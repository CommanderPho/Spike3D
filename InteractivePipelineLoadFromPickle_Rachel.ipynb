{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45f3f6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ InteractivePipelineLoadFromPickle (Independent Load-only Visualization Notebook) - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc319c3",
   "metadata": {},
   "source": [
    "#### Completed `2025-02-27`, got working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:34:16.396240Z",
     "start_time": "2025-01-07T11:34:09.057603Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "pho-run-2024",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "# !pip install viztracer\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "\n",
    "# %load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# # Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "from pyphocorehelpers.ipython_helpers import CustomFormatterMagics\n",
    "\n",
    "# Register the magic\n",
    "get_ipython().register_magics(CustomFormatterMagics)\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "from pyphocorehelpers.indexing_helpers import get_dict_subset\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "# from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import metadata_attributes\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "## Pho Programming Helpers:\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.notebook_helpers import NotebookCellExecutionLogger\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "_notebook_path:Path = Path(IPythonHelpers.try_find_notebook_filepath(IPython.extract_module_locals())).resolve() # Finds the path of THIS notebook\n",
    "# _notebook_execution_logger: NotebookCellExecutionLogger = NotebookCellExecutionLogger(notebook_path=_notebook_path, enable_logging_to_file=False) # Builds a logger that records info about this notebook\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme # used in perform_pipeline_save\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions,  RankOrderComputationsContainer, RankOrderResult, RankOrderAnalyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "from neuropy.utils.mixins.binning_helpers import transition_matrix\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.transition_matrix import TransitionMatrixComputations\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates, get_proper_global_spikes_df\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "def get_global_variable(var_name):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    return globals()[var_name]\n",
    "    \n",
    "def update_global_variable(var_name, value):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    globals()[var_name] = value\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/home/halechr/FastData'), Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/media/halechr/MAX/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/Users/pho/cloud/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "    global global_data_root_parent_path\n",
    "    new_global_data_root_parent_path = new_path.resolve()\n",
    "    global_data_root_parent_path = new_global_data_root_parent_path\n",
    "    print(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "    assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "            \n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "# active_data_mode_name = 'kdiba'\n",
    "# local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "# local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "# override_parameters = {\n",
    "#     'preprocessing.laps.use_direction_dependent_laps': True\n",
    "# }\n",
    "\n",
    "# # [*] - indicates bad or session with a problem\n",
    "# # 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # Recomputed 2024-12-16 18:51 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # Recomputed 2025-01-15 18:52 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # Recomputed 2025-01-16 03:21 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # Recomputed 2025-01-07 13:31 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46') # Recomputed 2024-12-16 19:23 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30') ## BLOCKING ERROR with pf2D computation (empty) for 5Hz 2024-12-02 15:24 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50') # Recomputed 2024-12-16 19:45 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54') # Recomputed 2024-12-16 19:29 -- about 3 good replays\n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3') # Recomputed 2024-12-16 19:32 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # Recomputed 2024-12-16 19:33 -- about 5 good replays\n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # Recomputed 2024-12-16 19:36 -- TONS of good replays, 10+ pages of them \n",
    "# local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "\n",
    "# # ==================================================================================================================== #\n",
    "# # BAPUN data format                                                                                                    #\n",
    "# # ==================================================================================================================== #\n",
    "# active_data_mode_name = 'bapun'\n",
    "# local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "# local_session_root_parent_path = global_data_root_parent_path.joinpath('Bapun')\n",
    "\n",
    "# # [*] - indicates bad or session with a problem\n",
    "# # 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='bapun',animal='RatN', session_name='Day4OpenField') # ,exper_name='one'\n",
    "# # Create a dictionary with the parameters to override\n",
    "# override_parameters = {\n",
    "#     'preprocessing.laps.use_direction_dependent_laps': False\n",
    "# }\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# RACHEL Data Format 2025-02-27 11:36                                                                                  #\n",
    "# ==================================================================================================================== #\n",
    "active_data_mode_name = 'rachel'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('Rachel')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='rachel', animal='cho', session_name='cho_241117_2_merged') # ,exper_name='one'\n",
    "# curr_context = IdentifyingContext(format_name='rachel', animal='cho', session_name='Cho_241117_Session2') # ,exper_name='one'\n",
    "curr_context = IdentifyingContext(format_name='rachel', animal='Petunia', session_name='Recordings/CA1/2024-12-09')\n",
    "\n",
    "# Create a dictionary with the parameters to override\n",
    "override_parameters = {\n",
    "    'preprocessing.laps.use_direction_dependent_laps': False\n",
    "}\n",
    "\n",
    "\n",
    "# Path('/media/halechr/MAX/Data/Rachel/cho_241117_2_merged')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name) #.resolve()\n",
    "\n",
    "# basedir: Path = Path('/media/halechr/MAX/Data/Rachel/cho/cho_241117_2_merged') # DO NOT `.resolve()``\n",
    "# basedir: Path = Path('/nfs/turbo/umms-kdiba/Data/Rachel/Cho_241117_Session2') # DO NOT `.resolve()``\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist = ['pf_computation', 'pfdt_computation', 'position_decoding']\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "selector, on_value_change = PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(update_global_variable_fn=update_global_variable, debug_print=False, enable_full_view=True)\n",
    "# selector.value = 'clean_run'\n",
    "selector.value = 'continued_run'\n",
    "# selector.value = 'final_run'\n",
    "on_value_change(dict(new=selector.value)) ## do update manually so the workspace variables reflect the set values\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "print(f\"saving_mode: {saving_mode}, force_reload: {force_reload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ca602",
   "metadata": {},
   "source": [
    "## Fresh direct unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497468e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core import Neurons, NeuronType\n",
    "\n",
    "print(f'basedir: \"{basedir}\"')\n",
    "\n",
    "# neurons = Neurons.from_file(basedir.joinpath('cho_241117_2_merged.paradigm.npy'))\n",
    "# neurons_path = basedir.joinpath('cho_241117_2_merged.paradigm.npy_bak')\n",
    "# neurons_path = basedir.joinpath('cho_241117_2_merged.paradigm.npy_bak')\n",
    "neurons_path = recording_path_found_files['npy'] # basedir.joinpath('cho_241117_2_merged.paradigm.npy_bak')\n",
    "neurons_path.exists()\n",
    "neurons = Neurons.from_file(neurons_path)\n",
    "assert neurons is not None\n",
    "\n",
    "# cho_241117_2_merged.neurons.npy_bak\n",
    "\n",
    "# neuronIDs = pd.read_csv(basedir.joinpath('cluster_q.tsv'))\n",
    "# neurons = Neurons(spiketrains=phydata.spiketrains, t_stop=2*3600, sampling_rate=30000, neuron_ids = {1:'pyr1',2:'pyr2',3:'pyr3',4:'int1',5:'int2',6:'int3',7:\"mua1\",8:'mua2',9:'mua3'})\n",
    "# neurons.filename = folder.joinpath(f'{filename}.neurons.npy')\n",
    "# neurons.save()\n",
    "neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7877ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _test_session = RachelDataSessionFormat.build_session(Path(r'R:\\data\\Rachel\\merged_M1_20211123_raw_phy'))\n",
    "_test_session = RachelDataSessionFormat.build_session(basedir)\n",
    "_test_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe104601",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "_test_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(_test_session)\n",
    "\n",
    "spikes_df: pd.DataFrame = deepcopy(_test_session.spikes_df)\n",
    "spikes_df\n",
    "spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = _test_session.position.to_dataframe()\n",
    "\n",
    "pos_df = pos_df.dropna()\n",
    "pos_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e18696",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df = _test_session.epochs.to_dataframe().to_numpy()\n",
    "epochs_df[1, 2] = pos_df.t.max() # 3320.0 Fixup max time\n",
    "epochs_df[1, 3] = pos_df.t.max() - epochs_df[1, 1] # fixup duration\n",
    "epochs_df = pd.DataFrame(epochs_df, columns=['label', 'start', 'stop', 'duration'])\n",
    "epochs_df[['start', 'stop', 'duration']] = epochs_df[['start', 'stop', 'duration']].astype(float)\n",
    "epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session.epochs = Epoch(epochs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4713bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe98de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_test_session.position =_test_session.position.time_slice(pos_df.t.min(), pos_df.t.max())\n",
    "_test_session.position.df['y_bak'] = deepcopy(_test_session.position.df['y'])\n",
    "_test_session.position.df['y'] = _test_session.position.df['z']\n",
    "_test_session.position.df['z'] = _test_session.position.df['y_bak']\n",
    "_test_session.position.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_epochs = Epoch(deepcopy(_test_session.epochs.epochs.time_slice(pos_df.t.min(), pos_df.t.max())))\n",
    "computation_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822592bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session.neurons\n",
    "_test_session.spikes_df['t_rel_seconds'] = _test_session.spikes_df['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import PlacefieldComputationParameters, perform_compute_placefields\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from neuropy.core.position import Position, PositionAccessor\n",
    "\n",
    "pf_computation_config = PlacefieldComputationParameters(computation_epochs=computation_epochs)\n",
    "active_session = _test_session\n",
    "\n",
    "# pf1D, pf2D = perform_compute_placefields(active_session_spikes_df=active_session.spikes_df, active_pos=active_session.position, computation_config=pf_computation_config, active_epoch_placefields1D=None, active_epoch_placefields2D=None, included_epochs=pf_computation_config.computation_epochs, should_force_recompute_placefields=True)\n",
    "# active_session.position.compute_higher_order_derivatives().position.compute_smoothed_position_info()\n",
    "\n",
    "# active_pos = deepcopy(active_session.position).to_dataframe().position.compute_higher_order_derivatives().position.compute_smoothed_position_info() #.position.com\n",
    "\n",
    "active_pos: Position = deepcopy(active_session.position)\n",
    "# active_pos = Position(lin_pos_df, metadata=None) ## build position object out of the dataframe\n",
    "active_pos.compute_higher_order_derivatives()\n",
    "active_pos.compute_smoothed_position_info()\n",
    "active_pos.speed; # ensure speed is calculated for the new object\n",
    "active_pos\n",
    "included_epochs = deepcopy(computation_epochs)\n",
    "computation_config = deepcopy(pf_computation_config)\n",
    "active_session_spikes_df = deepcopy(active_session.spikes_df)\n",
    "spikes_df = deepcopy(active_session_spikes_df).spikes.sliced_by_neuron_type('PYRAMIDAL') # Only use PYRAMIDAL neurons\n",
    "pf2D = PfND.from_config_values(spikes_df, deepcopy(active_pos), epochs=included_epochs,\n",
    "                                    speed_thresh=computation_config.speed_thresh, frate_thresh=computation_config.frate_thresh,\n",
    "                                    grid_bin=computation_config.grid_bin, grid_bin_bounds=computation_config.grid_bin_bounds, smooth=computation_config.smooth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf2D.plot_occupancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65664abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf2D.plot_ratemaps_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.dynamic_parameters import DynamicParameters\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputationResult\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.mixins.member_enumerating import AllFunctionEnumeratingMixin\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, BayesianPlacemapPositionDecoder, DecodedFilterEpochsResult, Zhang_Two_Step\n",
    "\n",
    "# placefield_computation_config = computation_result.computation_config.pf_params # should be a PlacefieldComputationParameters\n",
    "# if override_decoding_time_bin_size is not None:\n",
    "#     old_time_bin_size = computation_result.computation_config.pf_params.time_bin_size\n",
    "#     print(f'changing computation_result.computation_config.pf_params.time_bin_size from {old_time_bin_size} -> {override_decoding_time_bin_size}')\n",
    "#     did_change: bool = (override_decoding_time_bin_size != old_time_bin_size)\n",
    "#     computation_result.computation_config.pf_params.time_bin_size = override_decoding_time_bin_size\n",
    "#     print(f'\\tdid_change: {did_change}')\n",
    "    \n",
    "# ## filtered_spikes_df version:\n",
    "# computation_result.computed_data['pf1D_Decoder'] = BayesianPlacemapPositionDecoder(time_bin_size=placefield_computation_config.time_bin_size, pf=computation_result.computed_data['pf1D'], spikes_df=computation_result.computed_data['pf1D'].filtered_spikes_df.copy(), debug_print=False)\n",
    "# assert (len(computation_result.computed_data['pf1D_Decoder'].is_non_firing_time_bin) == computation_result.computed_data['pf1D_Decoder'].num_time_windows), f\"len(self.is_non_firing_time_bin): {len(computation_result.computed_data['pf1D_Decoder'].is_non_firing_time_bin)}, self.num_time_windows: {computation_result.computed_data['pf1D_Decoder'].num_time_windows}\"\n",
    "# computation_result.computed_data['pf1D_Decoder'].compute_all() # this is what breaks it\n",
    "\n",
    "pf2D_Decoder = BayesianPlacemapPositionDecoder(time_bin_size=0.5, pf=pf2D, spikes_df=pf2D.filtered_spikes_df.copy(), debug_print=False)\n",
    "pf2D_Decoder.compute_all() # Changing to fIXED grid_bin_bounds ===> MUCH (10x?) slower than before\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5318801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_most_likely_position_comparsions\n",
    "\n",
    "\n",
    "fig, axs = plot_most_likely_position_comparsions(pf2D_Decoder, _test_session.position.to_dataframe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(num='decoding_test')\n",
    "\n",
    "pf2D_Decoder.most_likely_positions.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51458391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import safeSaveData\n",
    "\n",
    "pkl_path = _test_session.basepath.joinpath('2025-02-27_pho_session.pkl')\n",
    "\n",
    "safeSaveData(pkl_path, _test_session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dccb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_pkl_path = _test_session.basepath.joinpath('2025-02-27_pho_computed_outputs.pkl')\n",
    "safeSaveData(decoded_pkl_path, (pf2D_Decoder, pf2D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59175f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm_df = pd.DataFrame(dict(label=['Pre','Maze'],\n",
    "    start = [0.0, 1208.01],\n",
    "    stop = [1208.0, np.inf],\n",
    "))\n",
    "\n",
    "paradigmdf = pd.DataFrame(data=paradigm_df)\n",
    "paradigm = Epoch(paradigmdf)\n",
    "paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paradigm.filename = Path('/home/wahlberg/Exp_Data/M1_Nov2021/20211123/merged_M1_20211123_raw/merged_M1_20211123_raw_phy/merged_M1_20211123_raw.paradigm.npy')\n",
    "paradigm.filename = basedir.joinpath(f'{curr_context.session_name}.paradigm.npy')\n",
    "paradigm.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63f2bb",
   "metadata": {},
   "source": [
    "## Resume Automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21a4df",
   "metadata": {
    "tags": [
     "run-group-0",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases, PipelinePickleFileSelectorWidget\n",
    "\n",
    "# ## INPUTS: basedir\n",
    "# active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir)\n",
    "\n",
    "extended_computations_include_includelist_phase_dict: Dict[str, CustomProcessingPhases] = CustomProcessingPhases.get_extended_computations_include_includelist_phase_dict()\n",
    "\n",
    "current_phase: CustomProcessingPhases = CustomProcessingPhases[selector.value]  # Assuming selector.value is an instance of CustomProcessingPhases\n",
    "extended_computations_include_includelist: List[str] = [key for key, value in extended_computations_include_includelist_phase_dict.items() if value <= current_phase]\n",
    "display(extended_computations_include_includelist)\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'] # \n",
    "\n",
    "# ## INPUTS: basedir\n",
    "active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir, on_update_global_variable_callback=update_global_variable, on_get_global_variable_callback=get_global_variable)\n",
    "\n",
    "_subfn_load, _subfn_save, _subfn_compute, _subfn_compute_new =active_session_pickle_file_widget._build_load_save_callbacks(global_data_root_parent_path=global_data_root_parent_path, active_data_mode_name=active_data_mode_name, basedir=basedir, saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                                             extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist)\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n",
    "\n",
    "# Display the widget\n",
    "display(active_session_pickle_file_widget.servable())\n",
    "# active_session_pickle_file_widget.local_file_browser_widget.servable()\n",
    "# active_session_pickle_file_widget.global_file_browser_widget.servable()\n",
    "# display(active_session_pickle_file_widget.local_file_browser_widget.servable())\n",
    "# display(active_session_pickle_file_widget.global_file_browser_widget.servable())\n",
    "\n",
    "# OUTPUTS: active_session_pickle_file_widget, widget.active_local_pkl, widget.active_global_pkl\n",
    "\n",
    "if selector.value == 'clean_run':\n",
    "\t## handle a clean run specially, this will create the pkls and not load them\n",
    "    print(f'clean run!')\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    # active_session_pickle_file_widget.is_compute_button_disabled = False # enable the compute button always during a clean run\n",
    "    # active_session_pickle_file_widget.is_load_button_disabled = True\n",
    "    \n",
    "    new_default_local_pkl_file: Path = active_session_pickle_file_widget.directory.joinpath(default_selected_local_file_name).resolve()\n",
    "    print(f'new_default_local_pkl_file: {new_default_local_pkl_file}')\n",
    "\n",
    "    active_session_pickle_file_widget.selected_local_pkl_files = [new_default_local_pkl_file]\n",
    "    active_session_pickle_file_widget.selected_global_pkl_files = []\n",
    "    active_session_pickle_file_widget._update_load_save_button_disabled_state()\n",
    "    print(f'active_session_pickle_file_widget.is_load_button_disabled: {active_session_pickle_file_widget.is_load_button_disabled}')\n",
    "    print(f'active_session_pickle_file_widget.is_compute_button_disabled: {active_session_pickle_file_widget.is_compute_button_disabled}')\n",
    "    print(f'active_local_pkl: \"{active_session_pickle_file_widget.active_local_pkl}\"')\n",
    "    print(f'active_global_pkl: \"{active_session_pickle_file_widget.active_global_pkl}\"')\n",
    "    active_session_pickle_file_widget.load_button.disabled = False\n",
    "    active_session_pickle_file_widget.compute_button.disabled = False\n",
    "else:\n",
    "    # not `clean_run` mode, continuing processing which might include loading from pickles\n",
    "    ## try selecting the first\n",
    "    did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "\n",
    "    ## Set default local comp pkl:\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    if not active_session_pickle_file_widget.is_local_file_names_list_empty:\n",
    "        default_local_section_indicies = [active_session_pickle_file_widget.local_file_browser_widget._data['File Name'].tolist().index(default_selected_local_file_name)]\n",
    "        active_session_pickle_file_widget.local_file_browser_widget.selection = default_local_section_indicies\n",
    "\n",
    "    ## Set default global computation pkl:\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    if not active_session_pickle_file_widget.is_global_file_names_list_empty:\n",
    "        default_global_section_indicies = [active_session_pickle_file_widget.global_file_browser_widget._data['File Name'].tolist().index(default_selected_global_file_name)]\n",
    "        active_session_pickle_file_widget.global_file_browser_widget.selection = default_global_section_indicies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "did_find_valid_selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_session_pickle_file_widget.on_load_callback()\n",
    "\n",
    "if did_find_valid_selection:\n",
    "\t_subfn_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d280d3",
   "metadata": {},
   "source": [
    "## From `test_non_interactive_crash.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd6416",
   "metadata": {
    "tags": [
     "run-compute"
    ]
   },
   "outputs": [],
   "source": [
    "### Bapun Open-Field Experiment (2022-08-09 Analysis)\n",
    "from neuropy.core.session.SessionSelectionAndFiltering import build_custom_epochs_filters # used particularly to build Bapun-style filters\n",
    "\n",
    "# active_data_mode_name = 'bapun'\n",
    "active_data_mode_name = 'rachel'\n",
    "print(f'active_data_session_types_registered_classes_dict: {active_data_session_types_registered_classes_dict}')\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "# basedir = Path('/media/halechr/MAX/Data/Rachel/Cho_241117_Session2').resolve()\n",
    "## INPUTS: basedir\n",
    "\n",
    "force_reload = force_reload #True\n",
    "print(f'force_reload: {force_reload}')\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties, override_basepath=Path(basedir), force_reload=force_reload) # , override_parameters_flat_keypaths_dict=override_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0a1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build epochs from track positions, not sure if correct:\n",
    "\n",
    "paradigm_file_path = Path(r\"W:\\Data\\Rachel\\Petunia\\Recordings\\CA1\\2024-12-09\\petunia_241209_merged.paradigm.npy\").resolve()\n",
    "\n",
    "epochs_df = pd.DataFrame.from_records([\n",
    "\tdict(label='maze1', start=pos_dict['position_track_1'].t_start, stop=pos_dict['position_track_1'].t_stop),\n",
    "    dict(label='maze2', start=pos_dict['position_track_2'].t_start, stop=pos_dict['position_track_2'].t_stop),\n",
    "]).epochs.get_valid_df()\n",
    "\n",
    "epochs_df\n",
    "\n",
    "epochs: Epoch = Epoch(epochs=epochs_df)\n",
    "\n",
    "epochs.filename = paradigm_file_path\n",
    "epochs.save()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.position import Position, PositionAccessor\n",
    "\n",
    "position_npy_paths = [\"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-09/petunia_241209_merged.position_2.npy\",\n",
    "    \"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-09/petunia_241209_merged.position_track_2.npy\",\n",
    "    \"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-09/petunia_241209_merged.position_1.npy\",\n",
    "    \"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-09/petunia_241209_merged.position_track_1.npy\",\n",
    "]\n",
    "\n",
    "# pos_dict = {Path(f).stem.split('.')[-1]:np.load(f, allow_pickle=True).item() for f in position_npy_paths}\n",
    "\n",
    "pos_dict: Dict[str, Position] = {Path(f).stem.split('.')[-1]:Position.init(**np.load(f, allow_pickle=True).item()) for f in position_npy_paths}\n",
    "merged_pos: Position = Position.concat(list(pos_dict.values()))\n",
    "\n",
    "# merged_pos.\n",
    "# merged_pos_df = pd.concat([p.to_dataframe() for p in pos_dict.values()], ignore_index=True).drop_duplicates(subset=['t'], keep='first', inplace=False, ignore_index=True).sort_values(by='t', axis='index', ascending=True, inplace=False, ignore_index=True)\n",
    "# ## Build merged metadata:\n",
    "# merged_metadata = pd.DataFrame.from_records([(p.metadata or {}) for p in pos_dict.values()])\n",
    "# merged_metadata = {'t_start': merged_metadata['t_start'].min(),\n",
    "#  't_stop': merged_metadata['t_stop'].max(),\n",
    "#  'sampling_rate': merged_metadata['sampling_rate'].min(),\n",
    "# }\n",
    "# merged_pos: Position = Position(pos_df=merged_pos_df, metadata=merged_metadata)\n",
    "\n",
    "\n",
    "# pos_file = sess.filePrefix.with_suffix(\".position.npy\")\n",
    "pos_file = Path(r\"W:\\Data\\Rachel\\Petunia\\Recordings\\CA1\\2024-12-09\\petunia_241209_merged.position.npy\").resolve() # sess.filePrefix.with_suffix(\".position.npy\")\n",
    "merged_pos.filename = pos_file\n",
    "merged_pos.save()\n",
    "pos_file\n",
    "# position_npy_paths\n",
    "\n",
    "\n",
    "# Position.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_npy_paths = [\n",
    "\t\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_dict['position_1'].tolist()\n",
    "\n",
    "# pd.concat((pos_dict['position_1'].to_dataframe(), pos_dict['position_2'].to_dataframe()), ignore_index=True).drop_duplicates(subset=['t'], keep='first', inplace=False, ignore_index=True)\n",
    "\n",
    "\n",
    "merged_pos_df = pd.concat([p.to_dataframe() for p in pos_dict.values()], ignore_index=True).drop_duplicates(subset=['t'], keep='first', inplace=False, ignore_index=True).sort_values(by='t', axis='index', ascending=True, inplace=False, ignore_index=True)\n",
    "merged_pos_df\n",
    "# {'traces': array([[-17.4369, -17.4409, -17.4545, ..., -43.729, -43.2822, -43.0087],\n",
    "#         [13.0881, 13.0861, 13.0837, ..., 3.83185, 3.92848, 3.99824],\n",
    "#         [-15.1044, -15.0894, -15.068, ..., 76.2707, 76.1355, 76.0378]]), 't_start': 8784.569933333334, 'sampling_rate': 60, 'traces_rot': array([[-0.682138, -0.681566, -0.680565, ..., -0.471912, -0.480161, -0.484554],\n",
    "#         [0.334609, 0.335222, 0.335841, ..., 0.236237, 0.243443, 0.244885],\n",
    "#         [0.623199, 0.622247, 0.621738, ..., -0.836285, -0.83127, -0.828997]]), 'metadata': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e19b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pos_df = pd.concat([p.to_dataframe() for p in pos_dict.values()], ignore_index=True).drop_duplicates(subset=['t', 'x', 'y'], keep='first', inplace=False, ignore_index=True).sort_values(by='t', axis='index', ascending=True, inplace=False, ignore_index=True)\n",
    "## Build merged metadata:\n",
    "merged_metadata = pd.DataFrame.from_records([(p.metadata or {}) for p in pos_dict.values()])\n",
    "merged_metadata = {'t_start': merged_metadata['t_start'].min(),\n",
    " 't_stop': merged_metadata['t_stop'].max(),\n",
    " 'sampling_rate': merged_metadata['sampling_rate'].min(),\n",
    "}\n",
    "merged_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43289acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_metadata = pd.DataFrame.from_records([(p.metadata or {}) for p in pos_dict.values()])\n",
    "merged_metadata = {'t_start': merged_metadata['t_start'].min(),\n",
    " 't_stop': merged_metadata['t_stop'].max(),\n",
    " 'sampling_rate': merged_metadata['sampling_rate'].min(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91daca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "a_dict = pos_dict['position_1']\n",
    "a_dict.t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.shape(a_dict['traces_rot'])\n",
    "# traces_rot = a_dict['traces_rot']\n",
    "# traces_rot[~np.isnan(traces_rot)]\n",
    "\n",
    "\n",
    "# np.all(np.isnan(a_dict['traces_rot']))\n",
    "\n",
    "\n",
    "\n",
    "a_pos: Position = Position.init(**a_dict)\n",
    "# a_pos.compute_higher_order_derivatives()\n",
    "a_pos.df.dropna(how='any', subset=['t', 'x', 'y'], inplace=False)\n",
    "\n",
    "# Position.legacy_from_dict(**pos_dict['position_1'])\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084806ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "['traces']\n",
    "\n",
    "\n",
    "\t\t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28dd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_test_session = RachelDataSessionFormat.build_session(Path(r'R:\\data\\Rachel\\merged_M1_20211123_raw_phy'))\n",
    "_test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "_test_session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76339c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.sess.epochs = deepcopy(curr_active_pipeline.sess.epochs_bak) \n",
    "del curr_active_pipeline.sess.__dict__['epochs_bak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56a78a",
   "metadata": {
    "tags": [
     "run-compute"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "\n",
    "# session_epochs: Epoch = BapunDataSessionFormatRegisteredClass.session_fixup_epochs(sess=curr_active_pipeline.sess)\n",
    "\n",
    "session_epochs: Epoch = RachelDataSessionFormat.session_fixup_epochs(sess=curr_active_pipeline.sess)\n",
    "session_epochs\n",
    "curr_active_pipeline.sess.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca441bee",
   "metadata": {
    "tags": [
     "run-compute"
    ]
   },
   "outputs": [],
   "source": [
    "# active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle']) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze', 'sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['roam', 'sprinkle']) # , 'maze'\n",
    "active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['Maze']) # , 'maze'\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_filters_pyramidal_epochs(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "active_session_computation_configs = active_data_mode_registered_class.build_default_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=0.5)\n",
    "curr_active_pipeline.filter_sessions(active_session_filter_configurations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7b2f2",
   "metadata": {
    "tags": [
     "run-compute"
    ]
   },
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.linearization_method = \"umap\"\n",
    "\n",
    "for an_epoch_name, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    ## forcibly compute the linearized position so it doesn't fallback to \"isomap\" method which eats all the memory\n",
    "    a_pos_df: pd.DataFrame = a_sess.position.compute_linearized_position(method='umap')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_filter_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.computation_epochs\n",
    "\n",
    "#    start   stop     label  duration\n",
    "# 0      0   7407       pre      7407\n",
    "# 1   7423  11483      maze      4060\n",
    "# 3  10186  11483  sprinkle      1297\n",
    "# 2  11497  25987      post     14490\n",
    "\n",
    "# [4 rows x 4 columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2caa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_merged_computation_function_validators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a058e33",
   "metadata": {
    "tags": [
     "run-compute"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfbf48",
   "metadata": {
    "tags": [
     "run-compute"
    ]
   },
   "outputs": [],
   "source": [
    "active_computation_functions_name_includelist = ['pf_computation', 'pfdt_computation', 'position_decoding'] #, 'extended_pf_peak_information' 'ratemap_peaks_prominence2d', 'position_decoding_two_step'\n",
    "\n",
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation', '_perform_velocity_vs_pf_density_computation', '_perform_velocity_vs_pf_simplified_count_density_computation']) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "\n",
    "curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_includelist=active_computation_functions_name_includelist, overwrite_extant_results=True, fail_on_exception=False, debug_print=True) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.computation_results['maze'].accumulated_errors\n",
    "curr_active_pipeline.clear_all_failed_computations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display(root_output_dir=basedir.joinpath('Output'), should_smooth_maze=True) # TODO: pass a display config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09507ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)\n",
    "# _out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-26.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f36c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-27.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4608e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_global_computation_results()#save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-27.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Computation Functions to be executed:\n",
    "# includelist Mode:\n",
    "computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                '_perform_position_decoding_computation', \n",
    "                                '_perform_firing_rate_trends_computation',\n",
    "                                '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                # '_perform_two_step_position_decoding_computation',\n",
    "                                # '_perform_recursive_latent_placefield_decoding'\n",
    "                            ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "computation_functions_name_excludelist=None\n",
    "\n",
    "batch_extended_computations(curr_active_pipeline, included_computation_filter_names=computation_functions_name_includelist,\n",
    "\t\t\t\t\t\t\t#  include_includelist=['maze', 'sprinkle'],\n",
    "\t\t\t\t\t\t\tinclude_includelist=['Maze', 'Post'],\n",
    "                            include_global_functions=True, fail_on_exception=False, progress_print=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52cc45",
   "metadata": {},
   "source": [
    "## 2024-06-25 - Load from saved custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337ddc6",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "# Loads custom pipeline pickles that were saved out via `custom_save_filepaths['pipeline_pkl'] = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename=custom_save_filenames['pipeline_pkl'])`\n",
    "\n",
    "## INPUTS: global_data_root_parent_path, active_data_mode_name, basedir, saving_mode, force_reload, custom_save_filenames\n",
    "# custom_suffix: str = '_withNewKamranExportedReplays'\n",
    "\n",
    "# custom_suffix: str = '_withNewComputedReplays'\n",
    "# custom_suffix: str = '_withNewComputedReplays-qclu_[1, 2]-frateThresh_5.0'\n",
    "\n",
    "# custom_save_filenames = {\n",
    "#     'pipeline_pkl':f'loadedSessPickle{custom_suffix}.pkl',\n",
    "#     'global_computation_pkl':f\"global_computation_results{custom_suffix}.pkl\",\n",
    "#     'pipeline_h5':f'pipeline{custom_suffix}.h5',\n",
    "# }\n",
    "# print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "# custom_save_filepaths = {k:v for k, v in custom_save_filenames.items()}\n",
    "\n",
    "# # ==================================================================================================================== #\n",
    "# # PIPELINE LOADING                                                                                                     #\n",
    "# # ==================================================================================================================== #\n",
    "# # load the custom saved outputs\n",
    "# active_pickle_filename = custom_save_filenames['pipeline_pkl'] # 'loadedSessPickle_withParameters.pkl'\n",
    "# print(f'active_pickle_filename: \"{active_pickle_filename}\"')\n",
    "# # assert active_pickle_filename.exists()\n",
    "# active_session_h5_filename = custom_save_filenames['pipeline_h5'] # 'pipeline_withParameters.h5'\n",
    "# print(f'active_session_h5_filename: \"{active_session_h5_filename}\"')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "## DO NOT allow recompute if the file doesn't exist!!\n",
    "# Computing loaded session pickle file results : \"W:/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_withNewComputedReplays.pkl\"... done.\n",
    "# Failure loading W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle_withNewComputedReplays.pkl.\n",
    "# proposed_load_pkl_path = basedir.joinpath(active_pickle_filename).resolve()\n",
    "\n",
    "## INPUTS: widget.active_global_pkl, widget.active_global_pkl\n",
    "\n",
    "if active_session_pickle_file_widget.active_global_pkl is None:\n",
    "    skip_global_load: bool = True\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skip_global_load: {skip_global_load}')\n",
    "else:\n",
    "    skip_global_load: bool = False\n",
    "    override_global_computation_results_pickle_path = active_session_pickle_file_widget.active_global_pkl.resolve()\n",
    "    Assert.path_exists(override_global_computation_results_pickle_path)\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "proposed_load_pkl_path = active_session_pickle_file_widget.active_local_pkl.resolve()\n",
    "Assert.path_exists(proposed_load_pkl_path)\n",
    "proposed_load_pkl_path\n",
    "\n",
    "custom_suffix: str = active_session_pickle_file_widget.try_extract_custom_suffix()\n",
    "print(f'custom_suffix: \"{custom_suffix}\"')\n",
    "\n",
    "## OUTPUTS: custom_suffix, proposed_load_pkl_path, (override_global_computation_results_pickle_path, skip_global_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_name_includelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eef26a",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: proposed_load_pkl_path\n",
    "# assert proposed_load_pkl_path.exists(), f\"for a saved custom the file must exist, but proposed_load_pkl_path: '{proposed_load_pkl_path}' does not!\"\n",
    "\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "\n",
    "with set_posix_windows():\n",
    "    curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                            computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                            saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                            skip_extended_batch_computations=True, debug_print=False, fail_on_exception=False, active_pickle_filename=proposed_load_pkl_path, \n",
    "                                            override_parameters_flat_keypaths_dict=override_parameters) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "print(f'Pipeline loaded from custom pickle!!')\n",
    "## OUTPUT: curr_active_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_load_pkl_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e10fea",
   "metadata": {},
   "source": [
    "## from `batch_load_session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From `pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing.batch_load_session` 2025-02-26 09:09 \n",
    "kwargs = {}\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "override_parameters_flat_keypaths_dict = override_parameters\n",
    "active_pickle_filename = proposed_load_pkl_path\n",
    "active_session_computation_configs = None\n",
    "fail_on_exception: bool = False\n",
    "\n",
    "saving_mode = PipelineSavingScheme.init(saving_mode)\n",
    "epoch_name_includelist = kwargs.get('epoch_name_includelist', ['maze1','maze2','maze'])\n",
    "debug_print = kwargs.get('debug_print', False)\n",
    "assert 'skip_save' not in kwargs, f\"use saving_mode=PipelineSavingScheme.SKIP_SAVING instead\"\n",
    "# skip_save = kwargs.get('skip_save', False)\n",
    "# active_pickle_filename = kwargs.get('active_pickle_filename', 'loadedSessPickle.pkl')\n",
    "\n",
    "# active_session_computation_configs = kwargs.get('active_session_computation_configs', None)\n",
    "# computation_functions_name_includelist = kwargs.get('computation_functions_name_includelist', None)\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "## Begin main run of the pipeline (load or execute):\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties,\n",
    "    override_basepath=Path(basedir), force_reload=force_reload, active_pickle_filename=active_pickle_filename, skip_save_on_initial_load=True, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n",
    "\n",
    "was_loaded_from_file: bool =  curr_active_pipeline.has_associated_pickle # True if pipeline was loaded from an existing file, False if it was created fresh\n",
    "\n",
    "# Get the previous configs:\n",
    "# curr_active_pipeline.filtered_sessions\n",
    "# ['filtered_session_names', 'filtered_contexts', 'filtered_epochs', 'filtered_sessions']\n",
    "# loaded_session_filter_configurations = {k:v.filter_config['filter_function'] for k,v in curr_active_pipeline.active_configs.items()}\n",
    "# loaded_pipeline_computation_configs = {k:v.computation_config for k,v in curr_active_pipeline.active_configs.items()}\n",
    "\n",
    "\n",
    "## Build updated ones from the current configs:\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "if debug_print:\n",
    "    print(f'active_session_filter_configurations: {active_session_filter_configurations}')\n",
    "\n",
    "## Skip the filtering, it used to be performed bere but NOT NOW\n",
    "\n",
    "## TODO 2023-05-16 - set `curr_active_pipeline.active_configs[a_name].computation_config.pf_params.computation_epochs = curr_laps_obj` equivalent\n",
    "## TODO 2023-05-16 - determine appropriate binning from `compute_short_long_constrained_decoders` so it's automatically from the long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef85735",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_on_exception: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43096f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if active_session_computation_configs is None:\n",
    "    \"\"\"\n",
    "    If there are is provided computation config, get the default:\n",
    "    \"\"\"\n",
    "    # ## Compute shared grid_bin_bounds for all epochs from the global positions:\n",
    "    # global_unfiltered_session = curr_active_pipeline.sess\n",
    "    # # ((22.736279243974774, 261.696733348342), (49.989466271998936, 151.2870218547401))\n",
    "    # first_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[0]]\n",
    "    # # ((22.736279243974774, 261.696733348342), (125.5644705153173, 151.21507349463707))\n",
    "    # second_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[1]]\n",
    "    # # ((71.67666779621361, 224.37820920766043), (110.51617463644946, 151.2870218547401))\n",
    "\n",
    "    # grid_bin_bounding_session = first_filtered_session\n",
    "    # grid_bin_bounds = PlacefieldComputationParameters.compute_grid_bin_bounds(grid_bin_bounding_session.position.x, grid_bin_bounding_session.position.y)\n",
    "\n",
    "    ## OR use no grid_bin_bounds meaning they will be determined dynamically for each epoch:\n",
    "    # grid_bin_bounds = None\n",
    "    # time_bin_size = 0.03333 #1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = 0.1 # 10 fps\n",
    "    time_bin_size = kwargs.get('time_bin_size', 0.03333) # 0.03333 = 1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = kwargs.get('time_bin_size', 0.1) # 10 fps\n",
    "\n",
    "    # lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "    # assert lap_estimation_parameters is not None\n",
    "    active_session_computation_configs: List[DynamicContainer] = active_data_mode_registered_class.build_active_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=time_bin_size, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # , grid_bin_bounds=grid_bin_bounds\n",
    "\n",
    "else:\n",
    "    # Use the provided `active_session_computation_configs`:\n",
    "    assert 'time_bin_size' not in kwargs, f\"time_bin_size kwarg provided but will not be used because a custom active_session_computation_configs was provided as well.\"\n",
    "\n",
    "active_session_computation_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation_functions_name_includelist = []\n",
    "computation_functions_name_includelist = ['pf_computation','firing_rate_trends', 'position_decoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Setup Computation Functions to be executed:\n",
    "if computation_functions_name_includelist is None:\n",
    "    # includelist Mode:\n",
    "    computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        # '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "    computation_functions_name_excludelist=None\n",
    "else:\n",
    "    print(f'using provided computation_functions_name_includelist: {computation_functions_name_includelist}')\n",
    "    computation_functions_name_excludelist=None\n",
    "\n",
    "## For every computation config we build a fake (duplicate) filter config).\n",
    "# OVERRIDE WITH TRUE:\n",
    "# curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps['use_direction_dependent_laps'] = True # override with True\n",
    "lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "assert lap_estimation_parameters is not None\n",
    "use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', False) # whether to split the laps into left and right directions\n",
    "# use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', True) # whether to split the laps into left and right directions\n",
    "\n",
    "if (use_direction_dependent_laps or (len(active_session_computation_configs) > 3)):\n",
    "    lap_direction_suffix_list = ['_odd', '_even', '_any'] # ['maze1_odd', 'maze1_even', 'maze1_any', 'maze2_odd', 'maze2_even', 'maze2_any', 'maze_odd', 'maze_even', 'maze_any']\n",
    "    # lap_direction_suffix_list = ['_odd', '_even', ''] # no '_any' prefix, instead reuses the existing names\n",
    "    # assert len(lap_direction_suffix_list) == len(active_session_computation_configs), f\"len(lap_direction_suffix_list): {len(lap_direction_suffix_list)}, len(active_session_computation_configs): {len(active_session_computation_configs)}, \"\n",
    "else:\n",
    "    print(f'not using direction-dependent laps.')\n",
    "    lap_direction_suffix_list = ['']\n",
    "\n",
    "# active_session_computation_configs: this should contain three configs, one for each Epoch    \n",
    "active_session_computation_configs = [deepcopy(a_config) for a_config in active_session_computation_configs]\n",
    "\n",
    "#TODO 2024-10-30 13:22: - [ ] This is where we should override the params using `override_parameters_flat_keypaths_dict`\n",
    "# if override_parameters_flat_keypaths_dict is not None:\n",
    "# \tfor a_config in active_session_computation_configs:\n",
    "# \t\tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\ta_config.set_by_keypath(k, deepcopy(v))\n",
    "# \t\t\texcept Exception as e:\n",
    "# \t\t\t\t# raise e\n",
    "# \t\t\t\tprint(f'cannot set_by_keypath: {k} -- error: {e}. Skipping for now.')\n",
    "\n",
    "assert len(lap_direction_suffix_list) == len(active_session_computation_configs)\n",
    "updated_active_session_pseudo_filter_configs = {} # empty list, woot!\n",
    "\n",
    "\n",
    "for a_computation_suffix_name, a_computation_config in zip(lap_direction_suffix_list, active_session_computation_configs): # these should NOT be the same length: lap_direction_suffix_list: ['_odd', '_even', '_any']\n",
    "    # We need to filter and then compute with the appropriate config iteratively.\n",
    "    for a_filter_config_name, a_filter_config_fn in active_session_filter_configurations.items():\n",
    "        # TODO: Build a context:\n",
    "        a_combined_name: str = f'{a_filter_config_name}{a_computation_suffix_name}'\n",
    "        # if a_computation_suffix_name != '':\n",
    "        updated_active_session_pseudo_filter_configs[a_combined_name] = deepcopy(a_filter_config_fn) # this copy is just so that the values are recomputed with the appropriate config. This is a HACK\n",
    "    # end for filter_configs\n",
    "\n",
    "    ## Actually do the filtering now. We have \n",
    "    curr_active_pipeline.filter_sessions(updated_active_session_pseudo_filter_configs, changed_filters_ignore_list=['maze1','maze2','maze'], debug_print=False)\n",
    "\n",
    "    ## TODO 2023-01-15 - perform_computations for all configs!!\n",
    "    #TODO 2024-10-30 13:22: - [ ] This is where we should override the params\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "    # \t\ta_filter_config_fn.set_by_keypath(k, deepcopy(v))\n",
    "\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tcurr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n",
    "    #TODO 2023-10-31 14:58: - [ ] This is where the computations are being done multiple times!\n",
    "    #TODO 2023-11-13 14:23: - [ ] With this approach, we can't actually properly filter the computation_configs for the relevant sessions ahead of time because they are calculated for a single computation config but across all sessions at once.\n",
    "    curr_active_pipeline.perform_computations(a_computation_config, computation_functions_name_includelist=computation_functions_name_includelist, computation_functions_name_excludelist=computation_functions_name_excludelist, fail_on_exception=fail_on_exception, debug_print=debug_print) #, overwrite_extant_results=False  ], fail_on_exception=True, debug_print=False)\n",
    "\n",
    "    if override_parameters_flat_keypaths_dict is not None:\n",
    "        curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not skip_extended_batch_computations:\n",
    "    batch_extended_computations(curr_active_pipeline, include_global_functions=False, fail_on_exception=fail_on_exception, progress_print=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation'], debug_print=False, fail_on_exception=False) # includelist: ['_perform_baseline_placefield_computation']\n",
    "\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.prepare_for_display(root_output_dir=global_data_root_parent_path.joinpath('Output'), should_smooth_maze=True) # TODO: pass a display config\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to do `curr_active_pipeline.prepare_for_display(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "if not saving_mode.shouldSave:\n",
    "    print(f'saving_mode.shouldSave == False, so not saving at the end of batch_load_session')\n",
    "\n",
    "## Load pickled global computations:\n",
    "# If previously pickled global results were saved, they will typically no longer be relevent if the pipeline was recomputed. We need a system of invalidating/versioning the global results when the other computations they depend on change.\n",
    "# Maybe move into `batch_extended_computations(...)` or integrate with that somehow\n",
    "# curr_active_pipeline.load_pickled_global_computation_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e374c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "skip_global_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cb3e1",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Global computations loading:                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "# Loads saved global computations that were saved out via: `custom_save_filepaths['global_computation_pkl'] = curr_active_pipeline.save_global_computation_results(override_global_pickle_filename=custom_save_filenames['global_computation_pkl'])`\n",
    "## INPUTS: custom_save_filenames\n",
    "## INPUTS: curr_active_pipeline, (override_global_computation_results_pickle_path, skip_global_load), extended_computations_include_includelist\n",
    "\n",
    "if skip_global_load:\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skipping global load because skip_global_load==True')\n",
    "else:\n",
    "    # override_global_computation_results_pickle_path = custom_save_filenames['global_computation_pkl']\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "# Pre-load ___________________________________________________________________________________________________________ #\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list\n",
    "\n",
    "# Try Unpickling Global Computations to update pipeline ______________________________________________________________ #\n",
    "if (not force_reload) and (not skip_global_load): # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # INPUTS: override_global_computation_results_pickle_path\n",
    "        with set_posix_windows():\n",
    "            sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(override_global_computation_results_pickle_path=override_global_computation_results_pickle_path,\n",
    "                                                                                            allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist, ) # is new\n",
    "            print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "            did_any_paths_change: bool = curr_active_pipeline.post_load_fixup_sess_basedirs(updated_session_basepath=deepcopy(basedir)) ## use INPUT: basedir\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'force_reload: {force_reload}, saving_mode: {saving_mode}')\n",
    "force_reload\n",
    "saving_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e54ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: curr_active_pipeline.global_computation_results_pickle_path, skip_global_load\n",
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "print(f'override_pickle_path = \"{curr_active_pipeline.pickle_path}\",\\nactive_pickle_filename = \"{curr_active_pipeline.pickle_path.name}\"')\n",
    "print(f'override_global_pickle_path = \"{curr_active_pipeline.global_computation_results_pickle_path}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9674fd9",
   "metadata": {},
   "source": [
    "## OUTPUTS: `curr_active_pipeline`  0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣0️⃣ RESUME Normal Pipeline Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f755b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 0️⃣ Shared Post-Pipeline load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Lab'\n",
    " \n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        BATCH_DATE_TO_USE = f'{BATCH_DATE_TO_USE}{custom_suffix}'\n",
    "        print(f'Adding custom suffix: \"{custom_suffix}\" - BATCH_DATE_TO_USE: \"{BATCH_DATE_TO_USE}\"')\n",
    "except NameError as err:\n",
    "    custom_suffix = None\n",
    "    print(f'NO CUSTOM SUFFIX.')\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                           r\"K:\\scratch\\collected_outputs\",\n",
    "                                                           '/Users/pho/data/collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "all",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607a444",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# / 🛑 End Run Section 🛑\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🎨 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "all",
     "run-group-display",
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5423e",
   "metadata": {},
   "source": [
    "## `LauncherWidget`: GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df7ce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Display import DisplayFunctionItem\n",
    "from pyphocorehelpers.gui.Qt.tree_helpers import find_tree_item_by_text\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_str: str = curr_active_pipeline.get_complete_session_identifier_string()\n",
    "widget.setWindowTitle(f'Spike3D Launcher: {session_id_str}')\n",
    "treeWidget.root\n",
    "# curr_active_pipeline.get_session_additional_parameters_context()\n",
    "# curr_active_pipeline.get_complete_session_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3ebce",
   "metadata": {},
   "source": [
    "## non-interactive batch plotting via `batch_perform_all_plots(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83293544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "\n",
    "_out = batch_perform_all_plots(curr_active_pipeline, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a240b9",
   "metadata": {},
   "source": [
    "## `Spike3DRasterWindowWidget` Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eea58",
   "metadata": {
    "tags": [
     "all",
     "spike_raster_window",
     "display",
     "gui",
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _setup_spike_raster_window_for_debugging\n",
    "\n",
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True)\n",
    "\n",
    "all_global_menus_actionsDict, global_flat_action_dict = _setup_spike_raster_window_for_debugging(spike_raster_window)\n",
    "\n",
    "# preview_overview_scatter_plot: pg.ScatterPlotItem  = active_2d_plot.plots.preview_overview_scatter_plot # ScatterPlotItem \n",
    "# preview_overview_scatter_plot.setDownsampling(auto=True, method='subsample', dsRate=10)\n",
    "main_graphics_layout_widget: pg.GraphicsLayoutWidget = active_2d_plot.ui.main_graphics_layout_widget\n",
    "wrapper_layout: pg.QtWidgets.QVBoxLayout = active_2d_plot.ui.wrapper_layout\n",
    "main_content_splitter = active_2d_plot.ui.main_content_splitter # QSplitter\n",
    "layout = active_2d_plot.ui.layout\n",
    "background_static_scroll_window_plot = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "active_window_container_layout = active_2d_plot.ui.active_window_container_layout # GraphicsLayout, first item of `main_graphics_layout_widget` -- just the active raster window I think, there is a strange black space above it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.plots.main_plot_widget\n",
    "\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "main_plot_widget.setMinimumHeight(20.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout\n",
    "# main_graphics_layout_widget.ci # GraphicsLayout\n",
    "main_graphics_layout_widget.ci.childItems()\n",
    "# main_graphics_layout_widget.setHidden(True) ## hides too much\n",
    "main_graphics_layout_widget.setHidden(False)\n",
    "\n",
    "# main_graphics_layout_widget\n",
    "\n",
    "active_window_container_layout.setBorder(pg.mkPen('yellow', width=4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout.allChildItems()\n",
    "active_window_container_layout.setPreferredHeight(200.0)\n",
    "active_window_container_layout.setMaximumHeight(800.0)\n",
    "active_window_container_layout.setSpacing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stretch factors to control priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(0, 400)  # Plot1: lowest priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(1, 2)  # Plot2: mid priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(2, 2)  # Plot3: highest priority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ParameterTreeWidget import create_parameter_tree_widget\n",
    "# win, param_tree = create_pipeline_filter_parameter_tree()\n",
    "win, param_tree = create_parameter_tree_widget(curr_active_pipeline.get_all_parameters())\n",
    "win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f61dd",
   "metadata": {
    "tags": [
     "_perform_plot_multi_decoder_meas_pred_position_track",
     "active-2025-01-16"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DecoderIdentityColors\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_plot_multi_decoder_meas_pred_position_track\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "## Build the new dock track:\n",
    "dock_identifier: str = 'Continuous Decoding Performance'\n",
    "ts_widget, fig, ax_list = active_2d_plot.add_new_matplotlib_render_plot_widget(name=dock_identifier)\n",
    "## Get the needed data:\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n",
    "\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "all_directional_continuously_decoded_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {k:v for k, v in (continuously_decoded_dict or {}).items() if k in TrackTemplates.get_decoder_names()} ## what is plotted in the `f'{a_decoder_name}_ContinuousDecode'` rows by `AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand`\n",
    "## OUT: all_directional_continuously_decoded_dict\n",
    "## Draw the position meas/decoded on the plot widget\n",
    "## INPUT: fig, ax_list, all_directional_continuously_decoded_dict, track_templates\n",
    "\n",
    "_out_artists =  _perform_plot_multi_decoder_meas_pred_position_track(curr_active_pipeline, fig, ax_list, desired_time_bin_size=0.058, enable_flat_line_drawing=True)\n",
    "\n",
    "\n",
    "## sync up the widgets\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget(dock_identifier, sync_mode=SynchronizedPlotMode.TO_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38325e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['truth_decoder_name'] = pos_df['truth_decoder_name'].fillna('')\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_color_dict: Dict[types.DecoderName, str] = DecoderIdentityColors.build_decoder_color_dict()\n",
    "\n",
    "decoded_pos_line_kwargs = dict(lw=1.0, color='gray', alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "inactive_decoded_pos_line_kwargs = dict(lw=0.3, alpha=0.2, marker='.', markersize=2, animated=False)\n",
    "active_decoded_pos_line_kwargs = dict(lw=1.0, alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "\n",
    "\n",
    "_out_data = {}\n",
    "_out_data_plot_kwargs = {}\n",
    "# curr_active_pipeline.global_computation_results.t\n",
    "for a_decoder_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    a_continuously_decoded_result = all_directional_continuously_decoded_dict[a_decoder_name]\n",
    "    a_decoder_color = decoder_color_dict[a_decoder_name]\n",
    "    \n",
    "    assert len(a_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = a_continuously_decoded_result.time_bin_containers[0]\n",
    "    time_window_centers = time_bin_containers.centers\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "    a_marginal_x = a_continuously_decoded_result.marginal_x_list[0]\n",
    "    # active_time_window_variable = a_decoder.active_time_window_centers\n",
    "    active_time_window_variable = time_window_centers\n",
    "    active_most_likely_positions_x = a_marginal_x['most_likely_positions_1D'] # a_decoder.most_likely_positions[:,0].T\n",
    "    _out_data[a_decoder_name] = pd.DataFrame({'t': time_window_centers, 'x': active_most_likely_positions_x, 'binned_time': np.arange(len(time_window_centers))})\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "    _out_data[a_decoder_name]['is_active_decoder_time'] = (_out_data[a_decoder_name]['truth_decoder_name'].fillna('', inplace=False) == a_decoder_name)\n",
    "\n",
    "    # is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "    active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "    active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "    ## could fill y with np.nan instead of getting shorter?\n",
    "    _out_data_plot_kwargs[a_decoder_name] = (dict(x=active_decoder_time_points, y=active_decoder_most_likely_positions_x, color=a_decoder_color, **active_decoded_pos_line_kwargs), dict(x=active_decoder_inactive_time_points, y=active_decoder_inactive_most_likely_positions_x, color=a_decoder_color, **inactive_decoded_pos_line_kwargs))\n",
    "\n",
    "_out_data_plot_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8972db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "\n",
    "# is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "\n",
    "_out_data[a_decoder_name] = ((active_decoder_time_points, active_decoder_most_likely_positions_x), (active_decoder_inactive_time_points, active_decoder_inactive_most_likely_positions_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_dfs = partition_df_dict(pos_df, partitionColumn='truth_decoder_name')\n",
    "\n",
    "a_decoder_name: str = 'short_LR'\n",
    "a_binned_time_grouped_df = partitioned_dfs[a_decoder_name].groupby('binned_time', axis='index', dropna=True)\n",
    "a_binned_time_grouped_df = a_binned_time_grouped_df.median().dropna(axis='index', subset=['x']) ## without the `.dropna(axis='index', subset=['x'])` part it gets an exhaustive df for all possible values of 'binned_time', even those not listed\n",
    "\n",
    "a_matching_binned_times = a_binned_time_grouped_df.reset_index(drop=False)['binned_time']\n",
    "a_matching_binned_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into two dfs for each decoder -- the supported and the unsupported\n",
    "partition\n",
    "\n",
    "PandasHelpers.safe_pandas_get_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.dropna(axis='index', subset=['lap', 'truth_decoder_name'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df: pd.DataFrame = global_laps_obj.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import find_epochs_overlapping_other_epochs\n",
    "\n",
    "## INPUTS: global_laps\n",
    "_out_split_pseudo2D_posteriors_dict = {}\n",
    "_out_split_pseudo2D_out_dict = {}\n",
    "pre_filtered_col_names = ['pre_filtered_most_likely_position_indicies', 'pre_filtered_most_likely_position'] # 'pre_filtered_time_bin_containers', 'pre_filtered_p_x_given_n', \n",
    "post_filtered_col_names = [a_col_name.removeprefix('pre_filtered_') for a_col_name in pre_filtered_col_names] # ['time_bin_containers', 'most_likely_position_indicies', 'most_likely_position']\n",
    "print(post_filtered_col_names)\n",
    "for a_time_bin_size, pseudo2D_decoder_continuously_decoded_result in continuously_decoded_pseudo2D_decoder_dict.items():\n",
    "    print(f'a_time_bin_size: {a_time_bin_size}')\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size] = {'pre_filtered_p_x_given_n': None, 'pre_filtered_time_bin_containers': None, 'pre_filtered_most_likely_position_indicies': None, 'pre_filtered_most_likely_position': None, \n",
    "                                                     'is_timebin_included': None, 'p_x_given_n': None} # , 'time_window_centers': None\n",
    "    # pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "    assert len(pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = pseudo2D_decoder_continuously_decoded_result.time_bin_containers[0]\n",
    "    # time_window_centers = time_bin_containers.centers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_position_indicies_list[0])\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_positions_list[0])\n",
    "    ## INPUTS: time_bin_containers, global_laps\n",
    "    left_edges = deepcopy(time_bin_containers.left_edges)\n",
    "    right_edges = deepcopy(time_bin_containers.right_edges)\n",
    "    continuous_time_binned_computation_epochs_df: pd.DataFrame = pd.DataFrame({'start': left_edges, 'stop': right_edges, 'label': np.arange(len(left_edges))})\n",
    "    is_timebin_included: NDArray = find_epochs_overlapping_other_epochs(epochs_df=continuous_time_binned_computation_epochs_df, epochs_df_required_to_overlap=deepcopy(global_laps))\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_p_x_given_n'] = p_x_given_n\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_time_bin_containers'] = time_bin_containers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['is_timebin_included'] = is_timebin_included\n",
    "    # continuous_time_binned_computation_epochs_df['is_in_laps'] = is_timebin_included\n",
    "    ## filter by whether it's included or not:\n",
    "    p_x_given_n = p_x_given_n[:, :, is_timebin_included]\n",
    "    # time_window_centers = \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['p_x_given_n'] = p_x_given_n\n",
    "    # _out_split_pseudo2D_out_dict[a_time_bin_size]['time_window_centers'] = time_window_centers[is_timebin_included]\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "\n",
    "    ## Split across the 2nd axis to make 1D posteriors that can be displayed in separate dock rows:\n",
    "    assert p_x_given_n.shape[1] == 4, f\"expected the 4 pseudo-y bins for the decoder in p_x_given_n.shape[1]. but found p_x_given_n.shape: {p_x_given_n.shape}\"\n",
    "    # split_pseudo2D_posteriors_dict = {k:np.squeeze(p_x_given_n[:, i, :]) for i, k in enumerate(('long_LR', 'long_RL', 'short_LR', 'short_RL'))}\n",
    "    _out_split_pseudo2D_posteriors_dict[a_time_bin_size] = deepcopy(p_x_given_n)\n",
    "    \n",
    "    # for a_col_name in pre_filtered_col_names:\n",
    "    #     filtered_col_name = a_col_name.removeprefix('pre_filtered_')\n",
    "    #     print(f'a_col_name: {a_col_name}, filtered_col_name: {filtered_col_name}, shape: {np.shape(_out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name])}')\n",
    "    #     _out_split_pseudo2D_out_dict[a_time_bin_size][filtered_col_name] = _out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name][is_timebin_included, :]\n",
    "        \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position_indicies'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'][:, is_timebin_included]\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'][is_timebin_included, :]\n",
    "    \n",
    "\n",
    "p_x_given_n.shape # (n_position_bins, n_decoding_models, n_time_bins) - (57, 4, 29951)\n",
    "\n",
    "## OUTPUTS: _out_split_pseudo2D_posteriors_dict, _out_split_pseudo2D_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_most_likely_position_comparsions\n",
    "\n",
    "# fig, axs = plot_most_likely_position_comparsions(pho_custom_decoder, axs=ax, sess.position.to_dataframe())\n",
    "fig, axs = plot_most_likely_position_comparsions(computation_result.computed_data['pf2D_Decoder'], computation_result.sess.position.to_dataframe(), **overriding_dict_with(lhs_dict={'show_posterior':True, 'show_one_step_most_likely_positions_plots':True}, **kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f34d9",
   "metadata": {},
   "source": [
    "## Overflow/Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f49198",
   "metadata": {},
   "source": [
    "# 💾 Save Export Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd78276",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_complete_session_context()\n",
    "custom_save_filepaths, custom_save_filenames, custom_suffix = curr_active_pipeline.get_custom_pipeline_filenames_from_parameters()\n",
    "custom_save_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "if curr_active_pipeline.pickle_path is None:\n",
    "    active_pickle_filename = 'loadedSessPickle.pkl'\n",
    "else:\n",
    "    active_pickle_filename = curr_active_pipeline.pickle_path.name\n",
    "    \n",
    "print(f'active_pickle_filename: {active_pickle_filename}')\n",
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename=active_pickle_filename) #active_pickle_filename=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results(override_global_pickle_path=curr_active_pipeline.global_computation_results_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb91c76",
   "metadata": {},
   "source": [
    "# 2025-01-30 - Rat Heading-Angle from Position Change Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a true global session encompassing all epochs\n",
    "# curr_active_pipeline.sess.epochs\n",
    "global_session = deepcopy(curr_active_pipeline.filtered_sessions['roam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import Position\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import post_process_non_kdiba\n",
    "\n",
    "post_process_non_kdiba(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74225fa6",
   "metadata": {},
   "source": [
    "# 2025-09-10 - Parsing Rachel's new session folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b54ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess_path = Path(r\"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-08/OpenEphys/2024-12-08_09-33-19\")\n",
    "# sess_path = Path(r\"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-09\")\n",
    "sess_path = Path(r\"W:/Data/Rachel/Petunia\")\n",
    "\n",
    "Assert.path_exists(sess_path)\n",
    "# W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-08/OpenEphys/2024-12-08_09-33-19/Record Node 106\n",
    "\n",
    "def get_recording_files(recordings_dir: Path):\n",
    "    found_recording_files = []\n",
    "    found_recording_files.extend(recordings_dir.glob(f\"**/*.xml\"))\n",
    "    return found_recording_files\n",
    "\n",
    "get_recording_files(recordings_dir=sess_path)\n",
    "\n",
    "search_path = sess_path\n",
    "xml_files = list(search_path.rglob(\"*.npy\"))\n",
    "xml_files\n",
    "\n",
    "all_found_files = {'xml': list(search_path.rglob(\"*.xml\")), 'dat': list(search_path.rglob(\"*.dat\")), 'npy': list(search_path.rglob(\"*.npy\")), 'csv': list(search_path.rglob(\"*.csv\"))}\n",
    "all_found_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path('W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-08/OpenEphys/2024-12-08_09-33-19/Record Node 106/settings.xml')\n",
    "# search_path = Path(r\"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-08/OpenEphys/2024-12-08_09-33-19/Record Node 106\").resolve()\n",
    "# search_path = Path(r\"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-08/OpenEphys/2024-12-08_09-33-19/Record Node 106\").resolve()\n",
    "\n",
    "sess_path: Path = Path(r\"W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-09\")\n",
    "# get_recording_files(recordings_dir=sess_path)\n",
    "\n",
    "\n",
    "## INPUTS: all_found_files\n",
    "found_sess_all_found_files = {}\n",
    "for an_extension, a_files_list in all_found_files.items():\n",
    "\tfiltered_files_list = [f for f in a_files_list if f.is_relative_to(sess_path)]\n",
    "\tfound_sess_all_found_files[an_extension] = filtered_files_list\n",
    "\t\n",
    "found_sess_all_found_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name: str = 'recording1'\n",
    "\n",
    "recording_path: Path = Path('W:/Data/Rachel/Petunia/Recordings/CA1/2024-12-08/OpenEphys/2024-12-08_09-33-19/Record Node 106/experiment1/recording1/continuous/Intan_Rec._Controller-100.0')\n",
    "recording_path_found_files = {'xml': list(recording_path.rglob(\"*.xml\")), 'dat': list(recording_path.rglob(\"*.dat\")), 'npy': list(recording_path.rglob(\"*.npy\"))} # , 'csv': list(recording_path.rglob(\"*.csv\"))\n",
    "recording_path_found_files = {k:v[0] for k, v in recording_path_found_files.items()}\n",
    "recording_path_found_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_path_found_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNUSED FOR RACHEL: KDiba Session Discovery - Determines contexts to process - 2024-09-19\n",
    "# # from pathlib import Path\n",
    "# # from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "# # from neuropy.utils.result_context import print_identifying_context_array_code\n",
    "\n",
    "# # known_global_data_root_parent_paths = [Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'/Volumes/MoverNew/data')] # , Path(r'/home/halechr/FastData'), Path(r'/home/halechr/turbo/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data')\n",
    "# # global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "# # assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# sessions_df, export_folder_path = RachelDataSessionFormat.find_build_and_save_sessions_experiment_datetime_df_csv(\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# global_data_root_parent_path=global_data_root_parent_path,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\texport_csv_path=Path('EXTERNAL/PhoDibaPaper2024Book/data/sessions_experiment_datetime_df.csv').resolve(),\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbad_sessions_csv_path=Path('EXTERNAL/PhoDibaPaper2024Book/data/2024-09-23_bad_sessions_table.csv').resolve(),\n",
    "# )\n",
    "# sessions_df = sessions_df.reset_index(drop=True, inplace=False)\n",
    "# sessions_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
