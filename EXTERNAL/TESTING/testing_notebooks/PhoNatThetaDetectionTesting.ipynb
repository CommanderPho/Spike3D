{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4f6b1-02ff-407f-a303-1946a2708a0e",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import sys\n",
    "import traceback # for stack trace formatting\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "import numpy as np\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "# %gui qt\n",
    "# !env QT_API=\"pyqt5\"\n",
    "%gui qt5\n",
    "# %gui qt6\n",
    "# from PyQt5.Qt import QApplication\n",
    "# # start qt event loop\n",
    "# _instance = QApplication.instance()\n",
    "# if not _instance:\n",
    "#     _instance = QApplication([])\n",
    "# app = _instance\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "from pyphoplacecellanalysis.General.NonInteractiveProcessing import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "# global_data_root_parent_path = Path(r'W:\\Data') # Windows Apogee\n",
    "global_data_root_parent_path = Path(r'/media/MAX/Data') # Diba Lab Workstation Linux\n",
    "# global_data_root_parent_path = Path(r'/Volumes/MoverNew/data') # rMBP\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d9aad-f22c-4607-8bb7-e315312b325a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fa1a4-56ac-49fe-aefc-c25669d862d4",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "## Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, excludelist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, excludelist=[])\n",
    "\n",
    "### Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, excludelist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, excludelist=[])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26506af-54f3-43f4-af6f-be88d37e2013",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Single basedir (non-batch) testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7768e9-3492-4312-abfc-c239adc7229c",
   "metadata": {
    "tags": [
     "load",
     "single_session"
    ]
   },
   "outputs": [],
   "source": [
    "%pdb on\n",
    "basedir = local_session_paths_list[0] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, force_reload=False, skip_extended_batch_computations=True, debug_print=False)\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=True, skip_extended_batch_computations=True, debug_print=False)\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=True, skip_extended_batch_computations=True) # temp no-save\n",
    "## SAVE AFTERWARDS!\n",
    "\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=False, active_pickle_filename='20221214200324-loadedSessPickle.pkl', skip_extended_batch_computations=True)\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=False, active_pickle_filename='loadedSessPickle - full-good.pkl', skip_extended_batch_computations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446496b-c286-4a76-b888-94be2f92aee3",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "# Future: theta-dependent placefields: build separate placefields for each phase of theta (binned in theta). There should be one set (where the animal is representing the present) that nearly perfectly predicts the animal's location.\n",
    "    # the rest of the variability \n",
    "\n",
    "    1. Basic Hilbert transform\n",
    "    2. But Theta wave-shape (sawtooth) at higher running speeds.\n",
    "        - do peak-to-trough and trough-to-peak separate\n",
    "        ** Nat will send me something\n",
    "        \n",
    "- remember Eloy's theta-dependent placefields. I'm ashamed that I fucked up with Eloy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ff852-0f53-4eac-89d7-3a890d7fd743",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36565a6-97ac-4ed7-be87-87d972fa3057",
   "metadata": {},
   "source": [
    "https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "Nat's code for detecting the sawtooth theta is here (lines 271-393ish): https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "\n",
    "It's all based on this paper: https://www.jneurosci.org/content/32/2/423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7b015-d93e-488c-8575-4525546a93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratchpad for opto\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import Analysis.python.LFP.preprocess_data as pd\n",
    "\n",
    "import scipy.signal as signal\n",
    "import pickle\n",
    "import os\n",
    "# import Analysis.python.LFP.helpers as helpers\n",
    "\n",
    "## LFP analysis functions from https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/lfp_analysis.py\n",
    "\n",
    "# instead of `import Analysis.python.LFP.lfp_analysis as lfp`\n",
    "class lfp(object):\n",
    "    ## Create Butterworth filter - copied from scipy-cookbook webpage\n",
    "    @classmethod\n",
    "    def butter_bandpass(cls, lowcut, highcut, fs, order=2):\n",
    "        \"\"\"\n",
    "        Simplify inputs for creating a Butterworth filter. copied from scipy-cookbook webpage.\n",
    "        :param lowcut: Hz\n",
    "        :param highcut: Hz\n",
    "        :param fs: Sampling rate in Hz\n",
    "        :param order: (optional) 2 = default\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        b, a = signal.butter(order, [low, high], btype='band')\n",
    "\n",
    "        return b, a\n",
    "\n",
    "\n",
    "    ## filter data through butterworth filter\n",
    "    @classmethod\n",
    "    def butter_bandpass_filter(cls, data, lowcut, highcut, fs, type='filtfilt', order=2):\n",
    "        \"\"\"\n",
    "        Filter data through butterworth bandpass filter. Copied from scipy-cookbook webpage.\n",
    "        :param data: array of data sampled at fs\n",
    "        :param lowcut: 4 Hz\n",
    "        :param highcut: 10 Hz\n",
    "        :param type: 'filtfilt' (default) filters both ways, 'lfilt' filters forward only (and likely induces a phase offset).\n",
    "        :param fs: 30000 Hz\n",
    "        :param order: (optional) default = 2 to match Sieglie et al., eLife (2014)\n",
    "        :return: filt_data: filtered data\n",
    "        \"\"\"\n",
    "        b, a = cls.butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        if type == 'lfilt':\n",
    "            filt_data = signal.lfilter(b, a, data)\n",
    "        elif type == 'filtfilt':\n",
    "            filt_data = signal.filtfilt(b, a, data)\n",
    "\n",
    "        return filt_data\n",
    "\n",
    "\n",
    "    ## Peak-trough detection via Belluscio et al. (2012) J. Neuro\n",
    "    @classmethod\n",
    "    def get_local_extrema(cls, trace, type='max'):\n",
    "        \"\"\" Get local extrema, assuming it occurs near the middle of the trace. spits out an np.nan if there a relative min\n",
    "        or max occurs at the edge.\n",
    "        :param trace: lfp trace\n",
    "        :param type: 'max' (default) or 'min'\n",
    "        :return: index in trace where max/min is located. np.nan if there is a relative minima/maxima at edge of trace.\n",
    "        \"\"\"\n",
    "        if type == 'max':\n",
    "            temp = signal.argrelmax(trace, order=int(len(trace)/2))[0]\n",
    "        elif type == 'min':\n",
    "            temp = signal.argrelmin(trace, order=int(len(trace)/2))[0]\n",
    "\n",
    "        if temp.size == 1:\n",
    "            ind_rel_extreme = temp[0]\n",
    "        else:\n",
    "            ind_rel_extreme = np.nan\n",
    "\n",
    "        return ind_rel_extreme\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def peak_trough(cls, trace, SRlfp, order=2, lowcut_bell=1, highcut_bell=80, peak_trough_offset_sec=0.07, debug_print=False):\n",
    "        \"\"\" Peak-trough method (Belluscio et al. 2012 J Neuro) - fold into lfp_analysis.peak_trough_detect eventually\n",
    "            from Nat's https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "            Detect the peaks and troughs in the wide-filtered trace using peak-trough method and return time points\n",
    "        Args:\n",
    "            trace: The trace from which to detect peaks and troughs.\n",
    "            SRlfp: Sampling rate (Hz)\n",
    "            order: The order of the filter used.\n",
    "            time_span: The length of the time window in which to detect peaks and troughs.\n",
    "            start_time: The start time of the time window.\n",
    "            peak_trough_offset_sec: seconds to look for trough of wide-filtered trace next to 4-10Hz filtered trace\n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            wide_peak_inds_good: The wide peaks\n",
    "            wide_trough_inds_good: The wide troughs\n",
    "\n",
    "        Usage:\n",
    "\n",
    "            lowcut = 4 # Hz\n",
    "            highcut = 10 # Hz\n",
    "            SRlfp = SRlfp = traces.sampling_rate\n",
    "            wave_phase_inds, wave_phases, wide_peak_inds_good, wide_trough_inds_good = lfp.peak_trough(trace, traces.sampling_rate, order=2, lowcut_bell=1, highcut_bell=80, peak_trough_offset_sec=0.07)\n",
    "\n",
    "        \"\"\"\n",
    "        trace_lfilt = lfp.butter_bandpass_filter(trace, lowcut, highcut, SRlfp, order=order, type='lfilt')\n",
    "        \n",
    "        wide_filt = lfp.butter_bandpass_filter(trace, lowcut_bell, highcut_bell, SRlfp, order=order)\n",
    "        offset_frames = np.round(peak_trough_offset_sec*SRlfp)\n",
    "\n",
    "        # First detect peak and trough off narrowband filtered signal - do hilbert transform\n",
    "        # trough = -pi->pi, peak = 0 (- -> +)\n",
    "        trace_analytic = signal.hilbert(trace_lfilt)  # get real and imaginary parts of signal\n",
    "        trig_trace_phase = np.angle(trace_analytic)\n",
    "        # ax.plot(time_plot, trig_trace_phase*v_range/8, 'r-')\n",
    "        peak_bool = np.bitwise_and(trig_trace_phase[0:-1] < 0, trig_trace_phase[1:] >= 0)\n",
    "        peak_bool = np.append(peak_bool, False)\n",
    "        trough_bool = np.bitwise_and(trig_trace_phase[0:-1] > 0, trig_trace_phase[1:] <= 0)\n",
    "        trough_bool = np.append(trough_bool, False)\n",
    "\n",
    "        # Indices to peak and trough of narrowband trace\n",
    "        peak_inds = np.where(peak_bool)[0]\n",
    "        trough_inds = np.where(trough_bool)[0]\n",
    "\n",
    "        ## now step through and find closest peak/trough in the wide-filtered trace when compared to the narrowband filtered trace.\n",
    "        # THIS IS ALL COMMENTED NOW SO THAT YOU DONT ACCIDENTALLY OVERWRITE EXISTING VALUES - NEED TO IMPLEMENT DOWNSAMPLING FIRST!!!\n",
    "        wide_peak_inds = []\n",
    "        wide_trough_inds = []\n",
    "\n",
    "        # Step through and look for each trough in the WIDE filtered signal between two peaks in the NARROW filtered signal\n",
    "        # how fast is this compared to just running it on all the trace and looking for closest inds? Bet it depends on if I\n",
    "        # downsample first...\n",
    "\n",
    "        n = 0\n",
    "        for idp, idp1 in zip(peak_inds[0:-1], peak_inds[1:]):\n",
    "            wide_trough_inds.append(lfp.get_local_extrema(wide_filt[idp:idp1], type='min') + idp)\n",
    "            n = n + 1\n",
    "            if int(n/100) == n/100:\n",
    "                if debug_print:\n",
    "                    print(n)\n",
    "\n",
    "        n = 0\n",
    "        # Ditto to above but for peaks\n",
    "        for idt, idt1 in zip(trough_inds[0:-1], trough_inds[1:]):\n",
    "            wide_peak_inds.append(lfp.get_local_extrema(wide_filt[idt:idt1], type='max') + idt)\n",
    "            n = n + 1\n",
    "            if int(n/100) == n/100:\n",
    "                if debug_print:\n",
    "                    print(n)\n",
    "\n",
    "        ## looks decent except when there is crappy theta. Filter out these epochs? Put on speed threshold?\n",
    "        wide_peak_inds_good = [idp for idp in wide_peak_inds if not np.isnan(idp)]\n",
    "        wide_trough_inds_good = [idt for idt in wide_trough_inds if not np.isnan(idt)]\n",
    "\n",
    "        ## Get rise and falling times of theta - trough = -pi/+pi, peak = 0\n",
    "\n",
    "        # if peak times generally lead trough times, chop off first peak value\n",
    "        if np.nanmean(np.array(wide_peak_inds) - np.array(wide_trough_inds)) < 0:\n",
    "            peak_inds_use = wide_peak_inds[1:]\n",
    "            trough_inds_use = wide_trough_inds[0:-1]\n",
    "            next_trough_inds = wide_trough_inds[1:]\n",
    "        else:\n",
    "            peak_inds_use = wide_peak_inds\n",
    "            trough_inds_use = wide_trough_inds\n",
    "            next_trough_inds = wide_trough_inds[1:]\n",
    "\n",
    "        wave_phase_inds = []\n",
    "        wave_phases = []\n",
    "        for idt, idp, idt1 in zip(trough_inds_use, peak_inds_use, next_trough_inds):\n",
    "            if not np.any(np.isnan([idt, idp, idt1])) and idt < idp < idt1:  # only run below if you have reliable peak/trough info\n",
    "                trace_snippet = wide_filt[idt:idt1]  # grab a snippet of the trace to use\n",
    "                if np.all(trace_snippet <= 0) or np.all(trace_snippet >= 0) or trace_snippet[0] > 0 or trace_snippet[-1] > 0\\\n",
    "                        or wide_filt[idp] < 0:  # Make sure trace is not all above or below zero and that peak/troughs are above/below zero\n",
    "                    wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "                else:\n",
    "                    rise_zero = np.max(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) < idp))[0])\n",
    "                    fall_zero = np.min(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) > idp))[0])\n",
    "                    wave_phase_inds.extend([idt, idt + rise_zero, idp, idt + fall_zero, idt1-1])\n",
    "            else:\n",
    "                wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "            wave_phases.extend([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\n",
    "\n",
    "            \n",
    "        return wave_phase_inds, wave_phases, wide_peak_inds_good, wide_trough_inds_good, peak_inds_use, trough_inds_use, next_trough_inds, trace_lfilt, wide_filt \n",
    "\n",
    "      \n",
    "    \n",
    "from neuropy.analyses import oscillations\n",
    "## Plot trace in a nice working window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b575b75-f436-47e7-a1aa-80c5c826d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ceaaf-6338-47aa-84cd-2948a3a5ffb1",
   "metadata": {},
   "source": [
    "## Main Theta Computation Run Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e757eb-cccb-4b67-8b9b-b2a313e6193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpFile = curr_active_pipeline.sess.eegfile # neuropy.io.binarysignalio.BinarysignalIO\n",
    "traces = lfpFile.get_signal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9620f-cddb-417b-86fb-8fe55af02c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_plot = 19  # channel you triggered off of\n",
    "artifact_chan = 13  # this channel should have good stimulation artifact on it for reference...\n",
    "\n",
    "# lfpFile = curr_active_pipeline.sess.eegfile # neuropy.io.binarysignalio.BinarysignalIO\n",
    "# traces = lfpFile.get_signal()\n",
    "\n",
    "order = 2\n",
    "lowcut = 4 # Hz\n",
    "highcut = 10 # Hz\n",
    "SRlfp = traces.sampling_rate\n",
    "\n",
    "trace = traces.traces[chan_plot, :]\n",
    "trace_lfilt = lfp.butter_bandpass_filter(trace, lowcut, highcut, SRlfp, order=order, type='lfilt')\n",
    "trace_filtfilt = lfp.butter_bandpass_filter(trace, lowcut, highcut, SRlfp, order=order, type='filtfilt')\n",
    "\n",
    "wave_phase_inds, wave_phases, wide_peak_inds_good, wide_trough_inds_good, peak_inds_use, trough_inds_use, next_trough_inds, trace_lfilt, wide_filt = lfp.peak_trough(trace, traces.sampling_rate, order=2, lowcut_bell=1, highcut_bell=80, peak_trough_offset_sec=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a9330-ecfa-4c87-b252-7d26458320a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa71c73-96ee-4923-bfd0-7889cca3f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_epochs = oscillations.detect_ripple_epochs(traces, curr_active_pipeline.sess.probegroup)\n",
    "ripple_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1f171-78c4-4dee-8639-150d43e77973",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = lfpFile.get_signal(channel_indx=19)\n",
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e47941-6e39-4deb-a2b5-405ae7d5a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpFile.n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62af39-2df5-482f-80a7-712ac145137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpFile.n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd3aed-55af-4ce7-92cd-6025a8fe9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c5d32-7e65-4d2e-bbda-8062b85af16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_ds = traces.traces\n",
    "np.shape(traces_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087854eb-80f1-4142-877c-f2bd11224962",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces.traces.shape # (96, 1099619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf9d89-db76-4de5-9e7d-c53ca89e78d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = traces.t_start\n",
    "time_span = 5.0 # traces.duration\n",
    "time_plot = traces.time\n",
    "wide_filt = trace_lfilt\n",
    "\n",
    "v_range = np.nanmax(trace)\n",
    "print(f'{v_range = }')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharex=True, sharey=True)\n",
    "fig.set_size_inches([26, 3])\n",
    "hraw = ax.plot(time_plot, trace)\n",
    "ax.plot(time_plot, wide_filt, 'm')\n",
    "ax.plot(time_plot, trace_lfilt, 'k--')\n",
    "ax.set_xlim([start_time*60, start_time*60 + time_span])\n",
    "# ax.set_ylim([-v_range, v_range])\n",
    "ax.set_xlabel(['Time (s)'])\n",
    "ax.set_ylabel('uV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f319565-b7f6-4fc1-b4c4-ece383283803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now plot\n",
    "wave_phase_inds_good = [idph for idph in wave_phase_inds if not np.isnan(idph)]\n",
    "wave_phases_good = [ph for ph, idph in zip(wave_phases, wave_phase_inds) if not np.isnan(idph)]\n",
    "ax.plot(time_plot[wave_phase_inds_good], np.asarray(wave_phases_good)*v_range/8, 'r-')\n",
    "\n",
    "## histogram of rise times vs fall times overlaid to prove I'm doing things correctly\n",
    "fig35, ax35 = plt.subplots(1, 2)\n",
    "fig35.set_size_inches([13.5, 4.8])\n",
    "rise_times = (np.array(peak_inds_use) - np.array(trough_inds_use))/SRlfp\n",
    "fall_times = (np.array(next_trough_inds) - np.array(peak_inds_use))/SRlfp\n",
    "ax35[0].hist(rise_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[0].set_title('Peak-Trough Method')\n",
    "ax35[0].set_xlabel('Rising Phase Times (s)')\n",
    "ax35[0].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(rise_times)) + ' sec')\n",
    "ax35[1].hist(fall_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[1].set_title('Peak-Trough Method')\n",
    "ax35[1].set_xlabel('Falling Phase Times (s)')\n",
    "ax35[1].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(fall_times)) + ' sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2907e-06a0-47ce-ac4b-028cdd2168b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Direct Import of Nat's Method (Pre 2023-02-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0117cd8-3177-4008-8972-5f4d8c34cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Peak-trough method (Belluscio et al. 2012 J Neuro) - fold into lfp_analysis.peak_trough_detect eventually\n",
    "# from Nat's https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "\n",
    "## Needs: trace, SRlfp, order\n",
    "order = 2\n",
    "\n",
    "lowcut_bell = 1  # Hz\n",
    "highcut_bell = 80  # Hz\n",
    "peak_trough_offset_sec = 0.07  # seconds to look for trough of wide-filtered trace next to 4-10Hz filtered trace\n",
    "\n",
    "wide_filt = lfp.butter_bandpass_filter(trace, lowcut_bell, highcut_bell, SRlfp, order=order)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharex=True, sharey=True)\n",
    "fig.set_size_inches([26, 3])\n",
    "hraw = ax.plot(time_plot, trace)\n",
    "ax.plot(time_plot, wide_filt, 'm')\n",
    "ax.plot(time_plot, trace_lfilt, 'k--')\n",
    "ax.set_xlim([start_time*60, start_time*60 + time_span])\n",
    "ax.set_ylim([-v_range, v_range])\n",
    "ax.set_xlabel(['Time (s)'])\n",
    "ax.set_ylabel('uV')\n",
    "\n",
    "offset_frames = np.round(peak_trough_offset_sec*SRlfp)\n",
    "\n",
    "# First detect peak and trough off narrowband filtered signal - do hilbert transform\n",
    "# trough = -pi->pi, peak = 0 (- -> +)\n",
    "trace_analytic = signal.hilbert(trace_lfilt)  # get real and imaginary parts of signal\n",
    "trig_trace_phase = np.angle(trace_analytic)\n",
    "# ax.plot(time_plot, trig_trace_phase*v_range/8, 'r-')\n",
    "peak_bool = np.bitwise_and(trig_trace_phase[0:-1] < 0, trig_trace_phase[1:] >= 0)\n",
    "peak_bool = np.append(peak_bool, False)\n",
    "trough_bool = np.bitwise_and(trig_trace_phase[0:-1] > 0, trig_trace_phase[1:] <= 0)\n",
    "trough_bool = np.append(trough_bool, False)\n",
    "\n",
    "# Indices to peak and trough of narrowband trace\n",
    "peak_inds = np.where(peak_bool)[0]\n",
    "trough_inds = np.where(trough_bool)[0]\n",
    "\n",
    "# Check that above code works...\n",
    "# ax.plot(time_plot[peak_bool], trace_lfilt[peak_bool], 'r*')\n",
    "# ax.plot(time_plot[trough_bool], trace_lfilt[trough_bool], 'g*')\n",
    "\n",
    "##  Plot times between peak and trough - seems likes looking 0.07 seconds to either side should be ok...\n",
    "fig2, ax2 = plt.subplots(1, 2)\n",
    "ax2[0].hist(np.diff(np.where(trough_bool))[0]/SRlfp)\n",
    "ax2[0].set_xlabel('Trough-to-trough times (s)')\n",
    "ax2[1].hist(np.diff(np.where(peak_bool))[0]/SRlfp)\n",
    "ax2[1].set_xlabel('Peak-to-peak times (s)')\n",
    "\n",
    "## now step through and find closest peak/trough in the wide-filtered trace when compared to the narrowband filtered trace.\n",
    "# THIS IS ALL COMMENTED NOW SO THAT YOU DONT ACCIDENTALLY OVERWRITE EXISTING VALUES - NEED TO IMPLEMENT DOWNSAMPLING FIRST!!!\n",
    "wide_peak_inds = []\n",
    "wide_trough_inds = []\n",
    "\n",
    "# Step through and look for each trough in the WIDE filtered signal between two peaks in the NARROW filtered signal\n",
    "# how fast is this compared to just running it on all the trace and looking for closest inds? Bet it depends on if I\n",
    "# downsample first...\n",
    "\n",
    "n = 0\n",
    "for idp, idp1 in zip(peak_inds[0:-1], peak_inds[1:]):\n",
    "    wide_trough_inds.append(lfp.get_local_extrema(wide_filt[idp:idp1], type='min') + idp)\n",
    "    n = n + 1\n",
    "    if int(n/100) == n/100:\n",
    "        print(n)\n",
    "\n",
    "n = 0\n",
    "# Ditto to above but for peaks\n",
    "for idt, idt1 in zip(trough_inds[0:-1], trough_inds[1:]):\n",
    "    wide_peak_inds.append(lfp.get_local_extrema(wide_filt[idt:idt1], type='max') + idt)\n",
    "    n = n + 1\n",
    "    if int(n/100) == n/100:\n",
    "        print(n)\n",
    "\n",
    "## looks decent except when there is crappy theta. Filter out these epochs? Put on speed threshold?\n",
    "wide_peak_inds_good = [idp for idp in wide_peak_inds if not np.isnan(idp)]\n",
    "wide_trough_inds_good = [idt for idt in wide_trough_inds if not np.isnan(idt)]\n",
    "\n",
    "ax.plot(time_plot[wide_peak_inds_good], wide_filt[wide_peak_inds_good], 'ro')\n",
    "ax.plot(time_plot[wide_trough_inds_good], wide_filt[wide_trough_inds_good], 'go')\n",
    "\n",
    "## Get rise and falling times of theta - trough = -pi/+pi, peak = 0\n",
    "\n",
    "# if peak times generally lead trough times, chop off first peak value\n",
    "if np.nanmean(np.array(wide_peak_inds) - np.array(wide_trough_inds)) < 0:\n",
    "    peak_inds_use = wide_peak_inds[1:]\n",
    "    trough_inds_use = wide_trough_inds[0:-1]\n",
    "    next_trough_inds = wide_trough_inds[1:]\n",
    "else:\n",
    "    peak_inds_use = wide_peak_inds\n",
    "    trough_inds_use = wide_trough_inds\n",
    "    next_trough_inds = wide_trough_inds[1:]\n",
    "\n",
    "\n",
    "wave_phase_inds = []\n",
    "wave_phases = []\n",
    "for idt, idp, idt1 in zip(trough_inds_use, peak_inds_use, next_trough_inds):\n",
    "    if not np.any(np.isnan([idt, idp, idt1])) and idt < idp < idt1:  # only run below if you have reliable peak/trough info\n",
    "        trace_snippet = wide_filt[idt:idt1]  # grab a snippet of the trace to use\n",
    "        if np.all(trace_snippet <= 0) or np.all(trace_snippet >= 0) or trace_snippet[0] > 0 or trace_snippet[-1] > 0\\\n",
    "                or wide_filt[idp] < 0:  # Make sure trace is not all above or below zero and that peak/troughs are above/below zero\n",
    "            wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "        else:\n",
    "            rise_zero = np.max(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) < idp))[0])\n",
    "            fall_zero = np.min(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) > idp))[0])\n",
    "            wave_phase_inds.extend([idt, idt + rise_zero, idp, idt + fall_zero, idt1-1])\n",
    "    else:\n",
    "        wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    wave_phases.extend([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e10e5a-5d95-4a95-bed7-fe99b179a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot\n",
    "wave_phase_inds_good = [idph for idph in wave_phase_inds if not np.isnan(idph)]\n",
    "wave_phases_good = [ph for ph, idph in zip(wave_phases, wave_phase_inds) if not np.isnan(idph)]\n",
    "ax.plot(time_plot[wave_phase_inds_good], np.asarray(wave_phases_good)*v_range/8, 'r-')\n",
    "\n",
    "\n",
    "## histogram of rise times vs fall times overlaid to prove I'm doing things correctly\n",
    "fig35, ax35 = plt.subplots(1, 2)\n",
    "fig35.set_size_inches([13.5, 4.8])\n",
    "rise_times = (np.array(peak_inds_use) - np.array(trough_inds_use))/SRlfp\n",
    "fall_times = (np.array(next_trough_inds) - np.array(peak_inds_use))/SRlfp\n",
    "ax35[0].hist(rise_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[0].set_title('Peak-Trough Method')\n",
    "ax35[0].set_xlabel('Rising Phase Times (s)')\n",
    "ax35[0].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(rise_times)) + ' sec')\n",
    "ax35[1].hist(fall_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[1].set_title('Peak-Trough Method')\n",
    "ax35[1].set_xlabel('Falling Phase Times (s)')\n",
    "ax35[1].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(fall_times)) + ' sec')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
