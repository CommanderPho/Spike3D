{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"2024-05-29-Dill Picking Errors with simple ComputationResult classes.ipynb\"\n",
    "based off of \"2024-01-02-Dill Picking Error with QApplication.ipynb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Simple `ComputedResult` subclasses cannot seem to be pickled!\n",
    "\n",
    "# Solution: The problem strangely resolved itself from last night. All pickle correctly after removing the `super(...).init(...)` call in my `.__setstate__()` func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation: Sometimes specific result can be successfully picked while entire pipeline cannot:\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import save_rank_order_results, SaveStringGenerator\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from neuropy.core import Epoch\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from attrs import define, field, Factory, asdict, astuple\n",
    "from functools import wraps\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, custom_define, serialized_field, serialized_attribute_field, non_serialized_field, keys_only_repr\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# 2024-05-28 - TrialByTrialActivity                                                                                    #\n",
    "# ==================================================================================================================== #\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "\n",
    "\n",
    "@define(slots=False, repr=False, eq=False)\n",
    "class TrialByTrialActivityResult(ComputedResult):\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    #TODO 2024-05-28 19:14: - [ ] Not yet picklable, and I think it just needs a recurrsive __getstate__(...) function\n",
    "\n",
    "    \n",
    "    Usage:\n",
    "    \n",
    "        from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "        from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "        directional_trial_by_trial_activity_result: TrialByTrialActivityResult = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None)\n",
    "        any_decoder_neuron_IDs = directional_trial_by_trial_activity_result.any_decoder_neuron_IDs\n",
    "        active_pf_dt: PfND_TimeDependent = directional_trial_by_trial_activity_result.active_pf_dt\n",
    "        directional_lap_epochs_dict: Dict[str, Epoch] = directional_trial_by_trial_activity_result.directional_lap_epochs_dict\n",
    "        directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts\n",
    "        directional_active_lap_pf_results_dicts\n",
    "\n",
    "    \"\"\"\n",
    "    _VersionedResultMixin_version: str = \"2024.05.28_0\" # to be updated in your IMPLEMENTOR to indicate its version\n",
    "\n",
    "    any_decoder_neuron_IDs: NDArray = serialized_field(default=None)\n",
    "    active_pf_dt: PfND_TimeDependent = serialized_field(default=None)\n",
    "    directional_lap_epochs_dict: Dict[str, Epoch] =  serialized_field(default=None)\n",
    "    directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = serialized_field(default=None)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" 2024-01-11 - Renders only the fields and their sizes\n",
    "        \"\"\"\n",
    "        from pyphocorehelpers.print_helpers import strip_type_str_to_classname\n",
    "        attr_reprs = []\n",
    "        for a in self.__attrs_attrs__:\n",
    "            attr_type = strip_type_str_to_classname(type(getattr(self, a.name)))\n",
    "            if 'shape' in a.metadata:\n",
    "                shape = ', '.join(a.metadata['shape'])  # this joins tuple elements with a comma, creating a string without quotes\n",
    "                attr_reprs.append(f\"{a.name}: {attr_type} | shape ({shape})\")  # enclose the shape string with parentheses\n",
    "            else:\n",
    "                attr_reprs.append(f\"{a.name}: {attr_type}\")\n",
    "        content = \",\\n\\t\".join(attr_reprs)\n",
    "        return f\"{type(self).__name__}({content}\\n)\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_trial_by_trial_result: TrialByTrialActivityResult = TrialByTrialActivityResult(is_global=True)\n",
    "a_trial_by_trial_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a_trial_by_trial_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData('test_a_trial_by_trial_result_data.pkl', a_trial_by_trial_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results = global_computation_results.computed_data['RankOrder'] # : \"RankOrderComputationsContainer\"\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "# included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "directional_laps_results: DirectionalLapsResult = global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [owning_pipeline_reference.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "\n",
    "## INPUTS: curr_active_pipeline, track_templates, global_epoch_name, (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)\n",
    "any_decoder_neuron_IDs: NDArray = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = owning_pipeline_reference.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "# ## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in owning_pipeline_reference.computation_results[global_epoch_name].computed_data:\n",
    "    # if `KeyError: 'pf1D_dt'` recompute\n",
    "    owning_pipeline_reference.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "# active_pf_2D_dt: PfND_TimeDependent = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = active_pf_1D_dt\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "## OUTPUTS: directional_active_lap_pf_results_dicts\n",
    "a_train_test_result: TrialByTrialActivityResult = TrialByTrialActivityResult(any_decoder_neuron_IDs=any_decoder_neuron_IDs,\n",
    "                                                                                active_pf_dt=active_pf_dt,\n",
    "                                                                                directional_lap_epochs_dict=directional_lap_epochs_dict,\n",
    "                                                                                directional_active_lap_pf_results_dicts=directional_active_lap_pf_results_dicts,\n",
    "                                                                                is_global=True)  # type: Tuple[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, BasePositionDecoder], Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-01-02 - Fails SOMETIMES, even when called immediately after the above save command which worked:\n",
    "curr_active_pipeline.save_global_computation_results()\n",
    "\n",
    "# Clearing and reloading the notebook, skipping these computations, and then re-applying them to the pipeline does allow resaving with `curr_active_pipeline.save_global_computation_results()` for some reason.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "def diagnose_pickling_issues(object_to_pickle):\n",
    "   \"\"\"Intellegently diagnoses which property on an object is causing pickling via Dill to fail.\"\"\"\n",
    "\n",
    "   try:\n",
    "       # Attempt to pickle the object directly\n",
    "       pickle.dumps(object_to_pickle)\n",
    "   except pickle.PicklingError as e:\n",
    "       # If pickling fails, initiate a diagnostic process\n",
    "       print(f\"Pickling error encountered: {e}\")\n",
    "\n",
    "       # Gather information about the object's attributes\n",
    "       object_attributes = [attr for attr in dir(object_to_pickle) if not attr.startswith(\"__\")]\n",
    "\n",
    "       # Isolate problematic attributes through iterative testing\n",
    "       problematic_attribute = None\n",
    "       for attribute in object_attributes:\n",
    "           try:\n",
    "               pickle.dumps(getattr(object_to_pickle, attribute))\n",
    "           except pickle.PicklingError:\n",
    "               problematic_attribute = attribute\n",
    "               break\n",
    "\n",
    "       # Provide informative output\n",
    "       if problematic_attribute:\n",
    "           print(f\"Identified problematic attribute: {problematic_attribute}\")\n",
    "           print(\"Potential causes:\")\n",
    "           print(\"- Attribute contains unpicklable data types (e.g., lambda functions, file objects).\")\n",
    "           print(\"- Attribute refers to external resources (e.g., database connections).\")\n",
    "           print(\"- Attribute has circular references within the object's structure.\")\n",
    "       else:\n",
    "           print(\"Unable to isolate the specific attribute causing the pickling error.\")\n",
    "           print(\"Consider:\")\n",
    "           print(\"- Examining the object's structure and dependencies for potential conflicts.\")\n",
    "           print(\"- Providing a minimal reproducible example for further analysis.\")\n",
    "\n",
    "   else:\n",
    "       # If pickling succeeds, indicate no issues found\n",
    "       print(\"No pickling issues detected.\")\n",
    "\n",
    "\n",
    "diagnose_pickling_issues(curr_active_pipeline.global_computation_results.computed_data['RankOrder'])\n",
    "\n",
    "# make a copy of an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_output_path = Path(r'W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\output\\2023-12-22_807pm-minimum_inclusion_fr-5-included_qclu_values-[1, 2]RankOrder.pkl').resolve()\n",
    "\n",
    "\n",
    "from pyphocorehelpers.Filesystem.pickling_helpers import custom_dump, custom_dumps\n",
    "import dill.detect\n",
    "dill.detect.trace(True)\n",
    "\n",
    "# dill.detect.badobjects(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].__dict__)\n",
    "\n",
    "test_obj = curr_active_pipeline.global_computation_results.computed_data['RankOrder'] # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.RankOrderComputationsContainer\n",
    "dill.detect.badtypes(test_obj)\n",
    "\n",
    "# dill.detect.errors(test_obj)\n",
    "\n",
    "# with dill.detect.trace(True):\n",
    "# saveData(rank_order_output_path, (curr_active_pipeline.global_computation_results.computed_data['RankOrder'].__dict__,))\n",
    "# custom_dumps(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].__dict__)\n",
    "#  with open(pkl_path, file_mode) as dbfile: \n",
    "# \t# source, destination\n",
    "# \t# pickle.dump(db, dbfile)\n",
    "# \tcustom_dump(db, dbfile) # ModuleExcludesPickler\n",
    "# \tdbfile.close()\n",
    "\t\n",
    "# # dumps(squared)\n",
    "# custom_dump(db, dbfile) # ModuleExcludesPickler\n",
    "# saveData(rank_order_output_path, (curr_active_pipeline.global_computation_results.computed_data['RankOrder'].__dict__,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\t\"name\": \"TypeError\",\n",
    "\t\"message\": \"<lambda>() missing 4 required positional arguments: 'short_stats_z_scorer', 'long_short_z_diff', 'long_short_naive_z_diff', and 'is_forward_replay'\",\n",
    "\t\"stack\": \"---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "c:\\\\Users\\\\pho\\\\repos\\\\Spike3DWorkEnv\\\\Spike3D\\\\ReviewOfWork_2024-01-02.ipynb Cell 88 line 1\n",
    "----> <a href='vscode-notebook-cell:/c%3A/Users/pho/repos/Spike3DWorkEnv/Spike3D/ReviewOfWork_2024-01-02.ipynb#Z2361sZmlsZQ%3D%3D?line=0'>1</a> diagnose_pickling_issues(rank_order_obj.to_dict())\n",
    "\n",
    "File ~\\\\repos\\\\Spike3DWorkEnv\\\\pyPhoPlaceCellAnalysis\\\\src\\\\pyphoplacecellanalysis\\\\General\\\\Pipeline\\\\Stages\\\\ComputationFunctions\\\\MultiContextComputationFunctions\\\\RankOrderComputations.py:685, in to_dict(self)\n",
    "    683 epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "    684 x_values = significant_ripple_epochs.midtimes\n",
    "--> 685 x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "    687 significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "    688 significant_ripple_epochs_df\n",
    "\n",
    "File c:\\\\Users\\\\pho\\\\repos\\\\Spike3DWorkEnv\\\\Spike3D\\\\.venv\\\\lib\\\\site-packages\\\\attr\\\\_next_gen.py:208, in asdict(inst, recurse, filter, value_serializer)\n",
    "    201 def asdict(inst, *, recurse=True, filter=None, value_serializer=None):\n",
    "    202     \\\"\\\"\\\"\n",
    "    203     Same as `attr.asdict`, except that collections types are always retained\n",
    "    204     and dict is always used as *dict_factory*.\n",
    "    205 \n",
    "    206     .. versionadded:: 21.3.0\n",
    "    207     \\\"\\\"\\\"\n",
    "--> 208     return _asdict(\n",
    "    209         inst=inst,\n",
    "    210         recurse=recurse,\n",
    "    211         filter=filter,\n",
    "    212         value_serializer=value_serializer,\n",
    "    213         retain_collection_types=True,\n",
    "    214     )\n",
    "\n",
    "File c:\\\\Users\\\\pho\\\\repos\\\\Spike3DWorkEnv\\\\Spike3D\\\\.venv\\\\lib\\\\site-packages\\\\attr\\\\_funcs.py:64, in asdict(inst, recurse, filter, dict_factory, retain_collection_types, value_serializer)\n",
    "     62 if recurse is True:\n",
    "     63     if has(v.__class__):\n",
    "---> 64         rv[a.name] = asdict(\n",
    "     65             v,\n",
    "     66             recurse=True,\n",
    "     67             filter=filter,\n",
    "     68             dict_factory=dict_factory,\n",
    "     69             retain_collection_types=retain_collection_types,\n",
    "     70             value_serializer=value_serializer,\n",
    "     71         )\n",
    "     72     elif isinstance(v, (tuple, list, set, frozenset)):\n",
    "     73         cf = v.__class__ if retain_collection_types is True else list\n",
    "\n",
    "File c:\\\\Users\\\\pho\\\\repos\\\\Spike3DWorkEnv\\\\Spike3D\\\\.venv\\\\lib\\\\site-packages\\\\attr\\\\_funcs.py:89, in asdict(inst, recurse, filter, dict_factory, retain_collection_types, value_serializer)\n",
    "     87 elif isinstance(v, dict):\n",
    "     88     df = dict_factory\n",
    "---> 89     rv[a.name] = df(\n",
    "     90         (\n",
    "     91             _asdict_anything(\n",
    "     92                 kk,\n",
    "     93                 is_key=True,\n",
    "     94                 filter=filter,\n",
    "     95                 dict_factory=df,\n",
    "     96                 retain_collection_types=retain_collection_types,\n",
    "     97                 value_serializer=value_serializer,\n",
    "     98             ),\n",
    "     99             _asdict_anything(\n",
    "    100                 vv,\n",
    "    101                 is_key=False,\n",
    "    102                 filter=filter,\n",
    "    103                 dict_factory=df,\n",
    "    104                 retain_collection_types=retain_collection_types,\n",
    "    105                 value_serializer=value_serializer,\n",
    "    106             ),\n",
    "    107         )\n",
    "    108         for kk, vv in v.items()\n",
    "    109     )\n",
    "    110 else:\n",
    "    111     rv[a.name] = v\n",
    "\n",
    "File c:\\\\Users\\\\pho\\\\repos\\\\Spike3DWorkEnv\\\\Spike3D\\\\.venv\\\\lib\\\\site-packages\\\\attr\\\\_funcs.py:99, in <genexpr>(.0)\n",
    "     87 elif isinstance(v, dict):\n",
    "     88     df = dict_factory\n",
    "     89     rv[a.name] = df(\n",
    "     90         (\n",
    "     91             _asdict_anything(\n",
    "     92                 kk,\n",
    "     93                 is_key=True,\n",
    "     94                 filter=filter,\n",
    "     95                 dict_factory=df,\n",
    "     96                 retain_collection_types=retain_collection_types,\n",
    "     97                 value_serializer=value_serializer,\n",
    "     98             ),\n",
    "---> 99             _asdict_anything(\n",
    "    100                 vv,\n",
    "    101                 is_key=False,\n",
    "    102                 filter=filter,\n",
    "    103                 dict_factory=df,\n",
    "    104                 retain_collection_types=retain_collection_types,\n",
    "    105                 value_serializer=value_serializer,\n",
    "    106             ),\n",
    "    107         )\n",
    "    108         for kk, vv in v.items()\n",
    "    109     )\n",
    "    110 else:\n",
    "    111     rv[a.name] = v\n",
    "\n",
    "File c:\\\\Users\\\\pho\\\\repos\\\\Spike3DWorkEnv\\\\Spike3D\\\\.venv\\\\lib\\\\site-packages\\\\attr\\\\_funcs.py:146, in _asdict_anything(val, is_key, filter, dict_factory, retain_collection_types, value_serializer)\n",
    "    143     else:\n",
    "    144         cf = list\n",
    "--> 146     rv = cf(\n",
    "    147         [\n",
    "    148             _asdict_anything(\n",
    "    149                 i,\n",
    "    150                 is_key=False,\n",
    "    151                 filter=filter,\n",
    "    152                 dict_factory=dict_factory,\n",
    "    153                 retain_collection_types=retain_collection_types,\n",
    "    154                 value_serializer=value_serializer,\n",
    "    155             )\n",
    "    156             for i in val\n",
    "    157         ]\n",
    "    158     )\n",
    "    159 elif isinstance(val, dict):\n",
    "    160     df = dict_factory\n",
    "\n",
    "TypeError: <lambda>() missing 4 required positional arguments: 'short_stats_z_scorer', 'long_short_z_diff', 'long_short_naive_z_diff', and 'is_forward_replay'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(rank_order_obj.__dict__.keys())) # ['is_global', 'LR_ripple', 'RL_ripple', 'LR_laps', 'RL_laps', 'ripple_most_likely_result_tuple', 'laps_most_likely_result_tuple', 'ripple_combined_epoch_stats_df', 'ripple_new_output_tuple', 'laps_combined_epoch_stats_df', 'laps_new_output_tuple', 'minimum_inclusion_fr_Hz', 'included_qclu_values']\n",
    "for k, v in rank_order_obj.__dict__.items():\n",
    "    print(f'trying to pickle: {k}')\n",
    "    try:\n",
    "        diagnose_pickling_issues(v)\n",
    "    except TypeError as e:\n",
    "        print(f'failed to pickle .{k} with error {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['is_global', 'LR_ripple', 'RL_ripple', 'LR_laps', 'RL_laps', 'ripple_most_likely_result_tuple', 'laps_most_likely_result_tuple', 'ripple_combined_epoch_stats_df', 'ripple_new_output_tuple', 'laps_combined_epoch_stats_df', 'laps_new_output_tuple', 'minimum_inclusion_fr_Hz', 'included_qclu_values']\n",
    "trying to pickle: is_global\n",
    "No pickling issues detected.\n",
    "trying to pickle: LR_ripple\n",
    "failed to pickle .LR_ripple with error cannot pickle 'QApplication' object\n",
    "trying to pickle: RL_ripple\n",
    "failed to pickle .RL_ripple with error cannot pickle 'QApplication' object\n",
    "trying to pickle: LR_laps\n",
    "failed to pickle .LR_laps with error cannot pickle 'QApplication' object\n",
    "trying to pickle: RL_laps\n",
    "failed to pickle .RL_laps with error cannot pickle 'QApplication' object\n",
    "trying to pickle: ripple_most_likely_result_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: laps_most_likely_result_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: ripple_combined_epoch_stats_df\n",
    "No pickling issues detected.\n",
    "trying to pickle: ripple_new_output_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: laps_combined_epoch_stats_df\n",
    "No pickling issues detected.\n",
    "trying to pickle: laps_new_output_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: minimum_inclusion_fr_Hz\n",
    "No pickling issues detected.\n",
    "trying to pickle: included_qclu_values\n",
    "No pickling issues detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['is_global', 'LR_ripple', 'RL_ripple', 'LR_laps', 'RL_laps', 'ripple_most_likely_result_tuple', 'laps_most_likely_result_tuple', 'ripple_combined_epoch_stats_df', 'ripple_new_output_tuple', 'laps_combined_epoch_stats_df', 'laps_new_output_tuple', 'minimum_inclusion_fr_Hz', 'included_qclu_values']\n",
    "trying to pickle: .is_global\n",
    "No pickling issues detected.\n",
    "trying to pickle: .LR_ripple\n",
    "Pickling error encountered: cannot pickle 'QApplication' object\n",
    "Identified problematic attribute: to_hdf\n",
    "Potential causes:\n",
    "- Attribute contains unpicklable data types (e.g., lambda functions, file objects).\n",
    "- Attribute refers to external resources (e.g., database connections).\n",
    "- Attribute has circular references within the object's structure.\n",
    "trying to pickle: .RL_ripple\n",
    "Pickling error encountered: cannot pickle 'QApplication' object\n",
    "Identified problematic attribute: to_hdf\n",
    "Potential causes:\n",
    "- Attribute contains unpicklable data types (e.g., lambda functions, file objects).\n",
    "- Attribute refers to external resources (e.g., database connections).\n",
    "- Attribute has circular references within the object's structure.\n",
    "trying to pickle: .LR_laps\n",
    "Pickling error encountered: cannot pickle 'QApplication' object\n",
    "Identified problematic attribute: to_hdf\n",
    "Potential causes:\n",
    "- Attribute contains unpicklable data types (e.g., lambda functions, file objects).\n",
    "- Attribute refers to external resources (e.g., database connections).\n",
    "- Attribute has circular references within the object's structure.\n",
    "trying to pickle: .RL_laps\n",
    "Pickling error encountered: cannot pickle 'QApplication' object\n",
    "Identified problematic attribute: to_hdf\n",
    "Potential causes:\n",
    "- Attribute contains unpicklable data types (e.g., lambda functions, file objects).\n",
    "- Attribute refers to external resources (e.g., database connections).\n",
    "- Attribute has circular references within the object's structure.\n",
    "trying to pickle: .ripple_most_likely_result_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: .laps_most_likely_result_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: .ripple_combined_epoch_stats_df\n",
    "No pickling issues detected.\n",
    "trying to pickle: .ripple_new_output_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: .laps_combined_epoch_stats_df\n",
    "No pickling issues detected.\n",
    "trying to pickle: .laps_new_output_tuple\n",
    "No pickling issues detected.\n",
    "trying to pickle: .minimum_inclusion_fr_Hz\n",
    "No pickling issues detected.\n",
    "trying to pickle: .included_qclu_values\n",
    "No pickling issues detected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear issue in `rank_order_obj.LR_ripple.ranked_aclus_stats_dict`: values contain LongShortStatsTuple which contains long_stats_z_scorer\n",
    "`pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.Zscorer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['is_global', 'ranked_aclus_stats_dict', 'selected_spikes_fragile_linear_neuron_IDX_dict', 'long_z_score', 'short_z_score', 'long_short_z_score_diff', 'spikes_df', 'epochs_df', 'selected_spikes_df', 'extra_info_dict']\n",
    "trying to pickle: .is_global\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .ranked_aclus_stats_dict\n",
    "\t\tPickling error encountered: Can't pickle <function make_set_closure_cell.<locals>.set_closure_cell at 0x000002D510CD7820>: it's not found as attr._compat.make_set_closure_cell.<locals>.set_closure_cell\n",
    "Identified problematic attribute: values\n",
    "Potential causes:\n",
    "- Attribute contains unpicklable data types (e.g., lambda functions, file objects).\n",
    "- Attribute refers to external resources (e.g., database connections).\n",
    "- Attribute has circular references within the object's structure.\n",
    "\n",
    "trying to pickle: .selected_spikes_fragile_linear_neuron_IDX_dict\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .long_z_score\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .short_z_score\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .long_short_z_score_diff\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .spikes_df\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .epochs_df\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .selected_spikes_df\n",
    "\t\tNo pickling issues detected.\n",
    "\n",
    "trying to pickle: .extra_info_dict\n",
    "\t\tNo pickling issues detected.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_black",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
