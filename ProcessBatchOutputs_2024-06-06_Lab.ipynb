{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "scripts_output_path: /home/halechr/cloud/turbo/Data/Output/gen_scripts\n",
      "collected_outputs_path: /home/halechr/cloud/turbo/Data/Output/collected_outputs\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# # required to enable non-blocking interaction:\n",
    "# %gui qt5 ## TODO 2024-01-18 - this causes kernel to crash when running notebook remotely via VSCode's ssh remote\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.exception_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import code_block_widget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import build_vscode_workspace, build_windows_powershell_run_script\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file, save_copydict_to_text_file, read_copydict_from_text_file, invert_filedict\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsHelpers\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget\n",
    "\n",
    "import inspect\n",
    "from jinja2 import Template\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import MAIN_get_template_string, write_test_script\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import export_session_h5_file_completion_function, curr_runtime_context_header_template, export_rank_order_results_completion_function, figures_rank_order_results_completion_function, compute_and_export_marginals_dfs_completion_function, determine_session_t_delta_completion_function, perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function, compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function, reload_exported_kdiba_session_position_info_mat_completion_function, compute_and_export_session_wcorr_shuffles_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import ProcessingScriptPhases\n",
    "\n",
    "# BATCH_DATE_TO_USE = '2024-06-06_GL' # used for filenames throught the notebook\n",
    "BATCH_DATE_TO_USE = '2024-06-06_Lab' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2024-06-06_Apogee' # used for filenames throught the notebook\n",
    "\n",
    "# scripts_output_path = Path('/home/halechr/cloud/turbo/Data/Output/gen_scripts/').resolve() # Greatlakes\n",
    "# scripts_output_path = Path('output/gen_scripts/').resolve() # Apogee\n",
    "# # scripts_output_path = Path('/home/halechr/FastData/gen_scripts/').resolve() # Lab\n",
    "# assert scripts_output_path.exists()\n",
    "known_scripts_output_paths = [Path(v).resolve() for v in ['/home/halechr/cloud/turbo/Data/Output/gen_scripts/', '/home/halechr/FastData/gen_scripts/', 'output/gen_scripts/']]\n",
    "scripts_output_path = find_first_extant_path(known_scripts_output_paths)\n",
    "assert scripts_output_path.exists(), f\"scripts_output_path: {scripts_output_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'scripts_output_path: {scripts_output_path}')\n",
    "\n",
    "collected_outputs_path = scripts_output_path.joinpath('../collected_outputs').resolve()\n",
    "collected_outputs_path.mkdir(exist_ok=True)\n",
    "assert collected_outputs_path.exists()\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d958aaf",
   "metadata": {},
   "source": [
    "## Build Processing Scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb673d3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the generated code for user-contributed functions:\n",
    "phase_any_run_custom_user_completion_functions_dict = {\n",
    "\"export_rank_order_results_completion_function\": export_rank_order_results_completion_function, # ran 2024-05-28 6am\n",
    "# \"figures_rank_order_results_completion_function\": figures_rank_order_results_completion_function,\n",
    "\"compute_and_export_marginals_dfs_completion_function\": compute_and_export_marginals_dfs_completion_function, # ran 2024-05-28 6am\n",
    "\"determine_session_t_delta_completion_function\": determine_session_t_delta_completion_function,  # ran 2024-05-28 6am\n",
    "'perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function': perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function, # ran 2024-05-28 6am\n",
    "'compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function': compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function, # ran 2024-05-28 6am\n",
    "'reload_exported_kdiba_session_position_info_mat_completion_function': reload_exported_kdiba_session_position_info_mat_completion_function,\n",
    "'export_session_h5_file_completion_function': export_session_h5_file_completion_function, # ran 2024-05-28 3am\n",
    "    # 'compute_and_export_session_wcorr_shuffles_completion_function': compute_and_export_session_wcorr_shuffles_completion_function,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa767589",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_user_completion_functions_dict: {'export_rank_order_results_completion_function': <function export_rank_order_results_completion_function at 0x7f0d5d8fd1f0>, 'compute_and_export_marginals_dfs_completion_function': <function compute_and_export_marginals_dfs_completion_function at 0x7f0d5d8fd310>, 'determine_session_t_delta_completion_function': <function determine_session_t_delta_completion_function at 0x7f0d5d8fd3a0>, 'perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function': <function perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function at 0x7f0d5d8fd4c0>, 'compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function': <function compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function at 0x7f0d5d8fd550>, 'reload_exported_kdiba_session_position_info_mat_completion_function': <function reload_exported_kdiba_session_position_info_mat_completion_function at 0x7f0d5d8fd430>, 'export_session_h5_file_completion_function': <function export_session_h5_file_completion_function at 0x7f0d5d8fd5e0>}\n",
      "custom_user_completion_function_template_code: \n",
      "BATCH_DATE_TO_USE = '2024-06-06_Lab'\n",
      "collected_outputs_path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs').resolve()\n",
      "\n",
      "# ==================================================================================================================== #\n",
      "# BEGIN USER COMPLETION FUNCTIONS                                                                                      #\n",
      "# ==================================================================================================================== #\n",
      "from copy import deepcopy\n",
      "\n",
      "\n",
      "custom_user_completion_functions = []\n",
      "\n",
      "@function_attributes(short_name=None, tags=['batch', 'rank-order'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-27 21:21', related_items=[])\n",
      "def export_rank_order_results_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
      "    # print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'export_rank_order_results_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
      "    long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
      "\n",
      "    assert self.collected_outputs_path.exists()\n",
      "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
      "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
      "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
      "\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import save_rank_order_results, SaveStringGenerator\n",
      "    save_rank_order_results(curr_active_pipeline, day_date=f\"{CURR_BATCH_OUTPUT_PREFIX}\", override_output_parent_path=self.collected_outputs_path) # \"2024-01-02_301pm\" \"2024-01-02_734pm\"\"\n",
      "\n",
      "    ## 2023-12-21 - Export to CSV:\n",
      "    spikes_df = curr_active_pipeline.sess.spikes_df\n",
      "    rank_order_results = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
      "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
      "    included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
      "    ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
      "    directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
      "    track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
      "    print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
      "    print(f'included_qclu_values: {included_qclu_values}')\n",
      "\n",
      "    print(f'\\t try saving to CSV...')\n",
      "    # active_csv_parent_output_path = curr_active_pipeline.get_output_path().resolve()\n",
      "    active_csv_parent_output_path = self.collected_outputs_path.resolve()\n",
      "    merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
      "    merged_complete_ripple_epoch_stats_df_output_path = active_csv_parent_output_path.joinpath(f'{CURR_BATCH_OUTPUT_PREFIX}_merged_complete_epoch_stats_df.csv').resolve()\n",
      "    merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
      "    print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')\n",
      "    \n",
      "    # across_session_results_extended_dict['merged_complete_epoch_stats_df'] = merged_complete_ripple_epoch_stats_df_output_path\n",
      "    \n",
      "    print(f'>>\\t done with {curr_session_context}')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    # return True\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(export_rank_order_results_completion_function)\n",
      "# END `export_rank_order_results_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "@function_attributes(short_name=None, tags=['marginal', 'across-sessions', 'CSV'], input_requires=[], output_provides=[], uses=['directional_merged_decoders_result.compute_and_export_marginals_df_csvs'], used_by=[], creation_date='2024-04-27 21:22', related_items=[])\n",
      "def compute_and_export_marginals_dfs_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'compute_and_export_marginals_dfs_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
      "    \n",
      "    assert self.collected_outputs_path.exists()\n",
      "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
      "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
      "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
      "\n",
      "    from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
      "    curr_active_pipeline.reload_default_computation_functions()\n",
      "    batch_extended_computations(curr_active_pipeline, include_includelist=['merged_directional_placefields'], include_global_functions=True, fail_on_exception=True, force_recompute=False)\n",
      "    directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
      "\n",
      "    active_context = curr_active_pipeline.get_session_context()\n",
      "    _out = directional_merged_decoders_result.compute_and_export_marginals_df_csvs(parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
      "    print(f'successfully exported marginals_df_csvs to \"{self.collected_outputs_path}\"!')\n",
      "    # (laps_marginals_df, laps_out_path), (ripple_marginals_df, ripple_out_path) = _out\n",
      "    (laps_marginals_df, laps_out_path, laps_time_bin_marginals_df, laps_time_bin_marginals_out_path), (ripple_marginals_df, ripple_out_path, ripple_time_bin_marginals_df, ripple_time_bin_marginals_out_path) = _out\n",
      "    print(f'\\tlaps_out_path: {laps_out_path}\\n\\tripple_out_path: {ripple_out_path}\\n\\tdone.')\n",
      "\n",
      "    # add to output dict\n",
      "    # across_session_results_extended_dict['compute_and_export_marginals_dfs_completion_function'] = _out\n",
      "\n",
      "    print(f'>>\\t done with {curr_session_context}')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(compute_and_export_marginals_dfs_completion_function)\n",
      "# END `compute_and_export_marginals_dfs_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "def determine_session_t_delta_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
      "    \"\"\" \n",
      "    from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import determine_computation_datetimes_completion_function\n",
      "    \n",
      "    Results can be extracted from batch output by \n",
      "    \n",
      "    # Extracts the callback results 'determine_session_t_delta_completion_function':\n",
      "    extracted_callback_fn_results = {a_sess_ctxt:a_result.across_session_results.get('determine_session_t_delta_completion_function', {}) for a_sess_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'determine_session_t_delta_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
      "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
      "    print(f'\\t{curr_session_basedir}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')\n",
      "    \n",
      "    callback_outputs = {\n",
      "     't_start': t_start, 't_delta':t_delta, 't_end': t_end   \n",
      "    }\n",
      "    across_session_results_extended_dict['determine_session_t_delta_completion_function'] = callback_outputs\n",
      "    \n",
      "    # print(f'>>\\t done with {curr_session_context}')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(determine_session_t_delta_completion_function)\n",
      "# END `determine_session_t_delta_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "@function_attributes(short_name=None, tags=['CSV', 'time_bin_sizes', 'marginals'], input_requires=['DirectionalMergedDecoders'], output_provides=[], uses=[], used_by=[], creation_date='2024-04-27 21:24', related_items=[])\n",
      "def perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict,\n",
      "                                                                             save_hdf=True, save_csvs=True, return_full_decoding_results:bool=False, \n",
      "                                                                             custom_all_param_sweep_options=None,\n",
      "                                                                             desired_shared_decoding_time_bin_sizes:Optional[NDArray]=None) -> dict:\n",
      "    \"\"\"\n",
      "    if `return_full_decoding_results` == True, returns the full decoding results for debugging purposes. `output_alt_directional_merged_decoders_result`\n",
      "\n",
      "    custom_all_param_sweep_options: if provided, these parameters will be used as the parameter sweeps instead of building new ones.\n",
      "\n",
      "    custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=np.linspace(start=0.030, stop=0.10, num=6),\n",
      "                                                                            use_single_time_bin_per_epoch=[False],\n",
      "                                                                            minimum_event_duration=[desired_shared_decoding_time_bin_sizes[-1]])\n",
      "\n",
      "    \"\"\"\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
      "    from copy import deepcopy\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    from typing_extensions import TypeAlias\n",
      "    from typing import NewType\n",
      "    from nptyping import NDArray\n",
      "\n",
      "    import neuropy.utils.type_aliases as types\n",
      "    from neuropy.utils.indexing_helpers import PandasHelpers\n",
      "    from neuropy.utils.debug_helpers import parameter_sweeps\n",
      "    from neuropy.core.laps import Laps\n",
      "    from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
      "    from pyphocorehelpers.print_helpers import get_now_day_str\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _check_result_laps_epochs_df_performance\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
      "    from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
      "\n",
      "\n",
      "    DecodedEpochsResultsDict = NewType('DecodedEpochsResultsDict', Dict[types.DecoderName, DecodedFilterEpochsResult]) # A Dict containing the decoded filter epochs result for each of the four 1D decoder names\n",
      "\n",
      "    # Export CSVs:\n",
      "    def export_marginals_df_csv(marginals_df: pd.DataFrame, data_identifier_str: str, parent_output_path: Path, active_context):\n",
      "        \"\"\" captures nothing\n",
      "        \"\"\"\n",
      "        # output_date_str: str = get_now_rounded_time_str()\n",
      "        output_date_str: str = get_now_day_str()\n",
      "        # parent_output_path: Path = Path('output').resolve()\n",
      "        # active_context = curr_active_pipeline.get_session_context()\n",
      "        session_identifier_str: str = active_context.get_description()\n",
      "        assert output_date_str is not None\n",
      "        out_basename = '-'.join([output_date_str, session_identifier_str, data_identifier_str]) # '2024-01-04|kdiba_gor01_one_2006-6-09_1-22-43|(laps_marginals_df).csv'\n",
      "        out_filename = f\"{out_basename}.csv\"\n",
      "        out_path = parent_output_path.joinpath(out_filename).resolve()\n",
      "        marginals_df.to_csv(out_path)\n",
      "        return out_path \n",
      "\n",
      "    def _subfn_process_time_bin_swept_results(output_extracted_result_tuples, active_context):\n",
      "        \"\"\" After the sweeps are complete and multiple (one for each time_bin_size swept) indepdnent dfs are had with the four results types this function concatenates each of the four into a single dataframe for all time_bin_size values with a column 'time_bin_size'. \n",
      "        It also saves them out to CSVs in a manner similar to what `compute_and_export_marginals_dfs_completion_function` did to be compatible with `2024-01-23 - Across Session Point and YellowBlue Marginal CSV Exports.ipynb`\n",
      "        Captures: save_csvs\n",
      "        GLOBAL Captures: collected_outputs_path\n",
      "        \n",
      "        \n",
      "        \"\"\"\n",
      "        several_time_bin_sizes_laps_time_bin_marginals_df_list = []\n",
      "        several_time_bin_sizes_laps_per_epoch_marginals_df_list = []\n",
      "\n",
      "        several_time_bin_sizes_ripple_time_bin_marginals_df_list = []\n",
      "        several_time_bin_sizes_ripple_per_epoch_marginals_df_list = []\n",
      "\n",
      "\n",
      "        # for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
      "        for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df, a_ripple_time_bin_marginals_df, a_ripple_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
      "            a_sweep_dict = dict(a_sweep_tuple)\n",
      "            \n",
      "\n",
      "            if 'desired_shared_decoding_time_bin_size' in a_sweep_dict:\n",
      "                # Shared\n",
      "                desired_laps_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
      "                desired_ripple_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
      "            else:\n",
      "                # Separate:\n",
      "                desired_laps_decoding_time_bin_size = float(a_sweep_dict.get('desired_laps_decoding_time_bin_size', None))\n",
      "                if desired_laps_decoding_time_bin_size is not None:\n",
      "                    desired_laps_decoding_time_bin_size = float(desired_laps_decoding_time_bin_size)\n",
      "                \n",
      "                desired_ripple_decoding_time_bin_size = a_sweep_dict.get('desired_ripple_decoding_time_bin_size', None)\n",
      "                if desired_ripple_decoding_time_bin_size is not None:\n",
      "                    desired_ripple_decoding_time_bin_size = float(desired_ripple_decoding_time_bin_size)\n",
      "            \n",
      "\n",
      "            if desired_laps_decoding_time_bin_size is not None:\n",
      "                df = a_laps_time_bin_marginals_df\n",
      "                df['time_bin_size'] = desired_laps_decoding_time_bin_size # desired_laps_decoding_time_bin_size\n",
      "                # df['session_name'] = session_name\n",
      "                df = a_laps_all_epoch_bins_marginals_df\n",
      "                df['time_bin_size'] = desired_laps_decoding_time_bin_size\n",
      "\n",
      "                several_time_bin_sizes_laps_time_bin_marginals_df_list.append(a_laps_time_bin_marginals_df)\n",
      "                several_time_bin_sizes_laps_per_epoch_marginals_df_list.append(a_laps_all_epoch_bins_marginals_df)\n",
      "                \n",
      "\n",
      "            if desired_ripple_decoding_time_bin_size is not None:\n",
      "                df = a_ripple_time_bin_marginals_df\n",
      "                df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
      "                df = a_ripple_all_epoch_bins_marginals_df\n",
      "                df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
      "\n",
      "                several_time_bin_sizes_ripple_time_bin_marginals_df_list.append(a_ripple_time_bin_marginals_df)\n",
      "                several_time_bin_sizes_ripple_per_epoch_marginals_df_list.append(a_ripple_all_epoch_bins_marginals_df)\n",
      "            \n",
      "\n",
      "        ## Build across_sessions join dataframes:\n",
      "        several_time_bin_sizes_time_bin_laps_df: Optional[pd.DataFrame] = PandasHelpers.safe_concat(several_time_bin_sizes_laps_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
      "        several_time_bin_sizes_laps_df: Optional[pd.DataFrame] = PandasHelpers.safe_concat(several_time_bin_sizes_laps_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
      "        several_time_bin_sizes_time_bin_ripple_df: Optional[pd.DataFrame] = PandasHelpers.safe_concat(several_time_bin_sizes_ripple_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
      "        several_time_bin_sizes_ripple_df: Optional[pd.DataFrame] = PandasHelpers.safe_concat(several_time_bin_sizes_ripple_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
      "\n",
      "        # Export time_bin_swept results to CSVs:\n",
      "        laps_time_bin_marginals_out_path, laps_out_path, ripple_time_bin_marginals_out_path, ripple_out_path = None, None, None, None\n",
      "        if save_csvs:\n",
      "            assert self.collected_outputs_path.exists()\n",
      "            assert active_context is not None\n",
      "            if several_time_bin_sizes_time_bin_laps_df is not None:\n",
      "                laps_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_laps_df, data_identifier_str=f'(laps_time_bin_marginals_df)', parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
      "            if several_time_bin_sizes_laps_df is not None:\n",
      "                laps_out_path = export_marginals_df_csv(several_time_bin_sizes_laps_df, data_identifier_str=f'(laps_marginals_df)', parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
      "            if several_time_bin_sizes_time_bin_ripple_df is not None:\n",
      "                ripple_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_ripple_df, data_identifier_str=f'(ripple_time_bin_marginals_df)', parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
      "            if several_time_bin_sizes_ripple_df is not None:\n",
      "                ripple_out_path = export_marginals_df_csv(several_time_bin_sizes_ripple_df, data_identifier_str=f'(ripple_marginals_df)', parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
      "\n",
      "        return (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
      "        # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
      "        \n",
      "    def add_session_df_columns(df: pd.DataFrame, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
      "        \"\"\" adds session-specific information to the marginal dataframes \"\"\"\n",
      "        df['session_name'] = session_name \n",
      "        if curr_session_t_delta is not None:\n",
      "            df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
      "        return df\n",
      "\n",
      "    ## Merged-only decode:\n",
      "    def _try_single_decode(owning_pipeline_reference, directional_merged_decoders_result: DirectionalPseudo2DDecodersResult, use_single_time_bin_per_epoch: bool,\n",
      "                            desired_laps_decoding_time_bin_size: Optional[float]=None, desired_ripple_decoding_time_bin_size: Optional[float]=None, desired_shared_decoding_time_bin_size: Optional[float]=None, minimum_event_duration: Optional[float]=None) -> DirectionalPseudo2DDecodersResult:\n",
      "        \"\"\" decodes laps and ripples for a single bin size. \n",
      "        \n",
      "        desired_laps_decoding_time_bin_size\n",
      "        desired_ripple_decoding_time_bin_size\n",
      "        minimum_event_duration: if provided, excludes all events shorter than minimum_event_duration\n",
      "\n",
      "        Looks like it updates:\n",
      "            .all_directional_laps_filter_epochs_decoder_result, .all_directional_ripple_filter_epochs_decoder_result, and whatever .perform_compute_marginals() updates\n",
      "\n",
      "        \n",
      "        Compared to `_compute_lap_and_ripple_epochs_decoding_for_decoder`, it looks like this only computes for the `*all*_directional_pf1D_Decoder` while `_compute_lap_and_ripple_epochs_decoding_for_decoder` is called for each separate directional pf1D decoder\n",
      "        \"\"\"\n",
      "        if desired_shared_decoding_time_bin_size is not None:\n",
      "            assert desired_laps_decoding_time_bin_size is None\n",
      "            assert desired_ripple_decoding_time_bin_size is None\n",
      "            desired_laps_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
      "            desired_ripple_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
      "            \n",
      "        # Separate the decoder first so they're all independent:\n",
      "        directional_merged_decoders_result = deepcopy(directional_merged_decoders_result)\n",
      "\n",
      "        ## Decode Laps:\n",
      "        laps_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs)\n",
      "        if not isinstance(laps_epochs_df, pd.DataFrame):\n",
      "            laps_epochs_df = laps_epochs_df.to_dataframe()\n",
      "        # global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
      "        min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(laps_epochs_df['duration'].to_numpy())\n",
      "        min_bounded_laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
      "        if desired_laps_decoding_time_bin_size < min_bounded_laps_decoding_time_bin_size:\n",
      "            print(f'WARN: desired_laps_decoding_time_bin_size: {desired_laps_decoding_time_bin_size} < min_bounded_laps_decoding_time_bin_size: {min_bounded_laps_decoding_time_bin_size}... hopefully it works.')\n",
      "        laps_decoding_time_bin_size: float = desired_laps_decoding_time_bin_size # allow direct use\n",
      "        if use_single_time_bin_per_epoch:\n",
      "            laps_decoding_time_bin_size = None\n",
      "        directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=laps_epochs_df,\n",
      "                                                                                                                                                        decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
      "\n",
      "        ## Decode Ripples: ripples are kinda optional (if `desired_ripple_decoding_time_bin_size is None` they are not computed.\n",
      "        if desired_ripple_decoding_time_bin_size is not None:\n",
      "            # global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(owning_pipeline_reference.filtered_sessions[global_epoch_name].replay))\n",
      "            replay_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs)\n",
      "            if not isinstance(replay_epochs_df, pd.DataFrame):\n",
      "                replay_epochs_df = replay_epochs_df.to_dataframe()\n",
      "            # min_possible_ripple_time_bin_size: float = find_minimum_time_bin_duration(replay_epochs_df['duration'].to_numpy())\n",
      "            # min_bounded_ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_ripple_time_bin_size) # 10ms # 0.002\n",
      "            # if desired_ripple_decoding_time_bin_size < min_bounded_ripple_decoding_time_bin_size:\n",
      "            #     print(f'WARN: desired_ripple_decoding_time_bin_size: {desired_ripple_decoding_time_bin_size} < min_bounded_ripple_decoding_time_bin_size: {min_bounded_ripple_decoding_time_bin_size}... hopefully it works.')\n",
      "            ripple_decoding_time_bin_size: float = desired_ripple_decoding_time_bin_size # allow direct use            \n",
      "            ## Drop those less than the time bin duration\n",
      "            print(f'DropShorterMode:')\n",
      "            pre_drop_n_epochs = len(replay_epochs_df)\n",
      "            if minimum_event_duration is not None:                \n",
      "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > minimum_event_duration]\n",
      "                post_drop_n_epochs = len(replay_epochs_df)\n",
      "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
      "                print(f'\\tminimum_event_duration present (minimum_event_duration={minimum_event_duration}).\\n\\tdropping {n_dropped_epochs} that are shorter than our minimum_event_duration of {minimum_event_duration}.', end='\\t')\n",
      "            else:\n",
      "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > desired_ripple_decoding_time_bin_size]\n",
      "                post_drop_n_epochs = len(replay_epochs_df)\n",
      "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
      "                print(f'\\tdropping {n_dropped_epochs} that are shorter than our ripple decoding time bin size of {desired_ripple_decoding_time_bin_size}', end='\\t') \n",
      "\n",
      "            print(f'{post_drop_n_epochs} remain.')\n",
      "            # returns a `DecodedFilterEpochsResult`\n",
      "            directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=replay_epochs_df,\n",
      "                                                                                                                                                                                            decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
      "\n",
      "        directional_merged_decoders_result.perform_compute_marginals()\n",
      "        return directional_merged_decoders_result\n",
      "        \n",
      "    \n",
      "\n",
      "    ## All templates AND merged decode:\n",
      "    def _try_all_templates_decode(owning_pipeline_reference, directional_merged_decoders_result: DirectionalPseudo2DDecodersResult, use_single_time_bin_per_epoch: bool,\n",
      "                            desired_laps_decoding_time_bin_size: Optional[float]=None, desired_ripple_decoding_time_bin_size: Optional[float]=None, desired_shared_decoding_time_bin_size: Optional[float]=None, minimum_event_duration: Optional[float]=None) -> Tuple[DirectionalPseudo2DDecodersResult, Tuple[DecodedEpochsResultsDict, DecodedEpochsResultsDict]]: #-> Dict[str, DirectionalPseudo2DDecodersResult]:\n",
      "        \"\"\" decodes laps and ripples for a single bin size but for each of the four track templates. \n",
      "        \n",
      "        Added 2024-05-23 04:23 \n",
      "\n",
      "        desired_laps_decoding_time_bin_size\n",
      "        desired_ripple_decoding_time_bin_size\n",
      "        minimum_event_duration: if provided, excludes all events shorter than minimum_event_duration\n",
      "\n",
      "        Looks like it updates:\n",
      "            .all_directional_laps_filter_epochs_decoder_result, .all_directional_ripple_filter_epochs_decoder_result, and whatever .perform_compute_marginals() updates\n",
      "\n",
      "        \n",
      "        Compared to `_compute_lap_and_ripple_epochs_decoding_for_decoder`, it looks like this only computes for the `*all*_directional_pf1D_Decoder` while `_compute_lap_and_ripple_epochs_decoding_for_decoder` is called for each separate directional pf1D decoder\n",
      "\n",
      "        Usage:\n",
      "\n",
      "            from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function, _try_all_templates_decode\n",
      "\n",
      "        \"\"\"\n",
      "        from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import EpochFilteringMode\n",
      "        \n",
      "        ripple_decoding_time_bin_size = None\n",
      "        if desired_shared_decoding_time_bin_size is not None:\n",
      "            assert desired_laps_decoding_time_bin_size is None\n",
      "            assert desired_ripple_decoding_time_bin_size is None\n",
      "            desired_laps_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
      "            desired_ripple_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
      "            \n",
      "        # Separate the decoder first so they're all independent:\n",
      "        directional_merged_decoders_result = deepcopy(directional_merged_decoders_result)\n",
      "\n",
      "        ## Decode Laps:\n",
      "        laps_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs)\n",
      "        if not isinstance(laps_epochs_df, pd.DataFrame):\n",
      "            laps_epochs_df = laps_epochs_df.to_dataframe()\n",
      "        # global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
      "        min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(laps_epochs_df['duration'].to_numpy())\n",
      "        min_bounded_laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
      "        if desired_laps_decoding_time_bin_size < min_bounded_laps_decoding_time_bin_size:\n",
      "            print(f'WARN: desired_laps_decoding_time_bin_size: {desired_laps_decoding_time_bin_size} < min_bounded_laps_decoding_time_bin_size: {min_bounded_laps_decoding_time_bin_size}... hopefully it works.')\n",
      "        laps_decoding_time_bin_size: float = desired_laps_decoding_time_bin_size # allow direct use\n",
      "        if use_single_time_bin_per_epoch:\n",
      "            laps_decoding_time_bin_size = None\n",
      "\n",
      "            \n",
      "        directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=laps_epochs_df,\n",
      "                                                                                                                                                        decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
      "\n",
      "        ## Decode Ripples: ripples are kinda optional (if `desired_ripple_decoding_time_bin_size is None` they are not computed.\n",
      "        if desired_ripple_decoding_time_bin_size is not None:\n",
      "            # global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(owning_pipeline_reference.filtered_sessions[global_epoch_name].replay))\n",
      "            replay_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs)\n",
      "            if not isinstance(replay_epochs_df, pd.DataFrame):\n",
      "                replay_epochs_df = replay_epochs_df.to_dataframe()\n",
      "            # min_possible_ripple_time_bin_size: float = find_minimum_time_bin_duration(replay_epochs_df['duration'].to_numpy())\n",
      "            # min_bounded_ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_ripple_time_bin_size) # 10ms # 0.002\n",
      "            # if desired_ripple_decoding_time_bin_size < min_bounded_ripple_decoding_time_bin_size:\n",
      "            #     print(f'WARN: desired_ripple_decoding_time_bin_size: {desired_ripple_decoding_time_bin_size} < min_bounded_ripple_decoding_time_bin_size: {min_bounded_ripple_decoding_time_bin_size}... hopefully it works.')\n",
      "            ripple_decoding_time_bin_size: float = desired_ripple_decoding_time_bin_size # allow direct use            \n",
      "            ## Drop those less than the time bin duration\n",
      "            print(f'DropShorterMode:')\n",
      "            pre_drop_n_epochs = len(replay_epochs_df)\n",
      "            if minimum_event_duration is not None:                \n",
      "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > minimum_event_duration]\n",
      "                post_drop_n_epochs = len(replay_epochs_df)\n",
      "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
      "                print(f'\\tminimum_event_duration present (minimum_event_duration={minimum_event_duration}).\\n\\tdropping {n_dropped_epochs} that are shorter than our minimum_event_duration of {minimum_event_duration}.', end='\\t')\n",
      "            else:\n",
      "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > desired_ripple_decoding_time_bin_size]\n",
      "                post_drop_n_epochs = len(replay_epochs_df)\n",
      "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
      "                print(f'\\tdropping {n_dropped_epochs} that are shorter than our ripple decoding time bin size of {desired_ripple_decoding_time_bin_size}', end='\\t') \n",
      "\n",
      "            print(f'{post_drop_n_epochs} remain.')\n",
      "\n",
      "            # returns a `DecodedFilterEpochsResult`\n",
      "            directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=replay_epochs_df,\n",
      "                                                                                                                                                                                            decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
      "\n",
      "\n",
      "        directional_merged_decoders_result.perform_compute_marginals() # this only works for the pseudo2D decoder, not the individual 1D ones\n",
      "\n",
      "\n",
      "        # directional_merged_decoders_result_dict: Dict[types.DecoderName, DirectionalPseudo2DDecodersResult] = {}\n",
      "\n",
      "        decoder_laps_filter_epochs_decoder_result_dict: DecodedEpochsResultsDict = {}\n",
      "        decoder_ripple_filter_epochs_decoder_result_dict: DecodedEpochsResultsDict = {}\n",
      "        \n",
      "        for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
      "            # external-function way:\n",
      "            decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_lap_and_ripple_epochs_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size, epochs_filtering_mode=EpochFilteringMode.DropShorter)\n",
      "\n",
      "            # # this function's way:\n",
      "            # directional_merged_decoders_result_dict[a_name] = deepcopy(directional_merged_decoders_result)\n",
      "            \n",
      "            # directional_merged_decoders_result_dict[a_name].all_directional_laps_filter_epochs_decoder_result = a_decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=deepcopy(laps_epochs_df),\n",
      "            #                                                                                                                                     decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
      "            # if ripple_decoding_time_bin_size is not None:\n",
      "            #     directional_merged_decoders_result_dict[a_name].all_directional_ripple_filter_epochs_decoder_result = a_decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=deepcopy(replay_epochs_df),\n",
      "            #                                                                                                                                                                                 decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
      "\n",
      "            # directional_merged_decoders_result_dict[a_name].perform_compute_marginals() # this only works for the pseudo2D decoder, not the individual 1D ones\n",
      "\n",
      "\n",
      "        # decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict = _perform_compute_custom_epoch_decoding(curr_active_pipeline, directional_merged_decoders_result, track_templates) # Dict[str, Optional[DecodedFilterEpochsResult]]\n",
      "\n",
      "        # ## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
      "        # (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(directional_merged_decoders_result, track_templates,\n",
      "        #                                                                                                                                                                                     decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
      "        #                                                                                                                                                                                     spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
      "        #                                                                                                                                                                                     should_skip_radon_transform=True)\n",
      "\n",
      "        # laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
      "        # decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple\n",
      "\n",
      "\n",
      "        # for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
      "        #     decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_lap_and_ripple_epochs_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
      "\n",
      "\n",
      "        return directional_merged_decoders_result, (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
      "        \n",
      "\n",
      "    def _update_result_laps(a_result: DecodedFilterEpochsResult, laps_df: pd.DataFrame) -> pd.DataFrame:\n",
      "        \"\"\" captures nothing. Can reusing the same laps_df as it makes no modifications to it. \n",
      "        \n",
      "        e.g. a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple]\n",
      "        \"\"\"\n",
      "        result_laps_epochs_df: pd.DataFrame = a_result.laps_epochs_df\n",
      "        ## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions\n",
      "        result_laps_epochs_df['maze_id'] = laps_df['maze_id'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
      "        ## add the 'is_LR_dir' groud-truth column in:\n",
      "        result_laps_epochs_df['is_LR_dir'] = laps_df['is_LR_dir'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
      "        \n",
      "        laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir = a_result.laps_directional_marginals_tuple\n",
      "        laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = a_result.laps_track_identity_marginals_tuple\n",
      "        ## Add the decoded results to the laps df:\n",
      "        result_laps_epochs_df['is_most_likely_track_identity_Long'] = laps_is_most_likely_track_identity_Long\n",
      "        result_laps_epochs_df['is_most_likely_direction_LR'] = laps_is_most_likely_direction_LR_dir\n",
      "\n",
      "        ## re-apply the laps changes:\n",
      "        a_result.laps_epochs_df = result_laps_epochs_df # 2024-04-05 - think this is good so the result has the updated columns, but not exactly certain.\n",
      "\n",
      "        return result_laps_epochs_df\n",
      "\n",
      "    # BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
      "    should_output_lap_decoding_performance_info: bool = False\n",
      "\n",
      "    assert self.collected_outputs_path.exists()\n",
      "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
      "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
      "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
      "\n",
      "    active_context = curr_active_pipeline.get_session_context()\n",
      "    session_ctxt_key:str = active_context.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
      "    \n",
      "    ## INPUT PARAMETER: time_bin_size sweep paraemters    \n",
      "    if custom_all_param_sweep_options is None:\n",
      "        if desired_shared_decoding_time_bin_sizes is None:\n",
      "            desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.10, num=6)\n",
      "        # Shared time bin sizes\n",
      "        custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_shared_decoding_time_bin_size=desired_shared_decoding_time_bin_sizes, use_single_time_bin_per_epoch=[False], minimum_event_duration=[desired_shared_decoding_time_bin_sizes[-1]]) # with Ripples\n",
      "\n",
      "        # ## Laps Only:\n",
      "        # custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_shared_decoding_time_bin_sizes,\n",
      "        #                                                                         use_single_time_bin_per_epoch=[False],\n",
      "        #                                                                         minimum_event_duration=[desired_shared_decoding_time_bin_sizes[-1]])\n",
      "\n",
      "    else:\n",
      "        assert desired_shared_decoding_time_bin_sizes is None, f\"when providing `custom_all_param_sweep_options`, desired_shared_decoding_time_bin_sizes must be None (not specified).\"\n",
      "\n",
      "\n",
      "    all_param_sweep_options = custom_all_param_sweep_options\n",
      "\n",
      "\n",
      "    ## Perfrom the computations:\n",
      "\n",
      "    # DirectionalMergedDecoders: Get the result after computation:\n",
      "    rank_order_results = curr_active_pipeline.global_computation_results.computed_data['RankOrder'] # : \"RankOrderComputationsContainer\"\n",
      "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
      "    # included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
      "    directional_laps_results: \"DirectionalLapsResult\" = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
      "    track_templates: \"TrackTemplates\" = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
      "\n",
      "    ## Copy the default result:\n",
      "    directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
      "    alt_directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = deepcopy(directional_merged_decoders_result)\n",
      "\n",
      "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size-{laps_decoding_time_bin_size}_{data_identifier_str}\"\n",
      "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size_sweep_results\"\n",
      "    out_path_basename_str: str = f\"{CURR_BATCH_OUTPUT_PREFIX}_time_bin_size_sweep_results\"\n",
      "    # out_path_filenname_str: str = f\"{out_path_basename_str}.csv\"\n",
      "\n",
      "    out_path_filenname_str: str = f\"{out_path_basename_str}.h5\"\n",
      "    out_path: Path = self.collected_outputs_path.resolve().joinpath(out_path_filenname_str).resolve()\n",
      "    print(f'\\out_path_str: \"{out_path_filenname_str}\"')\n",
      "    print(f'\\tout_path: \"{out_path}\"')\n",
      "    \n",
      "    # Ensure it has the 'lap_track' column\n",
      "    ## Compute the ground-truth information using the position information:\n",
      "    # adds columns: ['maze_id', 'is_LR_dir']\n",
      "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
      "    laps_obj: Laps = curr_active_pipeline.sess.laps\n",
      "    laps_obj.update_lap_dir_from_smoothed_velocity(pos_input=curr_active_pipeline.sess.position)\n",
      "    laps_obj.update_maze_id_if_needed(t_start=t_start, t_delta=t_delta, t_end=t_end)\n",
      "    laps_df = laps_obj.to_dataframe()\n",
      "    assert 'maze_id' in laps_df.columns, f\"laps_df is still missing the 'maze_id' column after calling `laps_obj.update_maze_id_if_needed(...)`. laps_df.columns: {print(list(laps_df.columns))}\"\n",
      "\n",
      "    # # BEGIN BLOCK ________________________________________________________________________________________________________ #\n",
      "\n",
      "    # # Uses: session_ctxt_key, all_param_sweep_options\n",
      "    # output_alt_directional_merged_decoders_result: Dict[Tuple, DirectionalPseudo2DDecodersResult] = {} # empty dict\n",
      "    # output_laps_decoding_accuracy_results_dict = {} # empty dict\n",
      "    # output_extracted_result_tuples = {}\n",
      "\n",
      "    # for a_sweep_dict in all_param_sweep_options:\n",
      "    #     a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
      "    #     print(f'a_sweep_dict: {a_sweep_dict}')\n",
      "    #     # Convert parameters to string because Parquet supports metadata as string\n",
      "    #     a_sweep_str_params = {key: str(value) for key, value in a_sweep_dict.items() if value is not None}\n",
      "        \n",
      "    #     output_alt_directional_merged_decoders_result[a_sweep_tuple] = _try_single_decode(curr_active_pipeline, alt_directional_merged_decoders_result, **a_sweep_dict)\n",
      "\n",
      "    #     laps_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_time_bin_marginals_df.copy()\n",
      "    #     laps_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_all_epoch_bins_marginals_df.copy()\n",
      "        \n",
      "    #     ## Ripples:\n",
      "    #     ripple_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_time_bin_marginals_df.copy()\n",
      "    #     ripple_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_all_epoch_bins_marginals_df.copy()\n",
      "\n",
      "    #     session_name = curr_session_name\n",
      "    #     curr_session_t_delta = t_delta\n",
      "        \n",
      "    #     for a_df, a_time_bin_column_name in zip((laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df), ('t_bin_center', 'lap_start_t', 't_bin_center', 'ripple_start_t')):\n",
      "    #         ## Add the session-specific columns:\n",
      "    #         a_df = add_session_df_columns(a_df, session_name, curr_session_t_delta, a_time_bin_column_name)\n",
      "\n",
      "    #     ## Build the output tuple:\n",
      "    #     output_extracted_result_tuples[a_sweep_tuple] = (laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df)\n",
      "        \n",
      "    #     # desired_laps_decoding_time_bin_size_str: str = a_sweep_str_params.get('desired_laps_decoding_time_bin_size', None)\n",
      "    #     laps_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_decoding_time_bin_size\n",
      "    #     # ripple_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_decoding_time_bin_size\n",
      "    #     actual_laps_decoding_time_bin_size_str: str = str(laps_decoding_time_bin_size)\n",
      "    #     if save_hdf and (actual_laps_decoding_time_bin_size_str is not None):\n",
      "    #         laps_time_bin_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_time_bin_marginals_df', format='table', data_columns=True)\n",
      "    #         laps_all_epoch_bins_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_all_epoch_bins_marginals_df', format='table', data_columns=True)\n",
      "\n",
      "    #     ## TODO: output ripple .h5 here if desired.\n",
      "            \n",
      "    #     # get the current lap object and determine the percentage correct:\n",
      "    #     result_laps_epochs_df: pd.DataFrame = _update_result_laps(a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple], laps_df=laps_df)\n",
      "    #     (is_decoded_track_correct, is_decoded_dir_correct, are_both_decoded_properties_correct), (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly) = _check_result_laps_epochs_df_performance(result_laps_epochs_df)\n",
      "    #     output_laps_decoding_accuracy_results_dict[laps_decoding_time_bin_size] = (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly)\n",
      "        \n",
      "\n",
      "    # BEGIN BLOCK 2 - modernizing from `_perform_compute_custom_epoch_decoding`  ________________________________________________________________________________________________________ #\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_lap_and_ripple_epochs_decoding_for_decoder, _perform_compute_custom_epoch_decoding, _compute_all_df_score_metrics\n",
      "\n",
      "    # Uses: session_ctxt_key, all_param_sweep_options\n",
      "    # output_alt_directional_merged_decoders_result: Dict[Tuple, Dict[types.DecoderName, DirectionalPseudo2DDecodersResult]] = {} # empty dict\n",
      "\n",
      "    output_alt_directional_merged_decoders_result: Dict[Tuple, DirectionalPseudo2DDecodersResult] = {} # empty dict\n",
      "    # output_alt_directional_merged_decoders_result: Dict[Tuple, Dict[types.DecoderName, DirectionalPseudo2DDecodersResult]] = {} # empty dict\n",
      "\n",
      "    # Tuple[DirectionalPseudo2DDecodersResult, Tuple[DecodedEpochsResultsDict, DecodedEpochsResultsDict]]\n",
      "    output_directional_decoders_epochs_decode_results_dict: Dict[Tuple, DecoderDecodedEpochsResult] = {} # `_decode_and_evaluate_epochs_using_directional_decoders`-style output\n",
      "\n",
      "    output_laps_decoding_accuracy_results_dict = {} # empty dict\n",
      "    output_extracted_result_tuples = {}\n",
      "\n",
      "\n",
      "    for a_sweep_dict in all_param_sweep_options:\n",
      "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
      "        print(f'a_sweep_dict: {a_sweep_dict}')\n",
      "\n",
      "        # for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
      "        #     decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_lap_and_ripple_epochs_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
      "\n",
      "\n",
      "        # output_alt_directional_merged_decoders_result[a_sweep_tuple] = _try_all_templates_decode(curr_active_pipeline, alt_directional_merged_decoders_result, **a_sweep_dict) # type: ignore\n",
      "\n",
      "        output_alt_directional_merged_decoders_result[a_sweep_tuple], (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _try_all_templates_decode(curr_active_pipeline, alt_directional_merged_decoders_result, **a_sweep_dict)\n",
      "        an_alt_dir_Pseudo2D_decoders_result = output_alt_directional_merged_decoders_result[a_sweep_tuple]\n",
      "\n",
      "\n",
      "        ## Decode epochs for all four decoders:\n",
      "        laps_time_bin_marginals_df: pd.DataFrame = an_alt_dir_Pseudo2D_decoders_result.laps_time_bin_marginals_df.copy()\n",
      "        laps_all_epoch_bins_marginals_df: pd.DataFrame = an_alt_dir_Pseudo2D_decoders_result.laps_all_epoch_bins_marginals_df.copy()\n",
      "        \n",
      "        ## Ripples:\n",
      "        ripple_time_bin_marginals_df: pd.DataFrame = an_alt_dir_Pseudo2D_decoders_result.ripple_time_bin_marginals_df.copy()\n",
      "        ripple_all_epoch_bins_marginals_df: pd.DataFrame = an_alt_dir_Pseudo2D_decoders_result.ripple_all_epoch_bins_marginals_df.copy()\n",
      "\n",
      "        session_name = curr_session_name\n",
      "        curr_session_t_delta = t_delta\n",
      "        \n",
      "        for a_df, a_time_bin_column_name in zip((laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df), ('t_bin_center', 'lap_start_t', 't_bin_center', 'ripple_start_t')):\n",
      "            ## Add the session-specific columns:\n",
      "            a_df = add_session_df_columns(a_df, session_name, curr_session_t_delta, a_time_bin_column_name)\n",
      "\n",
      "        ## Build the output tuple:\n",
      "        output_extracted_result_tuples[a_sweep_tuple] = (laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df) # output tuples are extracted here, where changes are needed I think\n",
      "        \n",
      "        ## Laps:\n",
      "        # desired_laps_decoding_time_bin_size_str: str = a_sweep_str_params.get('desired_laps_decoding_time_bin_size', None)\n",
      "        laps_decoding_time_bin_size: float = an_alt_dir_Pseudo2D_decoders_result.laps_decoding_time_bin_size\n",
      "        # ripple_decoding_time_bin_size: float = v.ripple_decoding_time_bin_size\n",
      "        actual_laps_decoding_time_bin_size_str: str = str(laps_decoding_time_bin_size)\n",
      "        if save_hdf and (actual_laps_decoding_time_bin_size_str is not None):\n",
      "            laps_time_bin_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_time_bin_marginals_df', format='table', data_columns=True)\n",
      "            laps_all_epoch_bins_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_all_epoch_bins_marginals_df', format='table', data_columns=True)\n",
      "\n",
      "        ## TODO: output ripple .h5 here if desired.\n",
      "        \n",
      "        # get the current lap object and determine the percentage correct:\n",
      "        if should_output_lap_decoding_performance_info:\n",
      "            result_laps_epochs_df: pd.DataFrame = _update_result_laps(a_result=an_alt_dir_Pseudo2D_decoders_result, laps_df=laps_df)\n",
      "            (is_decoded_track_correct, is_decoded_dir_correct, are_both_decoded_properties_correct), (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly) = _check_result_laps_epochs_df_performance(result_laps_epochs_df)\n",
      "            output_laps_decoding_accuracy_results_dict[laps_decoding_time_bin_size] = (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly)\n",
      "\n",
      "\n",
      "\n",
      "        # `_decode_and_evaluate_epochs_using_directional_decoders` post compute ______________________________________________ #\n",
      "\n",
      "        # decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict = _perform_compute_custom_epoch_decoding(curr_active_pipeline, directional_merged_decoders_result, track_templates) # Dict[str, Optional[DecodedFilterEpochsResult]]\n",
      "\n",
      "        ## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
      "        (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(an_alt_dir_Pseudo2D_decoders_result, track_templates,\n",
      "                                                                                                                                                                                            decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
      "                                                                                                                                                                                            spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
      "                                                                                                                                                                                            should_skip_radon_transform=True)\n",
      "        \n",
      "        laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
      "        decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple\n",
      "\n",
      "        ripple_decoding_time_bin_size = an_alt_dir_Pseudo2D_decoders_result.ripple_decoding_time_bin_size\n",
      "        laps_decoding_time_bin_size = an_alt_dir_Pseudo2D_decoders_result.laps_decoding_time_bin_size\n",
      "        pos_bin_size = an_alt_dir_Pseudo2D_decoders_result.all_directional_pf1D_Decoder.pos_bin_size\n",
      "\n",
      "        curr_sweep_directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = DecoderDecodedEpochsResult(is_global=True, **{'pos_bin_size': pos_bin_size, 'ripple_decoding_time_bin_size':ripple_decoding_time_bin_size, 'laps_decoding_time_bin_size':laps_decoding_time_bin_size,\n",
      "                                                                                                'decoder_laps_filter_epochs_decoder_result_dict':decoder_laps_filter_epochs_decoder_result_dict,\n",
      "            'decoder_ripple_filter_epochs_decoder_result_dict':decoder_ripple_filter_epochs_decoder_result_dict, 'decoder_laps_radon_transform_df_dict':decoder_laps_radon_transform_df_dict, 'decoder_ripple_radon_transform_df_dict':decoder_ripple_radon_transform_df_dict,\n",
      "            'decoder_laps_radon_transform_extras_dict': decoder_laps_radon_transform_extras_dict, 'decoder_ripple_radon_transform_extras_dict': decoder_ripple_radon_transform_extras_dict,\n",
      "            'laps_weighted_corr_merged_df': laps_weighted_corr_merged_df, 'ripple_weighted_corr_merged_df': ripple_weighted_corr_merged_df, 'decoder_laps_weighted_corr_df_dict': decoder_laps_weighted_corr_df_dict, 'decoder_ripple_weighted_corr_df_dict': decoder_ripple_weighted_corr_df_dict,\n",
      "            'laps_simple_pf_pearson_merged_df': laps_simple_pf_pearson_merged_df, 'ripple_simple_pf_pearson_merged_df': ripple_simple_pf_pearson_merged_df,\n",
      "            })\n",
      "        output_directional_decoders_epochs_decode_results_dict[a_sweep_tuple] = curr_sweep_directional_decoders_epochs_decode_result\n",
      "    \n",
      "    # END FOR a_sweep_dict in all_param_sweep_options\n",
      "\n",
      "\n",
      "\n",
      "    # END BLOCK __________________________________________________________________________________________________________ #\n",
      "\n",
      "    if should_output_lap_decoding_performance_info:\n",
      "        ## Output the performance:\n",
      "        output_laps_decoding_accuracy_results_df: pd.DataFrame = pd.DataFrame(output_laps_decoding_accuracy_results_dict.values(), index=output_laps_decoding_accuracy_results_dict.keys(), \n",
      "                        columns=['percent_laps_track_identity_estimated_correctly',\n",
      "                                'percent_laps_direction_estimated_correctly',\n",
      "                                'percent_laps_estimated_correctly'])\n",
      "        output_laps_decoding_accuracy_results_df.index.name = 'laps_decoding_time_bin_size'\n",
      "        ## Save out the laps peformance result\n",
      "        if save_hdf:\n",
      "            output_laps_decoding_accuracy_results_df.to_hdf(out_path, key=f'{session_ctxt_key}/laps_decoding_accuracy_results', format='table', data_columns=True)\n",
      "    else:\n",
      "        output_laps_decoding_accuracy_results_df = None\n",
      "\n",
      "    \n",
      "    ## Call the subfunction to process the time_bin_size swept result and produce combined output dataframes:\n",
      "    combined_multi_timebin_outputs_tuple = _subfn_process_time_bin_swept_results(output_extracted_result_tuples, active_context=curr_active_pipeline.get_session_context())\n",
      "    # Unpacking:    \n",
      "    # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
      "\n",
      "    # add to output dict\n",
      "    # across_session_results_extended_dict['compute_and_export_marginals_dfs_completion_function'] = _out\n",
      "    across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'] = [out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple]\n",
      "\n",
      "    if return_full_decoding_results:\n",
      "        # output_alt_directional_merged_decoders_result: Dict[Tuple, DirectionalPseudo2DDecodersResult]\n",
      "        across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'].append(output_alt_directional_merged_decoders_result) # append the real full results\n",
      "        across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'].append(output_directional_decoders_epochs_decode_results_dict) # append the real full results\n",
      "\n",
      "        ## Save out the laps peformance result\n",
      "        if save_hdf:\n",
      "            ## figure out how to save the actual dict out to HDF\n",
      "            print(f'`return_full_decoding_results` is True and `save_hdf` is True, but I do not yet know how to propperly output the `output_alt_directional_merged_decoders_result`')\n",
      "            # for a_sweep_dict in all_param_sweep_options:\n",
      "            #     a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
      "            #     print(f'a_sweep_dict: {a_sweep_dict}')\n",
      "            #     # Convert parameters to string because Parquet supports metadata as string\n",
      "            #     a_sweep_str_params = {key: str(value) for key, value in a_sweep_dict.items() if value is not None}\n",
      "            #     a_directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = output_alt_directional_merged_decoders_result[a_sweep_tuple]\n",
      "\n",
      "            #     # 2024-04-03 `DirectionalPseudo2DDecodersResult` is actually missing a `to_hdf` implementation, so no dice.\n",
      "\n",
      "        #     output_alt_directional_merged_decoders_result.to_hdf(out_path, key=f'{session_ctxt_key}/alt_directional_merged_decoders_result')\n",
      "        across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'] = tuple(across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'])\n",
      "\n",
      "\n",
      "    ## UNPACKING\n",
      "    # # with `return_full_decoding_results == False`\n",
      "    # out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
      "\n",
      "    # # with `return_full_decoding_results == True`\n",
      "    # out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result = across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
      "\n",
      "\n",
      "    # can unpack like:\n",
      "    (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
      "\n",
      "    print(f'>>\\t done with {curr_session_context}')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function)\n",
      "# END `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "@function_attributes(short_name=None, tags=['CSVs', 'export', 'across-sessions', 'batch', 'ripple_all_scores_merged_df'], input_requires=['DirectionalDecodersEpochsEvaluations'], output_provides=[], uses=[], used_by=[], creation_date='2024-04-27 21:20', related_items=[])\n",
      "def compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
      "    \"\"\"\n",
      "    Aims to export the results of the global 'directional_decoders_evaluate_epochs' calculation\n",
      "\n",
      "    Exports: 'ripple_all_scores_merged_df'\n",
      "\n",
      "    Uses result computed by `_decode_and_evaluate_epochs_using_directional_decoders`\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function(global_data_root_parent_path: \"{global_data_root_parent_path}\", curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)') # ,across_session_results_extended_dict: {across_session_results_extended_dict}\n",
      "    from pyphocorehelpers.Filesystem.path_helpers import file_uri_from_path\n",
      "    from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
      "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _workaround_validate_has_directional_decoded_epochs_heuristic_scoring\n",
      "\n",
      "    from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
      "\n",
      "    assert self.collected_outputs_path.exists()\n",
      "    active_context = curr_active_pipeline.get_session_context()\n",
      "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
      "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
      "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
      "\n",
      "    ## Doesn't force recompute! Assumes that the DirectionalDecodersEpochsEvaluations result is new\n",
      "    # curr_active_pipeline.reload_default_computation_functions()\n",
      "    # batch_extended_computations(curr_active_pipeline, include_includelist=['directional_decoders_evaluate_epochs'], include_global_functions=True, fail_on_exception=True, force_recompute=True)\n",
      "    \n",
      "    ## Extract Data:\n",
      "    directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'] # DirectionalLapsResult\n",
      "    rank_order_results = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
      "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
      "    included_qclu_values: float = rank_order_results.included_qclu_values\n",
      "    track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only # TrackTemplates\n",
      "\n",
      "\n",
      "    directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
      "    pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
      "    ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
      "    laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
      "    # decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
      "    # decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
      "\n",
      "    # Radon Transforms:\n",
      "    decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
      "    decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
      "    decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
      "    decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
      "\n",
      "    # Weighted correlations:\n",
      "    laps_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
      "    ripple_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
      "    decoder_laps_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
      "    decoder_ripple_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
      "\n",
      "    # Pearson's correlations:\n",
      "    laps_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
      "    ripple_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
      "\n",
      "    ## FILTERING FOR GOOD ROWS:\n",
      "    \n",
      "\n",
      "    ## INPUTS: decoder_ripple_filter_epochs_decoder_result_dict\n",
      "\n",
      "    # 2024-03-04 - Filter out the epochs based on the criteria:\n",
      "    _, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
      "    filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
      "    filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
      "\n",
      "    ## filter the epochs by something and only show those:\n",
      "    # INPUTS: filtered_epochs_df\n",
      "    # filtered_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(active_epochs_df[['start', 'stop']].to_numpy())\n",
      "\n",
      "    ## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
      "    decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
      "\n",
      "    ## run 'directional_decoders_epoch_heuristic_scoring',\n",
      "    directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
      "\n",
      "    #  2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
      "    if (not _workaround_validate_has_directional_decoded_epochs_heuristic_scoring(curr_active_pipeline)):\n",
      "        print(f'\\tmissing heuristic columns. Recomputing:')\n",
      "        directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
      "        print(f'\\tdone recomputing heuristics.')\n",
      "\n",
      "    print(f'\\tComputation complete. Exporting .CSVs...')\n",
      "\n",
      "    ## Export CSVs:\n",
      "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
      "    _output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=self.collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
      "                                                                              user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
      "                                                                              valid_epochs_selections={'ripple': filtered_valid_epoch_times})\n",
      "    \n",
      "\n",
      "    print(f'\\t\\tsuccessfully exported directional_decoders_epochs_decode_result to {self.collected_outputs_path}!')\n",
      "    _output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _output_csv_paths.items()])\n",
      "    # print(f'\\t\\t\\tCSV Paths: {_output_csv_paths}\\n')\n",
      "    print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')\n",
      "\n",
      "    # add to output dict\n",
      "    # across_session_results_extended_dict['compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function'] = _out\n",
      "\n",
      "    print(f'>>\\t done with {curr_session_context}')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function)\n",
      "# END `compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "def reload_exported_kdiba_session_position_info_mat_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
      "    \"\"\" \n",
      "    from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import reload_exported_kdiba_session_position_info_mat_completion_function\n",
      "    \n",
      "    Results can be extracted from batch output by \n",
      "    \n",
      "    # Extracts the callback results 'determine_session_t_delta_completion_function':\n",
      "    extracted_callback_fn_results = {a_sess_ctxt:a_result.across_session_results.get('determine_session_t_delta_completion_function', {}) for a_sess_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
      "    from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
      "    from neuropy.core.session.Formats.SessionSpecifications import SessionConfig\n",
      "\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'reload_exported_kdiba_session_position_info_mat_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
      "    active_data_mode_name: str = curr_active_pipeline.session_data_type\n",
      "\n",
      "    # known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()\n",
      "    active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
      "\n",
      "    active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
      "    # active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
      "\n",
      "    a_session = deepcopy(curr_active_pipeline.sess)\n",
      "    # sess_config: SessionConfig = SessionConfig(**deepcopy(session.config.to_dict()))\n",
      "    sess_config: SessionConfig = SessionConfig(**deepcopy(a_session.config.__getstate__()))\n",
      "    a_session.config = sess_config\n",
      "\n",
      "    # a_session = active_data_mode_registered_class._default_kdiba_exported_load_position_info_mat(basepath=curr_active_pipeline.sess.basepath, session_name=curr_active_pipeline.session_name, session=deepcopy(curr_active_pipeline.sess))\n",
      "    a_session = active_data_mode_registered_class._default_kdiba_exported_load_position_info_mat(basepath=curr_active_pipeline.sess.basepath, session_name=curr_active_pipeline.session_name, session=a_session)\n",
      "    # a_session\n",
      "\n",
      "    curr_active_pipeline.stage.sess = a_session ## apply the session\n",
      "    # curr_active_pipeline.sess.config = a_session.config # apply the config only...\n",
      "\n",
      "    loaded_track_limits = a_session.config.loaded_track_limits\n",
      "    \n",
      "    a_config_dict = a_session.config.to_dict()\n",
      "\n",
      "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
      "    print(f'\\t{curr_session_basedir}:\\tloaded_track_limits: {loaded_track_limits}, a_config_dict: {a_config_dict}')  # , t_end: {t_end}\n",
      "    \n",
      "    callback_outputs = {\n",
      "     'loaded_track_limits': loaded_track_limits, 'a_config_dict':a_config_dict, #'t_end': t_end   \n",
      "    }\n",
      "    across_session_results_extended_dict['position_info_mat_reload_completion_function'] = callback_outputs\n",
      "    \n",
      "    # print(f'>>\\t done with {curr_session_context}')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(reload_exported_kdiba_session_position_info_mat_completion_function)\n",
      "# END `reload_exported_kdiba_session_position_info_mat_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "def export_session_h5_file_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
      "    \"\"\"  Export the pipeline's HDF5 as 'pipeline_results.h5'\n",
      "    from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import reload_exported_kdiba_session_position_info_mat_completion_function\n",
      "    \n",
      "    Results can be extracted from batch output by \n",
      "    \n",
      "    # Extracts the callback results 'determine_session_t_delta_completion_function':\n",
      "    extracted_callback_fn_results = {a_sess_ctxt:a_result.across_session_results.get('determine_session_t_delta_completion_function', {}) for a_sess_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    import sys\n",
      "    from datetime import timedelta, datetime\n",
      "    from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata\n",
      "    from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
      "\n",
      "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
      "    print(f'export_session_h5_file_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
      "    \n",
      "\n",
      "    hdf5_output_path: Path = curr_active_pipeline.get_output_path().joinpath('pipeline_results.h5').resolve()\n",
      "    print(f'pipeline hdf5_output_path: {hdf5_output_path}')\n",
      "    err = None\n",
      "    # Only get files newer than date\n",
      "    skip_overwriting_files_newer_than_specified:bool = False\n",
      "\n",
      "    was_write_good: bool = False\n",
      "    newest_file_to_overwrite_date = datetime.now() - timedelta(days=1) # don't overwrite any files more recent than 1 day ago\n",
      "    can_skip_if_allowed: bool = (hdf5_output_path.exists() and (FilesystemMetadata.get_last_modified_time(hdf5_output_path)<=newest_file_to_overwrite_date))\n",
      "    if (not skip_overwriting_files_newer_than_specified) or (not can_skip_if_allowed):\n",
      "        # if skipping is disabled OR skipping is enabled but it's not valid to skip, overwrite.\n",
      "        # file is folder than the date to overwrite, so overwrite it\n",
      "        print(f'OVERWRITING (or writing) the file {hdf5_output_path}!')\n",
      "        # with ExceptionPrintingContext(suppress=False):\n",
      "        try:\n",
      "            curr_active_pipeline.export_pipeline_to_h5()\n",
      "            was_write_good = True\n",
      "        except BaseException as e:\n",
      "            exception_info = sys.exc_info()\n",
      "            err = CapturedException(e, exception_info)\n",
      "            print(f\"ERROR: encountered exception {err} while trying to build the session HDF output for {curr_session_context}\")\n",
      "            hdf5_output_path = None # set to None because it failed.\n",
      "            if self.fail_on_exception:\n",
      "                raise err.exc\n",
      "    else:\n",
      "        print(f'WARNING: file {hdf5_output_path} is newer than the allowed overwrite date, so it will be skipped.')\n",
      "        print(f'\\t\\tnewest_file_to_overwrite_date: {newest_file_to_overwrite_date}\\t can_skip_if_allowed: {can_skip_if_allowed}\\n')\n",
      "        # return (hdf5_output_path, None)\n",
      "\n",
      "\n",
      "    callback_outputs = {\n",
      "     'hdf5_output_path': hdf5_output_path, 'e':err, #'t_end': t_end   \n",
      "    }\n",
      "    across_session_results_extended_dict['export_session_h5_file_completion_function'] = callback_outputs\n",
      "    \n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
      "\n",
      "    return across_session_results_extended_dict\n",
      "\n",
      "custom_user_completion_functions.append(export_session_h5_file_completion_function)\n",
      "# END `export_session_h5_file_completion_function` USER COMPLETION FUNCTION  _______________________________________________________________________________________ #\n",
      "\n",
      "\n",
      "# ==================================================================================================================== #\n",
      "# END USER COMPLETION FUNCTIONS                                                                                        #\n",
      "# ==================================================================================================================== #\n",
      "\n",
      "generate_figures_script_paths: ['/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-08_14-26-15/figures_kdiba_gor01_one_2006-6-08_14-26-15.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-09_1-22-43/figures_kdiba_gor01_one_2006-6-09_1-22-43.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-12_15-55-31/figures_kdiba_gor01_one_2006-6-12_15-55-31.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-07_16-40-19/figures_kdiba_gor01_two_2006-6-07_16-40-19.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-08_21-16-25/figures_kdiba_gor01_two_2006-6-08_21-16-25.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-09_22-24-40/figures_kdiba_gor01_two_2006-6-09_22-24-40.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-12_16-53-46/figures_kdiba_gor01_two_2006-6-12_16-53-46.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_one_2006-4-09_17-29-30/figures_kdiba_vvp01_one_2006-4-09_17-29-30.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_one_2006-4-10_12-25-50/figures_kdiba_vvp01_one_2006-4-10_12-25-50.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_two_2006-4-09_16-40-54/figures_kdiba_vvp01_two_2006-4-09_16-40-54.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_two_2006-4-10_12-58-3/figures_kdiba_vvp01_two_2006-4-10_12-58-3.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_11-02_17-46-44/figures_kdiba_pin01_one_11-02_17-46-44.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_11-02_19-28-0/figures_kdiba_pin01_one_11-02_19-28-0.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_11-03_12-3-25/figures_kdiba_pin01_one_11-03_12-3-25.py', '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_fet11-01_12-58-54/figures_kdiba_pin01_one_fet11-01_12-58-54.py']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f685f12106434e0fba889111c6bea746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Box(children=(Label(value='run_kdiba_gor01_one_2006-6-08_14-26-15.py', layout=Layout(width='aut"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import BatchScriptsCollection, generate_batch_single_session_scripts, display_generated_scripts_ipywidget\n",
    "\n",
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # Other file\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), \n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # \n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # Other tab\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'), \n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), #\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), #\n",
    "]\n",
    "\n",
    "## Setup Functions:\n",
    "force_recompute_override_computations_includelist = []\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_evaluate_epochs',]\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps','merged_directional_placefields','directional_decoders_evaluate_epochs','directional_decoders_epoch_heuristic_scoring']\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_evaluate_epochs','directional_decoders_epoch_heuristic_scoring']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps','merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['long_short_decoding_analyses', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding',]\n",
    "# force_recompute_override_computations_includelist = ['wcorr_shuffle_analysis','trial_by_trial_metrics']\n",
    "\n",
    "extra_extended_computations_include_includelist = [\n",
    "\t# 'wcorr_shuffle_analysis'\n",
    "]\n",
    "\n",
    "custom_phase_extended_computations_include_includelist = None\n",
    "# custom_phase_extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "# \t# 'pf_dt_sequential_surprise',\n",
    "# \t'extended_stats',\n",
    "# \t'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', \n",
    "# \t# 'ratemap_peaks_prominence2d',\n",
    "# \t'long_short_inst_spike_rate_groups',\n",
    "# \t# 'long_short_endcap_analysis',\n",
    "# \t# 'spike_burst_detection',\n",
    "# \t'split_to_directional_laps',\n",
    "# \t'merged_directional_placefields',\n",
    "# \t'rank_order_shuffle_analysis',\n",
    "# \t# 'directional_train_test_split',\n",
    "# \t# 'directional_decoders_decode_continuous',\n",
    "# \t'directional_decoders_evaluate_epochs',\n",
    "# \t# 'directional_decoders_epoch_heuristic_scoring',\n",
    "#     'wcorr_shuffle_analysis',\n",
    "#     'trial_by_trial_metrics',\n",
    "# ]\n",
    "\n",
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'W:/Data'), Path(r'/home/halechr/FastData'), Path(r'/Volumes/MoverNew/data')] # Path(r'/home/halechr/cloud/turbo/Data'), , Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/turbo/Data'), \n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "\n",
    "## Different run configurations:\n",
    "\n",
    "# active_phase = ProcessingScriptPhases.clean_run\n",
    "active_phase = ProcessingScriptPhases.continued_run\n",
    "# active_phase = ProcessingScriptPhases.final_run\n",
    "# active_phase = ProcessingScriptPhases.figure_run\n",
    "\n",
    "custom_user_completion_functions_dict = active_phase.get_custom_user_completion_functions_dict(extra_run_functions=phase_any_run_custom_user_completion_functions_dict)\n",
    "print(f'custom_user_completion_functions_dict: {custom_user_completion_functions_dict}')\n",
    "custom_user_completion_function_template_code, custom_user_completion_functions_dict = MAIN_get_template_string(BATCH_DATE_TO_USE=BATCH_DATE_TO_USE, collected_outputs_path=collected_outputs_path, override_custom_user_completion_functions_dict=custom_user_completion_functions_dict)\n",
    "print(f'custom_user_completion_function_template_code: {custom_user_completion_function_template_code}')\n",
    "active_phase_dict = active_phase.get_run_configuration(custom_user_completion_function_template_code=custom_user_completion_function_template_code, extra_extended_computations_include_includelist=extra_extended_computations_include_includelist)\n",
    "\n",
    "## Completely replace with custom functions:\n",
    "if custom_phase_extended_computations_include_includelist is not None:\n",
    "\tprint(f'WARNING: custom_phase_extended_computations_include_includelist: {custom_phase_extended_computations_include_includelist} is provided so only these extended_computation_fns will be used (overwritting the phase defaults!).')\n",
    "\tactive_phase_dict['extended_computations_include_includelist'] = custom_phase_extended_computations_include_includelist\n",
    "\n",
    "\n",
    "## Build Slurm Scripts:\n",
    "session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "batch_scripts_collection: BatchScriptsCollection = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts,\n",
    "                                                                                        output_directory=scripts_output_path, use_separate_run_directories=True,\n",
    "                                                                                        # should_freeze_pipeline_updates=False, \n",
    "                                                                                        # create_slurm_scripts=True, should_create_vscode_workspace=True,\n",
    "                                                                                        # extended_computations_include_includelist=extended_computations_include_includelist,\n",
    "                                                                                        force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, # ['split_to_directional_laps', 'rank_order_shuffle_analysis'],\n",
    "                                                                                        # should_perform_figure_generation_to_file=False,\n",
    "                                                                                        # batch_session_completion_handler_kwargs=dict(enable_hdf5_output=False),\n",
    "                                                                                        # custom_user_completion_functions=custom_user_completion_functions,\n",
    "                                                                                        # custom_user_completion_function_template_code=custom_user_completion_function_template_code,\n",
    "                                                                                        # should_force_reload_all=True,\n",
    "                                                                                        # should_force_reload_all=False,\n",
    "                                                                                        **active_phase_dict\n",
    "                                                                                    )\n",
    "\n",
    "\n",
    "# batch_scripts_collection.included_session_contexts, output_python_scripts, output_slurm_scripts\n",
    "\n",
    "output_python_scripts = batch_scripts_collection.output_python_scripts\n",
    "output_slurm_scripts = batch_scripts_collection.output_slurm_scripts\n",
    "vscode_workspace_path = batch_scripts_collection.vscode_workspace_path\n",
    "if vscode_workspace_path is not None:\n",
    "    display(fullwidth_path_widget(vscode_workspace_path, file_name_label='vscode_workspace_path:'))\n",
    "\n",
    "computation_script_paths = [x[0] for x in output_python_scripts]\n",
    "generate_figures_script_paths = [x[1] for x in output_python_scripts]\n",
    "print(f'generate_figures_script_paths: {generate_figures_script_paths}')\n",
    "_out_widget = display_generated_scripts_ipywidget(batch_scripts_collection.included_session_contexts, output_python_scripts)\n",
    "display(_out_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b61a8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(active_phase_dict['extended_computations_include_includelist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e718bbde33744cebbf707f6a98de7d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value=\"sbatch '/home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 2\n",
    "# List of your script paths\n",
    "# if active_phase.value == ProcessingScriptPhases.figure_run:\n",
    "if active_phase.is_figure_phase:\n",
    "    print(f'fig mode!')\n",
    "    script_paths = generate_figures_script_paths\n",
    "else:\n",
    "\tscript_paths = computation_script_paths\n",
    "\n",
    "if (platform.system() == 'Windows'):\n",
    "    powershell_script_path = build_windows_powershell_run_script(script_paths, max_concurrent_jobs=max_parallel_executions, script_name='run_scripts')\n",
    "    # print(f'powershell_script_path: {powershell_script_path}')\n",
    "    display(fullwidth_path_widget(powershell_script_path, file_name_label='powershell_script_path:'))\n",
    "\n",
    "\n",
    "    if active_phase_dict['should_perform_figure_generation_to_file']:\n",
    "        powershell_figures_script_path = build_windows_powershell_run_script(generate_figures_script_paths, max_concurrent_jobs=1, script_name='run_figure_scripts')\n",
    "        display(fullwidth_path_widget(powershell_figures_script_path, file_name_label='powershell_figures_script_path:'))\n",
    "\n",
    "\n",
    "if (platform.system() != 'Windows'):\n",
    "    ## Linux Only: Slurm Scripts\n",
    "    # sbatch_start_slurm_scripts: str = \"\\n\".join([f\"sbatch '{slurm_script}'\" for slurm_script in output_slurm_scripts])\n",
    "    sbatch_run_start_slurm_scripts: str = \"\\n\".join([f\"sbatch '{slurm_script}'\" for slurm_script in output_slurm_scripts['run']])\n",
    "    sbatch_figs_start_slurm_scripts: str = \"\\n\".join([f\"sbatch '{slurm_script}'\" for slurm_script in output_slurm_scripts['figs']])\n",
    "\n",
    "    if active_phase.is_figure_phase:\n",
    "        # print(sbatch_figs_start_slurm_scripts)\n",
    "        initial_code = sbatch_figs_start_slurm_scripts\n",
    "    else:\n",
    "        # print(sbatch_run_start_slurm_scripts)\n",
    "        initial_code = sbatch_run_start_slurm_scripts\n",
    "\n",
    "    # Create and display the code block widget\n",
    "    slurm_code_block = code_block_widget(initial_code, label=\"Python Code:\")\n",
    "\n",
    "\n",
    "\n",
    "# Batch 1\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_pin01_one_11-02_17-46-44/run_kdiba_pin01_one_11-02_17-46-44.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_pin01_one_11-02_19-28-0/run_kdiba_pin01_one_11-02_19-28-0.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_pin01_one_11-03_12-3-25/run_kdiba_pin01_one_11-03_12-3-25.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_pin01_one_fet11-01_12-58-54/run_kdiba_pin01_one_fet11-01_12-58-54.sh'\n",
    "\n",
    "# Batch 2:\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_vvp01_one_2006-4-09_17-29-30/run_kdiba_vvp01_one_2006-4-09_17-29-30.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_vvp01_one_2006-4-10_12-25-50/run_kdiba_vvp01_one_2006-4-10_12-25-50.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_vvp01_two_2006-4-09_16-40-54/run_kdiba_vvp01_two_2006-4-09_16-40-54.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_vvp01_two_2006-4-10_12-58-3/run_kdiba_vvp01_two_2006-4-10_12-58-3.sh'\n",
    "\n",
    "# Batch 3\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-08_14-26-15/run_kdiba_gor01_one_2006-6-08_14-26-15.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-09_1-22-43/run_kdiba_gor01_one_2006-6-09_1-22-43.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-07_16-40-19/run_kdiba_gor01_two_2006-6-07_16-40-19.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-08_21-16-25/run_kdiba_gor01_two_2006-6-08_21-16-25.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-09_22-24-40/run_kdiba_gor01_two_2006-6-09_22-24-40.sh'\n",
    "# sbatch '/nfs/turbo/umms-kdiba/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-12_16-53-46/run_kdiba_gor01_two_2006-6-12_16-53-46.sh'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f98344",
   "metadata": {},
   "source": [
    "## Execute the generated scripts in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2fb7d6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5172c59b609446ea8f6fe1d682548648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Running:', max=15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-09_1-22-43/run_kdiba_gor01_one_2006-6-09_1-22-43.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-08_14-26-15/run_kdiba_gor01_one_2006-6-08_14-26-15.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_one_2006-6-12_15-55-31/run_kdiba_gor01_one_2006-6-12_15-55-31.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-08_21-16-25/run_kdiba_gor01_two_2006-6-08_21-16-25.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-07_16-40-19/run_kdiba_gor01_two_2006-6-07_16-40-19.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-12_16-53-46/run_kdiba_gor01_two_2006-6-12_16-53-46.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_one_2006-4-09_17-29-30/run_kdiba_vvp01_one_2006-4-09_17-29-30.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_one_2006-4-10_12-25-50/run_kdiba_vvp01_one_2006-4-10_12-25-50.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_two_2006-4-09_16-40-54/run_kdiba_vvp01_two_2006-4-09_16-40-54.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_gor01_two_2006-6-09_22-24-40/run_kdiba_gor01_two_2006-6-09_22-24-40.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_11-02_17-46-44/run_kdiba_pin01_one_11-02_17-46-44.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_vvp01_two_2006-4-10_12-58-3/run_kdiba_vvp01_two_2006-4-10_12-58-3.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_11-02_19-28-0/run_kdiba_pin01_one_11-02_19-28-0.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_11-03_12-3-25/run_kdiba_pin01_one_11-03_12-3-25.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n",
      "Error in /home/halechr/cloud/turbo/Data/Output/gen_scripts/run_kdiba_pin01_one_fet11-01_12-58-54/run_kdiba_pin01_one_fet11-01_12-58-54.py: [neptune] [warning] /home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NeptuneAiHelpers.py:78: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "## recieves: max_parallel_executions, script_paths\n",
    "\"\"\"\n",
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 5\n",
    "# List of your script paths\n",
    "script_paths = output_python_scripts\n",
    "\"\"\"\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    python_executable = 'python'\n",
    "    # python_executable = '/home/halechr/Library/VSCode/green/.venv_green/bin/python'\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run([python_executable, script_path], capture_output=True, text=True)\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except Exception as e:\n",
    "        return script_path, None, str(e)\n",
    "\n",
    "# Create a progress bar\n",
    "progress_bar = widgets.IntProgress(value=0, min=0, max=len(script_paths), description='Running:', bar_style='info')\n",
    "display(progress_bar)\n",
    "\n",
    "# Run scripts in parallel with a limit on the number of parallel instances\n",
    "with ProcessPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "    futures = {executor.submit(run_script, path): path for path in script_paths}\n",
    "    for future in as_completed(futures):\n",
    "        script, stdout, stderr = future.result()\n",
    "        progress_bar.value += 1  # Update the progress bar\n",
    "        if stderr:\n",
    "            print(f\"Error in {script}: {stderr}\")\n",
    "        else:\n",
    "            print(f\"Completed {script}\")\n",
    "\n",
    "# Progress bar will automatically update as scripts complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f583772",
   "metadata": {},
   "source": [
    "## Resume normal stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: file_size < 0.01 for /home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>modification_time</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>file_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/gor01/one...</td>\n",
       "      <td>2024-06-05 13:24:35</td>\n",
       "      <td>2024-06-05 13:24:35</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/gor01/one...</td>\n",
       "      <td>2024-06-05 13:28:52</td>\n",
       "      <td>2024-06-05 13:28:52</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/gor01/one...</td>\n",
       "      <td>2024-06-05 13:12:43</td>\n",
       "      <td>2024-06-05 13:12:43</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/gor01/two...</td>\n",
       "      <td>2024-06-05 13:32:58</td>\n",
       "      <td>2024-06-05 13:32:58</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/gor01/two...</td>\n",
       "      <td>2024-06-05 13:27:18</td>\n",
       "      <td>2024-06-05 13:27:18</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/gor01/two...</td>\n",
       "      <td>2024-06-05 13:26:53</td>\n",
       "      <td>2024-06-05 13:26:53</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/vvp01/one...</td>\n",
       "      <td>2024-06-05 13:14:08</td>\n",
       "      <td>2024-06-05 13:14:08</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/vvp01/one...</td>\n",
       "      <td>2024-06-05 13:14:08</td>\n",
       "      <td>2024-06-05 13:14:08</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/vvp01/two...</td>\n",
       "      <td>2024-06-05 13:13:42</td>\n",
       "      <td>2024-06-05 13:13:42</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/vvp01/two...</td>\n",
       "      <td>2024-06-05 13:14:38</td>\n",
       "      <td>2024-06-05 13:14:38</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/pin01/one...</td>\n",
       "      <td>2024-06-05 13:13:16</td>\n",
       "      <td>2024-06-05 13:13:16</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/pin01/one...</td>\n",
       "      <td>2024-06-05 13:13:12</td>\n",
       "      <td>2024-06-05 13:13:12</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/halechr/cloud/turbo/Data/KDIBA/pin01/one...</td>\n",
       "      <td>2024-06-05 13:16:19</td>\n",
       "      <td>2024-06-05 13:16:19</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path   modification_time  \\\n",
       "0   /home/halechr/cloud/turbo/Data/KDIBA/gor01/one... 2024-06-05 13:24:35   \n",
       "1   /home/halechr/cloud/turbo/Data/KDIBA/gor01/one... 2024-06-05 13:28:52   \n",
       "2   /home/halechr/cloud/turbo/Data/KDIBA/gor01/one... 2024-06-05 13:12:43   \n",
       "3   /home/halechr/cloud/turbo/Data/KDIBA/gor01/two... 2024-06-05 13:32:58   \n",
       "4   /home/halechr/cloud/turbo/Data/KDIBA/gor01/two... 2024-06-05 13:27:18   \n",
       "5   /home/halechr/cloud/turbo/Data/KDIBA/gor01/two... 2024-06-05 13:26:53   \n",
       "6   /home/halechr/cloud/turbo/Data/KDIBA/vvp01/one... 2024-06-05 13:14:08   \n",
       "7   /home/halechr/cloud/turbo/Data/KDIBA/vvp01/one... 2024-06-05 13:14:08   \n",
       "8   /home/halechr/cloud/turbo/Data/KDIBA/vvp01/two... 2024-06-05 13:13:42   \n",
       "9   /home/halechr/cloud/turbo/Data/KDIBA/vvp01/two... 2024-06-05 13:14:38   \n",
       "10  /home/halechr/cloud/turbo/Data/KDIBA/pin01/one... 2024-06-05 13:13:16   \n",
       "11  /home/halechr/cloud/turbo/Data/KDIBA/pin01/one... 2024-06-05 13:13:12   \n",
       "12  /home/halechr/cloud/turbo/Data/KDIBA/pin01/one... 2024-06-05 13:16:19   \n",
       "\n",
       "         creation_time  file_size  \n",
       "0  2024-06-05 13:24:35       1.17  \n",
       "1  2024-06-05 13:28:52       0.62  \n",
       "2  2024-06-05 13:12:43       0.25  \n",
       "3  2024-06-05 13:32:58       1.11  \n",
       "4  2024-06-05 13:27:18       0.45  \n",
       "5  2024-06-05 13:26:53       0.83  \n",
       "6  2024-06-05 13:14:08       0.22  \n",
       "7  2024-06-05 13:14:08       0.21  \n",
       "8  2024-06-05 13:13:42       0.25  \n",
       "9  2024-06-05 13:14:38       0.19  \n",
       "10 2024-06-05 13:13:16       0.19  \n",
       "11 2024-06-05 13:13:12       0.13  \n",
       "12 2024-06-05 13:16:19       0.28  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'/Volumes/MoverNew/data')] # , Path(r'/home/halechr/FastData'), Path(r'/home/halechr/turbo/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = UserAnnotationsManager.get_hardcoded_good_sessions()\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "\n",
    "# Output Paths:\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "check_output_h5_files(included_h5_paths)\n",
    "# from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts\n",
    "\n",
    "# ## Build Slurm Scripts:\n",
    "# session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "# included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts, output_directory=Path('output/generated_slurm_scripts/').resolve(), use_separate_run_directories=True, should_perform_figure_generation_to_file=False)\n",
    "# display(output_python_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15077fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: file_size < 0.01 for /home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n",
      "h5_filelist_output_file_path: /home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-06-06_Lab_all_sessions_h5_filelist.txt\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "/home/halechr/cloud/turbo/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n",
      "saving out to \"/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-06-06_Lab_all_sessions_h5_filelist.txt\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-06-06_Lab_all_sessions_h5_filelist.txt')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import save_filelist_to_text_file\n",
    "\n",
    "## INPUTS: good_session_concrete_folders, target_dir, BATCH_DATE_TO_USE\n",
    "\n",
    "# target_dir: Path = Path(global_data_root_parent_path)\n",
    "target_dir: Path = collected_outputs_path\n",
    "\n",
    "included_h5_paths = [Path(get_file_str_if_file_exists(v.pipeline_results_h5)).resolve() for v in good_session_concrete_folders]\n",
    "check_output_h5_files(included_h5_paths)\n",
    "included_h5_paths\n",
    "\n",
    "def _across_session_h5_output_basename_fn(session_context: Optional[IdentifyingContext], session_descr: Optional[str], basename: str, *args, separator_char: str = \"_\"):\n",
    "    \"\"\" Captures `BATCH_DATE_TO_USE` \"\"\"\n",
    "    # a_session_folder.context\n",
    "    if session_context is not None:\n",
    "        session_descr = session_context.session_name # '2006-6-07_16-40-19'\n",
    "    _filename_list = [BATCH_DATE_TO_USE, session_descr, basename]\n",
    "    if len(args) > 0:\n",
    "        _filename_list.extend([str(a_part) for a_part in args if a_part is not None])\n",
    "    return separator_char.join(_filename_list)\n",
    "\n",
    "copy_h5_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, target_dir=collected_outputs_path, backup_mode=BackupMethods.CommonTargetDirectory, rename_backup_basename_fn=_across_session_h5_output_basename_fn, only_include_file_types=['h5']) # , rename_backup_suffix=BATCH_DATE_TO_USE\n",
    "copy_h5_dict\n",
    "\n",
    "\n",
    "## INPUTS: target_dir, BATCH_DATE_TO_USE\n",
    "h5_filelist_output_filename=f'{BATCH_DATE_TO_USE}_all_sessions_h5_filelist.txt'\n",
    "h5_filelist_output_file_path = Path(target_dir).joinpath(h5_filelist_output_filename).resolve() # Use Default\n",
    "print(f'h5_filelist_output_file_path: {h5_filelist_output_file_path}')\n",
    "_out_string, filelist_path = save_filelist_to_text_file(included_h5_paths, filelist_path=h5_filelist_output_file_path, debug_print=True) # r\"W:\\Data\\all_sessions_h5_filelist_2024-03-28_Apogee.txt\"\n",
    "filelist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_h5_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import read_filelist_from_text_file\n",
    "    \n",
    "an_h5_filelist_path = Path(r'L:/Scratch/collected_outputs/2024-03-28_Apogee_all_sessions_h5_filelist.txt').resolve()\n",
    "read_hdf5_output_paths = read_filelist_from_text_file(filelist_path=an_h5_filelist_path, debug_print=False)\n",
    "read_hdf5_output_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Output File Processing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying \"/home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\"\n",
      "\t\t -> \"/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-06-06_Lab_2006-6-08_14-26-15_pipeline_results.h5\"...\n",
      "done.\n",
      "copying \"/home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\"\n",
      "\t\t -> \"/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-06-06_Lab_2006-6-09_1-22-43_pipeline_results.h5\"...\n"
     ]
    }
   ],
   "source": [
    "## INPUT a_copy_dict\n",
    "a_copy_dict = copy_h5_dict\n",
    "\n",
    "moved_files_dict_h5_files = copy_movedict(a_copy_dict)\n",
    "moved_files_dict_h5_files\n",
    "# INPUTS: active_filelist_prefix, target_dir\n",
    "# active_filelist_prefix: str = 'backed_up_files'\n",
    "active_filelist_prefix: str = 'session_h5_files'\n",
    "\n",
    "# target_dir: Path = Path(global_data_root_parent_path)\n",
    "target_dir: Path = collected_outputs_path\n",
    "\n",
    "moved_files_copydict_output_filename=f'{active_filelist_prefix}_copydict_{BATCH_DATE_TO_USE}.csv'\n",
    "moved_files_copydict_file_path = target_dir.joinpath(moved_files_copydict_output_filename).resolve() # Use Default\n",
    "print(f'moved_files_copydict_file_path: \"{moved_files_copydict_file_path}\"')\n",
    "\n",
    "_out_string, filedict_out_path = save_copydict_to_text_file(moved_files_dict_h5_files, moved_files_copydict_file_path, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab869730",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: moved_files_copydict_file_path\n",
    "moved_files_copydict_file_path = Path(r\"W:\\Data\\session_h5_files_copydict_2024-03-28_Apogee.csv\").resolve()\n",
    "assert moved_files_copydict_file_path.exists()\n",
    "\n",
    "read_moved_files_dict_files = read_copydict_from_text_file(moved_files_copydict_file_path, debug_print=False)\n",
    "read_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_moved_files_dict_files\n",
    "restore_moved_files_dict_files = invert_filedict(read_moved_files_dict_files)\n",
    "restore_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123baf6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data, process, and produce figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "graphics_output_dict = AcrossSessionsResults.post_compute_all_sessions_processing(global_data_root_parent_path=global_data_root_parent_path, BATCH_DATE_TO_USE=BATCH_DATE_TO_USE, plotting_enabled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d829b90",
   "metadata": {},
   "source": [
    "## 2023-11-15 - For manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load the saved across-session results:\n",
    "# Outputs: across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list, neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table\n",
    "\n",
    "inst_fr_output_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "## Load all across-session tables from the pickles:\n",
    "output_path_suffix: str = f'{BATCH_DATE_TO_USE}'\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=output_path_suffix) # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "# neuron_replay_stats_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abca14",
   "metadata": {},
   "source": [
    "# 2024-01-18 - Extracts the callback results 'determine_computation_datetimes_completion_function':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_callback_fn_results = {a_sess_ctxt:a_result.across_session_results.get('determine_session_t_delta_completion_function', {}) for a_sess_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "extracted_callback_fn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abede5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_white",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
