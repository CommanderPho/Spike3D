{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "doc_output_parent_folder: C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\EXTERNAL\\DEVELOPER_NOTES\\DataStructureDocumentation\n",
      "DAY_DATE_STR: 2024-05-30, DAY_DATE_TO_USE: 2024-05-30\n",
      "NOW_DATETIME: 2024-05-30_0630PM, NOW_DATETIME_TO_USE: 2024-05-30_0630PM\n",
      "global_data_root_parent_path changed to W:\\Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152a9557a10b4905a0ae434404536927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Data Root:', layout=Layout(width='auto'), options=(WindowsPath('W:/Data'),), style=ToggleButtonsStyle(button_width='max-content'), tooltip='global_data_root_parent_path', value=WindowsPath('W:/Data'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "# !pip install viztracer\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "from pyphocorehelpers.print_helpers import array_preview_with_graphical_shape_repr_html\n",
    "\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "# Register the custom display function for NumPy arrays\n",
    "ip.display_formatter.formatters['text/html'].for_type(np.ndarray, lambda arr: array_preview_with_graphical_shape_repr_html(arr))\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns, parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions,  RankOrderComputationsContainer, RankOrderResult, RankOrderAnalyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/Users/pho/cloud/turbo/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "\tglobal global_data_root_parent_path\n",
    "\tnew_global_data_root_parent_path = new_path.resolve()\n",
    "\tglobal_data_root_parent_path = new_global_data_root_parent_path\n",
    "\tprint(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "\tassert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\t\t\t\n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir: W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\n",
      "Computing loaded session pickle file results : \"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\loadedSessPickle.pkl\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:==========================================================================================\n",
      "========== Logger INIT \"2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43\" ==============================\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:NeuropyPipeline.__setstate__(state=\"{'pipeline_name': 'kdiba_pipeline', 'session_data_type': 'kdiba', '_stage': <pyphoplacecellanalysis.General.Pipeline.Stages.Display.DisplayPipelineStage object at 0x0000026434B3EAC0>}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_logger(full_logger_string=\"2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43\", file_logging_dir: None):\n",
      "done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\loadedSessPickle.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:select_filters(...) with: []\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing global computations...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:select_filters(...) with: []\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing global computations...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:select_filters(...) with: []\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:Performing global computations...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:NeuropyPipeline.on_stage_changed(new_stage=\"PipelineStage.Displayed\")\n",
      "WARNING:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # 2024-04-30 - Completely cleaned.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # Working wcorr_shuffle and trial_by_trial - DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays ---- VERY weird effect of the replays, a sharp drop to strongly negative values more than 3/4 through the experiment.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays! Interesting see-saw!\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # 2024-05-28 __ DEAD # 2024-01-10 new RANKORDER APOGEE | DONE, Added replay selections. A TON of putative replays in general, most bad, but some good. LOOKIN GOOD!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='twolong_LR_pf1Dsession_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # BAD: Confirmed frequent jumping off of the track in this session. DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few  # BAD: Seems like in 3D view there's also jumping off of the track in this session.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3') ### KeyError: 'maze1_odd'\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5') ### \n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # NEWDONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bf3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "list(curr_active_pipeline.global_computation_results.computed_data.keys())\n",
    "\n",
    "\n",
    "# 2024-01-22 ERROR: when the pipeline is manually saved, its global_computations seem to be saved to the pickle too. After modifying how global computations are loaded from pickle, the following global computations code block no longer appropriately overwrites the existing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "curr_active_pipeline.get_merged_computation_function_validators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_problem_result_names = ['directional_decoders_evaluate_epochs', 'directional_decoders_decode_continuous']\n",
    "# pickle_problem_global_result_key_names = ['DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations']\n",
    "pickle_problem_global_result_key_names = ['SequenceBased',]\n",
    "# pickle_problem_global_result_key_names = ['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded']\n",
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=pickle_problem_global_result_key_names, debug_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba46b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.574268400Z",
     "start_time": "2023-11-16T23:21:35.966373700Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "#### GLOBAL COMPUTE via `batch_extended_computations(...)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b6c927",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# extended_computations_include_includelist = ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "#     'extended_stats',\n",
    "#     # 'pf_dt_sequential_surprise',\n",
    "#     #  'ratemap_peaks_prominence2d',\n",
    "#     'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', \n",
    "#     'long_short_post_decoding', # #TODO 2024-01-19 05:49: - [ ] `'long_short_post_decoding' is broken for some reason `AttributeError: 'NoneType' object has no attribute 'active_filter_epochs'``\n",
    "#     # 'long_short_rate_remapping',\n",
    "#     'long_short_inst_spike_rate_groups',\n",
    "#     'long_short_endcap_analysis',\n",
    "#     # 'spike_burst_detection',\n",
    "#     'split_to_directional_laps',\n",
    "#     'merged_directional_placefields',\n",
    "#     # 'rank_order_shuffle_analysis',\n",
    "#     # 'directional_train_test_split',\n",
    "#     # 'directional_decoders_decode_continuous',\n",
    "#     # 'directional_decoders_evaluate_epochs',\n",
    "#     # 'directional_decoders_epoch_heuristic_scoring',\n",
    "# ] # do only specified\n",
    "\n",
    "\n",
    "extended_computations_include_includelist = ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    'extended_stats',\n",
    "    'pf_dt_sequential_surprise',\n",
    "     'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', \n",
    "    'long_short_post_decoding', # #TODO 2024-01-19 05:49: - [ ] `'long_short_post_decoding' is broken for some reason `AttributeError: 'NoneType' object has no attribute 'active_filter_epochs'``\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    'directional_train_test_split',\n",
    "    'directional_decoders_decode_continuous',\n",
    "    'directional_decoders_evaluate_epochs',\n",
    "    'directional_decoders_epoch_heuristic_scoring',\n",
    "    'perform_wcorr_shuffle_analysis',\n",
    "    'trial_by_trial_metrics',\n",
    "] # do only specified\n",
    "\n",
    "\n",
    "\n",
    "# extended_computations_include_includelist = ['lap_direction_determination', # needs to be first. Directly changes the laps\n",
    "# \t\t\t\t\t\t\t\t\t\t\t  'pf_computation',\n",
    "#     'split_to_directional_laps', # must come after `lap_direction_determination`\n",
    "#     'merged_directional_placefields',\n",
    "# \t] # do only specified\n",
    "\n",
    "\n",
    "\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_decode_continuous'] # \n",
    "# force_recompute_override_computations_includelist = ['wcorr_shuffle_analysis','trial_by_trial_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb47e03a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring', 'perform_wcorr_shuffle_analysis', 'trial_by_trial_metrics'], so only performing these extended computations.\n",
      "Running batch_evaluate_required_computations(...) with global_epoch_name: \"maze_any\"\n",
      "WARNING: FIXME TODO 2024-05-08 08:20: - [ ] Disabled checking for 'combined_best_direction_indicies', which is missing from both the laps and ripple dataframes after fresh computation for some reason.\n",
      "done with all batch_evaluate_required_computations(...).\n",
      "Pre-load global computations: needs_computation_output_dict: ['pf_dt_sequential_surprise']\n"
     ]
    }
   ],
   "source": [
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28cf10d2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loaded session pickle file results : \"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\global_computation_results.pkl\"... done.\n",
      "sucessfully_updated_keys: ['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations', 'TrainTestSplit', 'long_short_leave_one_out_decoding_analysis', 'short_long_pf_overlap_analyses', 'long_short_fr_indicies_analysis', 'jonathan_firing_rate_analysis', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap', 'TrialByTrialActivity', 'SequenceBased']\n",
      "successfully_loaded_keys: ['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations', 'TrainTestSplit', 'long_short_leave_one_out_decoding_analysis', 'short_long_pf_overlap_analyses', 'long_short_fr_indicies_analysis', 'jonathan_firing_rate_analysis', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap', 'TrialByTrialActivity', 'SequenceBased']\n"
     ]
    }
   ],
   "source": [
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # curr_active_pipeline.load_pickled_global_computation_results()\n",
    "        sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist) # is new\n",
    "        print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except BaseException as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].laps_most_likely_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02cdcca0",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring', 'perform_wcorr_shuffle_analysis', 'trial_by_trial_metrics'], so only performing these extended computations.\n",
      "Running batch_evaluate_required_computations(...) with global_epoch_name: \"maze_any\"\n",
      "WARNING: FIXME TODO 2024-05-08 08:20: - [ ] Disabled checking for 'combined_best_direction_indicies', which is missing from both the laps and ripple dataframes after fresh computation for some reason.\n",
      "done with all batch_evaluate_required_computations(...).\n",
      "Post-load global computations: needs_computation_output_dict: ['pf_dt_sequential_surprise']\n"
     ]
    }
   ],
   "source": [
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81782608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['split_to_directional_laps', 'rank_order_shuffle_analysis', 'extended_stats', 'pfdt_computation']\n",
    "# ['split_to_directional_laps', 'extended_stats', 'pfdt_computation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76313826",
   "metadata": {},
   "outputs": [],
   "source": [
    "needs_computation_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0fe66c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring', 'perform_wcorr_shuffle_analysis', 'trial_by_trial_metrics'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "WARNING: FIXME TODO 2024-05-08 08:20: - [ ] Disabled checking for 'combined_best_direction_indicies', which is missing from both the laps and ripple dataframes after fresh computation for some reason.\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_dt_sequential_surprise', 'maze_any')].\n",
      "\n",
      "\n",
      "!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"\n",
      "\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     exception_info = sys.exc_info()\n",
    "#     e = CapturedException(e, exception_info)\n",
    "#     print(f'second half threw: {e}')\n",
    "\n",
    "# 4m 5.2s for inst fr computations\n",
    "\n",
    "#34m 6.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4793f3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring', 'perform_wcorr_shuffle_analysis', 'trial_by_trial_metrics'], so only performing these extended computations.\n",
      "Running batch_evaluate_required_computations(...) with global_epoch_name: \"maze_any\"\n",
      "WARNING: FIXME TODO 2024-05-08 08:20: - [ ] Disabled checking for 'combined_best_direction_indicies', which is missing from both the laps and ripple dataframes after fresh computation for some reason.\n",
      "done with all batch_evaluate_required_computations(...).\n",
      "Post-compute validation: needs_computation_output_dict: []\n"
     ]
    }
   ],
   "source": [
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=False, force_recompute_override_computations_includelist=[], debug_print=True)\n",
    "print(f'Post-compute validation: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_all_extra_epoch_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e0148",
   "metadata": {},
   "source": [
    "## Specific Recomputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_most_recent_computation_time, each_epoch_latest_computation_time, each_epoch_each_result_computation_completion_times, (global_computations_latest_computation_time, global_computation_completion_times) = curr_active_pipeline.get_computation_times(debug_print=False)\n",
    "# each_epoch_latest_computation_time\n",
    "each_epoch_each_result_computation_completion_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config.instantaneous_time_bin_size_seconds = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820977",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "4    'extended_stats',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    # 'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs',\n",
    "    # 'directional_decoders_epoch_heuristic_scoring',\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = [\n",
    "    # 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring',\n",
    "    # 'lap_direction_determination', 'DirectionalLaps',\n",
    "    # 'merged_directional_placefields',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "]\n",
    "# force_recompute_override_computations_includelist = None\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recompute_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# force_recompute_override_computations_includelist = ['_decode_continuous_using_directional_decoders']\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(extended_computations_include_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.02}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.20}, {'time_bin_size': 0.20}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "# 2024-04-20 - HACK -- FIXME: Invert the 'is_LR_dir' column since it is clearly reversed. No clue why.\n",
    "# fails due to some types thing?\n",
    "# \terr: Length of values (82) does not match length of index (80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_evaluate_epochs'], computation_kwargs_list=[{'should_skip_radon_transform': False}], enabled_filter_names=None, fail_on_exception=True, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe380f10",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_epoch_heuristic_scoring'], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # OK FOR PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372737e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39417243",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['lap_direction_determination'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89757a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['extended_stats'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da92d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['perform_wcorr_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 50}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.2}, {'time_bin_size': 0.025}, {'should_skip_radon_transform': False}, {}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62493eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    ], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # , computation_kwargs_list=[{'should_skip_radon_transform': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c267dae5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# directional_merged_decoders_result = deepcopy(directional_decoders_epochs_decode_result)\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "\n",
    "spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "\n",
    "global_computation_results = curr_active_pipeline.global_computation_results\n",
    "\n",
    " # spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results = global_computation_results.computed_data['RankOrder'] # : \"RankOrderComputationsContainer\"\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "# included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "directional_laps_results: DirectionalLapsResult = global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "# print(f'included_qclu_values: {included_qclu_values}')\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "# pos_bin_size = _recover_position_bin_size(track_templates.get_decoders()[0]) # 3.793023081021702\n",
    "# print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "# pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "\n",
    "## Simple Pearson Correlation\n",
    "assert spikes_df is not None\n",
    "(laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df), corr_column_names = directional_merged_decoders_result.compute_simple_spike_time_v_pf_peak_x_by_epoch(track_templates=track_templates, spikes_df=deepcopy(spikes_df))\n",
    "## OUTPUTS: (laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df), corr_column_names\n",
    "## Computes the highest-valued decoder for this score:\n",
    "best_decoder_index_col_name: str = 'best_decoder_index'\n",
    "laps_simple_pf_pearson_merged_df[best_decoder_index_col_name] = laps_simple_pf_pearson_merged_df[corr_column_names].abs().apply(lambda row: np.argmax(row.values), axis=1)\n",
    "ripple_simple_pf_pearson_merged_df[best_decoder_index_col_name] = ripple_simple_pf_pearson_merged_df[corr_column_names].abs().apply(lambda row: np.argmax(row.values), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a452a66",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(active_epochs_df): 412\n",
      "min_num_unique_aclu_inclusions: 18\n",
      "len(active_epochs_df): 136\n",
      "pos_bin_size: 3.8054171165052444\n",
      "ripple_decoding_time_bin_size: 0.025\n",
      "laps_decoding_time_bin_size: 0.2\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "print(f'pos_bin_size: {pos_bin_size}')\n",
    "print(f'ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}')\n",
    "\n",
    "# Radon Transforms:\n",
    "decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "# Weighted correlations:\n",
    "laps_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "ripple_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "decoder_laps_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "decoder_ripple_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "# Pearson's correlations:\n",
    "laps_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "# laps_simple_pf_pearson_merged_df\n",
    "# ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## Drop rows where all are missing\n",
    "corr_column_names = ['long_LR_pf_peak_x_pearsonr', 'long_RL_pf_peak_x_pearsonr', 'short_LR_pf_peak_x_pearsonr', 'short_RL_pf_peak_x_pearsonr']\n",
    "# ripple_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='all') # 350/412 rows\n",
    "filtered_laps_simple_pf_pearson_merged_df: pd.DataFrame = laps_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='any') # 320/412 rows\n",
    "filtered_ripple_simple_pf_pearson_merged_df: pd.DataFrame = ripple_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='any') # 320/412 rows\n",
    "\n",
    "## Update the `decoder_ripple_filter_epochs_decoder_result_dict` with the included epochs:\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epochs(filtered_ripple_simple_pf_pearson_merged_df.index) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epochs(filtered_laps_simple_pf_pearson_merged_df.index) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_ripple_simple_pf_pearson_merged_df[['start', 'stop']].to_numpy()) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_laps_simple_pf_pearson_merged_df[['start', 'stop']].to_numpy()) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_ripple_simple_pf_pearson_merged_df['start'].to_numpy()) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_laps_simple_pf_pearson_merged_df['start'].to_numpy()) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed38171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "\n",
    "# directional_decoders_epochs_decode_result\n",
    "# save_path = Path(\"/Users/pho/data/KDIBA/gor01/one/2006-6-09_1-22-43/output/2024-04-25_CustomDecodingResults.pkl\").resolve()\n",
    "# save_path = curr_active_pipeline.get_output_path().joinpath(\"2024-04-28_CustomDecodingResults.pkl\").resolve()\n",
    "save_path = curr_active_pipeline.get_output_path().joinpath(f\"{DAY_DATE_TO_USE}_CustomDecodingResults.pkl\").resolve()\n",
    "\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "xbin_centers = deepcopy(long_pf2D.xbin_centers)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "ybin_centers = deepcopy(long_pf2D.ybin_centers)\n",
    "\n",
    "print(xbin_centers)\n",
    "save_dict = {\n",
    "'directional_decoders_epochs_decode_result': directional_decoders_epochs_decode_result.__getstate__(),\n",
    "'xbin': xbin, 'xbin_centers': xbin_centers}\n",
    "\n",
    "saveData(save_path, save_dict)\n",
    "print(f'save_path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b4531",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#  Export CSVs: \n",
    "## INPUTS: directional_decoders_epochs_decode_result,\n",
    "\n",
    "extracted_merged_scores_df = directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "# extracted_merged_scores_df\n",
    "\n",
    "print(f'\\tAll scores df CSV exporting...')\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "export_df_dict = {'ripple_all_scores_merged_df': extracted_merged_scores_df}\n",
    "_csv_export_paths = directional_decoders_epochs_decode_result.perform_export_dfs_dict_to_csvs(extracted_dfs_dict=export_df_dict, parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                            #   user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                            #   valid_epochs_selections={'ripple': filtered_valid_epoch_times},\n",
    "                                                                            )\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported ripple_all_scores_merged_df to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _csv_export_paths.items()])\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extracted_merged_scores_df.to_csv('test_(ripple_all_scores_merged_df).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a860a1",
   "metadata": {},
   "outputs": [],
   "source": [
    " decoder_ripple_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_extras_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbf34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ripple_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_extras_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_laps_simple_pf_pearson_merged_df\n",
    "# filtered_ripple_simple_pf_pearson_merged_df\n",
    "# decoder_ripple_weighted_corr_df_dict\n",
    "ripple_weighted_corr_merged_df['ripple_start_t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a95450",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_column_names = ['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']\n",
    "filtered_ripple_simple_pf_pearson_merged_df.label = filtered_ripple_simple_pf_pearson_merged_df.label.astype('int64')\n",
    "ripple_weighted_corr_merged_df['label'] = ripple_weighted_corr_merged_df['ripple_idx'].astype('int64')\n",
    "\n",
    "filtered_ripple_simple_pf_pearson_merged_df.join(ripple_weighted_corr_merged_df[wcorr_column_names], on='start') # , on='label'\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ripple_weighted_corr_merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_decoded_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3599f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "\n",
    "a_decoded_filter_epochs_decoder_result_dict.epochs.find_data_indicies_from_epoch_times([380.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77babf98",
   "metadata": {},
   "source": [
    "##  Continue Saving/Exporting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea89da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_computation_results_pickle_path: W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\global_computation_results.pkl\n",
      "Saving (file mode 'w+b') session pickle file results : \"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\global_computation_results.pkl\"... \tmoving new output at 'W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\20240530195807-global_computation_results.pkltmp' -> to desired location: 'W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\global_computation_results.pkl'\n",
      "saved session pickle file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "837f39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:save_pipeline(): Attempting to save pipeline to disk...\n",
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\tfinalized_loaded_sess_pickle_path: W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\loadedSessPickle.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_loaded_sess_pickle_path: W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\loadedSessPickle.pkl\n",
      "Saving (file mode 'w+b') session pickle file results :... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:moving new output at 'W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\20240530195910-loadedSessPickle.pkl' -> to desired location: 'W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\loadedSessPickle.pkl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved session pickle file\n",
      "moving new output at 'W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\20240530195910-loadedSessPickle.pkl' -> to desired location: 'W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\loadedSessPickle.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-05-30_18-05-35.Apogee.kdiba.gor01.one.2006-6-09_1-22-43:\t save complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) ## #TODO 2024-02-16 14:25: - [ ] PicklingError: Can't pickle <function make_set_closure_cell.<locals>.set_closure_cell at 0x7fd35e66b700>: it's not found as attr._compat.make_set_closure_cell.<locals>.set_closure_cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10989b4",
   "metadata": {},
   "source": [
    "#### Get computation times/info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_most_recent_computation_time, each_epoch_latest_computation_time, each_epoch_each_result_computation_completion_times, (global_computations_latest_computation_time, global_computation_completion_times) = curr_active_pipeline.get_computation_times(debug_print=False)\n",
    "# each_epoch_latest_computation_time\n",
    "# each_epoch_each_result_computation_completion_times\n",
    "# global_computation_completion_times\n",
    "\n",
    "# curr_active_pipeline.get_merged_computation_function_validators()\n",
    "# Get the names of the global and non-global computations:\n",
    "all_validators_dict = curr_active_pipeline.get_merged_computation_function_validators()\n",
    "global_only_validators_dict = {k:v for k, v in all_validators_dict.items() if v.is_global}\n",
    "non_global_only_validators_dict = {k:v for k, v in all_validators_dict.items() if (not v.is_global)}\n",
    "non_global_comp_names: List[str] = [v.short_name for k, v in non_global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))] # ['firing_rate_trends', 'spike_burst_detection', 'pf_dt_sequential_surprise', 'extended_stats', 'placefield_overlap', 'ratemap_peaks_prominence2d', 'velocity_vs_pf_simplified_count_density', 'EloyAnalysis', '_perform_specific_epochs_decoding', 'recursive_latent_pf_decoding', 'position_decoding_two_step', 'position_decoding', 'lap_direction_determination', 'pfdt_computation', 'pf_computation']\n",
    "global_comp_names: List[str] = [v.short_name for k, v in global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))] # ['long_short_endcap_analysis', 'long_short_inst_spike_rate_groups', 'long_short_post_decoding', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_decoding_analyses', 'PBE_stats', 'rank_order_shuffle_analysis', 'directional_decoders_epoch_heuristic_scoring', 'directional_decoders_evaluate_epochs', 'directional_decoders_decode_continuous', 'merged_directional_placefields', 'split_to_directional_laps']\n",
    "\n",
    "# mappings between the long computation function names and their short names:\n",
    "non_global_comp_names_map: Dict[str, str] = {v.computation_fn_name:v.short_name for k, v in non_global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))}\n",
    "global_comp_names_map: Dict[str, str] = {v.computation_fn_name:v.short_name for k, v in global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))} # '_perform_long_short_endcap_analysis': 'long_short_endcap_analysis', '_perform_long_short_instantaneous_spike_rate_groups_analysis': 'long_short_inst_spike_rate_groups', ...}\n",
    "\n",
    "# convert long function names to short-names:\n",
    "each_epoch_each_result_computation_completion_times = {an_epoch:{non_global_comp_names_map.get(k, k):v for k,v in a_results_dict.items()} for an_epoch, a_results_dict in each_epoch_each_result_computation_completion_times.items()}\n",
    "global_computation_completion_times = {global_comp_names_map.get(k, k):v for k,v in global_computation_completion_times.items()}\n",
    "\n",
    "each_epoch_each_result_computation_completion_times\n",
    "global_computation_completion_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967369f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "\n",
    "active_probe_includelist = extended_computations_include_includelist\n",
    "# active_probe_includelist = ['lap_direction_determination']\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=active_probe_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "needs_computation_output_dict\n",
    "# valid_computed_results_output_list\n",
    "# remaining_include_function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59374622",
   "metadata": {},
   "outputs": [],
   "source": [
    "'lap_direction_determination'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recompute_earlier_than_date = datetime(2024, 4, 1, 0, 0, 0)\n",
    "recompute_earlier_than_date\n",
    "\n",
    "each_epoch_needing_recompute = [an_epoch for an_epoch, last_computed_datetime in each_epoch_latest_computation_time.items() if (last_computed_datetime < recompute_earlier_than_date)]\n",
    "each_epoch_needing_recompute\n",
    "each_epoch_each_result_needing_recompute = {an_epoch:{a_computation_name:last_computed_datetime for a_computation_name, last_computed_datetime in last_computed_datetimes_dict.items() if (last_computed_datetime < recompute_earlier_than_date)} for an_epoch, last_computed_datetimes_dict in each_epoch_each_result_computation_completion_times.items()}\n",
    "each_epoch_each_result_needing_recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ffa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_times\n",
    "curr_active_pipeline.global_computation_results\n",
    "# curr_active_pipeline.try_load_split_pickled_global_computation_results\n",
    "\n",
    "global_computation_times = deepcopy(curr_active_pipeline.global_computation_results.computation_times.to_dict()) # DynamicParameters({'perform_rank_order_shuffle_analysis': datetime.datetime(2024, 4, 3, 5, 41, 31, 287680), '_decode_continuous_using_directional_decoders': datetime.datetime(2024, 4, 3, 5, 12, 7, 337326), '_perform_long_short_decoding_analyses': datetime.datetime(2024, 4, 3, 5, 43, 10, 361685), '_perform_long_short_pf_overlap_analyses': datetime.datetime(2024, 4, 3, 5, 43, 10, 489296), '_perform_long_short_firing_rate_analyses': datetime.datetime(2024, 4, 3, 5, 45, 3, 73472), '_perform_jonathan_replay_firing_rate_analyses': datetime.datetime(2024, 4, 3, 5, 45, 5, 168790), '_perform_long_short_post_decoding_analysis': datetime.datetime(2024, 2, 16, 18, 13, 4, 734621), '_perform_long_short_endcap_analysis': datetime.datetime(2024, 4, 3, 5, 45, 24, 274261), '_decode_and_evaluate_epochs_using_directional_decoders': datetime.datetime(2024, 4, 3, 5, 14, 37, 935482), '_perform_long_short_instantaneous_spike_rate_groups_analysis': datetime.datetime(2024, 4, 3, 5, 45, 24, 131955), '_split_to_directional_laps': datetime.datetime(2024, 4, 3, 5, 11, 22, 627789), '_build_merged_directional_placefields': datetime.datetime(2024, 4, 3, 5, 11, 28, 376078)})\n",
    "global_computation_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {},
   "source": [
    "# Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8d73eec6e041748a8f3a20745cb7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Box(children=(Label(value='session path:', layout=Layout(width='auto')), HTML(value=\"<b style='font-size: larger;'>W:\\\\Data\\\\KDIBA\\\\gor01\\\\one\\\\2006-6-09_1-22-43\\\\output</b>\", layout=Layout(flex='1 1 auto', margin='2px', width='auto')), Button(button_style='info', description='Copy', icon='clipboard', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Copy to Clipboard'), Button(button_style='info', description='Reveal', icon='folder-open-o', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Reveal in System Explorer')), layout=Layout(align_items='center', display='flex', flex_flow='row nowrap', justify_content='flex-start', width='70%')), HBox(children=(Button(description='Output Folder', style=ButtonStyle()), Button(description='global pickle', style=ButtonStyle()), Button(description='pipeline pickle', style=ButtonStyle()), Button(description='.h5 export', style=ButtonStyle()), Button(description='TEST - Dialog', style=ButtonStyle()), Button(description='Save Pipeline', style=ButtonStyle()))), HBox(children=(Button(description='Reload display functions...', style=ButtonStyle()), Button(description='Reload computation functions...', style=ButtonStyle()))), ToggleButton(value=True, description='Figures Displaying')))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1029.316608761903, 1737.1968310000375)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_epoch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "WARNING: PAPER_FIGURE_figure_1_add_replay_epoch_rasters(...): no user-assigned manually labeled replay epochs. Reeturning all epochs.\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n"
     ]
    }
   ],
   "source": [
    "## long_short_decoding_analyses:\n",
    "from attrs import astuple\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import LeaveOneOutDecodingAnalysis\n",
    "\n",
    "curr_long_short_decoding_analyses: LeaveOneOutDecodingAnalysis = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult, DirectionalLapsResult, DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93346114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.BatchCompletionHandler import BatchSessionCompletionHandler\n",
    "\n",
    "BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(directional_laps_results.directional_lap_specific_configs.keys()) # ['maze1_odd', 'maze1_even', 'maze2_odd', 'maze2_even']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912656a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "\n",
    "## UNPACK HERE via direct property access:\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "ripple_decoding_time_bin_size = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "decoder_laps_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "\n",
    "# New items:\n",
    "decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "# Weighted correlations:\n",
    "laps_weighted_corr_merged_df = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "decoder_laps_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "decoder_ripple_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "# Pearson's correlations:\n",
    "laps_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "def _long_running_compute_peak_prominence_2d_function(task_id: str, active_config_name: str):\n",
    "    print(f\"Starting a long-running task named '{task_id}' in background...\")\n",
    "    _out = curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=[active_config_name], fail_on_exception=True, debug_print=False)\n",
    "\n",
    "    return (task_id, f\"Task '{task_id}' finished.\", _out, active_config_name)\n",
    "\n",
    "\n",
    "def _active_peak_prominence_2d_results_done_callback(future):\n",
    "    result = future.result()\n",
    "    task_name, desc, _out_result, active_config_name = result # unpack\n",
    "    print(f'task_name: {task_name}, desc: {desc}\\n\\tassigning global variable named \"{task_name}\".')\n",
    "    active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "\n",
    "    globals()['active_peak_prominence_2d_results'] = active_peak_prominence_2d_results\n",
    "\n",
    "    # Notify user that the task is done by printing a message.\n",
    "    display(Javascript(f'alert(\"Background task `_active_peak_prominence_2d_results_done_callback` is complete, global variable named \"{task_name}\" has been defined!\")')) # used to give the headsup in the Jupyter notebook\n",
    "\n",
    "# Create a ThreadPoolExecutor to run the function in a separate thread\n",
    "executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "# Starts the task and assigns a callback function that will be called upon completion\n",
    "# future = executor.submit(curr_active_pipeline.perform_specific_computation, computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "future = executor.submit(_long_running_compute_peak_prominence_2d_function, 'bg_peak_prominence_2d', global_any_name)\n",
    "future.add_done_callback(_active_peak_prominence_2d_results_done_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881402df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_config_name: str = 'maze_any'\n",
    "active_config_name: str = global_epoch_name\n",
    "## INPUTS: curr_active_pipeline, active_config_name\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "if active_peak_prominence_2d_results is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=False, debug_print=False)\n",
    "    active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "    assert active_peak_prominence_2d_results is not None, f\"bad even after computation\"\n",
    "\n",
    "active_peak_prominence_2d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f141f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=[short_LR_name, short_RL_name, long_any_name, short_any_name], fail_on_exception=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previously_decoded time_bin_sizes: [0.025]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "if 'DirectionalDecodersDecoded' in curr_active_pipeline.global_computation_results.computed_data:\n",
    "    directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "    all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "    pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "    spikes_df = directional_decoders_decode_result.spikes_df\n",
    "    continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "    previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "    print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7b4e959",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wcorr_ripple_shuffle.n_completed_shuffles: 1000\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "wcorr_shuffle_results: SequenceBasedComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('SequenceBased', None)\n",
    "if wcorr_shuffle_results is not None:    \n",
    "    wcorr_ripple_shuffle: WCorrShuffle = wcorr_shuffle_results.wcorr_ripple_shuffle\n",
    "    print(f'wcorr_ripple_shuffle.n_completed_shuffles: {wcorr_ripple_shuffle.n_completed_shuffles}')\n",
    "else:\n",
    "    print(f'SequenceBased is not computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fba865e4",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "directional_trial_by_trial_activity_result: TrialByTrialActivityResult = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None)\n",
    "\n",
    "if directional_trial_by_trial_activity_result is not None:\n",
    "    any_decoder_neuron_IDs = directional_trial_by_trial_activity_result.any_decoder_neuron_IDs\n",
    "    active_pf_dt: PfND_TimeDependent = directional_trial_by_trial_activity_result.active_pf_dt\n",
    "    directional_lap_epochs_dict: Dict[str, Epoch] = directional_trial_by_trial_activity_result.directional_lap_epochs_dict\n",
    "    directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts\n",
    "    ## OUTPUTS: directional_trial_by_trial_activity_result, directional_active_lap_pf_results_dicts\n",
    "else:\n",
    "    print(f'TrialByTrialActivity is not computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "most_recent_time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# most_recent_time_bin_size\n",
    "most_recent_continuously_decoded_dict = deepcopy(directional_decoders_decode_result.most_recent_continuously_decoded_dict)\n",
    "# most_recent_continuously_decoded_dict\n",
    "\n",
    "## Adds in the 'pseudo2D' decoder in:\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# time_bin_size: float = 0.01\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = continuously_decoded_result_cache_dict[time_bin_size]\n",
    "pseudo2D_decoder_continuously_decoded_result = continuously_decoded_dict.get('pseudo2D', None)\n",
    "if pseudo2D_decoder_continuously_decoded_result is None:\n",
    "\t# compute here...\n",
    "\t## Currently used for both cases to decode:\n",
    "\tt_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\tsingle_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]}) # Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "\tsingle_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "\tspikes_df = directional_decoders_decode_result.spikes_df\n",
    "\tpseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = pseudo2D_decoder.decode_specific_epochs(spikes_df=deepcopy(spikes_df), filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "\tcontinuously_decoded_dict['pseudo2D'] = pseudo2D_decoder_continuously_decoded_result\n",
    "\tcontinuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c260739a4f36c662",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestSplitResult\n",
    "\n",
    "directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "test_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "train_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_extended_stats = global_results['extended_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1c6006aba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d3bf5955d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "appearing_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {},
   "source": [
    "# POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n",
      "laps_decoding_time_bin_size: 0.2, ripple_decoding_time_bin_size: 0.025\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "\n",
    "## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_replay_epochs_df\n",
    "\n",
    "# Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "# directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "# directional_likelihoods_df\n",
    "\n",
    "# 2023-12-15 - Newest method:\n",
    "# laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "# ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "# ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "# # Concatenate the three DataFrames along the columns axis:\n",
    "# # Assert that all DataFrames have the same number of rows:\n",
    "# assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "# # Assert that all DataFrames have at least one row:\n",
    "# assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "# assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "# assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "# merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "# merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "# merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08508b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_replay_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ripple_is_most_likely_direction_LR_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753ca336",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(active_epochs_df): 412\n",
      "min_num_unique_aclu_inclusions: 18\n",
      "len(active_epochs_df): 136\n",
      "df_column_names: [['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch'], ['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch'], ['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch'], ['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>n_unique_aclus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105.400143</td>\n",
       "      <td>105.562560</td>\n",
       "      <td>8</td>\n",
       "      <td>0.162417</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132.511389</td>\n",
       "      <td>132.791003</td>\n",
       "      <td>17</td>\n",
       "      <td>0.279613</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.959357</td>\n",
       "      <td>150.254392</td>\n",
       "      <td>28</td>\n",
       "      <td>0.295035</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>191.608560</td>\n",
       "      <td>191.948753</td>\n",
       "      <td>48</td>\n",
       "      <td>0.340193</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>251.416775</td>\n",
       "      <td>251.811957</td>\n",
       "      <td>62</td>\n",
       "      <td>0.395182</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>438.266686</td>\n",
       "      <td>438.448211</td>\n",
       "      <td>87</td>\n",
       "      <td>0.181524</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1693.342058</td>\n",
       "      <td>1693.481926</td>\n",
       "      <td>400</td>\n",
       "      <td>0.139868</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1699.225242</td>\n",
       "      <td>1699.356969</td>\n",
       "      <td>403</td>\n",
       "      <td>0.131727</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1705.053099</td>\n",
       "      <td>1705.140866</td>\n",
       "      <td>405</td>\n",
       "      <td>0.087767</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1707.712068</td>\n",
       "      <td>1707.918998</td>\n",
       "      <td>406</td>\n",
       "      <td>0.206930</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1725.278891</td>\n",
       "      <td>1725.595092</td>\n",
       "      <td>414</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1731.111480</td>\n",
       "      <td>1731.287905</td>\n",
       "      <td>418</td>\n",
       "      <td>0.176425</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          start         stop label  duration  n_unique_aclus\n",
       "0    105.400143   105.562560     8  0.162417              26\n",
       "1    132.511389   132.791003    17  0.279613              33\n",
       "2    149.959357   150.254392    28  0.295035              30\n",
       "3    191.608560   191.948753    48  0.340193              34\n",
       "4    251.416775   251.811957    62  0.395182              23\n",
       "5    438.266686   438.448211    87  0.181524              21\n",
       "..          ...          ...   ...       ...             ...\n",
       "33  1693.342058  1693.481926   400  0.139868              23\n",
       "34  1699.225242  1699.356969   403  0.131727              31\n",
       "35  1705.053099  1705.140866   405  0.087767              18\n",
       "36  1707.712068  1707.918998   406  0.206930              24\n",
       "37  1725.278891  1725.595092   414  0.316201              30\n",
       "38  1731.111480  1731.287905   418  0.176425              21\n",
       "\n",
       "[39 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import HeuristicReplayScoring\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_filter_replay_epochs\n",
    "\n",
    "filtered_epochs_df, filtered_decoder_filter_epochs_decoder_result_dict, filtered_ripple_all_epoch_bins_marginals_df = _perform_filter_replay_epochs(curr_active_pipeline, global_epoch_name, track_templates, decoder_ripple_filter_epochs_decoder_result_dict, ripple_all_epoch_bins_marginals_df, ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "filtered_epochs_df\n",
    "# filtered_ripple_all_epoch_bins_marginals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6e077",
   "metadata": {},
   "source": [
    "### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((filtered_ripple_all_epoch_bins_marginals_df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "# is_included_large_wcorr_diff\n",
    "is_included_high_wcorr = np.any((filtered_ripple_all_epoch_bins_marginals_df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = filtered_ripple_all_epoch_bins_marginals_df[is_included_large_wcorr_diff]\n",
    "# df = filtered_ripple_all_epoch_bins_marginals_df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t\n",
    "\n",
    "significant_epochs_start_ts = np.squeeze(df['ripple_start_t'].to_numpy()) ## for filtering\n",
    "\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(significant_epochs_start_ts) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "filtered_decoder_filter_epochs_decoder_result_dict\n",
    "filtered_epochs_df = filtered_epochs_df.epochs.matching_epoch_times_slice(significant_epochs_start_ts)\n",
    "# filtered_ripple_all_epoch_bins_marginals_df = filtered_ripple_all_epoch_bins_marginals_df.epochs.matching_epoch_times_slice(significant_epochs_start_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15332e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "included_qclu_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(filtered_ripple_all_epoch_bins_marginals_df, t_split=split_time_t, active_context=active_context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripple_merged_complete_epoch_stats_df\n",
    "laps_merged_complete_epoch_stats_df\n",
    "['long_best_direction_indices', 'short_best_direction_indices', 'combined_best_direction_indicies', 'long_relative_direction_likelihoods', 'short_relative_direction_likelihoods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time series of Long-likely events\n",
    "# type(long_RL_results) # DynamicParameters\n",
    "long_LR_pf1D_Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df\n",
    "laps_most_likely_direction_from_decoder\n",
    "long_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "\t\"\"\" \n",
    "\tnum='RipplesRankOrderZscore'\n",
    "\t\"\"\"\n",
    "\tprint(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "\tfig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "\tax_dict = fig.subplot_mosaic(\n",
    "\t\t[\n",
    "\t\t\t[\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "\t\t\t[\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "\t\t],\n",
    "\t)\n",
    "\tplots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "\t)\n",
    "\treturn MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CSVs \n",
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "merged_complete_epoch_stats_df\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732743e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns\n",
    "\n",
    "dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))\n",
    "reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43327521",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index).any())\n",
    "# ripple_combined_epoch_stats_df.label.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.label).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdone. building global result.\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc018065",
   "metadata": {},
   "source": [
    "# Call perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6522e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from attrs import make_class\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "\n",
    "## Settings:\n",
    "return_full_decoding_results: bool = True\n",
    "desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=10)\n",
    "save_hdf: bool = False\n",
    "\n",
    "SimpleBatchComputationDummy = make_class('SimpleBatchComputationDummy', attrs=['BATCH_DATE_TO_USE', 'collected_outputs_path'])\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path)\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=save_hdf, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                desired_shared_decoding_time_bin_sizes=desired_shared_decoding_time_bin_sizes,\n",
    "                                                )\n",
    "if return_full_decoding_results:\n",
    "    # with `return_full_decoding_results == True`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result, output_directional_decoders_epochs_decode_results_dict = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    # validate the result:\n",
    "    {k:v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size for k,v in output_full_directional_merged_decoders_result.items()}\n",
    "    assert np.all([np.isclose(dict(k)['desired_shared_decoding_time_bin_size'], v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size) for k,v in output_full_directional_merged_decoders_result.items()]), f\"the desired time_bin_size in the parameters should match the one used that will appear in the decoded result\"\n",
    "\n",
    "else:\n",
    "    # with `return_full_decoding_results == False`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    output_full_directional_merged_decoders_result = None\n",
    "\n",
    "\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v: DecoderDecodedEpochsResult = list(output_directional_decoders_epochs_decode_results_dict.values())[0]\n",
    "type(v)\n",
    "\n",
    "_out = v.export_csvs(parent_output_path=collected_outputs_path, active_context=curr_active_pipeline.get_session_context(), session_name=curr_active_pipeline.session_name, curr_session_t_delta=t_delta)\n",
    "\n",
    "# assert self.collected_outputs_path.exists()\n",
    "# curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "# CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "# print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "# from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# batch_extended_computations(curr_active_pipeline, include_includelist=['merged_directional_placefields'], include_global_functions=True, fail_on_exception=True, force_recompute=False)\n",
    "# directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "# active_context = curr_active_pipeline.get_session_context()\n",
    "# _out = directional_merged_decoders_result.compute_and_export_marginals_df_csvs(parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
    "# print(f'successfully exported marginals_df_csvs to {self.collected_outputs_path}!')\n",
    "# (laps_marginals_df, laps_out_path), (ripple_marginals_df, ripple_out_path) = _out\n",
    "(laps_marginals_df, laps_out_path, laps_time_bin_marginals_df, laps_time_bin_marginals_out_path), (ripple_marginals_df, ripple_out_path, ripple_time_bin_marginals_df, ripple_time_bin_marginals_out_path) = _out\n",
    "print(f'\\tlaps_out_path: {laps_out_path}\\n\\tripple_out_path: {ripple_out_path}\\n\\tdone.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take extra computations from `_decode_and_evaluate_epochs_using_directional_decoders` and integrate into the multi-time-bin results from `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_all_df_score_metrics\n",
    "\n",
    "should_skip_radon_transform = True\n",
    "## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
    "\n",
    "a_sweep_tuple, a_pseudo_2D_result = list(output_full_directional_merged_decoders_result.items())[0]\n",
    "a_decoder_laps_filter_epochs_decoder_result_dict = deepcopy(a_pseudo_2D_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "a_decoder_ripple_filter_epochs_decoder_result_dict = deepcopy(a_pseudo_2D_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "\n",
    "(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(directional_merged_decoders_result, track_templates,\n",
    "                                                                                                                                                                                    decoder_laps_filter_epochs_decoder_result_dict=a_decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict=a_decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                    spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
    "                                                                                                                                                                                    should_skip_radon_transform=should_skip_radon_transform)\n",
    "laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
    "decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_sweep_tuple, a_pseudo_2D_result in output_full_directional_merged_decoders_result.items():\n",
    "    a_pseudo_2D_result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfda868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `_perform_compute_custom_epoch_decoding`\n",
    "\n",
    "a_sweep_tuple\n",
    "# a_pseudo_2D_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# a_pseudo_2D_result\n",
    "# a_pseudo_2D_result.short_directional_decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_keys_if_possible('several_time_bin_sizes_laps_df', several_time_bin_sizes_laps_df)\n",
    "print_keys_if_possible('output_full_directional_merged_decoders_result', output_full_directional_merged_decoders_result, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file_pat\n",
    "collected_outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_laps_decoding_accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# from neuropy.utils.matplotlib_helpers import pho_jointplot\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import pho_jointplot, plot_histograms\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# def pho_jointplot(*args, **kwargs):\n",
    "# \t\"\"\" wraps sns.jointplot to allow adding titles/axis labels/etc.\"\"\"\n",
    "# \ttitle = kwargs.pop('title', None)\n",
    "# \t_out = sns.jointplot(*args, **kwargs)\n",
    "# \tif title is not None:\n",
    "# \t\tplt.suptitle(title)\n",
    "# \treturn _out\n",
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_histograms\n",
    "\n",
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "plot_histograms('Ripples', 'One Session', several_time_bin_sizes_time_bin_ripple_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(\n",
    "#     several_time_bin_sizes_laps_df, x=\"P_Long\", col=\"species\", row=\"time_bin_size\",\n",
    "#     binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    "# )\n",
    "\n",
    "sns.displot(\n",
    "    several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', row=\"time_bin_size\",\n",
    "    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be351c18",
   "metadata": {},
   "source": [
    "# 2024-01-31 - Reinvestigation regarding remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "911d7495",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncationCheckingResults(_VersionedResultMixin_version='2024.01.10_0', disappearing_endcap_aclus=Int64Index([], dtype='int64'), non_disappearing_endcap_aclus=Int64Index([14, 15, 18, 19, 25, 26, 28, 32, 33, 34, 40, 47, 52, 57, 58, 59, 60, 61, 67, 68, 70, 77, 85, 87, 92, 95, 101, 102], dtype='int64'), significant_distant_remapping_endcap_aclus=Int64Index([14, 25, 47, 58, 61, 70, 77], dtype='int64'), minor_remapping_endcap_aclus=Int64Index([15, 18, 19, 26, 28, 32, 33, 34, 40, 52, 57, 59, 60, 67, 68, 85, 87, 92, 95, 101, 102], dtype='int64'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d9b54",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## From Jonathan Long/Short Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3641aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df = deepcopy(jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "## try to add the 2D peak information to the cells in `neuron_replay_stats_df`:\n",
    "neuron_replay_stats_df['long_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['long_pf2D_peak_y'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_y'] = pd.NA\n",
    "\n",
    "# flat_peaks_df: pd.DataFrame = deepcopy(active_peak_prominence_2d_results['flat_peaks_df']).reset_index(drop=True)\n",
    "long_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[long_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "short_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[short_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), long_filtered_flat_peaks_df.neuron_id.to_numpy()), ['long_pf2D_peak_x', 'long_pf2D_peak_y']] = long_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), short_filtered_flat_peaks_df.neuron_id.to_numpy()), ['short_pf2D_peak_x', 'short_pf2D_peak_y']] = short_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "\n",
    "both_included_neuron_stats_df = deepcopy(neuron_replay_stats_df[neuron_replay_stats_df['LS_pf_peak_x_diff'].notnull()]).drop(columns=['track_membership', 'neuron_type'])\n",
    "both_included_neuron_stats_df\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# _out_scatter = sns.scatterplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', y='aclu') # , hue='aclu'\n",
    "# _out_scatter.show()\n",
    "# _out_hist = sns.histplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jonathan_firing_rate_analysis_result) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.JonathanFiringRateAnalysisResult\n",
    "\n",
    "rdf_df: pd.DataFrame = deepcopy(jonathan_firing_rate_analysis_result.rdf.rdf)\n",
    "rdf_df\n",
    "\n",
    "np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to JSON\n",
    "output_path = Path(f'output/{get_now_day_str()}_rdf_df.json').resolve()\n",
    "rdf_df.to_json(output_path, orient='records', lines=True) ## This actually looks pretty good!\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b794a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_columns = ['start', 'end']\n",
    "invalid_columns = ['active_aclus', 'is_neuron_active', 'firing_rates']\n",
    "invalid_df_subset = rdf_df[join_columns + invalid_columns]\n",
    "invalid_df_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload DataFrame from JSON\n",
    "df_read: pd.DataFrame = pd.read_json(output_path, orient='records', lines=True)\n",
    "df_read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6170689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdf_df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_df.dtypes\n",
    "\n",
    "# firing_rates                            object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_long_pf].to_numpy()\n",
    "short_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_short_pf].to_numpy()\n",
    "\n",
    "long_pf_aclus, short_pf_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c34fd9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# 2024-04-09 - Maximum peaks only for each template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import NumpyHelpers\n",
    "from neuropy.utils.indexing_helpers import intersection_of_arrays, union_of_arrays\n",
    "from neuropy.utils.indexing_helpers import unwrap_single_item\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing import NewType\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "DecoderName = NewType('DecoderName', str)\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _get_directional_pf_peaks_dfs\n",
    "\n",
    "# (LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=False)\n",
    "\n",
    "(LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=False)\n",
    "\n",
    "\n",
    "AnyDir_decoder_aclu_MAX_peak_maps_df\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df\n",
    "# RL_only_decoder_aclu_MAX_peak_maps_df\n",
    "\n",
    "long_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['long_LR'].to_numpy()\n",
    "short_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['short_LR'].to_numpy()\n",
    "peak_x_diff = LR_only_decoder_aclu_MAX_peak_maps_df['peak_diff'].to_numpy()\n",
    "# decoder_aclu_peak_maps_dict\n",
    "\n",
    "## OUTPUTS: AnyDir_decoder_aclu_MAX_peak_maps_df,\n",
    "## OUTPUTS: LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "## OUTPUTS: RL_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fe1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_track_remapping_diagram(track_templates, grid_bin_bounds, active_context=None, perform_write_to_file_callback=None, defer_render: bool=False, **kwargs):    \n",
    "    \"\"\" Based off of `plot_bidirectional_track_remapping_diagram`\n",
    "    Usage:\n",
    "    \n",
    "        from pyphoplacecellanalysis.Pho2D.track_shape_drawing import plot_bidirectional_track_remapping_diagram\n",
    "\n",
    "        collector = plot_combined_track_remapping_diagram(track_templates, grid_bin_bounds=long_pf2D.config.grid_bin_bounds, active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='plot_bidirectional_track_remapping_diagram'))\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "    from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "    from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "    from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    from neuropy.utils.matplotlib_helpers import build_or_reuse_figure, perform_update_title_subtitle\n",
    "    from pyphoplacecellanalysis.Pho2D.track_shape_drawing import _plot_track_remapping_diagram\n",
    "\n",
    "    use_separate_plot_for_each_direction: bool = True\n",
    "\n",
    "    if active_context is not None:\n",
    "            display_context = active_context.adding_context('display_fn', display_fn_name='bidir_track_remap')\n",
    "        \n",
    "    with mpl.rc_context({'figure.figsize': (10, 4), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "        # Create a FigureCollector instance\n",
    "        with FigureCollector(name='plot_bidirectional_track_remapping_diagram', base_context=display_context) as collector:\n",
    "\n",
    "            ## Define common operations to do after making the figure:\n",
    "            def setup_common_after_creation(a_collector, fig, axes, sub_context, title=f'<size:22>Track <weight:bold>Remapping</></>'):\n",
    "                \"\"\" Captures:\n",
    "\n",
    "                t_split\n",
    "                \"\"\"\n",
    "                a_collector.contexts.append(sub_context)\n",
    "                \n",
    "                # `flexitext` version:\n",
    "                text_formatter = FormattedFigureText()\n",
    "                # ax.set_title('')\n",
    "                fig.suptitle('')\n",
    "                text_formatter.setup_margins(fig)\n",
    "                title_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin,\n",
    "                                        title,\n",
    "                                        va=\"bottom\", xycoords=\"figure fraction\")\n",
    "                footer_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "                                            text_formatter._build_footer_string(active_context=sub_context),\n",
    "                                            va=\"top\", xycoords=\"figure fraction\")\n",
    "            \n",
    "                if ((perform_write_to_file_callback is not None) and (sub_context is not None)):\n",
    "                    perform_write_to_file_callback(sub_context, fig)\n",
    "\n",
    "\n",
    "            # BEGIN FUNCTION BODY\n",
    "            drop_aclu_if_missing_long_or_short=True\n",
    "            # LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)\n",
    "            # drop_aclu_if_missing_long_or_short =False\n",
    "            (LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)\n",
    "\n",
    "            ## Make a single figure for both LR/RL remapping cells:\n",
    "            # kwargs = dict(draw_point_aclu_labels=True, enable_interactivity=True)\n",
    "            kwargs = dict(draw_point_aclu_labels=True, enable_interactivity=False)\n",
    "\n",
    "            if use_separate_plot_for_each_direction:\n",
    "                fig, axs = collector.subplots(nrows=2, ncols=1, sharex=True, sharey=True, num='Track Remapping', figsize=kwargs.pop('figsize', (10, 4)), dpi=kwargs.pop('dpi', None), constrained_layout=True, clear=True)\n",
    "                assert len(axs) == 2, f\"{len(axs)}\"\n",
    "                ax_dict = {'ax_LR': axs[0], 'ax_RL': axs[1]}\n",
    "\n",
    "                fig, ax_LR, _outputs_tuple_LR = _plot_track_remapping_diagram(LR_only_decoder_aclu_MAX_peak_maps_df, grid_bin_bounds=grid_bin_bounds, long_column_name='long_LR', short_column_name='short_LR', ax=ax_dict['ax_LR'], defer_render=defer_render, **kwargs)\n",
    "                perform_update_title_subtitle(fig=fig, ax=ax_LR, title_string=None, subtitle_string=f\"LR Track Remapping - {len(LR_only_decoder_aclu_MAX_peak_maps_df)} aclus\")\n",
    "                fig, ax_RL, _outputs_tuple_RL = _plot_track_remapping_diagram(RL_only_decoder_aclu_MAX_peak_maps_df, grid_bin_bounds=grid_bin_bounds, long_column_name='long_RL', short_column_name='short_RL', ax=ax_dict['ax_RL'], defer_render=defer_render, **kwargs)\n",
    "                perform_update_title_subtitle(fig=fig, ax=ax_RL, title_string=None, subtitle_string=f\"RL Track Remapping - {len(RL_only_decoder_aclu_MAX_peak_maps_df)} aclus\")\n",
    "\n",
    "                setup_common_after_creation(collector, fig=fig, axes=[ax_LR, ax_RL], sub_context=display_context.adding_context('subplot', subplot_name='Track Remapping'))\n",
    "            else:\n",
    "                fig, axs = collector.subplots(nrows=1, ncols=1, sharex=True, sharey=True, num='Track Remapping', figsize=kwargs.pop('figsize', (10, 4)), dpi=kwargs.pop('dpi', None), constrained_layout=True, clear=True)\n",
    "                # assert len(axs) == 1, f\"{len(axs)}\"\n",
    "                ax = axs\n",
    "\n",
    "                fig, ax, _outputs_tuple = _plot_track_remapping_diagram(AnyDir_decoder_aclu_MAX_peak_maps_df, grid_bin_bounds=grid_bin_bounds, long_column_name='long_LR', short_column_name='short_LR', ax=ax, defer_render=defer_render, **kwargs)\n",
    "                perform_update_title_subtitle(fig=fig, ax=ax, title_string=None, subtitle_string=f\"LR+RL Track Remapping - {len(LR_only_decoder_aclu_MAX_peak_maps_df)} aclus\")\n",
    "\n",
    "                setup_common_after_creation(collector, fig=fig, axes=[ax, ], sub_context=display_context.adding_context('subplot', subplot_name='Track Remapping'))\n",
    "\n",
    "\n",
    "    return collector\n",
    "\n",
    "\n",
    "collector = plot_combined_track_remapping_diagram(track_templates, grid_bin_bounds=long_pf2D.config.grid_bin_bounds, active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='plot_bidirectional_track_remapping_diagram'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Qt.color_helpers import ColorFormatConverter, build_adjusted_color, debug_print_color\n",
    "\n",
    "selection_color = (1, 0, 0, 1)  # Red color in RGBA format\n",
    "\n",
    "\n",
    "# pg.QtGui.QColor(selection_color)\n",
    "selection_qcolor = pg.mkColor(*selection_color[:-1])\n",
    "selection_qcolor.setAlphaF(selection_color[-1])\n",
    "\n",
    "ColorFormatConverter.qColor_to_hexstring(qcolor=selection_qcolor, include_alpha=True)\n",
    "\n",
    "\n",
    "debug_print_color(selection_qcolor)\n",
    "curr_color_copy = build_adjusted_color(selection_qcolor, hue_shift=0.0, saturation_scale=1.00, value_scale=0.35) # darker\n",
    "debug_print_color(curr_color_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b90dc",
   "metadata": {},
   "source": [
    "## 2024-02-08 - Filter to find only the clear remap examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphocorehelpers.indexing_helpers import dict_to_full_array\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0cf56",
   "metadata": {},
   "source": [
    "### Get num peaks exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: `directional_active_lap_pf_results_dicts`, not sure why\n",
    "\n",
    "neuron_ids_dict = {k:v.neuron_ids for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "neuron_ids_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42a8d",
   "metadata": {},
   "source": [
    "### Get stability for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ec2d7",
   "metadata": {},
   "source": [
    "#### 2024-02-08 - 3pm - new stability dataframe to look at stability of each cell across decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57869e27",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## INPUTS: directional_active_lap_pf_results_dicts\n",
    "\n",
    "# for k,v in directional_active_lap_pf_results_dicts.items():\n",
    "# stability_dict = {k:v.aclu_to_stability_score_dict for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict = {k:dict_to_full_array(v.aclu_to_stability_score_dict, full_indicies=any_decoder_neuron_IDs, fill_value=0.0) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "\n",
    "\n",
    "# list(stability_dict.values())\n",
    "\n",
    "stability_dict = {k:list(v.aclu_to_stability_score_dict.values()) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "## all the same size hopefully!\n",
    "# [len(v) for v in list(stability_dict.values())]\n",
    "\n",
    "stability_df: pd.DataFrame = pd.DataFrame({'aclu': any_decoder_neuron_IDs, **stability_dict})\n",
    "# stability_df.rename(dict(zip([], [])))\n",
    "stability_df\n",
    "\n",
    "## OUTPUTS: stability_df, stability_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef13541",
   "metadata": {},
   "source": [
    "# 2024-02-02 - napari_plot_directional_trial_by_trial_activity_viz Trial-by-trial Correlation Matrix C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd735e",
   "metadata": {},
   "source": [
    "###  Show Trial-by-trial Correlation Matrix C in `napari`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72df59",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "# import afinder\n",
    "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_plot_directional_trial_by_trial_activity_viz, napari_trial_by_trial_activity_viz, napari_export_image_sequence\n",
    "\n",
    "## Directional\n",
    "directional_viewer, directional_image_layer_dict, custom_direction_split_layers_dict = napari_plot_directional_trial_by_trial_activity_viz(directional_active_lap_pf_results_dicts, include_trial_by_trial_correlation_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d79d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: directional_trial_by_trial_activity_result\n",
    "for a_decoder_name, a_result in directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts.items():\n",
    "    ## Global:\n",
    "    viewer, image_layer_dict = napari_trial_by_trial_activity_viz(a_result.z_scored_tuning_map_matrix, a_result.C_trial_by_trial_correlation_matrix, title=f'Trial-by-trial Correlation Matrix C - Decoder {a_decoder_name}', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4ec86",
   "metadata": {},
   "source": [
    "# 2023-09-07 - Track Graphics Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b53254",
   "metadata": {},
   "source": [
    "##  2024-02-16 - NOW - Working Track Remapping Diagram Figure!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import plot_bidirectional_track_remapping_diagram, _plot_track_remapping_diagram\n",
    "\n",
    "collector = plot_bidirectional_track_remapping_diagram(track_templates, grid_bin_bounds=long_pf2D.config.grid_bin_bounds, active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='plot_bidirectional_track_remapping_diagram'), enable_adjust_overlapping_text=False, draw_point_aclu_labels=False, enable_interactivity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "curr_active_pipeline.display('_display_directional_track_remapping_diagram', save_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import build_shared_sorted_neuron_color_maps\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColorFormatConverter\n",
    "\n",
    "decoders_dict = track_templates.get_decoders_dict() # decoders_dict = {'long_LR': track_templates.long_LR_decoder, 'long_RL': track_templates.long_RL_decoder, 'short_LR': track_templates.short_LR_decoder, 'short_RL': track_templates.short_RL_decoder, }\n",
    "neuron_IDs_lists = [deepcopy(a_decoder.neuron_IDs) for a_decoder in decoders_dict.values()] # [A, B, C, D, ...]\n",
    "# _unit_qcolors_map, unit_colors_map = build_shared_sorted_neuron_color_maps(neuron_IDs_lists)\n",
    "unit_colors_map, _unit_colors_ndarray_map = build_shared_sorted_neuron_color_maps(neuron_IDs_lists, return_255_array=False)\n",
    "# _unit_colors_ndarray_map = {k:np.squeeze(ColorFormatConverter.Colors_NDArray_Convert_to_zero_to_one_array(np.array([v]))) for k, v in _unit_colors_ndarray_map.items()} # convert to 0.0-1.0 based colors\n",
    "\n",
    "# `unit_colors_map` is main colors output\n",
    "\n",
    "included_neuron_ids = np.array(list(unit_colors_map.keys())) # one list for all decoders\n",
    "n_neurons = len(included_neuron_ids)\n",
    "\n",
    "RL_neuron_ids = track_templates.shared_RL_aclus_only_neuron_IDs.copy() # (69, )\n",
    "LR_neuron_ids = track_templates.shared_LR_aclus_only_neuron_IDs.copy() # (64, )\n",
    "\n",
    "included_any_context_neuron_ids_dict = dict(zip(['long_LR', 'long_RL', 'short_LR', 'short_RL'], (LR_neuron_ids, RL_neuron_ids, LR_neuron_ids, RL_neuron_ids)))\n",
    "\n",
    "sortable_values_list_dict = {k:deepcopy(a_decoder.pf.peak_tuning_curve_center_of_masses) for k, a_decoder in decoders_dict.items()} # tuning_curve CoM location\n",
    "sorted_neuron_IDs_lists, sort_helper_neuron_id_to_neuron_colors_dicts, sort_helper_neuron_id_to_sort_IDX_dicts, (unsorted_original_neuron_IDs_lists, unsorted_neuron_IDs_lists, unsorted_sortable_values_lists, unsorted_unit_colors_map) = paired_separately_sort_neurons(decoders_dict, included_any_context_neuron_ids_dict, sortable_values_list_dict=sortable_values_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4701a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_aclu_if_missing_long_or_short = True\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)\n",
    "# drop_aclu_if_missing_long_or_short =False\n",
    "(LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c35494",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnyDir_decoder_aclu_MAX_peak_maps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfe113",
   "metadata": {},
   "outputs": [],
   "source": [
    "_by_ANY = AnyDir_decoder_aclu_MAX_peak_maps_df.sort_values(by=['long_LR', 'long_RL'], inplace=False)\n",
    "long_peak_sorted_unit_colors_ndarray_map = dict(zip(_by_ANY.index.to_numpy(), list(_unit_colors_ndarray_map.values())))\n",
    "long_peak_sorted_unit_colors_ndarray_map\n",
    "\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5286baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnyDir_decoder_aclu_MAX_peak_maps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_helper_neuron_id_to_sort_IDX_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7183e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_peak_sorted_unit_colors_ndarray_map_LR = dict(zip(sorted_neuron_IDs_lists[0], list(_unit_colors_ndarray_map.values())))\n",
    "long_peak_sorted_unit_colors_ndarray_map_RL = dict(zip(sorted_neuron_IDs_lists[1], list(_unit_colors_ndarray_map.values())))\n",
    "long_peak_sorted_unit_colors_ndarray_map_LR\n",
    "long_peak_sorted_unit_colors_ndarray_map_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colormap = mcolors.ListedColormap(['white'])\n",
    "normalize = mcolors.Normalize(vmin=active_aclus.min(), vmax=active_aclus.max())\n",
    "scalar_map = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "\n",
    "# Create a constant colormap with only white color\n",
    "\n",
    "color = scalar_map.to_rgba(active_aclus)\n",
    "\n",
    "color = [_unit_colors_ndarray_map[an_aclu] for an_aclu in active_aclus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d58d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS:\n",
    "neuron_replay_stats_df\n",
    "\n",
    "_active_LR_aclus = np.array(list(_output_by_aclu_dict_LR.keys()))\n",
    "_active_LR_aclus\n",
    "\n",
    "is_active_LR_aclus = np.isin(neuron_replay_stats_df.aclu, _active_LR_aclus)\n",
    "_temp_neuron_replay_stats_df = neuron_replay_stats_df[is_active_LR_aclus]\n",
    "\n",
    "is_active_LR_long_peak_either_cap_dict = _temp_neuron_replay_stats_df['is_long_peak_either_cap'].to_dict()\n",
    "is_active_LR_long_peak_either_cap_dict\n",
    "\n",
    "\n",
    "# either_cap_aclu = {k:v for k,v in is_active_LR_long_peak_either_cap_dict.items() if (v is True)}\n",
    "\n",
    "active_LR_either_cap_aclus = np.array([k for k,v in is_active_LR_long_peak_either_cap_dict.items() if (v is True)])\n",
    "active_LR_either_cap_aclus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dac9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Selected ACLUS manually:\n",
    "\n",
    "## `FakePickEvent` is used to highlight specified aclus by emulating a selection event.\n",
    "#  matplotlib.backend_bases.PickEvent\n",
    "import attrs\n",
    "FakePickEvent = attrs.make_class(\"FakePickEvent\", {k:field() for k in (\"ind\", )})\n",
    "\n",
    "included_aclus = [45, 24, 17, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: included_aclus, LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df, _outputs_tuple_LR, _outputs_tuple_RL\n",
    "included_aclus = active_LR_either_cap_aclus\n",
    "# LR:\n",
    "LR_included_indicies = np.where(np.isin(LR_only_decoder_aclu_MAX_peak_maps_df.index, included_aclus))[0] # LR_included_indicies # [ 6,  9, 22, 36]\n",
    "LR_fake_event: FakePickEvent = FakePickEvent(ind=np.array(LR_included_indicies))\n",
    "_output_dict_LR, _output_by_aclu_dict_LR = _outputs_tuple_LR\n",
    "scatter_select_function_LR = _output_dict_LR['scatter_select_function']\n",
    "scatter_select_function_LR(LR_fake_event)\n",
    "\n",
    "## RL:\n",
    "RL_included_indicies = np.where(np.isin(RL_only_decoder_aclu_MAX_peak_maps_df.index, included_aclus))[0]\n",
    "RL_fake_event: FakePickEvent = FakePickEvent(ind=np.array(RL_included_indicies))\n",
    "_output_dict_RL, _output_by_aclu_dict_RL = _outputs_tuple_RL\n",
    "scatter_select_function_RL = _output_dict_RL['scatter_select_function']\n",
    "scatter_select_function_RL(RL_fake_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {},
   "source": [
    "#  2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2fa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _directional_laps_overview = curr_active_pipeline.plot._display_directional_laps_overview(curr_active_pipeline.computation_results, a)\n",
    "# _directional_laps_overview = curr_active_pipeline.display('_display_directional_laps_overview')\n",
    "# _directional_laps_overview = curr_active_pipeline.display('_display_grid_bin_bounds_validation')\n",
    "_directional_laps_overview = curr_active_pipeline.display('_display_long_short_pf1D_comparison')\n",
    "\n",
    "_directional_laps_overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539711fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194e75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4929b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.displayContextSelectorWidget.enable_context_selection(is_enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e87f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(widget.ui, 'displayContextSelectorWidget', IdentifyingContextSelectorWidget(owning_pipeline=widget.owning_pipeline, enable_multi_context_select=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.ui.displayContextSelectorWidget = IdentifyingContextSelectorWidget(owning_pipeline=widget.owning_pipeline, enable_multi_context_select=False)\n",
    "# a_layout = widget.ui.displayContextSelectorWidgetContainer.layout\n",
    "# widget.ui.gridLayout_displayContext\n",
    "widget.ui.gridLayout_displayContext.addWidget(widget.ui.displayContextSelectorWidget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff893ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeError: _display_rank_order_z_stats_results() missing 2 required positional arguments: 'computation_results' and 'active_configs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_fcn = widget._perform_get_display_function_code(a_fcn_name='_display_two_step_decoder_prediction_error_2D')\n",
    "print(str(curr_fcn.__code__.co_varnames))\n",
    "fcn_defn_str: str = inspect.getsource(curr_fcn)\n",
    "print(fcn_defn_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_two_step_decoder_prediction_error_2D', 'maze_any') # 'maze_any'\n",
    "\n",
    "update_fn = _out_graphics_dict.plot_data['draw_update_fn']\n",
    "num_frames = _out_graphics_dict.plot_data['num_frames']\n",
    "\n",
    "print(f'num_frames: {num_frames}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b374e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "all_save_paths = {}\n",
    "\n",
    "ani = animation.FuncAnimation(_out_graphics_dict.figures[0], update_fn, frames=num_frames, blit=False, repeat=False, interval=20, save_count=50)\n",
    "\n",
    "# ani.to_html5_video()\n",
    "\n",
    "# # To save the animation using Pillow as a gif\n",
    "# _temp_gif_save_path = Path('scatter.gif').resolve()\n",
    "# writer = animation.PillowWriter(fps=15, metadata=dict(artist='Pho Hale'), bitrate=1800)\n",
    "# ani.save(_temp_gif_save_path, writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f69ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save the animation to a BytesIO buffer\n",
    "# buf = io.BytesIO()\n",
    "# ani.save(buf, codec='gif', writer='imagemagick', fps=10)\n",
    "# buf.seek(0)\n",
    "\n",
    "# # Display the GIF\n",
    "# display(Image(data=buf.getvalue(), format='gif'))\n",
    "# Display the GIF\n",
    "# assert _temp_gif_save_path.exists()\n",
    "# Image(_temp_gif_save_path)\n",
    "\n",
    "\n",
    "# for i in np.arange(num_frames):\n",
    "#     update_fn(i) ## Adjust the slider, using its callbacks as well to update the displayed epoch.\n",
    "    \n",
    "#     # _out_rank_order_event_raster_debugger.on_update_epoch_IDX(an_epoch_idx=i)\n",
    "#     active_epoch_label = self.active_epoch_label\n",
    "\n",
    "#     save_paths = []\n",
    "\n",
    "#     for a_decoder, a_plot in self.root_plots_dict.items():\n",
    "#         curr_filename_prefix = f'Epoch{active_epoch_label}_{a_decoder}'\n",
    "#         # a_plot.setYRange(-0.5, float(self.max_n_neurons))\n",
    "#         out_path = export_path.joinpath(f'{curr_filename_prefix}_plot.png').resolve()\n",
    "#         export_pyqtgraph_plot(a_plot, savepath=out_path, background=pg.mkColor(0, 0, 0, 0))\n",
    "#         save_paths.append(out_path)\n",
    "\n",
    "#     all_save_paths[active_epoch_label] = save_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77194b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_long_short_laps', '_display_long_short_pf1D_comparison', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b267e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_two_step_decoder_prediction_error_2D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adcd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import Image, display\n",
    "import io\n",
    "from pyphocorehelpers.plotting.media_output_helpers import fig_to_clipboard\n",
    "\n",
    "\n",
    "# Generate the frames for the animation\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "line, = ax.plot(x, np.sin(x))\n",
    "\n",
    "def update(frame):\n",
    "    line.set_ydata(np.sin(x + frame / 10.0))\n",
    "    return line,\n",
    "\n",
    "frames = len(x) - 1\n",
    "ani = animation.FuncAnimation(fig, update, frames=frames, blit=True, repeat=True, interval=50)\n",
    "\n",
    "# To save the animation using Pillow as a gif\n",
    "_temp_gif_save_path = Path('scatter.gif').resolve()\n",
    "writer = animation.PillowWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "ani.save(_temp_gif_save_path, writer=writer)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save the animation to a BytesIO buffer\n",
    "# buf = io.BytesIO()\n",
    "# ani.save(buf, codec='gif', writer='imagemagick', fps=10)\n",
    "# buf.seek(0)\n",
    "\n",
    "# # Display the GIF\n",
    "# display(Image(data=buf.getvalue(), format='gif'))\n",
    "# Display the GIF\n",
    "assert _temp_gif_save_path.exists()\n",
    "Image(_temp_gif_save_path)\n",
    "\n",
    "\n",
    "# fig_to_clipboard(fig, format='gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7be129",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_long_short_laps')\n",
    "graphics_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedac3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs, plot_data = graphics_output_dict['fig'], graphics_output_dict['axs'], graphics_output_dict['plot_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_grid_bin_bounds_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68008d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_long_short_laps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685113bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_spike_rasters_pyqtplot_2D', 'maze_any') # 'maze_any'\n",
    "assert isinstance(_out_graphics_dict, dict)\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = _out_graphics_dict['spike_raster_plt_2d'], _out_graphics_dict['spike_raster_plt_3d'], _out_graphics_dict['spike_raster_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_renderables_menu = active_2d_plot.ui.menus.custom_context_menus.add_renderables[0].programmatic_actions_dict\n",
    "menu_commands = ['AddTimeIntervals.PBEs', 'AddTimeIntervals.Ripples', 'AddTimeIntervals.Replays', 'AddTimeIntervals.Laps', 'AddTimeIntervals.SessionEpochs']\n",
    "for a_command in menu_commands:\n",
    "    add_renderables_menu[a_command].trigger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5650ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(add_renderables_menu.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('add_renderables_menu', add_renderables_menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d_interactive_tuning_curves_plotter\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_3d_interactive_tuning_curves_plotter', active_session_configuration_context=global_epoch_context,\n",
    "                                            active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                            params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                           )\n",
    "ipcDataExplorer = _out_graphics_dict['ipcDataExplorer'] # InteractivePlaceCellTuningCurvesDataExplorer \n",
    "p = _out_graphics_dict['plotter']\n",
    "pane = _out_graphics_dict['pane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context) # , computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}]\n",
    "ipspikesDataExplorer = _out['ipspikesDataExplorer']\n",
    "p = _out['plotter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "\n",
    "an_image_file_path = Path('an_image.png').resolve()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_image_plotter', active_session_configuration_context=global_epoch_context, image_file=an_image_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ce93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_config in curr_active_pipeline.active_configs.items():\n",
    "    print(f'a_config.plotting_config.should_use_linear_track_geometry: {a_config.plotting_config.should_use_linear_track_geometry}')\n",
    "    a_config.plotting_config.should_use_linear_track_geometry = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d560b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "\n",
    "_out = TemplateDebugger.init_templates_debugger(track_templates) # , included_any_context_neuron_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1323cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "\n",
    "\n",
    "_out = batch_perform_all_plots(curr_active_pipeline=curr_active_pipeline, enable_neptune=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2D matrix\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import pv\n",
    "\n",
    "matrix = np.random.rand(10, 10)\n",
    "\n",
    "# Coordinates\n",
    "x, y = np.meshgrid(np.arange(matrix.shape[1]), np.arange(matrix.shape[0]))\n",
    "z = matrix.flatten()\n",
    "\n",
    "# Colors based on recency of updates (for example purposes, random values)\n",
    "colors = np.random.rand(matrix.size)\n",
    "\n",
    "# Create the plotter\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "# Add points (dots)\n",
    "points = np.column_stack((x.flatten(), y.flatten(), z))\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud['colors'] = colors\n",
    "plotter.add_mesh(point_cloud, render_points_as_spheres=True, point_size=10, scalars='colors', cmap='viridis')\n",
    "\n",
    "# Add stems\n",
    "for i in range(len(z)):\n",
    "    line = pv.Line([x.flatten()[i], y.flatten()[i], 0], [x.flatten()[i], y.flatten()[i], z[i]])\n",
    "    plotter.add_mesh(line, color='black')\n",
    "\n",
    "# Show plot\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23611eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot.display_function_items\n",
    "\n",
    "# '_display_directional_template_debugger'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "directional_laps_overview = curr_active_pipeline.display(display_function='_display_directional_laps_overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ee6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pic_placefields = curr_active_pipeline.display('_display_1d_placefields', long_LR_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pic_placefields_short_LR = curr_active_pipeline.display('_display_1d_placefields', short_LR_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c334080",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eafe14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f93040",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd15f5",
   "metadata": {},
   "source": [
    "#  2024-04-23 - 3D Posterior Plot\n",
    "<!-- t_delta -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7ac95",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=global_epoch_context,\n",
    "                                    params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                    )\n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "pActiveInteractiveLapsPlotter = _out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "pActiveInteractiveLapsPlotter[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "pActiveInteractiveLapsPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer.active_config.plotting_config.subplots_shape # '1|5'\n",
    "iplapsDataExplorer.active_config.plotting_config.plotter_type # 'BackgroundPlotter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots_shape_str: str = '1|5'\n",
    "subplots_shape_arr_strs = subplots_shape_str.split('|')\n",
    "\n",
    "subplots_shape = [int(k) for k in subplots_shape_arr_strs]\n",
    "subplots_shape\n",
    "\n",
    "total_n_plots: int = np.prod(subplots_shape)\n",
    "if total_n_plots > 1:\n",
    "    iplapsDataExplorer.active_config.plotting_config.plotter_type = 'BackgroundPlotter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iplapsDataExplorer.p.\n",
    "\n",
    "p = iplapsDataExplorer.p[0,0]\n",
    "p\n",
    "# p = self.p[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_global = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context) # , config_override_kwargs={'plotting_config': {'should_use_linear_track_geometry': True}}\n",
    "# ipspikesDataExplorer = _out_global['ipspikesDataExplorer']\n",
    "# p = _out_global['plotter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: active_config\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "_out_global = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context,\n",
    "                                            active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                            params_kwargs=dict(enable_historical_spikes=False, enable_recent_spikes=False, should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                           )\n",
    "ipspikesDataExplorer = _out_global['ipspikesDataExplorer']\n",
    "p = _out_global['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k, v in active_config_modifiying_kwargs.items():\n",
    "    curr_subdict = active_config.get(k, {})\n",
    "    for sub_k, sub_v in v.items():\n",
    "        try:\n",
    "            curr_subdict[sub_k] = sub_v # apply the update\n",
    "        except TypeError as err:\n",
    "            # TypeError: 'PlottingConfig' object does not support item assignment\n",
    "            setattr(curr_subdict, sub_k, sub_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27140db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_config.plotting_config.should_use_linear_track_geometry\n",
    "active_config.plotting_config.t_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.time_animations import TrackConfigurationTimeAnimationRoutine\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "custom_track_animatior: TrackConfigurationTimeAnimationRoutine = TrackConfigurationTimeAnimationRoutine(t_start=t_start, t_delta=t_delta, t_end=t_end, \n",
    "        long_maze_bg=ipspikesDataExplorer.plots['long_maze_bg'], short_maze_bg=ipspikesDataExplorer.plots['short_maze_bg'],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveSliderWrapper import InteractiveSliderWrapper\n",
    "\n",
    "# interactive_plotter = ipspikesDataExplorer.ui.interactive_plotter # PhoInteractivePlotter\n",
    "\n",
    "active_timestamp_slider_wrapper: InteractiveSliderWrapper = ipspikesDataExplorer.ui.interactive_plotter.interface_properties.active_timestamp_slider_wrapper # InteractiveSliderWrapper \n",
    "active_timestamp_slider_wrapper.curr_value # 17659.517659\n",
    "active_timestamp_slider_wrapper.curr_index # 17659\n",
    "\n",
    "\n",
    "curr_i: int = int(active_timestamp_slider_wrapper.curr_index)\n",
    "active_window_sample_indicies = np.squeeze(ipspikesDataExplorer.params.pre_computed_window_sample_indicies[curr_i,:]) # Get the current precomputed indicies for this curr_i\n",
    "\n",
    "## Spike Plotting:\n",
    "# Get the times that fall within the current plot window:\n",
    "curr_time_fixedSegments = ipspikesDataExplorer.t[active_window_sample_indicies] # New Way\n",
    "t_start = curr_time_fixedSegments[0]\n",
    "t_stop = curr_time_fixedSegments[-1]\n",
    "\n",
    "# \n",
    "t_start, t_stop\n",
    "# custom_track_animatior.on_update_current_window(t_start=t_start, t_stop=t_stop)\n",
    "# curr_index\n",
    "active_timestamp_slider_wrapper.slider_obj.SetEnabled(False) # hide the typical timestamp slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_one_step_decoder = deepcopy(global_results.pf2D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _update_nearest_decoded_most_likely_position_callback, _conn = add_nearest_decoded_position_indicator_circle(self, active_one_step_decoder, _debug_print = False)\n",
    "\n",
    "_update_nearest_decoded_most_likely_position_callback, _conn = ipspikesDataExplorer.add_nearest_decoded_position_indicator_circle(active_one_step_decoder=active_one_step_decoder, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter\n",
    "\n",
    "## plots a decoder posterior viewer with two sliders: one for epoch_idx and another for epoch_time_bin_idx within that epoch\n",
    "active_one_step_decoder = deepcopy(global_results.pf2D_Decoder) # just used for position binning info\n",
    "# a_result: DecodedFilterEpochsResult = deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'])\n",
    "a_result: DecodedFilterEpochsResult = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'])\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = ipspikesDataExplorer.add_decoded_posterior_bars(a_result=a_result,\n",
    "                                                                                                                         xbin=active_one_step_decoder.xbin, xbin_centers=active_one_step_decoder.xbin_centers, ybin=active_one_step_decoder.ybin, ybin_centers=active_one_step_decoder.ybin_centers,\n",
    "                                                                                                                         enable_plot_all_time_bins_in_epoch_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer.params.curr_view_window_length_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79651414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer.clear_all_added_decoded_posterior_plots()\n",
    "ipspikesDataExplorer.p.clear_slider_widgets() # does not actually clear the added sliders\n",
    "ipspikesDataExplorer.on_slider_update_mesh(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ipspikesDataExplorer.params.curr_view_window_length_samples # 299\n",
    "ipspikesDataExplorer.params.curr_view_window_length_samples = 60.0 * 5.0 * ipspikesDataExplorer.active_session.position.sampling_rate # 5 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_interactions.widgets import RangeSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipspikesDataExplorer.add_grid_bin_bounds_box(\n",
    "ipspikesDataExplorer.on_slider_update_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59bafd",
   "metadata": {},
   "source": [
    "#  2024-02-28 - WE gotta see the replays on the 3D track. Or the 2D track.\n",
    "2024-04-28 - This is working in both 3D and 2D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ad549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: directional_laps_results, global_replays, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "# global_pf1D\n",
    "# long_replays\n",
    "# direction_max_indices = ripple_all_epoch_bins_marginals_df[['P_Long', 'P_Short']].values.argmax(axis=1)\n",
    "# track_identity_max_indices = ripple_all_epoch_bins_marginals_df[['P_Long', 'P_Short']].values.argmax(axis=1)\n",
    "\n",
    "## How do I get the replays?\n",
    "# long_replay_df: pd.DataFrame = long_replays.to_dataframe() ## These work.\n",
    "# global_replay_df: pd.DataFrame = global_replays.to_dataframe() ## These work.\n",
    "# global_replay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1D version:\n",
    "## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "xbin = deepcopy(directional_laps_results.get_decoders()[0].xbin)\n",
    "xbin_centers = deepcopy(directional_laps_results.get_decoders()[0].xbin_centers)\n",
    "ybin_centers = None\n",
    "ybin = None\n",
    "\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "## 1D:\n",
    "a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "\n",
    "## OUTPUTS: a_decoded_filter_epochs_decoder_result_dict, xbin_centers, ybin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cdc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D version:\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_lap_and_ripple_epochs_decoding_for_decoder\n",
    "\n",
    "## INPUTS: long_results, short_results\n",
    "# long_one_step_decoder_2D\n",
    "\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "one_step_decoder_dict_2D: Dict[str, BayesianPlacemapPositionDecoder] = dict(zip(('long', 'short'), (long_one_step_decoder_2D, short_one_step_decoder_2D)))\n",
    "long_pf2D = long_results.pf2D\n",
    "# short_pf2D = short_results.pf2D\n",
    "\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "xbin_centers = deepcopy(long_pf2D.xbin_centers)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "ybin_centers = deepcopy(long_pf2D.ybin_centers)\n",
    "\n",
    "## OUTPUTS: one_step_decoder_dict_2D, xbin_centers, ybin_centers\n",
    "\n",
    "## INPUTS: one_step_decoder_dict_2D\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "pos_bin_size: Tuple[float, float] = list(one_step_decoder_dict_2D.values())[0].pos_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "\n",
    "## Decode epochs for the two decoders ('long', 'short'):\n",
    "LS_decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {}\n",
    "LS_decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {}\n",
    "\n",
    "for a_name, a_decoder in one_step_decoder_dict_2D.items():\n",
    "    LS_decoder_laps_filter_epochs_decoder_result_dict[a_name], LS_decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_lap_and_ripple_epochs_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "\n",
    "# LS_decoder_ripple_filter_epochs_decoder_result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D:\n",
    "# Choose the ripple epochs to plot:\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(LS_decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long'] # 2D\n",
    "# Choose the laps epochs to plot:\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(LS_decoder_laps_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "\n",
    "# a_result: DecodedFilterEpochsResult = LS_decoder_laps_filter_epochs_decoder_result_dict['long'] # 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "\n",
    "## INPUTS: a_result: DecodedFilterEpochsResult, an_epoch_idx: int = 18\n",
    "# e.g. `a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR']`\n",
    "\n",
    "# a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "\n",
    "## Convert to plottable posteriors\n",
    "# an_epoch_idx: int = 0\n",
    "\n",
    "# valid_aclus = deepcopy(decoder_aclu_peak_location_df_merged.aclu.unique())\n",
    "num_filter_epochs: int = a_result.num_filter_epochs\n",
    "a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers)\n",
    "fig, axs, laps_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(global_session, curr_num_subplots=8, active_page_index=0, plot_actual_lap_lines=False, use_theoretical_tracks_instead=True)\n",
    "\n",
    "integer_slider = a_decoded_traj_plotter.plot_epoch_with_slider_widget(an_epoch_idx=6)\n",
    "integer_slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(laps_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps[0].remove()\n",
    "\n",
    "# an_ax.remove(heatmaps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_ax = axs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plotActors, data_dict = plot_3d_stem_points(pCustom, active_epoch_placefields2D.ratemap.xbin, active_epoch_placefields2D.ratemap.ybin, active_epoch_placefields2D.ratemap.occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4627224",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_plot(value=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cfc42",
   "metadata": {},
   "source": [
    "## add to 3D plotter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12058a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter\n",
    "from pyphoplacecellanalysis.Pho3D.PyVista.graphs import plot_3d_stem_points, plot_3d_binned_bars\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=global_epoch_context,\n",
    "                                    params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                    )\n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "pActiveInteractiveLapsPlotter = _out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b011ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: a_result, xbin_centers, ybin_centers, iplapsDataExplorer\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = DecodedTrajectoryPyVistaPlotter(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, p=iplapsDataExplorer.p)\n",
    "# a_decoded_trajectory_pyvista_plotter.build_ui()\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=True)\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=False, active_plot_fn=plot_3d_stem_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=False, active_plot_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer.clear_all_added_decoded_posterior_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968821ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_plot_fn = a_decoded_trajectory_pyvista_plotter.data_dict['plot_3d_binned_bars[55.63197815967686]']['update_plot_fn']\n",
    "# update_plot_fn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_posterior_p_x_given_n, n_epoch_timebins = a_decoded_trajectory_pyvista_plotter._perform_get_curr_posterior(a_result=a_result, an_epoch_idx=a_decoded_trajectory_pyvista_plotter.curr_epoch_idx, time_bin_index=np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins))\n",
    "# np.shape(a_posterior_p_x_given_n)\n",
    "\n",
    "\n",
    "a_posterior_p_x_given_n, n_epoch_timebins = a_decoded_trajectory_pyvista_plotter.get_curr_posterior(an_epoch_idx=a_decoded_trajectory_pyvista_plotter.curr_epoch_idx, time_bin_index=np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins))\n",
    "np.shape(a_posterior_p_x_given_n)\n",
    "\n",
    "n_epoch_timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = a_decoded_trajectory_pyvista_plotter.plotActors['plot_3d_binned_bars[49.11980797704307]']\n",
    "# v['main'].remove()\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter.p.remove_actor(v['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.graphs import clear_3d_binned_bars_plots\n",
    "\n",
    "clear_3d_binned_bars_plots(p=a_decoded_trajectory_pyvista_plotter.p, plotActors=a_decoded_trajectory_pyvista_plotter.plotActors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.plotActors_CenterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.perform_update_plot_epoch_time_bin_range(value=None) # select all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.perform_clear_existing_decoded_trajectory_plots()\n",
    "iplapsDataExplorer.p.update()\n",
    "iplapsDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_index = np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins)\n",
    "type(time_bin_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.slider_epoch.RemoveAllObservers()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch.Off()\n",
    "# a_decoded_trajectory_pyvista_plotter.slider_epoch.FastDelete()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch = None\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.RemoveAllObservers()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.Off()\n",
    "# a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.FastDelete()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin = None\n",
    "iplapsDataExplorer.p.clear_slider_widgets()\n",
    "iplapsDataExplorer.p.update()\n",
    "iplapsDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66de9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecoderRenderingPyVistaMixin\n",
    "\n",
    "(plotActors, data_dict), (plotActors_CenterLabels, data_dict_CenterLabels) = DecoderRenderingPyVistaMixin.perform_plot_posterior_bars(iplapsDataExplorer.p, xbin=xbin, ybin=ybin, xbin_centers=xbin_centers, ybin_centers=ybin_centers,\n",
    "                                               posterior_p_x_given_n=a_posterior_p_x_given_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d46f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.20720657697753883 * 24.130508176591324"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293adac0",
   "metadata": {},
   "source": [
    "#  Rasters Debugger (via `RankOrderRastersDebugger`)\n",
    "<!-- ![image.png|350](attachment:image.png) -->\n",
    "![image.png](attachment:image.png){ width=300; max-width: 300px; }\n",
    "<!-- <img src=\"path_to_your_image.png\" style=\"max-width: 300px;\" /> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff576652",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "global_laps_epochs_df = global_laps.to_dataframe()\n",
    "\n",
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "_out_laps_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, global_laps_epochs_df, track_templates, rank_order_results, RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_laps_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a586d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "# global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "# global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "# global_laps_epochs_df = global_laps.to_dataframe()\n",
    "global_ripple_epochs_df = global_replays.to_dataframe()\n",
    "\n",
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "_out_ripple_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, global_ripple_epochs_df, track_templates, rank_order_results, RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_ripple_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "# rank_order_results\n",
    "# used_rank_order_results = deepcopy(rank_order_results)\n",
    "used_rank_order_results = None\n",
    "_out_ripple_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, deepcopy(filtered_ripple_simple_pf_pearson_merged_df),\n",
    "                                                                                                   track_templates, used_rank_order_results,\n",
    "                                                                                                    RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_ripple_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "_out_ripple_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, deepcopy(filtered_ripple_simple_pf_pearson_merged_df),\n",
    "                                                                                                   track_templates, None,\n",
    "                                                                                                    None, None,\n",
    "                                                                                                    dock_add_locations = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (['right'], ['right'], ['right'], ['right']))),\n",
    "                                                                                                    )\n",
    "_out_ripple_rasters.set_top_info_bar_visibility(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_ripple_rasters.set_top_info_bar_visibility(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700afa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide top info bar:\n",
    "LongShortColumnsInfo_dock_layout, LongShortColumnsInfo_dock_Dock = _out_ripple_rasters.plots.dock_widgets['LongShortColumnsInfo_dock']\n",
    "# LongShortColumnsInfo_dock_layout.hide() # No use\n",
    "# _out_ripple_rasters.ui.long_short_info_layout.hide() # No use\n",
    "LongShortColumnsInfo_dock_Dock.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95532207",
   "metadata": {},
   "outputs": [],
   "source": [
    "LongShortColumnsInfo_dock_Dock.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_IDX = _out_ripple_rasters.find_nearest_time_index(193.65)\n",
    "# if found_IDX is not None:\n",
    "#     print(f'found_IDX: {found_IDX}')\n",
    "#     _out_ripple_rasters.programmatically_update_epoch_IDX(found_IDX)\n",
    "\n",
    "\n",
    "_out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(193.65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965556b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_ripple_rasters.on_update_epoch_IDX(45)\n",
    "# on_update_epoch_IDX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a_ScrollBarWithSpinBox = _out_ripple_rasters.ui.ctrls_widget # ScrollBarWithSpinBox \n",
    "_a_ScrollBarWithSpinBox.setValue(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_directional_template_debugger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_template_debugger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_two_step_decoder_prediction_error_2D', global_epoch_context, variable_name='p_x_given_n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_plot_most_likely_position_comparisons', global_epoch_context) # , variable_name='p_x_given_n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d805c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_display_directional_merged_pfs'\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=False, plot_long_directional=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47076a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_1d_placefield_occupancy'\n",
    "'_display_placemaps_pyqtplot_2D'\n",
    " '_display_2d_placefield_occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_2d_placefield_occupancy', global_any_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_grid_bin_bounds_validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58951b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import add_rectangular_selector, add_range_selector\n",
    "\n",
    "\n",
    "# epoch_name = global_any_name\n",
    "epoch_name = short_epoch_name\n",
    "computation_result = curr_active_pipeline.computation_results[epoch_name]\n",
    "grid_bin_bounds = computation_result.computation_config['pf_params'].grid_bin_bounds\n",
    "epoch_context = curr_active_pipeline.filtered_contexts[epoch_name]\n",
    "            \n",
    "fig, ax = computation_result.computed_data.pf2D.plot_occupancy(identifier_details_list=[epoch_name], active_context=epoch_context) \n",
    "\n",
    "# rect_selector, set_extents, reset_extents = add_rectangular_selector(fig, ax, initial_selection=grid_bin_bounds) # (24.82, 257.88), (125.52, 149.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_vertical_track_bounds_lines\n",
    "\n",
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_track_line_collection, short_track_line_collection = add_vertical_track_bounds_lines(grid_bin_bounds=grid_bin_bounds, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b862a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import compute_placefield_center_of_mass_positions\n",
    "\n",
    "\n",
    "epoch_name = global_any_name\n",
    "computation_result = curr_active_pipeline.computation_results[epoch_name]\n",
    "grid_bin_bounds = deepcopy(computation_result.computation_config['pf_params'].grid_bin_bounds)\n",
    "epoch_context = curr_active_pipeline.filtered_contexts[epoch_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_pf2D.xbin\n",
    "long_pf2D.ybin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = deepcopy(long_pf2D.occupancy) # occupancy.shape # (60, 15)\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0416d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage # used for `compute_placefield_center_of_masses`\n",
    "from neuropy.utils.mixins.peak_location_representing import compute_occupancy_center_of_mass_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6352663",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_x_center_dict = {k:compute_occupancy_center_of_mass_positions(v.pf.occupancy, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n",
    "occupancy_x_center_dict # {'long_LR': 162.99271603199625, 'long_RL': 112.79866056603696, 'short_LR': 138.45611791646, 'short_RL': 130.78889937230684}\n",
    "\n",
    "occupancy_mask_x_center_dict = {k:compute_occupancy_center_of_mass_positions(v.pf.visited_occupancy_mask, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n",
    "occupancy_mask_x_center_dict # {'long_LR': 135.66781520875904, 'long_RL': 130.0042755113645, 'short_LR': 133.77996864296085, 'short_RL': 143.21920147195175}\n",
    "\n",
    "\n",
    "# {k:compute_occupancy_center_of_mass_positions(v.pf.occupancy, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb029d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = deepcopy(long_pf2D.occupancy) # occupancy.shape # (60, 15)\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "\n",
    "# masked_nonzero_occupancy = deepcopy(long_pf2D.nan_never_visited_occupancy)\n",
    "\n",
    "masked_nonzero_occupancy = deepcopy(long_pf2D.visited_occupancy_mask)\n",
    "\n",
    "# occupancy_CoM_positions = compute_occupancy_center_of_mass_positions(occupancy, xbin=long_pf2D.xbin, ybin=long_pf2D.ybin)\n",
    "occupancy_CoM_positions = compute_occupancy_center_of_mass_positions(masked_nonzero_occupancy, xbin=long_pf2D.xbin, ybin=long_pf2D.ybin) # array([127.704, 145.63])\n",
    "occupancy_CoM_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf2D.nan_never_visited_occupancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4caa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict\n",
    "\n",
    "\n",
    "'_display_grid_bin_bounds_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f963a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting on 2024-02-06 to display the LR/RL directions instead of the All/Long/Short pfs:\n",
    "def _display_directional_merged_pfs(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "\t\t\t\t\t\t\t\t\tplot_all_directions=True, plot_long_directional=False, plot_short_directional=False, **kwargs):\n",
    "\t\"\"\" Plots the merged pseduo-2D pfs/ratemaps. Plots: All-Directions, Long-Directional, Short-Directional in seperate windows. \n",
    "\t\n",
    "\tHistory: this is the Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "\t\"\"\"\n",
    "\tfrom pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "\tfrom pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow \n",
    "\tfrom pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import LayoutScrollability\n",
    "\n",
    "\tdefer_render = kwargs.pop('defer_render', False)\n",
    "\tdirectional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\tactive_merged_pf_plots_data_dict = {} #empty dict\n",
    "\t\n",
    "\tif plot_all_directions:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.all_directional_pf1D_Decoder.pf # all-directions\n",
    "\tif plot_long_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.long_directional_pf1D_Decoder.pf # Long-only\n",
    "\tif plot_short_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Short-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.short_directional_pf1D_Decoder.pf # Short-only\n",
    "\n",
    "\tout_plots_dict = {}\n",
    "\t\n",
    "\tfor active_context, active_pf_2D in active_merged_pf_plots_data_dict.items():\n",
    "\t\t# figure_format_config = {} # empty dict for config\n",
    "\t\tfigure_format_config = {'scrollability_mode': LayoutScrollability.NON_SCROLLABLE} # kwargs # kwargs as default figure_format_config\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig: BasicBinnedImageRenderingWindow  = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "\t\n",
    "\t\t# Set the window title from the context\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "\t\tout_plots_dict[active_context] = out_all_pf_2D_pyqtgraph_binned_image_fig\n",
    "\n",
    "\t\t# Tries to update the display of the item:\n",
    "\t\tnames_list = [v for v in list(out_all_pf_2D_pyqtgraph_binned_image_fig.plots.keys()) if v not in ('name', 'context')]\n",
    "\t\tfor a_name in names_list:\n",
    "\t\t\t# Adjust the size of the text for the item by passing formatted text\n",
    "\t\t\ta_plot: pg.PlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[a_name].mainPlotItem # PlotItem \n",
    "\t\t\t# no clue why 2 is a good value for this...\n",
    "\t\t\ta_plot.titleLabel.setMaximumHeight(2)\n",
    "\t\t\ta_plot.layout.setRowFixedHeight(0, 2)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif not defer_render:\n",
    "\t\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.show()\n",
    "\n",
    "\treturn out_plots_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e434945",
   "metadata": {},
   "source": [
    "# 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import ContinuousPeakLocationRepresentingMixin\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from scipy.signal import find_peaks\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', sortby=None)\n",
    "\n",
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap: Ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, peaks_results_df = active_ratemap.compute_tuning_curve_modes(height=0.2, width=None)\n",
    "\n",
    "\n",
    "included_columns = ['pos', 'peak_heights'] # the columns of interest that you want in the final dataframe.\n",
    "included_columns_renamed = dict(zip(included_columns, ['peak', 'peak_height']))\n",
    "decoder_peaks_results_dfs = [a_decoder.pf.ratemap.get_tuning_curve_peak_df(height=0.2, width=None) for a_decoder in (track_templates.long_LR_decoder, track_templates.long_RL_decoder, track_templates.short_LR_decoder, track_templates.short_RL_decoder)]\n",
    "prefix_names = [f'{a_decoder_name}_' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "all_included_columns = ['aclu', 'series_idx', 'subpeak_idx'] + included_columns # Used to filter out the unwanted columns from the output\n",
    "\n",
    "# [['aclu', 'series_idx', 'subpeak_idx', 'pos']]\n",
    "\n",
    "# rename_list_fn = lambda a_prefix: {'pos': f\"{a_prefix}pos\"}\n",
    "rename_list_fn = lambda a_prefix: {a_col_name:f\"{a_prefix}{included_columns_renamed[a_col_name]}\" for a_col_name in included_columns}\n",
    "\n",
    "# column_names = [f'{a_decoder_name}_peak' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "\n",
    "# dataFrames = decoder_peaks_results_dfs\n",
    "# names = self.get_decoder_names()\n",
    "\n",
    "# rename 'pos' column in each dataframe and then reduce to perform cumulative outer merge\n",
    "result_df = decoder_peaks_results_dfs[0][all_included_columns].rename(columns=rename_list_fn(prefix_names[0]))\n",
    "for df, a_prefix in zip(decoder_peaks_results_dfs[1:], prefix_names[1:]):\n",
    "    result_df = pd.merge(result_df, df[all_included_columns].rename(columns=rename_list_fn(a_prefix)), on=['aclu', 'series_idx', 'subpeak_idx'], how='outer')\n",
    "\n",
    "# result = reorder_columns(result, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "# list(filter(lambda column: column.endswith('_peak_heights'), result.columns))\n",
    "# result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))\n",
    "# result_df\n",
    "\n",
    "# print(list(result.columns))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "result_df: pd.DataFrame = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), relative_mode='end').sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually Excluded endcap aclus:\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "excluded_endcap_aclus: NDArray = np.array(list(set([40, 60, 85, 102, 52, 6] + [83, 60, 52, 102, 40] + [59, 67, 95, 28, 101] + [14, 15, 87, 71] + [43, 84, 87, 19, 33, 51, 53])))\n",
    "excluded_endcap_aclus\n",
    "\n",
    "\n",
    "np.array([  6,  14,  15,  19,  28,  33,  40,  43,  51,  52,  53,  59,  60,  67,  71,  83,  84,  85,  87,  95, 101, 102])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_results_df = track_templates.get_decoders_aclu_peak_location_df().sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True) ## Does not seem to merge entries as I would expect via intution. It keeps LR/RL peaks distinct and leaves pd.NA values for the entries.\n",
    "peaks_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c29838",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict: Dict = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index().set_index('aclu').to_dict()['subpeak_idx_count'] # number of peaks (\"models\" for each aclu)\n",
    "aclu_n_peaks_dict\n",
    "\n",
    "# peaks_results_df = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index()\n",
    "\n",
    "# peaks_results_df[peaks_results_df.aclu == 5]\n",
    "# peaks_results_df.aclu.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ratemap.n_neurons\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=active_ratemap.neuron_ids, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aclu_n_peaks_dict\n",
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c9e39",
   "metadata": {},
   "source": [
    "#  2024-02-08 - `PhoPaginatedMultiDecoderDecodedEpochsWindow` - Plot Ripple Metrics like Radon Transforms, WCorr, Simple Pearson, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf989bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "## INPUTS: decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "\n",
    "## filter the epochs by something and only show those:\n",
    "# INPUTS: filtered_epochs_df\n",
    "# filtered_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(active_epochs_df[['start', 'stop']].to_numpy())\n",
    "## Update the `decoder_ripple_filter_epochs_decoder_result_dict` with the included epochs:\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# print(f\"any_good_selected_epoch_times.shape: {any_good_selected_epoch_times.shape}\") # (142, 2)\n",
    "\n",
    "pre_cols = {a_name:set(a_result.filter_epochs.columns) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "\n",
    "#  2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "filtered_decoder_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "# _out_new_scores\n",
    "## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# ## Constrain again now by the user selections\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(any_good_selected_epoch_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Instead, add in the 'is_user_annotated_epoch' column instead of filtering\n",
    "## INPUTS: any_good_selected_epoch_times\n",
    "num_user_selected_times: int = len(any_good_selected_epoch_times)\n",
    "print(f'num_user_selected_times: {num_user_selected_times}')\n",
    "any_good_selected_epoch_indicies = None\n",
    "print(f'adding user annotation column!')\n",
    "\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "\n",
    "## OUT: filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "# ## specifically long_LR\n",
    "# filter_epochs: pd.DataFrame = deepcopy(ensure_dataframe(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs))\n",
    "# filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65255fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## specifically long_LR\n",
    "filter_epochs: pd.DataFrame = deepcopy(ensure_dataframe(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs))\n",
    "filter_epochs\n",
    "\n",
    "# np.sum(filter_epochs['pearsonr'].isna())\n",
    "\n",
    "new_heuristic_checking_columns = ['total_variation', 'integral_second_derivative', 'stddev_of_diff', 'score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79929f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epochs.groupby('is_user_annotated_epoch').agg(['mean', 'std', 'var', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaaf5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epochs.to_csv('output/2024-05-10_new_heuristics_for_ripples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd98310",
   "metadata": {},
   "source": [
    "### 2024-05-09 - get the most-likely decoder for each epoch using the sequenceless probabilities and used this to selected the appopriate column for each of the heuristic measures.\n",
    "Modifies `extracted_merged_scores_df`, adding \"*_BEST\" columns for each specified heuristic score column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_merged_scores_df: pd.DataFrame =  directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "extracted_merged_scores_df\n",
    "\n",
    "ripple_weighted_corr_merged_df = deepcopy(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df)\n",
    "\n",
    "## Need 'best_decoder_index':... actually 'most_likely_decoder_index'\n",
    "\n",
    "# best_decoder_index = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs['best_decoder_index']) # hope this is correct and not just like the best wcorr or something\n",
    "best_decoder_index = deepcopy(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df['most_likely_decoder_index'])\n",
    "\n",
    "new_heuristic_checking_columns = ['total_variation', 'integral_second_derivative', 'stddev_of_diff', 'score'] # , 'integral_second_derivative', 'stddev_of_diff', 'score'\n",
    "# best_decoder_names = [['long_LR', 'long_RL', 'short_LR', 'short_RL'][an_idx] for an_idx in best_decoder_index]\n",
    "## Example: extracted_merged_scores_df[['total_variation_long_LR', 'total_variation_long_RL', 'total_variation_short_LR', 'total_variation_short_RL']]\n",
    "\n",
    "for a_score_col in new_heuristic_checking_columns:\n",
    "    curr_score_col_decoder_col_names = [f\"{a_score_col}_{a_decoder_name}\" for a_decoder_name in ['long_LR', 'long_RL', 'short_LR', 'short_RL']]\n",
    "    print(f'curr_score_col_decoder_col_names: {curr_score_col_decoder_col_names}')\n",
    "    # extracted_merged_scores_df\n",
    "    _final_out = [extracted_merged_scores_df[curr_score_col_decoder_col_names].to_numpy()[epoch_idx, a_decoder_idx] for epoch_idx, a_decoder_idx in zip(np.arange(np.shape(extracted_merged_scores_df)[0]), best_decoder_index.to_numpy())]\n",
    "    extracted_merged_scores_df[f\"{a_score_col}_BEST\"] = _final_out # extracted_merged_scores_df[curr_score_col_decoder_col_names].to_numpy()[best_decoder_index]\n",
    "\n",
    "extracted_merged_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_merged_scores_df.groupby('is_user_annotated_epoch').agg(['mean', 'min', 'max', 'std']) ## successfully got the most-likely decoder for each epoch using the sequenceless probabilities and used this to selected the appopriate column for each of the heuristic measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34a23a",
   "metadata": {},
   "source": [
    "### Continue something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epochs_df_dict = {k:deepcopy(v.filter_epochs) for k,v in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "# filter_epochs_df_dict\n",
    "\n",
    "high_wcorr_filter_epochs_dict = {k:np.where((v['wcorr'].abs() >= 0.9))[0] for k,v in filter_epochs_df_dict.items()}\n",
    "# high_wcorr_filter_epochs_dict\n",
    "\n",
    "high_wcorr_any_epochs = union_of_arrays(*[v for k,v in high_wcorr_filter_epochs_dict.items()]) # get unique indicies\n",
    "# high_wcorr_any_epochs\n",
    "\n",
    "# high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "high_wcorr_included_epoch_times = {k:v.iloc[high_wcorr_any_epochs][['start', 'stop']].to_numpy() for k,v in filter_epochs_df_dict.items()}\n",
    "# high_wcorr_included_epoch_times\n",
    "\n",
    "high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(high_wcorr_included_epoch_times[a_name]) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find high wcorr values:\n",
    "filter_epochs = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "\n",
    "# np.sum((filter_epochs['wcorr'].abs() > 0.9))\n",
    "\n",
    "np.where((filter_epochs['wcorr'].abs() > 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a6b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs.directionality_ratio.unique()\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs.sweep_score.unique()\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs.laplacian_smoothness.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a merged (single df) frame\n",
    "decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb235522",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epochs_df\n",
    "filtered_epochs_df[filtered_epochs_df['start'] >= t_delta]\n",
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_decoded_epochs_result_dict: generic\n",
    "app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                decoder_decoded_epochs_result_dict=filtered_decoder_filter_epochs_decoder_result_dict,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict,\n",
    "                                                                                                epochs_name='ripple',\n",
    "                                                                                                included_epoch_indicies=None, debug_print=False,\n",
    "                                                                                                params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 10,\n",
    "                                                                                                    'scrollable_figure': False,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': False,\n",
    "                                                                                                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "# _tmp_out_selections = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca820df",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_out_selections = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289385ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6097ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get radon transform data:\n",
    "a_pagination_controller = pagination_controller_dict['long_LR']\n",
    "radon_transform_data = a_pagination_controller.plots_data['radon_transform_data']\n",
    "radon_transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30830fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120293e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_selections_dict = paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "# paginated_multi_decoder_decoded_epochs_window.ui.print = print\n",
    "_annotations = paginated_multi_decoder_decoded_epochs_window.print_user_annotations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d82308",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination_controller_dict['long_LR'].params.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eeaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpldatacursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f785638",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3435812",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.params.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show crosshair at cursor position\n",
    "plt.connect('motion_notify_event', lambda event: plt.gcf().gca().format_coord(event.xdata, event.ydata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c382b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import ClickedCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc42b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.params.debug_print = True\n",
    "    if not a_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_middle_click_item_callbacks'] = {}    \n",
    "    a_pagination_controller.params.on_middle_click_item_callbacks['copy_to_epoch_times_to_clipboard_callback'] = ClickedCallbacks.copy_to_epoch_times_to_clipboard_callback\n",
    "\n",
    "clicked_epoch = np.array([132.51138943410479, 132.79100273095537])\n",
    "\n",
    "clicked_epoch = np.array([149.95935746072792, 150.25439218967222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _build_attached_raster_viewer\n",
    "\n",
    "_out_ripple_rasters = _build_attached_raster_viewer(paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=filtered_ripple_simple_pf_pearson_merged_df)\n",
    "\n",
    "## Enable programmatically updating the rasters viewer to the clicked epoch index when middle clicking on a posterior.\n",
    "@function_attributes(short_name=None, tags=['callback', 'raster'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:13', related_items=[])\n",
    "def update_attached_raster_viewer_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" Enable programmatically updating the rasters viewer to the clicked epoch index when middle clicking on a posterior. \n",
    "    called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'update_attached_raster_viewer_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)\n",
    "\n",
    "\n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.params.debug_print = True\n",
    "    if not a_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_middle_click_item_callbacks'] = {}\n",
    "    a_pagination_controller.params.on_middle_click_item_callbacks['update_attached_raster_viewer_epoch_callback'] = update_attached_raster_viewer_epoch_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_epoch_start_stop_time = [488.296 488.484]\n",
    "start_t = 488.29642327222973\n",
    "found_IDX = 24\n",
    "\n",
    "# ripple_idx=80, ripple_start_t=488.29642327222973\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_attributes(short_name=None, tags=['callback'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:16', related_items=[])\n",
    "def an_alt_clicked_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'an_alt_clicked_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1998f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable programmatically updating the rasters viewer to the clicked epoch index when middle clicking on a posterior.\n",
    "@function_attributes(short_name=None, tags=['callback'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:16', related_items=[])\n",
    "def an_alt_clicked_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'an_alt_clicked_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)\n",
    "\n",
    "\n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.params.debug_print = True\n",
    "    if not a_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_middle_click_item_callbacks'] = {}    \n",
    "    a_pagination_controller.params.on_middle_click_item_callbacks['an_alt_clicked_epoch_callback'] = an_alt_clicked_epoch_callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to set identical low and high xlims makes transformation singular; automatically expanding. Is this what is causing the white posteriors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR'].params.posterior_heatmap_imshow_kwargs = dict(vmin=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23369f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.update_params(posterior_heatmap_imshow_kwargs = dict(vmin=0.0))\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.update_params(enable_per_epoch_action_buttons = True)\n",
    "paginated_multi_decoder_decoded_epochs_window.refresh_current_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.get_children_props('params')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('plots')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('plots.fig')\n",
    "paginated_multi_decoder_decoded_epochs_window.get_children_props('plots.fig')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('params.posterior_heatmap_imshow_kwargs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ca528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window# AttributeError: 'PhoPaginatedMultiDecoderDecodedEpochsWindow' object has no attribute 'params'\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR'].params.should_suppress_callback_exceptions = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a19394",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea69a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f136d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2150f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # v.params.enable_radon_transform_info = False\n",
    "    # v.params.enable_weighted_correlation_info = False\n",
    "    v._subfn_clear_selectability_rects()\n",
    "    \n",
    "# paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_ctrlr in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    a_ctrlr.perform_update_selections(defer_render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009775d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[785.7379401021171, 785.9232737672282]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[427.4610240198672, 427.55720829055645]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[833.3391086903866, 833.4508065531263]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[491.7975491596153, 492.17844624456484], [940.0164351915009, 940.2191870877286]]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [array([785.738, 785.923])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [array([427.461, 427.557])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [array([833.339, 833.451])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [array([491.798, 492.178]), array([940.016, 940.219])]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[785.7379401021171, 785.9232737672282]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[427.4610240198672, 427.55720829055645]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[833.3391086903866, 833.4508065531263]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[491.7975491596153, 492.17844624456484], [940.0164351915009, 940.2191870877286]]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[208.356, 208.523], [693.842, 693.975], [954.574, 954.679]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[224.037, 224.312]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[145.776, 146.022], [198.220, 198.582], [220.041, 220.259], [511.570, 511.874], [865.238, 865.373]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[191.817, 192.100], [323.147, 323.297]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-paginated_multi_decoder_decoded_epochs_window_page.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "    paginated_multi_decoder_decoded_epochs_window.jump_to_page(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f513296",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.jump_to_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98478063",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.get_decoder_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # v.params.enable_radon_transform_info = False\n",
    "    # v.params.enable_weighted_correlation_info = False\n",
    "    v.params.enable_radon_transform_info = True\n",
    "    v.params.enable_weighted_correlation_info = True\n",
    "    v.params.debug_enabled = True\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    print(f'decoder[{k}]:')\n",
    "    v.params.name\n",
    "    # v.params.on_render_page_callbacks\n",
    "    # v.params.enable_radon_transform_info\n",
    "    len(v.plots_data.radon_transform_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ff3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b447b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.refresh_current_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sub_subfn_wrapped_in_brackets(s: str, bracket_strings = (\"[\", \"]\")) -> str:\n",
    "        return bracket_strings[0] + s + bracket_strings[1]\n",
    "    \n",
    "def _sub_subfn_format_nested_list(arr, precision:int=3, num_sep=\", \", array_sep=', ') -> str:\n",
    "    \"\"\"\n",
    "    Converts a nested list of floats into a single string,\n",
    "    with each float formatted to the specified precision.\n",
    "    \n",
    "    arr = np.array([[491.798, 492.178], [940.016, 940.219]])\n",
    "    _sub_subfn_format_nested_list(arr)\n",
    "\n",
    "    >> '[[491.798, 492.178], [940.016, 940.219]]'\n",
    "\n",
    "    arr = np.array([[785.738, 785.923]])\n",
    "    _sub_subfn_format_nested_list(arr)\n",
    "    >> '[[785.738, 785.923]]'\n",
    "    \"\"\"\n",
    "    return _sub_subfn_wrapped_in_brackets(array_sep.join([_sub_subfn_wrapped_in_brackets(num_sep.join([f\"{num:.{precision}f}\" for num in row])) for row in arr]))\n",
    "    \n",
    "# arr = np.array([[491.798, 492.178], [940.016, 940.219]])\n",
    "arr = np.array([[785.738, 785.923]])\n",
    "_sub_subfn_format_nested_list(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0ab5d",
   "metadata": {},
   "source": [
    "### 2024-02-29 3pm - Get the active user-annotated epoch times from the `paginated_multi_decoder_decoded_epochs_window` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c982e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inputs: paginated_multi_decoder_decoded_epochs_window, filtered_ripple_simple_pf_pearson_merged_df\n",
    "any_good_selected_epoch_times = deepcopy(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = deepcopy(paginated_multi_decoder_decoded_epochs_window.find_data_indicies_from_epoch_times(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1f903",
   "metadata": {},
   "source": [
    "##  2024-03-01 - Get the active user-annotated epoch times from the `UserAnnotationsManager` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc751d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.misc import numpyify_array\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.epoch import EpochsAccessor\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "## Get from UserAnnotations directly instead of the intermediate viewer\n",
    "\n",
    "## # inputs: any_good_selected_epoch_times, any_good_selected_epoch_times, any_good_selected_epoch_indicies \n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies\n",
    "# Add user-selection columns to df\n",
    "a_df = deepcopy(filtered_ripple_simple_pf_pearson_merged_df)\n",
    "# a_df = deepcopy(ripple_weighted_corr_merged_df)\n",
    "a_df['is_user_annotated_epoch'] = False\n",
    "# any_good_selected_epoch_indicies = a_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, np.squeeze(any_good_selected_epoch_times[:,0]), t_column_names=['ripple_start_t',])\n",
    "# any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, any_good_selected_epoch_times, t_column_names=['ripple_start_t',])\n",
    "any_good_selected_epoch_indicies\n",
    "# a_df['is_user_annotated_epoch'] = np.isin(a_df.index.to_numpy(), any_good_selected_epoch_indicies)\n",
    "a_df['is_user_annotated_epoch'].loc[any_good_selected_epoch_indicies] = True # Here's another .iloc issue! Changing to .loc\n",
    "a_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DecoderDecodedEpochsResult.filter_epochs_dfs_by_annotation_times(curr_active_pipeline, any_good_selected_epoch_times, ripple_decoding_time_bin_size, filtered_ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dee7b3",
   "metadata": {},
   "source": [
    "### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "is_included_high_wcorr = np.any((df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifts the absolute times to delta-relative values, as would be needed to draw on a 'delta_aligned_start_t' axis:\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end = np.array([earliest_delta_aligned_t_start, t_delta, latest_delta_aligned_t_end]) - t_delta\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_wcorr_y_col'] = df['long_best_wcorr'].abs()\n",
    "df['_wcorr_y_col_y_diff_col'] = df['long_best_wcorr'].abs() - df['short_best_wcorr'].abs()\n",
    "# df.plot.scatter(x='ripple_start_t', y='wcorr_y_col')\n",
    "df.plot.scatter(x='delta_aligned_start_t', y='_wcorr_y_col_y_diff_col')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5438dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pearsonr_long_abs'] = df['long_best_pf_peak_x_pearsonr'].abs()\n",
    "# df['pearsonr_short_abs'] = df['short_best_pf_peak_x_pearsonr'].abs()\n",
    "# df['pearsonr_diff'] = df['long_best_pf_peak_x_pearsonr'].abs() - df['short_best_pf_peak_x_pearsonr'].abs()\n",
    "\n",
    "# df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_long_abs')\n",
    "# df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_short_abs')\n",
    "df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_abs_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f951c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02556ea",
   "metadata": {},
   "source": [
    "### Add utility footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3888f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig, get_utility_dock_colors\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "\n",
    "ui = paginated_multi_decoder_decoded_epochs_window.ui._contents\n",
    "# ui.dock_widgets\n",
    "# ui.dock_configs\n",
    "\n",
    "\n",
    "## Build the utility controls at the bottom:\n",
    "ctrls_dock_config = CustomDockDisplayConfig(custom_get_colors_callback_fn=get_utility_dock_colors, showCloseButton=False)\n",
    "\n",
    "button_bar_height = 21\n",
    "ctrls_button_bar_widget = ThinButtonBarWidget()\n",
    "ctrls_button_bar_widget.setObjectName(\"ctrls_button_bar\")\n",
    "\n",
    "ctrl_layout = pg.LayoutWidget()\n",
    "ctrl_layout.addWidget(ctrls_button_bar_widget, row=1, rowspan=1, col=1, colspan=2)\n",
    "ctrl_widgets_dict = dict(ctrls_widget=ctrls_button_bar_widget)\n",
    "# ctrl_layout.setSizePolicy(\n",
    "\n",
    "def onCopySelectionsClicked():\n",
    "    print(f'onCopySelectionsClicked()')\n",
    "    saved_selections_contexts_dict = paginated_multi_decoder_decoded_epochs_window.print_user_annotations()\n",
    "\n",
    "ctrl_widgets_dict['copy_selection_connection'] = ctrls_button_bar_widget.sigCopySelections.connect(onCopySelectionsClicked)\n",
    "\n",
    "ui.dock_widgets['bottom_controls'] = paginated_multi_decoder_decoded_epochs_window.add_display_dock(identifier='bottom_controls', widget=ctrl_layout, dockSize=(600, button_bar_height), dockAddLocationOpts=['bottom'], display_config=ctrls_dock_config)\n",
    "ui.dock_widgets['bottom_controls'][1].hideTitleBar()\n",
    "ui.dock_widgets['bottom_controls']\n",
    "\n",
    "button_bar_height = 21\n",
    "\n",
    "a_layout = ui.dock_widgets['bottom_controls'][0]\n",
    "a_layout.size()\n",
    "a_layout.setContentsMargins(0,0,0,0)\n",
    "a_layout.setFixedHeight(21)\n",
    "ui.dock_widgets['bottom_controls'][1].size()\n",
    "ui.dock_widgets['bottom_controls'][1].setContentsMargins(0,0,0,0)\n",
    "# ui.dock_widgets['bottom_controls'][1].hideTitleBar()\n",
    "# ui.dock_widgets['bottom_controls'][1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "setMargin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.dock_widgets['bottom_controls'][0].resize(600, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39be52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.find_display_dock('bottom_controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_display_dock('bottom_controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "## Set epoch annotations from selections epochs \n",
    "annotations_man = UserAnnotationsManager()\n",
    "user_annotations = annotations_man.get_user_annotations()\n",
    "new_selections_dict = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(user_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf471332",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_selections_objs_dict = {a_name:EpochSelectionsObject(epoch_times=a_selections_values) for a_name, a_selections_values in loaded_selections_dict.items()}\n",
    "loaded_selections_objs_dict\n",
    "\n",
    "## Select just the selected epoch times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_selections_context_dict = {a_name:v.figure_ctx.adding_context_if_missing(user_annotation='selections') for a_name, v in saved_selections_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d312d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.print_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ee6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the excessively long plot titles?\n",
    "# root_dockAreaWindow.update\n",
    "pagination_controller_dict = paginated_multi_decoder_decoded_epochs_window.pagination_controllers\n",
    "all_widgets = {a_decoder_name:a_pagination_controller.ui.mw for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_windows = {a_decoder_name:a_pagination_controller.ui.mw.window() for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_plots = {a_decoder_name:a_pagination_controller.plots for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_plots_data = {a_decoder_name:a_pagination_controller.plots_data for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_params = {a_decoder_name:a_pagination_controller.params for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_current_page_idx = {a_decoder_name:a_pagination_controller.current_page_idx for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_current_page_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_separate_plots\n",
    "\n",
    "all_separate_weighted_corr_plots = {a_decoder_name:a_pagination_controller.plots.get('weighted_corr', {}) for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_weighted_corr_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ui.print = self.private_print # builtins.print # the print function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69313c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import EpochsAccessor\n",
    "\n",
    "# MLM\n",
    "# {a_name:a_ctrlr.params.is_selected for a_name, a_ctrlr in root_dockAreaWindow.pagination_controllers.items()}\n",
    "# {a_name:a_ctrlr.selected_epoch_times for a_name, a_ctrlr in root_dockAreaWindow.pagination_controllers.items()}\n",
    "\n",
    "any_good_selected_epoch_times: NDArray = paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times # drops duplicate rows (present in multiple decoders), and sorts them ascending\n",
    "# any_good_selected_epoch_times\n",
    "# Only at the decoder-level\n",
    "any_good_epoch_idxs_list = [a_ctrlr.find_data_indicies_from_epoch_times(any_good_selected_epoch_times) for a_name, a_ctrlr in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()]\n",
    "any_good_epoch_idxs: NDArray = any_good_epoch_idxs_list[0]\n",
    "any_good_epoch_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29282203",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531db971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "\n",
    "found_data_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(epoch_times=any_good_selected_epoch_times)\n",
    "df = filtered_ripple_simple_pf_pearson_merged_df.epochs._obj.iloc[found_data_indicies].copy().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1677479",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_selected_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.iloc[any_good_epoch_idxs, :].reset_index(drop=True)\n",
    "hand_selected_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d400bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand_selected_ripple_simple_pf_pearson_merged_df['best_decoder_index']\n",
    "\n",
    "is_most_likely_long = (hand_selected_ripple_simple_pf_pearson_merged_df['P_Long'] >= 0.5)\n",
    "# is_most_likely_long\n",
    "\n",
    "long_likely_hand_selected_ripple_simple_pf_pearson_merged_df = hand_selected_ripple_simple_pf_pearson_merged_df[is_most_likely_long]\n",
    "long_likely_hand_selected_ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d33190",
   "metadata": {},
   "source": [
    "##  Plot laps to compare between decoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4873ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "\n",
    "# decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs # looks like 'lap_dir' column is wrong\n",
    "updated_laps_dfs_dict = {}\n",
    "\n",
    "## Update the .filter_epochs:\n",
    "for k, v in decoder_laps_filter_epochs_decoder_result_dict.items():\n",
    "    updated_laps_dfs_dict[k] = Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs)))\n",
    "    decoder_laps_filter_epochs_decoder_result_dict[k].filter_epochs =  updated_laps_dfs_dict[k]\n",
    "\n",
    "# updated_laps_dfs_dict['long_LR']\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_app, laps_paginated_multi_decoder_decoded_epochs_window, laps_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                            decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', included_epoch_indicies=None, \n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': False, \n",
    "    # 'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': False,\n",
    "    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "    # 'disable_y_label': True,\n",
    "    # 'isPaginatorControlWidgetBackedMode': True,\n",
    "    # 'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "    # 'debug_print': True,\n",
    "    'max_subplots_per_page': 10,\n",
    "    'scrollable_figure': True,\n",
    "    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "    'use_AnchoredCustomText': False,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec05d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "\n",
    "## INPUTS: decoder_laps_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Highlight the correct ones:\n",
    "# {k:Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs))) for k, v in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "\n",
    "## Select the true laps by emulating user_annotations:\n",
    "filter_epochs = ensure_dataframe(deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)) \n",
    "# filter_epochs\n",
    "\n",
    "decoder_name_idx_map = {'long_LR': 0, 'long_RL': 1, 'short_LR': 2, 'short_RL': 3} \n",
    "selections_dict = {}\n",
    "figure_ctx_dict = laps_paginated_multi_decoder_decoded_epochs_window.figure_ctx_dict\n",
    "loaded_selections_context_dict = {a_name:a_figure_ctx.adding_context_if_missing(user_annotation='selections') for a_name, a_figure_ctx in figure_ctx_dict.items()}\n",
    "\n",
    "for a_name, an_idx in decoder_name_idx_map.items():\n",
    "    a_selections_context = loaded_selections_context_dict[a_name]\n",
    "    selections_dict[a_selections_context] = filter_epochs[filter_epochs['true_decoder_index'] == an_idx][['start', 'stop']].to_numpy()\n",
    "\n",
    "\n",
    "## Clearing the existing selection rects and them having them rebuilt when the selection is updated fixes them being shifted.\n",
    "for k, v in laps_pagination_controller_dict.items():\n",
    "    v._subfn_clear_selectability_rects()\n",
    "\n",
    "# _tmp_out_selections = laps_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(user_annotations=selections_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6607cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e840a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f558fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89caf8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.remove_data_overlays(defer_refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.remov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clearing the existing selection rects and them having them rebuilt when the selection is updated fixes them being shifted.\n",
    "for k, v in laps_pagination_controller_dict.items():\n",
    "    v._subfn_clear_selectability_rects()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f847f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1556870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(decoder_laps_filter_epochs_decoder_result_dict.keys())\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the figure from the axes:\n",
    "a_fig = ax.get_figure()\n",
    "a_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller = laps_paginated_multi_decoder_decoded_epochs_window.contents.pagination_controllers['long_LR'] # DecodedEpochSlicesPaginatedFigureController\n",
    "a_pagination_controller_figure_widget = paginator_controller_widget = a_controlling_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "paginator_controller_widget = a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget # PaginationControlWidget\n",
    "# paginator_controller_widget\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92048bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = a_controlling_pagination_controller.plots.axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_figure().canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47481538",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_rectangles_dict = a_controlling_pagination_controller.plots.get('selection_rectangles_dict', None)\n",
    "selection_rectangles_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa452e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_controlling_pagination_controller.plots.fig.canvas.draw_idle()\n",
    "# a_controlling_pagination_controller.plots.fig.canvas.draw()\n",
    "# paginator_controller_widget.update()\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator_controller_widget.go_to_page(3)\n",
    "# paginator_controller_widget.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget.jump_to_page\n",
    "\n",
    "new_obj.plots_data.paginator\n",
    "new_obj.params.active_identifying_figure_ctx\n",
    "new_obj.on_paginator_control_widget_jump_to_page(page_idx=0)\n",
    "new_obj.ui.connections['paginator_controller_widget_jump_to_page']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c54dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, extant_plots in a_plots['weighted_corr'].items():\n",
    "    extant_wcorr_text = extant_plots.get('wcorr_text', None)\n",
    "    # extant_wcorr_text = extant_plots.pop('wcorr_text', None)\n",
    "    print(f'extant_wcorr_text: {extant_wcorr_text}')\n",
    "    # plot the radon transform line on the epoch:\n",
    "    if (extant_wcorr_text is not None):\n",
    "        # already exists, clear the existing ones. \n",
    "        # Let's assume we want to remove the 'Quadratic' line (line2)\n",
    "        print(f'removing extant text object at index: {i}.')\n",
    "        # extant_wcorr_text.remove()\n",
    "        extant_wcorr_text.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "    # Get context for current page of items:\n",
    "    current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "    a_paginator = a_pagination_controller.paginator\n",
    "    total_num_pages = int(a_paginator.num_pages)\n",
    "    page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "    print(page_context)\n",
    "\n",
    "    ## Get the figure/axes:\n",
    "    a_plots = a_pagination_controller.plots # RenderPlots\n",
    "    a_plot_data = a_pagination_controller.plots_data\n",
    "\n",
    "    a_params = a_pagination_controller.params\n",
    "    a_params.skip_plotting_measured_positions\n",
    "\n",
    "    figs = a_plots.fig\n",
    "    axs = a_plots.axs\n",
    "\n",
    "    # # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    # with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    #     curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d32342",
   "metadata": {},
   "source": [
    "##  Export Paginated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laps_paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)\n",
    "paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export_decoder_pagination_controller_figure_page(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd2637",
   "metadata": {},
   "source": [
    "##  Single Decoder Version (`DecodedEpochSlicesPaginatedFigureController`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d880d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import _subfn_update_decoded_epoch_slices\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController # `plot_decoded_epoch_slices_paginated`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import WeightedCorrelationPaginatedPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import DecodedPositionsPlotDataProvider, DecodedAndActualPositionsPlotData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import perform_plot_1D_single_most_likely_position_curve\n",
    "\n",
    "# Inputs: epochs_name, decoder_ripple_filter_epochs_decoder_result_dict, curr_active_pipeline\n",
    "epochs_name = 'ripple'\n",
    "\n",
    "(a_name, a_decoder) = tuple(track_templates.get_decoders_dict().items())[0]\n",
    "\n",
    "# a_decoder_decoded_epochs_result = decoder_ripple_filter_epochs_decoder_result_dict[a_name]\n",
    "\n",
    "# a_decoder_decoded_epochs_result = decoder_ripple_filter_epochs_decoder_result_dict[a_name]\n",
    "a_decoder_decoded_epochs_result = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict[a_name]) ## FILTERED\n",
    "\n",
    "_out_pagination_controller = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(active_filter_epochs=a_decoder_decoded_epochs_result.filter_epochs,\n",
    "                                                                                    filter_epochs_decoder_result= a_decoder_decoded_epochs_result,\n",
    "                                                                                    xbin=a_decoder.xbin, global_pos_df=curr_active_pipeline.sess.position.df,\n",
    "                                                                                    a_name=f'DecodedEpochSlices[{a_name}]', active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs=epochs_name, decoder=a_name),\n",
    "                                                                                    max_subplots_per_page=32,\n",
    "                                                                                    params_kwargs={'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, 'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': True, #'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 32,\n",
    "                                                                                                    'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': True,\n",
    "                                                                                                    'disable_toolbar': False,\n",
    "                                                                                    }, \n",
    "                                                                                    # disable_toolbar=False\n",
    "                                                                                    )\n",
    "\n",
    "_out_pagination_controller.params.should_suppress_callback_exceptions = False\n",
    "_out_pagination_controller.add_data_overlays(a_decoder_decoded_epochs_result)\n",
    "_tmp_out_selections = _out_pagination_controller.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c780e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = _out_pagination_controller.plots.fig\n",
    "# fig.toolbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(_out_pagination_controller)\n",
    "\n",
    "_out_pagination_controller.plot_widget._buildUI_setup_statusbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff74414",
   "metadata": {},
   "source": [
    "single_epoch_field_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e429a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_selected_epochs_changed\n",
    "\n",
    "active_captured_single_epoch_result: SingleEpochDecodedResult = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=3)\n",
    "\n",
    "def get_selected_posterior_on_secondary_clicked_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user alt-clicks an epoch \n",
    "    \n",
    "    captures: active_captured_single_epoch_result\n",
    "    \"\"\"\n",
    "    global active_captured_single_epoch_result\n",
    "    if self.params.debug_print:\n",
    "        print(f'get_selected_posterior_on_secondary_clicked_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            # print(f'start_t: {start_t}')\n",
    "            clicked_data_index: int = _out_pagination_controller.find_data_indicies_from_epoch_times(epoch_times=np.array([start_t, end_t]))[0]\n",
    "            if self.params.debug_print:\n",
    "                print(f'\\tclicked_data_index: {clicked_data_index}')            \n",
    "            active_captured_single_epoch_result = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=clicked_data_index)\n",
    "            if self.params.debug_print:\n",
    "                print(f'\\tactive_captured_single_epoch_result.epoch_info_tuple: {active_captured_single_epoch_result.epoch_info_tuple}')\n",
    "                print(f'\\tdone.')\n",
    "\n",
    "\n",
    "# BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "if not _out_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "    _out_pagination_controller.params['on_middle_click_item_callbacks'] = {}\n",
    "\n",
    "_out_pagination_controller.params.on_middle_click_item_callbacks['get_selected_posterior_on_secondary_clicked_callback'] = get_selected_posterior_on_secondary_clicked_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_decoded_epochs_result.active_filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a619872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image\n",
    "\n",
    "posterior_image = active_captured_single_epoch_result.get_posterior_as_image(desired_width=2048)\n",
    "posterior_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Define 8x8 blur filter kernel\n",
    "blur_kernel = np.ones((8, 8)) / 64\n",
    "\n",
    "# Apply blur to a 2D matrix\n",
    "blurred_matrix = convolve2d(active_captured_single_epoch_result.p_x_given_n, blur_kernel, mode='same', boundary='wrap')\n",
    "\n",
    "get_array_as_image(blurred_matrix, desired_height=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "{i:col for i, col in enumerate(a_decoder_decoded_epochs_result.active_filter_epochs.columns)}\n",
    "\n",
    "column_indicies = np.arange(12, 19)\n",
    "column_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_pagination_controller.params.debug_print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc81f8",
   "metadata": {},
   "source": [
    "## 2024-04-30 Heuristic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21abb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *position_relative\": mapped between the ends of the track, 0.0 to 1.0\n",
    "most_likely_position_relative = (np.squeeze(active_captured_single_epoch_result.most_likely_position_indicies) / float(active_captured_single_epoch_result.n_xbins-1))\n",
    "most_likely_position_relative\n",
    "\n",
    "\n",
    "plt.hlines([0], colors='k', xmin=active_captured_single_epoch_result.time_bin_edges[0], xmax=active_captured_single_epoch_result.time_bin_edges[-1])\n",
    "plt.step(active_captured_single_epoch_result.time_bin_container.centers[1:], np.diff(most_likely_position_relative))\n",
    "plt.scatter(active_captured_single_epoch_result.time_bin_container.centers, most_likely_position_relative, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c52d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.Qt import QtGui, QtCore, QtWidgets\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph.parametertree.parameterTypes.file import popupFilePicker\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.widgets.FileDialog import FileDialog\n",
    "\n",
    "from silx.gui import qt\n",
    "from silx.gui.dialog.ImageFileDialog import ImageFileDialog\n",
    "from silx.gui.dialog.DataFileDialog import DataFileDialog\n",
    "import silx.io\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import saveFile\n",
    "\n",
    "app = pg.mkQApp('silx_testing')\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc644214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from silx.gui.plot import Plot2D\n",
    "\n",
    "matrix = np.random.rand(10, 10)  # Example 2D matrix\n",
    "plot = Plot2D()\n",
    "plot.addImage(matrix, colormap=\"viridis\", vmin=0, vmax=1)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "HeuristicReplayScoring.bin_wise_track_coverage_score_fn(a_result=a_decoder_decoded_epochs_result, an_epoch_idx=active_captured_single_epoch_result.epoch_data_index, a_decoder_track_length=170.0)\n",
    "\n",
    "# np.diff(active_captured_single_epoch_result.most_likely_position_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5407a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CodeConversion.convert_dictionary_to_class_defn(\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _out_pagination_controller.plots.axs[0]\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d52d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.format_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f28b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ascending sequences of most-likely positions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = round(x)\n",
    "    row = round(y)\n",
    "    nrows, ncols = X.shape\n",
    "    if 0 <= col < ncols and 0 <= row < nrows:\n",
    "        z = X[row, col]\n",
    "        return f'x={x:1.4f}, y={y:1.4f}, z={z:1.4f}'\n",
    "    else:\n",
    "        return f'x={x:1.4f}, y={y:1.4f}'\n",
    "\n",
    "\n",
    "ax.format_coord = format_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de603c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_pagination_controller.plot_widget.setStatusTip('LONG STATUS TIP TEST')\n",
    "\n",
    "_out_pagination_controller.plot_widget.update_status('LONG STATUS TIP TEST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_pagination_controller.plots.radon_transform\n",
    "fig = _out_pagination_controller.plots.fig\n",
    "\n",
    "# plt.subplots_adjust(left=0.15, right=0.85, top=0.9, bottom=0.1)\n",
    "# Adjust the margins using subplots_adjust\n",
    "fig.subplots_adjust(left=0.15, right=0.85, bottom=0.15, top=0.85)\n",
    "\n",
    "# Adjust the margins using the Figure object\n",
    "# fig.set_tight_layout(dict(rect=[0.1, 0.2, 0.8, 0.8]))\n",
    "# fig.tight_layout(dict(rect=[0.1, 0.2, 0.8, 0.8]))\n",
    "# fig.tight_layout(pad=1.0, rect=[0.1, 0.1, 0.8, 0.8])\n",
    "_out_pagination_controller.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9643ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a_name, a_decoder) = tuple(track_templates.get_decoders_dict().items())[0]\n",
    "a_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d1a25",
   "metadata": {},
   "source": [
    "##  2024-03-06 - Uni Page Scrollable Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_decoded_epochs_result_dict: generic\n",
    "single_page_app, single_page_paginated_multi_decoder_decoded_epochs_window, single_page_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                included_epoch_indicies=None, debug_print=False,\n",
    "                                                                                                params_kwargs={'skip_plotting_most_likely_positions': False, 'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                               'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                                # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                                # 'disable_y_label': True,\n",
    "                                                                                                                'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                                'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                                # 'debug_print': True,\n",
    "                                                                                                                'max_subplots_per_page': 64,\n",
    "                                                                                                                'scrollable_figure': True,\n",
    "                                                                                                                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2565e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "_tmp_out_selections = single_page_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curr_results_obj: LeaveOneOutDecodingAnalysisResult object\n",
    "num_filter_epochs:int = curr_results_obj.active_filter_epochs.n_epochs\n",
    "\n",
    "# `active_filter_epochs_df` native columns approach\n",
    "active_filter_epochs_df = curr_results_obj.active_filter_epochs.to_dataframe().copy()\n",
    "assert np.isin(['score', 'velocity', 'intercept', 'speed'], active_filter_epochs_df.columns).all()\n",
    "epochs_linear_fit_df = active_filter_epochs_df[['score', 'velocity', 'intercept', 'speed']].copy() # get the `epochs_linear_fit_df` as a subset of the filter epochs df\n",
    "# epochs_linear_fit_df approach\n",
    "assert curr_results_obj.all_included_filter_epochs_decoder_result.num_filter_epochs == np.shape(epochs_linear_fit_df)[0]\n",
    "\n",
    "num_filter_epochs:int = curr_results_obj.all_included_filter_epochs_decoder_result.num_filter_epochs # curr_results_obj.num_filter_epochs\n",
    "try:\n",
    "    time_bin_containers: List[BinningContainer] = deepcopy(curr_results_obj.time_bin_containers)\n",
    "except AttributeError as e:\n",
    "    # AttributeError: 'LeaveOneOutDecodingAnalysisResult' object has no attribute 'time_bin_containers' is expected when `curr_results_obj: LeaveOneOutDecodingAnalysisResult - for Long/Short plotting`\n",
    "    time_bin_containers: List[BinningContainer] = deepcopy(curr_results_obj.all_included_filter_epochs_decoder_result.time_bin_containers) # for curr_results_obj: LeaveOneOutDecodingAnalysisResult - for Long/Short plotting\n",
    "\n",
    "radon_transform_data = RadonTransformPlotDataProvider._subfn_build_radon_transform_plotting_data(active_filter_epochs_df=active_filter_epochs_df,\n",
    "            num_filter_epochs = num_filter_epochs, time_bin_containers = time_bin_containers, radon_transform_column_names=['score', 'velocity', 'intercept', 'speed'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fa458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _display_long_and_short_stacked_epoch_slices\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out_dict = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f3190",
   "metadata": {},
   "source": [
    "## Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = _out_pagination_controller.plots['radon_transform'][7]\n",
    "extant_line = _out['line'] # matplotlib.lines.Line2D\n",
    "extant_line.linestyle = 'none'\n",
    "# extant_line.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e00165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(curr_active_pipeline.filtered_contexts.keys())) # ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Converting between decoder names and filtered epoch names:\n",
    "# {'long':'maze1', 'short':'maze2'}\n",
    "# {'LR':'odd', 'RL':'even'}\n",
    "long_LR_name, short_LR_name, long_RL_name, short_RL_name = ['maze1_odd', 'maze2_odd', 'maze1_even', 'maze2_even']\n",
    "decoder_name_to_session_context_name: Dict[str,str] = dict(zip(track_templates.get_decoder_names(), (long_LR_name, long_RL_name, short_LR_name, short_RL_name))) # {'long_LR': 'maze1_odd', 'long_RL': 'maze1_even', 'short_LR': 'maze2_odd', 'short_RL': 'maze2_even'}\n",
    "session_context_to_decoder_name: Dict[str,str] = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), track_templates.get_decoder_names())) # {'maze1_odd': 'long_LR', 'maze1_even': 'long_RL', 'maze2_odd': 'short_LR', 'maze2_even': 'short_RL'}\n",
    "\n",
    "decoder_name_to_session_context_name\n",
    "session_context_to_decoder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_num_slices: int = _out_pagination_controller.params.active_num_slices\n",
    "single_plot_fixed_height: float = _out_pagination_controller.params.single_plot_fixed_height\n",
    "all_plots_height: float = _out_pagination_controller.params.all_plots_height\n",
    "print(f'all_plots_height: {all_plots_height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fed6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import _add_maze_id_to_epochs\n",
    "\n",
    "\n",
    "## Add new weighted correlation results as new columns in existing filter_epochs df:\n",
    "active_filter_epochs = long_results_obj.active_filter_epochs\n",
    "# Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "active_filter_epochs = _add_maze_id_to_epochs(active_filter_epochs, short_session.t_start)\n",
    "active_filter_epochs._df['weighted_corr_LONG'] = epoch_long_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_SHORT'] = epoch_short_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_spearman_LONG'] = epoch_long_weighted_corr_results[:,1]\n",
    "active_filter_epochs._df['weighted_corr_spearman_SHORT'] = epoch_short_weighted_corr_results[:,1]\n",
    "\n",
    "\n",
    "active_filter_epochs\n",
    "active_filter_epochs.to_dataframe()\n",
    "## plot the `weighted_corr_LONG` over time\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=1, nrows=active_num_rows, sharex=True, sharey=sharey, figsize=figsize)\n",
    "\n",
    "## Weighted Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_LONG', title='weighted_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "## Weighted Spearman Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_LONG', title='weighted_spearman_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Spearman Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='score_LONG', title='Radon Transform Score during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='score_SHORT', xlabel='Replay Epoch Time', ylabel='Replay Radon Transform Score', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "example_stacked_epoch_graphics = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', defer_render=False, save_figure=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4f9a0",
   "metadata": {},
   "source": [
    "# 2024-02-15 - Do simple spike-t vs. template pf peak correlation like Kamran suggested this morning\n",
    "\n",
    "Replays can be of trajectories on either the current track configuration or on a temporally distant one (such as a trajectory on the long track after the track has been shortened). \n",
    "The goal of the decoder scoring methods are to evaluate how likely each decoder was. This means for each Epoch we obtain a score for all four decoders: Long_LR, Long_RL, Short_LR, Short_RL\n",
    "\n",
    "#### `posterior decoder likelihoods` - This scoring method produces a probability that the\n",
    "\n",
    "#### Radon Transform - TODO\n",
    "\n",
    "#### `compute_simple_spike_time_v_pf_peak_x_by_epoch` - This epoch scoring metric plots the placefield peak x position against the time in seconds of each spike relative to the start of the epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1e967",
   "metadata": {},
   "source": [
    "## TODO 2024-02-15 8pm - Add in to previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd539a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "laps_simple_pf_pearson_merged_df\n",
    "# laps_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=False))\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74381521",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result: TrialByTrialActivity = directional_active_lap_pf_results_dicts['long_LR']\n",
    "# a_result.sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421fd1a",
   "metadata": {},
   "source": [
    "#  2024-03-04 - Export `DecoderDecodedEpochsResult` CSVs with user annotations for epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0ae73",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "_, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
    "\n",
    "## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
    "decoder_user_selected_epoch_times_dict, any_user_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "\n",
    "a_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# {a_name:ensure_dataframe(a_result.filter_epochs) for a_name, a_result in a_result_dict.items()}\n",
    "\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
    "\n",
    "#  2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "## Merge the heuristic columns into the wcorr df columns for exports\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "\n",
    "# {a_name:DecoderDecodedEpochsResult.try_add_is_user_annotated_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=filtered_valid_epoch_times) for a_name, a_result in a_result_dict.items()}\n",
    "\n",
    "for a_name, a_result in a_result_dict.items():\n",
    "    # a_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
    "\n",
    "    ## Merge the heuristic columns into the wcorr df columns for exports\n",
    "    # directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "    a_wcorr_result = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict[a_name]\n",
    "    \n",
    "    # did_update_user_annotation_col = DecoderDecodedEpochsResult.try_add_is_user_annotated_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=any_user_selected_epoch_times, t_column_names=None)\n",
    "    # print(f'did_update_user_annotation_col: {did_update_user_annotation_col}')\n",
    "    # did_update_is_valid = DecoderDecodedEpochsResult.try_add_is_valid_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=filtered_valid_epoch_times, t_column_names=None)\n",
    "    # print(f'did_update_is_valid: {did_update_is_valid}')\n",
    "\n",
    "# ['start',]\n",
    "\n",
    "a_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "# {a_name:ensure_dataframe(a_result.filter_epochs) for a_name, a_result in a_result_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04fb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_new_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ba9f746",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected_outputs_path: \"K:\\scratch\\collected_outputs\"\n",
      "CURR_BATCH_OUTPUT_PREFIX: 2024-05-30_APOGEE-2006-6-09_1-22-43\n",
      "\tComputation complete. Exporting .CSVs...\n",
      "len(active_epochs_df): 412\n",
      "min_num_unique_aclu_inclusions: 18\n",
      "len(active_epochs_df): 136\n",
      "num_user_selected_times: 49\n",
      "adding user annotation column!\n",
      "\t succeded at getting 49 selected indicies (of 49 user selections) for ripple_weighted_corr_merged_df. got 49 indicies!\n",
      "num_valid_epoch_times: 136\n",
      "adding valid filtered epochs column!\n",
      "\t succeded at getting 136 selected indicies (of 136 valid filter epoch times) for ripple_weighted_corr_merged_df. got 136 indicies!\n",
      "num_user_selected_times: 49\n",
      "adding user annotation column!\n",
      "\t succeded at getting 49 selected indicies (of 49 user selections) for ripple_simple_pf_pearson_merged_df. got 49 indicies!\n",
      "num_valid_epoch_times: 136\n",
      "adding valid filtered epochs column!\n",
      "\t succeded at getting 136 selected indicies (of 136 valid filter epoch times) for ripple_simple_pf_pearson_merged_df. got 136 indicies!\n",
      "\t\tsuccessfully exported directional_decoders_epochs_decode_result to K:\\scratch\\collected_outputs!\n",
      "\t\t\tCSV Paths: laps_weighted_corr_merged_df: \"file:///K:/scratch/collected_outputs/2024-05-30_0630PM-kdiba_gor01_one_2006-6-09_1-22-43-%28laps_weighted_corr_merged_df%29_tbin-0.2.csv\"\n",
      "ripple_weighted_corr_merged_df: \"file:///K:/scratch/collected_outputs/2024-05-30_0630PM-kdiba_gor01_one_2006-6-09_1-22-43-%28ripple_weighted_corr_merged_df%29_tbin-0.025.csv\"\n",
      "laps_simple_pf_pearson_merged_df: \"file:///K:/scratch/collected_outputs/2024-05-30_0630PM-kdiba_gor01_one_2006-6-09_1-22-43-%28laps_simple_pf_pearson_merged_df%29_tbin-0.2.csv\"\n",
      "ripple_simple_pf_pearson_merged_df: \"file:///K:/scratch/collected_outputs/2024-05-30_0630PM-kdiba_gor01_one_2006-6-09_1-22-43-%28ripple_simple_pf_pearson_merged_df%29_tbin-0.025.csv\"\n",
      "ripple_all_scores_merged_df: \"file:///K:/scratch/collected_outputs/2024-05-30_0630PM-kdiba_gor01_one_2006-6-09_1-22-43-%28ripple_all_scores_merged_df%29_tbin-0.025.csv\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pathlib import Path\n",
    "\n",
    "#  export_csvs\n",
    "\n",
    "BATCH_DATE_TO_USE: str = f'{get_now_day_str()}_APOGEE' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "\n",
    "known_collected_outputs_paths = [Path(v).resolve() for v in ('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs', '/Users/pho/Dropbox (University of Michigan)/MED-DibaLabDropbox/Data/Pho/Outputs/output/collected_outputs', '/home/halechr/cloud/turbo/Data/Output/collected_outputs', '/home/halechr/FastData/gen_scripts/', '/home/halechr/FastData/collected_outputs/', 'output/gen_scripts/')]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_outputs_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: '{collected_outputs_path}' does not exist! Is the right computer's config commented out above?\"\n",
    "print(f'collected_outputs_path: \"{collected_outputs_path}\"')\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "print(f'\\tComputation complete. Exporting .CSVs...')\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "_, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                              user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                              valid_epochs_selections={'ripple': filtered_valid_epoch_times})\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported directional_decoders_epochs_decode_result to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _output_csv_paths.items()])\n",
    "# print(f'\\t\\t\\tCSV Paths: {_output_csv_paths}\\n')\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')\n",
    "\n",
    "# {'laps_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'laps_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_simple_pf_pearson_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_simple_pf_pearson_merged_df)_tbin-0.025.csv')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b77273",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0661fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88335249",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_good_selected_epoch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7420982",
   "metadata": {},
   "source": [
    "# 2024-03-04 - Filter out the epochs based on the criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.mixins.time_slicing import add_epochs_id_identity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, required_min_percentage_of_active_cells=0.333333, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68233f",
   "metadata": {},
   "source": [
    "# 2024-03-27 - Look at active set cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.HDF5_representable import HDFConvertableEnum\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import TruncationCheckingResults\n",
    "\n",
    "\n",
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_aclus_dict, jonathan_firing_rate_analysis_result.neuron_replay_stats_df = truncation_checking_result.build_truncation_checking_aclus_dict(neuron_replay_stats_df=jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "frs_index_inclusion_magnitude:float = 0.5\n",
    "\n",
    "jonathan_firing_rate_analysis_result = JonathanFiringRateAnalysisResult(**curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis.to_dict())\n",
    "\n",
    "## Unrefined:\n",
    "# neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "\n",
    "## Refine the LxC/SxC designators using the firing rate index metric:\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "jonathan_firing_rate_analysis_result.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "\n",
    "neuron_replay_stats_df, *exclusivity_tuple = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "# short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = exclusivity_tuple\n",
    "exclusivity_aclus_tuple = [v.track_exclusive_aclus for v in exclusivity_tuple]\n",
    "exclusivity_aclus_dict = dict(zip(['short_exclusive', 'long_exclusive', 'BOTH', 'EITHER', 'XOR', 'NEITHER'], exclusivity_aclus_tuple))\n",
    "any_aclus = union_of_arrays(*exclusivity_aclus_tuple)\n",
    "exclusivity_aclus_dict['any'] = any_aclus\n",
    "refined_exclusivity_aclus_tuple = [v.get_refined_track_exclusive_aclus() for v in exclusivity_tuple]\n",
    "neuron_replay_stats_df: pd.DataFrame = HDFConvertableEnum.convert_dataframe_columns_for_hdf(neuron_replay_stats_df)\n",
    "\n",
    "# These keys exhaustively span all aclus:\n",
    "exhaustive_key_names = ['short_exclusive', 'long_exclusive', 'BOTH', 'NEITHER']\n",
    "assert np.all(any_aclus == union_of_arrays(*[exclusivity_aclus_dict[k] for k in exhaustive_key_names]))\n",
    "exhaustive_key_dict = {k:v for k, v in exclusivity_aclus_dict.items() if k in exhaustive_key_names}\n",
    "\n",
    "\n",
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_any_aclus = np.array([  3,   4,   5,   7,  10,  11,  13,  14,  15,  17,  23,  24,  25,  26,  31,  32,  33,  34,  45,  49,  50,  51,  52,  54,  55,  58,  61,  64,  68,  69,  70,  71,  73,  74,  75,  76,  78,  81,  82,  83,  84,  85,  87,  90,  92,  93,  96,  97, 102, 109])\n",
    "old_appearing_aclus = np.array([ 4, 11, 13, 23, 52, 58, 87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af361ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_aclus = union_of_arrays(*[v for v in truncation_checking_aclus_dict.values() if len(v) > 0])\n",
    "any_aclus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520650ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import perform_sweep_lap_groud_truth_performance_testing, _perform_variable_time_bin_lap_groud_truth_performance_testing\n",
    "\n",
    "desired_laps_decoding_time_bin_size: float = 0.75\n",
    "\n",
    "## INPUTS: exclusivity_aclus_tuple, desired_laps_decoding_time_bin_size: float\n",
    "# short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = exclusivity_aclus_tuple\n",
    "# included_neuron_ids_list = [short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset]\n",
    "\n",
    "# included_neuron_ids_list = [*exclusivity_aclus_tuple]\n",
    "\n",
    "## INPUTS: truncation_checking_aclus_dict\n",
    "included_neuron_ids_list = list(truncation_checking_aclus_dict.values())\n",
    "row_names = list(truncation_checking_aclus_dict.keys())\n",
    "\n",
    "_output_tuples_list = perform_sweep_lap_groud_truth_performance_testing(curr_active_pipeline, \n",
    "                                                                        included_neuron_ids_list=included_neuron_ids_list,\n",
    "                                                                        desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size)\n",
    "\n",
    "percent_laps_correctness_df: pd.DataFrame = pd.DataFrame.from_records([complete_decoded_context_correctness_tuple.percent_correct_tuple for (a_directional_merged_decoders_result, result_laps_epochs_df, complete_decoded_context_correctness_tuple) in _output_tuples_list],\n",
    "                          columns=(\"track_ID_correct\", \"dir_correct\", \"complete_correct\"), index=row_names)\n",
    "percent_laps_correctness_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb00384",
   "metadata": {},
   "source": [
    "# 2024-03-29 - Rigorous Decoder Performance assessment\n",
    "Quantify cell contributions to decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f9715a6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_portion: 0.8333333333333334, test_data_portion: 0.16666666666666663\n"
     ]
    }
   ],
   "source": [
    "# Inputs: all_directional_pf1D_Decoder, alt_directional_merged_decoders_result\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestLapsSplitting, CustomDecodeEpochsResult, decoder_name, epoch_split_key, get_proper_global_spikes_df\n",
    "\n",
    "## INPUTS: directional_laps_results, track_templates, directional_laps_results\n",
    "\n",
    "## Split the lap epochs into training and test periods.\n",
    "##### Ideally we could test the lap decoding error by sampling randomly from the time bins and omitting 1/6 of time bins from the placefield building (effectively the training data). These missing bins will be used as the \"test data\" and the decoding error will be computed by decoding them and subtracting the actual measured position during these bins.\n",
    "\n",
    "# ### Get the laps to train on\n",
    "# training_data_portion: float = 9.0/10.0\n",
    "# test_data_portion: float = 1.0 - training_data_portion # test data portion is 1/6 of the total duration\n",
    "# print(f'training_data_portion: {training_data_portion}, test_data_portion: {test_data_portion}')\n",
    "\n",
    "# decoders_dict = deepcopy(track_templates.get_decoders_dict())\n",
    "\n",
    "# # debug_output_hdf5_file_path = Path('output', 'laps_train_test_split.h5').resolve()\n",
    "# debug_output_hdf5_file_path = None\n",
    "\n",
    "# # (train_epochs_dict, test_epochs_dict), train_lap_specific_pf1D_Decoder_dict, split_train_test_lap_specific_configs = TrainTestLapsSplitting.compute_train_test_split_laps_decoders(directional_laps_results, track_templates, training_data_portion=training_data_portion,\n",
    "# #                                                                                                                                                              debug_output_hdf5_file_path=debug_output_hdf5_file_path, debug_plot=False, debug_print=True)  # type: Tuple[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, BasePositionDecoder], Any]\n",
    "\n",
    "# train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = train_lap_specific_pf1D_Decoder_dict\n",
    "\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestSplitResult\n",
    "\n",
    "directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "\n",
    "training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "print(f'training_data_portion: {training_data_portion}, test_data_portion: {test_data_portion}')\n",
    "\n",
    "test_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "train_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict\n",
    "\n",
    "\n",
    "# Tuple[Tuple[Dict, Dict], Dict[str, BasePositionDecoder], Dict]\n",
    "\n",
    "# OUTPUTS: train_test_split_laps_df_dict\n",
    "\n",
    "# ## Get test epochs:\n",
    "# train_epoch_names: List[str] = [k for k in train_test_split_laps_df_dict.keys() if k.endswith('_train')]\n",
    "# test_epoch_names: List[str] = [k for k in train_test_split_laps_df_dict.keys() if k.endswith('_test')]\n",
    "# train_lap_specific_pf1D_Decoder_dict: Dict[str,BasePositionDecoder] = {k.split('_train', maxsplit=1)[0]:split_train_test_lap_specific_pf1D_Decoder_dict[k] for k in train_epoch_names} # the `k.split('_train', maxsplit=1)[0]` part just gets the original key like 'long_LR'\n",
    "# test_epochs_dict: Dict[str,Epoch] = {k.split('_test', maxsplit=1)[0]:v for k,v in train_test_split_laps_epoch_obj_dict.items() if k.endswith('_test')} # the `k.split('_test', maxsplit=1)[0]` part just gets the original key like 'long_LR'\n",
    "\n",
    "# a_training_test_split_laps_epoch_obj_dict[a_training_test_names[0]].to_hdf('output/laps_train_test_split.h5', f'{a_train_epoch_name}/laps_training_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _do_custom_decode_epochs_dict\n",
    "\n",
    "active_laps_decoding_time_bin_size: float = 0.75\n",
    "\n",
    "global_spikes_df: pd.DataFrame = get_proper_global_spikes_df(curr_active_pipeline)\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "\n",
    "# Dict[epoch_split_key, Dict[decoder_name, CustomDecodeEpochsResult]]\n",
    "\n",
    "## INPUTS: flat_epochs_to_decode_dict, active_laps_decoding_time_bin_size\n",
    "## Decoding of the test epochs (what matters):\n",
    "test_decoder_results_dict: Dict[decoder_name, CustomDecodeEpochsResult] = _do_custom_decode_epochs_dict(global_spikes_df=global_spikes_df, global_measured_position_df=global_measured_position_df,\n",
    "                                                                                                                                 pf1D_Decoder_dict=train_lap_specific_pf1D_Decoder_dict,\n",
    "                                                                                                                                 epochs_to_decode_dict=test_epochs_dict, \n",
    "                                                                                                                                 decoding_time_bin_size=active_laps_decoding_time_bin_size,\n",
    "                                                                                                                                 decoder_and_epoch_keys_independent=False)\n",
    "\n",
    "\n",
    "# flat_epochs_to_decode_dict = {f'{k}_train':v for k,v in train_epochs_dict.items()} | {f'{k}_test':v for k,v in test_epochs_dict.items()} # (train_epochs_dict, test_epochs_dict)\n",
    "# final_decoder_results_dict: Dict[epoch_split_key, Dict[decoder_name, CustomDecodeEpochsResult]] = _do_custom_decode_epochs_dict(curr_active_pipeline,\n",
    "#                                                                                                                                  pf1D_Decoder_dict=train_lap_specific_pf1D_Decoder_dict,\n",
    "#                                                                                                                                  epochs_to_decode_dict=flat_epochs_to_decode_dict,\n",
    "#                                                                                                                                  decoding_time_bin_size=active_laps_decoding_time_bin_size,\n",
    "#                                                                                                                                  decoder_and_epoch_keys_independent=True) # epochs_to_decode_dict.keys(): ['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train', 'long_LR_test', 'long_RL_test', 'short_LR_test', 'short_RL_test']\n",
    "# matched_decoder_epochs_final_decoder_results_dict: Dict[decoder_name, CustomDecodeEpochsResult] = {k:v[k.replace('_train', '').replace('_test', '')] for k, v in final_decoder_results_dict.items()} # flatten down to only the matching decoder\n",
    "# matched_decoder_epochs_final_decoder_results_dict\n",
    "# print(list(matched_decoder_epochs_final_decoder_results_dict.keys())) # ['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train', 'long_LR_test', 'long_RL_test', 'short_LR_test', 'short_RL_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import compute_weighted_correlations\n",
    "\n",
    "train_decoded_results_dict: Dict[str, DecodedFilterEpochsResult] = {k:v.decoder_result for k, v in test_decoder_results_dict.items()}\n",
    "\n",
    "weighted_corr_data_dict = compute_weighted_correlations(train_decoded_results_dict, debug_print=True)\n",
    "weighted_corr_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_decoded_wcorr_df = pd.concat(weighted_corr_data_dict)\n",
    "train_decoded_wcorr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result.p_x_given_n_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_decoded_measured_diff_df: pd.DataFrame = test_decoder_results_dict['long_LR'].measured_decoded_position_comparion.decoded_measured_diff_df\n",
    "\n",
    "\n",
    "train_decoded_measured_diff_df_dict: Dict[str, pd.DataFrame] = {k:v.measured_decoded_position_comparion.decoded_measured_diff_df for k, v in test_decoder_results_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce176e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import pho_jointplot\n",
    "import seaborn as sns\n",
    "\n",
    "plot_key: str = 'err_cm'\n",
    "\n",
    "# Plot each list as a separate time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, value in train_decoded_measured_diff_df_dict.items():\n",
    "    # sns.lineplot(x=range(len(value)), y=value, label=key)\n",
    "    _out_line = sns.lineplot(data=value, x='t', y=plot_key, label=key)\n",
    "    _out_scatter = sns.scatterplot(data=value, x='t', y=plot_key) # no `, label=key` because we only want one entry in the legend\n",
    "\n",
    "plt.xlabel('lap_center_t (sec)')\n",
    "plt.ylabel('mean_error [cm]')\n",
    "plt.title('LAp Decoding Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10117f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_dict = {k:Epoch(ensure_dataframe(v.measured_decoded_position_comparion.decoded_measured_diff_df)) for k, v in test_decoder_results_dict.items()}\n",
    "active_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_dict = {k:Epoch(ensure_dataframe(v)) for k, v in train_decoded_measured_diff_df_dict.items()}\n",
    "active_epochs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8f168",
   "metadata": {},
   "source": [
    "# 2024-04-03 - Time-bin effect on lap decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrs import make_class\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "\n",
    "return_full_decoding_results: bool = True\n",
    "desired_laps_decoding_time_bin_size = np.linspace(start=0.030, stop=1.0, num=4)\n",
    "\n",
    "\n",
    "SimpleBatchComputationDummy = make_class('SimpleBatchComputationDummy', attrs=['BATCH_DATE_TO_USE', 'collected_outputs_path'])\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path)\n",
    "\n",
    "custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size,\n",
    "                                                                        use_single_time_bin_per_epoch=[False],\n",
    "                                                                        minimum_event_duration=[desired_laps_decoding_time_bin_size[-1]])\n",
    "\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                save_hdf=True, save_csvs=True,\n",
    "                                                # desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=4),\n",
    "                                                custom_all_param_sweep_options=custom_all_param_sweep_options, # directly provide the parameter sweeps\n",
    "                                                )\n",
    "if return_full_decoding_results:\n",
    "    # with `return_full_decoding_results == True`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    # validate the result:\n",
    "    # {k:v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size for k,v in output_full_directional_merged_decoders_result.items()}\n",
    "    # assert np.all([np.isclose(dict(k)['desired_shared_decoding_time_bin_size'], v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size) for k,v in output_full_directional_merged_decoders_result.items()]), f\"the desired time_bin_size in the parameters should match the one used that will appear in the decoded result\"\n",
    "\n",
    "\n",
    "else:\n",
    "    # with `return_full_decoding_results == False`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    output_full_directional_merged_decoders_result = None\n",
    "\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _show_sweep_result\n",
    "\n",
    "## INPUTS: output_full_directional_merged_decoders_result\n",
    "\n",
    "\n",
    "## RUN\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "# sweep_key_name: str=\"desired_shared_decoding_time_bin_size\"\n",
    "sweep_key_name: str=\"desired_laps_decoding_time_bin_size\"\n",
    "_out_pagination_controller, (all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict) = _show_sweep_result(output_full_directional_merged_decoders_result, global_measured_position_df=global_measured_position_df,\n",
    "                                                                                                                                                        xbin=long_results_obj.original_1D_decoder.xbin,\n",
    "                                                                                                                                                        active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='laps', decoder='all_dir'),\n",
    "                                                                                                                                                        sweep_params_idx=2, sweep_key_name=sweep_key_name, max_subplots_per_page=4)\n",
    "# _out_pagination_controller\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed630cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_laps_decoding_time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context Mask - provides additional information about an Identifying context, like whether a certain component of it should print:\n",
    "# has tags like 'print_debug', 'print_session', 'print_across_sessions'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import CustomDecodeEpochsResult\n",
    "\n",
    "\n",
    "## INPUTS: output_full_directional_merged_decoders_result\n",
    "\n",
    "# Interpolated measured position DataFrame - looks good\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict = CustomDecodeEpochsResult.build_measured_decoded_position_comparison({k:deepcopy(v.all_directional_laps_filter_epochs_decoder_result) for k, v in output_full_directional_merged_decoders_result.items()}, global_measured_position_df=global_measured_position_df)\n",
    "# all_swept_decoded_measured_diff_df_dict = {k:pd.DataFrame(v, columns=['t', 'err']) for k, v in all_swept_decoded_measured_diff_df_dict.items()}\n",
    "\n",
    "# global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "# test_measured_positions_dfs_dict, test_decoded_positions_df_dict, test_decoded_measured_diff_df_dict = CustomDecodeEpochsResult.build_measured_decoded_position_comparison(test_laps_decoder_results_dict, global_measured_position_df=global_measured_position_df)\n",
    "# train_measured_positions_dfs_dict, train_decoded_positions_df_dict, train_decoded_measured_diff_df_dict = CustomDecodeEpochsResult.build_measured_decoded_position_comparison(train_laps_decoder_results_dict, global_measured_position_df=global_measured_position_df)\n",
    "\n",
    "\n",
    "## OUTPUTS: all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict\n",
    "all_swept_decoded_measured_diff_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# # Plot the time series using Seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.lineplot(data=df_melted, x=df_melted.index, y='value', hue='type')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Time Series Plot')\n",
    "# plt.show()\n",
    "\n",
    "sweep_key_name: str=\"desired_laps_decoding_time_bin_size\"\n",
    "\n",
    "## INPUTS: all_swept_decoded_measured_diff_df_dict, sweep_key_name,\n",
    "\n",
    "\n",
    "# Plot each list as a separate time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, error_df in all_swept_decoded_measured_diff_df_dict.items():\n",
    "    # convert frozenset back to dict\n",
    "    a_sweep_params_dict = {s[0]:s[1] for i, s in enumerate(key)}\n",
    "    # error_df['err_cm'] = np.sqrt(error_df['err'])\n",
    "\n",
    "    # plot_key: str = 'err'\n",
    "    plot_key: str = 'err_cm'\n",
    "    \n",
    "    key_label = f'{round(a_sweep_params_dict[sweep_key_name], ndigits=3)}s'\n",
    "    # sns.lineplot(x=range(len(value)), y=value, label=key)\n",
    "    _out_line = sns.lineplot(data=error_df, x='t', y=plot_key, label=key_label)\n",
    "    _out_scatter = sns.scatterplot(data=error_df, x='t', y=plot_key) #  label=key_label, legend=None\n",
    "\n",
    "\n",
    "plt.xlabel('lap_center_t (sec)')\n",
    "plt.ylabel('mean_squared_error')\n",
    "plt.title('All Swept Time Bin Sizes Lap Decoding Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2024-04-03 - Interactively show the lap decoding performance for a single time bin size:\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController\n",
    "\n",
    "\n",
    "_out_pagination_controller = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(an_all_directional_laps_filter_epochs_decoder_result.active_filter_epochs,\n",
    "                                                                                            an_all_directional_laps_filter_epochs_decoder_result,\n",
    "                                                                                            xbin=long_results_obj.original_1D_decoder.xbin, global_pos_df=global_session.position.df,\n",
    "                                                                                            active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices',\n",
    "                                                                                                                                                                #   t_bin=f'{an_all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size}s',\n",
    "                                                                                                                                                                  t_bin=f\"{a_sweep_params_dict['desired_shared_decoding_time_bin_size']}s\",\n",
    "                                                                                                                                                                   epochs='laps', decoder='all_dir'),\n",
    "                                                                                            a_name='an_all_directional_laps_filter_epochs_decoder_result', max_subplots_per_page=20)\n",
    "_out_pagination_controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import compute_weighted_correlations\n",
    "\n",
    "\n",
    "# out_wcorr_df_dict = compute_weighted_correlations({k:[a_p_x_given_n for a_p_x_given_n in deepcopy(v.all_directional_laps_filter_epochs_decoder_result.p_x_given_n_list)] for k, v in output_full_directional_merged_decoders_result.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result: DirectionalPseudo2DDecodersResult = list(output_full_directional_merged_decoders_result.values())[-2]\n",
    "all_directional_laps_filter_epochs_decoder_result: DecodedFilterEpochsResult = a_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "# out_wcorr_df_dict = compute_weighted_correlations({k:deepcopy(v.all_directional_laps_filter_epochs_decoder_result) for k, v in output_full_directional_merged_decoders_result.items()})\n",
    "# out_wcorr_df_dict = compute_weighted_correlations({'out': all_directional_laps_filter_epochs_decoder_result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997b1e4",
   "metadata": {},
   "source": [
    "# Completely Different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "\n",
    "laps_weighted_corr_merged_df = add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=laps_weighted_corr_merged_df)\n",
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5436c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df[laps_weighted_corr_merged_df['best_decoder_index'] != laps_weighted_corr_merged_df['true_decoder_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df[laps_weighted_corr_merged_df['best_decoder_index'] == laps_weighted_corr_merged_df['true_decoder_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e57782",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325be3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult], decoder_laps_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict,\n",
    "# laps_weighted_corr_merged_df: pd.DataFrame, decoder_laps_weighted_corr_df_dict: Dict[str, pd.DataFrame],\n",
    "# laps_simple_pf_pearson_merged_df: pd.DataFrame\n",
    "# laps_simple_pf_pearson_merged_df\n",
    "laps_weighted_corr_merged_df\n",
    "\n",
    "# ['best_decoder_index'] # gives the index of the decoder with the best value of wcorr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the ground truth for the decoded laps:\n",
    "groudtruth_laps_epochs_df: pd.DataFrame = directional_merged_decoders_result.add_groundtruth_information(curr_active_pipeline=curr_active_pipeline)\n",
    "groudtruth_laps_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_column_names = ['maze_id', 'is_LR_dir', 'is_most_likely_track_identity_Long', 'is_most_likely_direction_LR']\n",
    "groudtruth_laps_epochs_df[groundtruth_column_names]\n",
    "\n",
    "lap_idxs = groudtruth_laps_epochs_df['lap_id'] - 1\n",
    "lap_idxs\n",
    "## add the truth columns to `laps_weighted_corr_merged_df`:\n",
    "laps_weighted_corr_merged_df[groundtruth_column_names] = groudtruth_laps_epochs_df[groundtruth_column_names]\n",
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6402fe8",
   "metadata": {},
   "source": [
    "##  2024-03-07 - measured v. best-decoded Position + Derivatives Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import _compute_pos_derivs\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring, HeuristicScoresTuple\n",
    "\n",
    "\n",
    "measured_position_df = deepcopy(curr_active_pipeline.sess.position.to_dataframe())\n",
    "# # lap positions only:\n",
    "# measured_position_df = measured_position_df[~measured_position_df['lap'].isnull()] # only get the positions during the laps\n",
    "# measured_position_df['lap'] = measured_position_df['lap'].astype('int64')\n",
    "measured_position_df\n",
    "\n",
    "new_measured_pos_df: pd.DataFrame = _compute_pos_derivs(measured_position_df['t'].to_numpy(), position = deepcopy(measured_position_df['lin_pos'].to_numpy()), decoding_time_bin_size=laps_decoding_time_bin_size) \n",
    "# new_measured_pos_df\n",
    "\n",
    "extra_column_names = ['lap', 'lap_dir'] # 'y', \n",
    "assert np.shape(new_measured_pos_df)[0] == np.shape(measured_position_df)[0]\n",
    "new_measured_pos_df[extra_column_names] = measured_position_df[extra_column_names].copy()\n",
    "\n",
    "# lap positions only:\n",
    "new_measured_pos_df = new_measured_pos_df[~new_measured_pos_df['lap'].isnull()] # only get the positions during the laps\n",
    "new_measured_pos_df['lap'] = new_measured_pos_df['lap'].astype('int64')\n",
    "new_measured_pos_df\n",
    "\n",
    "\n",
    "# new_measured_pos_df['lap_dir'] = new_measured_pos_df['lap_dir'].astype('int64')\n",
    "\n",
    "# new_measured_pos_df\n",
    "\n",
    "# new_measured_pos_df.describe()\n",
    "\n",
    "\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "\n",
    "# all_epochs_position_derivatives_df_dict_dict = {}\n",
    "all_epochs_position_derivatives_df_dict: Dict[str, pd.DataFrame] = {}\n",
    "_out_new_scores = {}\n",
    "\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "for an_epoch_type_name, a_decoded_filter_epochs_decoder_result_dict in zip(('lap', 'ripple'), (deepcopy(decoder_laps_filter_epochs_decoder_result_dict), deepcopy(decoder_ripple_filter_epochs_decoder_result_dict))):\n",
    "    # all_epochs_position_derivatives_df_dict_dict[an_epoch_type_name] =\n",
    "    # any_good_selected_epoch_times = deepcopy(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times)\n",
    "    # any_good_selected_epoch_indicies = deepcopy(paginated_multi_decoder_decoded_epochs_window.find_data_indicies_from_epoch_times(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times))\n",
    "    # any_good_selected_epoch_indicies\n",
    "\n",
    "    # with suppress_print_context():\n",
    "    # with disable_function_context(builtins, \"print\"):\n",
    "    # _out_new_scores = {}\n",
    "    # an_epoch_idx: int = 0 # 7\n",
    "    axs = None\n",
    "    # all_epochs_position_derivatives_df_dict: Dict[str, pd.DataFrame] = {}\n",
    "    for a_decoder_name, a_result in a_decoded_filter_epochs_decoder_result_dict.items():\n",
    "        # print(f'\\na_name: {a_name}')\n",
    "        combined_key: str = f\"_\".join([an_epoch_type_name, a_decoder_name])\n",
    "                                      \n",
    "        #  2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "        # directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "        _out_new_scores[combined_key] = [HeuristicReplayScoring.compute_pho_heuristic_replay_scores(a_result=a_result, an_epoch_idx=an_epoch_idx, enable_debug_plot=False, debug_plot_axs=axs, debug_plot_name=f\"{combined_key}\") for an_epoch_idx in np.arange(a_result.num_filter_epochs)]\n",
    "        all_epochs_position_derivatives_df_dict[combined_key] = pd.concat([a_scores.position_derivatives_df for a_scores in _out_new_scores[combined_key]], ignore_index=True)\n",
    "\n",
    "_out_new_scores\n",
    "\n",
    "## Outputs: all_epochs_position_derivatives_df_dict, _out_new_scores, (measured_position_df, new_measured_pos_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41abbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import debug_plot_helper_add_position_and_derivatives\n",
    "\n",
    "\n",
    "fig, debug_plot_axs = debug_plot_helper_add_position_and_derivatives(new_measured_pos_df['t'].to_numpy(), new_measured_pos_df['x'].to_numpy(), new_measured_pos_df['vel_x'].to_numpy(), new_measured_pos_df['accel_x'].to_numpy(),\n",
    "                                                                        debug_plot_axs=None, debug_plot_name='measured', common_plot_kwargs=dict(color='k', markersize='5', marker='.', linestyle='None', alpha=0.35))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import debug_plot_position_and_derivatives_figure\n",
    "\n",
    "## INPUTS: new_measured_pos_df, all_epochs_position_derivatives_df_dict\n",
    "fig, debug_plot_axs = debug_plot_position_and_derivatives_figure(new_measured_pos_df, all_epochs_position_derivatives_df_dict, debug_plot_axs=None, debug_figure_title=None, enable_debug_plot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import debug_plot_position_derivatives_stack\n",
    "\n",
    "# fig = debug_plot_position_derivatives_stack(new_measured_pos_df, all_epochs_position_derivatives_df_dict)\n",
    "fig = debug_plot_position_derivatives_stack(new_measured_pos_df, all_epochs_position_derivatives_df_dict, show_scatter=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e791125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import sanitize_filename_for_Windows\n",
    "\n",
    "def save_plotly(a_fig, a_fig_context):\n",
    "    \"\"\" \n",
    "    captures: TODAY_DAY_DATE\n",
    "    \"\"\"\n",
    "    fig_save_path: Path = figures_folder.joinpath('_'.join([get_now_day_str(), sanitize_filename_for_Windows(a_fig_context.get_description())])).resolve()\n",
    "    figure_out_paths = {'.html': fig_save_path.with_suffix('.html'), '.png': fig_save_path.with_suffix('.png')}\n",
    "    a_fig.write_html(figure_out_paths['.html'])\n",
    "    display(fullwidth_path_widget(figure_out_paths['.html'], file_name_label='.html'))\n",
    "    # print(file_uri_from_path(figure_out_paths['.html']))\n",
    "    a_fig.write_image(figure_out_paths['.png'])\n",
    "    # print(file_uri_from_path(figure_out_paths['.png']))\n",
    "    display(fullwidth_path_widget(figure_out_paths['.png'], file_name_label='.png'))\n",
    "    return figure_out_paths\n",
    "\n",
    "\n",
    "figures_folder = Path('output').resolve()\n",
    "a_fig_context = curr_active_pipeline.build_display_context_for_session(display_fn_name='debug_plot_position_derivatives_stack')\n",
    "save_plotly(fig, a_fig_context)\n",
    "\n",
    "# fig_save_path: Path = figures_folder.joinpath('_'.join([get_now_day_str(), sanitize_filename_for_Windows(a_fig_context.get_description())])).resolve()\n",
    "# figure_out_paths = {'.html': fig_save_path.with_suffix('.html'), '.png': fig_save_path.with_suffix('.png')}\n",
    "# a_fig.write_html(figure_out_paths['.html'])\n",
    "# display(fullwidth_path_widget(figure_out_paths['.html'], file_name_label='.html'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7fbba6",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: df1, df2\n",
    "position_deriv_column_names1 = pos_deriv_column_names\n",
    "df1 = measured_position_df[position_deriv_column_names1]\n",
    "\n",
    "position_deriv_column_names2 = ['x', 'vel_x', 'accel_x']\n",
    "df2 = deepcopy(all_epochs_position_derivatives_df[position_deriv_column_names2])\n",
    "\n",
    "# Set up the figure and axes.\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 6))\n",
    "\n",
    "# List of columns to compare\n",
    "columns_to_compare = ['col1', 'col2', 'col3']\n",
    "\n",
    "\n",
    "# Loop through the list of columns and create a histogram for each.\n",
    "for i, (col1, col2) in enumerate(zip(position_deriv_column_names1, position_deriv_column_names2)):\n",
    "# for i, col in enumerate(columns_to_compare):\n",
    "    # Use the same bin edges for both histograms by computing them from the combined range of both DataFrames\n",
    "    combined_range = pd.concat([df1[col1], df2[col2]])\n",
    "    bins = np.histogram_bin_edges(combined_range, bins='auto')\n",
    "\n",
    "    # Plot the first DataFrame histogram\n",
    "    df1[col1].hist(bins=bins, ax=axes[i], alpha=0.5, label='Decoded')\n",
    "\n",
    "    # Plot the second DataFrame histogram\n",
    "    df2[col2].hist(bins=bins, ax=axes[i], alpha=0.5, label='Measured')\n",
    "\n",
    "    # Set the title and labels\n",
    "    axes[i].set_title(f'Histogram of {col1}')\n",
    "    axes[i].set_xlabel(col1)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    # Add a legend\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout for readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5670d2",
   "metadata": {},
   "source": [
    "#  2024-04-27 - Save Posteriors as Yellow-Blue plots to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import save_posterior\n",
    "\n",
    "\n",
    "directional_decoders_decode_result\n",
    "\n",
    "collapsed_per_lap_epoch_marginal_track_identity_point = laps_marginals_df[['P_Long', 'P_Short']].to_numpy().astype(float)\n",
    "collapsed_per_lap_epoch_marginal_dir_point = laps_marginals_df[['P_LR', 'P_RL']].to_numpy().astype(float)\n",
    "\n",
    "for epoch_id in np.arange(laps_filter_epochs_decoder_result.num_filter_epochs):\n",
    "\traw_tuple, marginal_dir_tuple, marginal_track_identity_tuple, marginal_dir_point_tuple, marginal_track_identity_point_tuple = save_posterior(raw_posterior_laps_marginals, laps_directional_marginals, laps_track_identity_marginals, collapsed_per_lap_epoch_marginal_dir_point, collapsed_per_lap_epoch_marginal_track_identity_point,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tparent_array_as_image_output_folder=parent_array_as_image_output_folder, epoch_id_identifier_str='lap', epoch_id=epoch_id)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f77b16",
   "metadata": {},
   "source": [
    "#  2024-05-24 07:24 - Shuffed WCorr Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2244da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wcorr_ripple_shuffle.n_completed_shuffles: 1200\n",
      "n_epochs: 136\n",
      "n_completed_shuffles: 1200\n"
     ]
    }
   ],
   "source": [
    "## wcorr is computed on each decoded posterior: `curr_results_obj.p_x_given_n_list`\n",
    "\n",
    "## Actually need to shuffle the unit idenities and recompute the posteriors\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "from typing import NewType\n",
    "\n",
    "import neuropy.utils.type_aliases as types\n",
    "from neuropy.utils.misc import build_shuffled_ids, shuffle_ids # used in _SHELL_analyze_leave_one_out_decoding_results\n",
    "from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "DecodedEpochsResultsDict = NewType('DecodedEpochsResultsDict', Dict[types.DecoderName, DecodedFilterEpochsResult]) # A Dict containing the decoded filter epochs result for each of the four 1D decoder names\n",
    "ShuffleIdx = NewType('ShuffleIdx', int)\n",
    "\n",
    "wcorr_shuffle_results: SequenceBasedComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('SequenceBased', None)\n",
    "if wcorr_shuffle_results is not None:    \n",
    "    wcorr_ripple_shuffle: WCorrShuffle = wcorr_shuffle_results.wcorr_ripple_shuffle\n",
    "    print(f'wcorr_ripple_shuffle.n_completed_shuffles: {wcorr_ripple_shuffle.n_completed_shuffles}')\n",
    "else:\n",
    "    print(f'SequenceBased is not computed.')\n",
    "\n",
    "# wcorr_ripple_shuffle: WCorrShuffle = WCorrShuffle.init_from_templates(curr_active_pipeline=curr_active_pipeline, enable_saving_entire_decoded_shuffle_result=True)\n",
    "\n",
    "n_epochs: int = wcorr_ripple_shuffle.n_epochs\n",
    "print(f'n_epochs: {n_epochs}')\n",
    "n_completed_shuffles: int = wcorr_ripple_shuffle.n_completed_shuffles\n",
    "print(f'n_completed_shuffles: {n_completed_shuffles}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## modernize class:\n",
    "WCorrShuffle(**wcorr_ripple_shuffle.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f3e92d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 36 69 72 57 10  9 30 31 24 27 48 19 77 53 66 15 42  1 59 60  6 12 74 47 26 33  2 68 75 22 50 34 61 64 13 65 28 43 20 39  7 76 45 67  3 18 40  0 25 62 70 52 29 58 37 32 71 51 56 23 14 41 73 49 78  5 55 44 21  4 63 35 46 16 38 54 17 11  8], a_shuffle_aclus: [104  48  91  95  77  14  13  39  40  31  34  63  26 102  70  87  20  57   3  80  81   9  16  98  62  33  44   4  90 100  29  67  45  82  85  18  86  35  58  27  53  11 101  60  89   5  25  55   2  32  83  92  69  38  79  51  43  93  68  75  30  19  56  97  66 103   8  72  59  28   6  84  47  61  23  52  71  24  15  12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29 41 45 12 18 53 28 69 56 15 13 67 35 46 64 39 44 20 34 51 40  7 17 23  9 16 37 73 63 50 77 71 11 54 52 24 14 59 74 61  1 43  6  2 70 19 48 79 10 57 55 78 26  5  4 25 49  0 76 62 33 60 38 72 42 68 58 66 22 30 47 75  8 36 21 32 65 31 27  3], a_shuffle_aclus: [ 38  56  60  16  25  70  35  91  75  20  18  89  47  61  85  53  59  27  45  68  55  11  24  30  13  23  51  97  84  67 102  93  15  71  69  31  19  80  98  82   3  58   9   4  92  26  63 104  14  77  72 103  33   8   6  32  66   2 101  83  44  81  52  95  57  90  79  87  29  39  62 100  12  48  28  43  86  40  34   5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [49 16 75 40 54 58 47 78 22 60 52 67 17 25 73 65 34 38 53 21  2 42  5 72 74 35 62 41 27 26 56 46 18 43 63  6 61 39 28 45 79  1 12 24 71  9 23 59  0 19 68 30 11 33  4 51 29 13 44 10 69 14  3 50 32 70 48  7 55 36 15 66 37 76 64 57 20  8 77 31], a_shuffle_aclus: [ 66  23 100  55  71  79  62 103  29  81  69  89  24  32  97  86  45  52  70  28   4  57   8  95  98  47  83  56  34  33  75  61  25  58  84   9  82  53  35  60 104   3  16  31  93  13  30  80   2  26  90  39  15  44   6  68  38  18  59  14  91  19   5  67  43  92  63  11  72  48  20  87  51 101  85  77  27  12 102  40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 30 68 70 40 17 73 62 66 23 44  8 61 49 26  2 67 39 41 11  9 31 77 42  6 29 52 65 35  4 43 20 50 27  5 18 13 74 57 53  1  3 32  0 36 22 72 45 15 69 55 59 25 76 58 54 63 19 75 71 28 51 34 64 38 56 46 48 14 21 47 33 12 60 24 16 37 10 78 79], a_shuffle_aclus: [ 11  39  90  92  55  24  97  83  87  30  59  12  82  66  33   4  89  53  56  15  13  40 102  57   9  38  69  86  47   6  58  27  67  34   8  25  18  98  77  70   3   5  43   2  48  29  95  60  20  91  72  80  32 101  79  71  84  26 100  93  35  68  45  85  52  75  61  63  19  28  62  44  16  81  31  23  51  14 103 104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [43  7 17 34 78 76 44 56 12 31 19 10 79 24 39 41 28 67 36 54 15  5 27 16 20 38 58 47 70 13 66 21 61  9 22 18 40  0 77 35 33 14 60 23  6 46 37 45 69 49 68 30 25 75 55 59 52  3 64 72 50 73 26  4 71  2 74 11 65  8 62 32 48 51 63 53 57 29  1 42], a_shuffle_aclus: [ 58  11  24  45 103 101  59  75  16  40  26  14 104  31  53  56  35  89  48  71  20   8  34  23  27  52  79  62  92  18  87  28  82  13  29  25  55   2 102  47  44  19  81  30   9  61  51  60  91  66  90  39  32 100  72  80  69   5  85  95  67  97  33   6  93   4  98  15  86  12  83  43  63  68  84  70  77  38   3  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22 43  9 62 65 29 58 72 34 52 77 64 26  6 28 17 48 69 75  5 11 73 68 45 71 61  1 50 60 53 32 42 31  7 59 41 14 38 24 44 35 15 19 79 55 13 33 40 63  0 46 25  4 78  2 67 27 20 54 12 23 37 21 49 30 66 57  3 74 16 10  8 56 47 76 36 70 18 51 39], a_shuffle_aclus: [ 29  58  13  83  86  38  79  95  45  69 102  85  33   9  35  24  63  91 100   8  15  97  90  60  93  82   3  67  81  70  43  57  40  11  80  56  19  52  31  59  47  20  26 104  72  18  44  55  84   2  61  32   6 103   4  89  34  27  71  16  30  51  28  66  39  87  77   5  98  23  14  12  75  62 101  48  92  25  68  53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 10  7 35 23 34 41 65 62 53 69 26 27 63 44 31 15 28 64 25 17 40 79 33 24 37  4 29 51 45  6 48  9 67 73 38 61 50 58 60 52  5 55 20  8 76 16 70 13  0 11 19 42 66 21 77 43 30 49 74 68 57 22 32 12 36 54 14 71  3 72 75 18 39 78 59  1 47  2 46], a_shuffle_aclus: [ 75  14  11  47  30  45  56  86  83  70  91  33  34  84  59  40  20  35  85  32  24  55 104  44  31  51   6  38  68  60   9  63  13  89  97  52  82  67  79  81  69   8  72  27  12 101  23  92  18   2  15  26  57  87  28 102  58  39  66  98  90  77  29  43  16  48  71  19  93   5  95 100  25  53 103  80   3  62   4  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [59 78 61  6 14 56 46 18 64 52 49 36 40 75 11 25 13 32 77 29 31 21 27 44 42 15  3 23  0  7 53 16  5 69 35 71 47 34 73 38 63 66 12 19 67 33  2  1 76 70  4 50 22 10 54 41 43 20 58 26 68 37  9 48 57 45 30 17 39 51 62 74 79 65 24 55 28  8 72 60], a_shuffle_aclus: [ 80 103  82   9  19  75  61  25  85  69  66  48  55 100  15  32  18  43 102  38  40  28  34  59  57  20   5  30   2  11  70  23   8  91  47  93  62  45  97  52  84  87  16  26  89  44   4   3 101  92   6  67  29  14  71  56  58  27  79  33  90  51  13  63  77  60  39  24  53  68  83  98 104  86  31  72  35  12  95  81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 33 31 55  3 25 18 56 57 20  0  6 41 28 14  4 46  5 72 21 79  9 45 43 71 63 70 36  2 35  7 52 27 51 32 15 62 38 59 60  1 75 29 47 49 76 11 10 26 34 40 22 13  8 17 65 54 58 77 73 42 67 68 37 44 23 69 16 50 53 12 78 24 66 30 39 74 48 61 19], a_shuffle_aclus: [ 85  44  40  72   5  32  25  75  77  27   2   9  56  35  19   6  61   8  95  28 104  13  60  58  93  84  92  48   4  47  11  69  34  68  43  20  83  52  80  81   3 100  38  62  66 101  15  14  33  45  55  29  18  12  24  86  71  79 102  97  57  89  90  51  59  30  91  23  67  70  16 103  31  87  39  53  98  63  82  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22  7 39 28 45  5 41 14 65  9 40 55 49 71 54 17 30 60 63 31 12 11 74 33 47 44 42 59 57 48 36 26  4 53 23 34 35 78 43  0 21  2 16  3 19 51 58 62  8  1 61 68  6 77 73 15 37 32 56 70 24 46 67 52 13 38 50 76 27 75 69 66 10 72 79 29 18 64 20 25], a_shuffle_aclus: [ 29  11  53  35  60   8  56  19  86  13  55  72  66  93  71  24  39  81  84  40  16  15  98  44  62  59  57  80  77  63  48  33   6  70  30  45  47 103  58   2  28   4  23   5  26  68  79  83  12   3  82  90   9 102  97  20  51  43  75  92  31  61  89  69  18  52  67 101  34 100  91  87  14  95 104  38  25  85  27  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66 14  5  2 78  1 24 75 29 38 47 71 18 31 79 27 76  8 46 77 54 40 70 57 11 33 49 63 59 45 15 35 36 48 41 73 10 22 19 67 16  7 25 21 64 39 69 61 74 13 68 44 20 58  3 32 62 12 55 60 34  6  4 56 42 72 23 52 37 65 28 30 50 17  0 26  9 43 51 53], a_shuffle_aclus: [ 87  19   8   4 103   3  31 100  38  52  62  93  25  40 104  34 101  12  61 102  71  55  92  77  15  44  66  84  80  60  20  47  48  63  56  97  14  29  26  89  23  11  32  28  85  53  91  82  98  18  90  59  27  79   5  43  83  16  72  81  45   9   6  75  57  95  30  69  51  86  35  39  67  24   2  33  13  58  68  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [69 63 17 11 76  6 71 65 67 20 57 34 72 39 59 74 50 19  7 16 21 28 66 64 33 18 41 43 30 44 29 14 54 23 62 46 10 61 47 12 73 49  1  0 45  5 13 56 51 35 38 15 70  9 78 52 37 79 42 27 22 75  3 26  4 77 31 36 24 55 40 58 53 68  8  2 48 60 25 32], a_shuffle_aclus: [ 91  84  24  15 101   9  93  86  89  27  77  45  95  53  80  98  67  26  11  23  28  35  87  85  44  25  56  58  39  59  38  19  71  30  83  61  14  82  62  16  97  66   3   2  60   8  18  75  68  47  52  20  92  13 103  69  51 104  57  34  29 100   5  33   6 102  40  48  31  72  55  79  70  90  12   4  63  81  32  43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [23  0 74 78 76  5 24 34  8 33 40 19 32 49 28 31 45 77 57 71 53 26 61 55 50 51 59 10  1 42 36 25  7  9 73 30 58  2 38 52 54 75 22 79 65 39 37 18 29 70 17 13 27 44 35 14 11 12  6 64 62 69 21  4 47 63 72 48  3 15 16 60 46 66 41 67 20 68 43 56], a_shuffle_aclus: [ 30   2  98 103 101   8  31  45  12  44  55  26  43  66  35  40  60 102  77  93  70  33  82  72  67  68  80  14   3  57  48  32  11  13  97  39  79   4  52  69  71 100  29 104  86  53  51  25  38  92  24  18  34  59  47  19  15  16   9  85  83  91  28   6  62  84  95  63   5  20  23  81  61  87  56  89  27  90  58  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 45 48 60 39 35 76 58 49 36 22  1 34 71 55 79 29  7 59 18  3 78 50 70 72 73 51  6 63 69  4 42 25  9 52 10 12 75  0 23 40 53 56  5 33 21 31 11 37 68 74 77 57 47 27 14 44 32 17 65 13 28 15 41 26 46  2 30 24 61 67 66 38 43 54 62 20 19 16 64], a_shuffle_aclus: [ 12  60  63  81  53  47 101  79  66  48  29   3  45  93  72 104  38  11  80  25   5 103  67  92  95  97  68   9  84  91   6  57  32  13  69  14  16 100   2  30  55  70  75   8  44  28  40  15  51  90  98 102  77  62  34  19  59  43  24  86  18  35  20  56  33  61   4  39  31  82  89  87  52  58  71  83  27  26  23  85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [39 28 38 20 58  6 62 36 61 49 54 55 34 21 35 52 68 22  1  3 18 50 41 26  5 46 70 43 13 66 59 31 67 11 16 71 73 37 23  0 42 57 24 60 74 40 51 32  7 19 78 12 65 30 69  4 72 53 76  9 47  8 79 77 14 10 25 27 64 75 63 45 48 15 29 17  2 44 56 33], a_shuffle_aclus: [ 53  35  52  27  79   9  83  48  82  66  71  72  45  28  47  69  90  29   3   5  25  67  56  33   8  61  92  58  18  87  80  40  89  15  23  93  97  51  30   2  57  77  31  81  98  55  68  43  11  26 103  16  86  39  91   6  95  70 101  13  62  12 104 102  19  14  32  34  85 100  84  60  63  20  38  24   4  59  75  44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [34 79 22 48 62 42 30 17 14 19  9 25 59 13  3 78 10 31 75 54 70  5 27 68 74 26  1 58 67  0 12 18 47 43  2 23 28 41 44 35 56 36 64 72 52 69 49 38 37 45  8 76 20 15 40 16 61 77  7 57 32 60 50 63 24 55 53 21 29  4 66 46 33  6 71 51 65 39 11 73], a_shuffle_aclus: [ 45 104  29  63  83  57  39  24  19  26  13  32  80  18   5 103  14  40 100  71  92   8  34  90  98  33   3  79  89   2  16  25  62  58   4  30  35  56  59  47  75  48  85  95  69  91  66  52  51  60  12 101  27  20  55  23  82 102  11  77  43  81  67  84  31  72  70  28  38   6  87  61  44   9  93  68  86  53  15  97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 46 29 18 45 26 30 43 71 50 39  7 37 33 41 40 56 65 61  4 74  6 19 24 21 22 13 49 64 35 73 62 25 72 31 34 77 63 32  3 52 12 55 67 53 57 79  2 76  9 70 11 44  1 48 15 59 75 51 28 47 36 16 58 54 68 10 66 42 14 23  0 60  8 38 20 78  5 27 69], a_shuffle_aclus: [ 24  61  38  25  60  33  39  58  93  67  53  11  51  44  56  55  75  86  82   6  98   9  26  31  28  29  18  66  85  47  97  83  32  95  40  45 102  84  43   5  69  16  72  89  70  77 104   4 101  13  92  15  59   3  63  20  80 100  68  35  62  48  23  79  71  90  14  87  57  19  30   2  81  12  52  27 103   8  34  91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 29  7 10 40  8 52 48 33 36 16 66 14 62 42 38  6  0  1 30 20 58 44 19 56 21 13 49 46 75 67 59 73  4 71 63 79 78 51 53 39 26 31 60 23 54 25 70 18 24 37 15 22 69 77 47 65 34  9 43 35 27 17  5 32 74 64 28 76 55 41 72 57 68 61 11 12  3 50 45], a_shuffle_aclus: [  4  38  11  14  55  12  69  63  44  48  23  87  19  83  57  52   9   2   3  39  27  79  59  26  75  28  18  66  61 100  89  80  97   6  93  84 104 103  68  70  53  33  40  81  30  71  32  92  25  31  51  20  29  91 102  62  86  45  13  58  47  34  24   8  43  98  85  35 101  72  56  95  77  90  82  15  16   5  67  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [13 61 77 47 24 56 78 36 44 19 25 16 28 63 38 54  8 39 14 70 30  4  3 20 52  9 42 18 74  5 40 17 62 23  7 12 51 53 10 45 27 21 29  6 35 71 64 66 68 76 33 67 72 32 11 34 60 46 59 37 73 50 58 31 41 48 49 57 65  2 26 75 55 79  0 69 15  1 22 43], a_shuffle_aclus: [ 18  82 102  62  31  75 103  48  59  26  32  23  35  84  52  71  12  53  19  92  39   6   5  27  69  13  57  25  98   8  55  24  83  30  11  16  68  70  14  60  34  28  38   9  47  93  85  87  90 101  44  89  95  43  15  45  81  61  80  51  97  67  79  40  56  63  66  77  86   4  33 100  72 104   2  91  20   3  29  58]\n",
      "a_shuffle_IDXs: [66 51 60 71 36 58  9 55 26  3 15 47 54 63 16 18 14 72 52  4 28 23 33  2 48 44 24 12 41 11 17 38 68 43  7  6 78 34 42 65 62 73 35 22 46 67 70 49 40 45 53 19 27 56 30  1 69 13  5  0 64 50 37 79 61 21 74 20  8 77 75 10 29 59 76 25 57 39 32 31], a_shuffle_aclus: [ 87  68  81  93  48  79  13  72  33   5  20  62  71  84  23  25  19  95  69   6  35  30  44   4  63  59  31  16  56  15  24  52  90  58  11   9 103  45  57  86  83  97  47  29  61  89  92  66  55  60  70  26  34  75  39   3  91  18   8   2  85  67  51 104  82  28  98  27  12 102 100  14  38  80 101  32  77  53  43  40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 38 48 55  2 56 32 37 65 67 47 72 52 29 43 73 35 42 57 20 61 75 23 15 41 33 78 49  8  1 70 10 51 59 28  9  6 60 31 11 66 68 79 63 58 22 71 25 62 36 34 13 46 40 24 12 30 39 16 19  4 50 26  5 21 69 77 45 27  3 14  0 64 17 54 18 44 53  7 76], a_shuffle_aclus: [ 98  52  63  72   4  75  43  51  86  89  62  95  69  38  58  97  47  57  77  27  82 100  30  20  56  44 103  66  12   3  92  14  68  80  35  13   9  81  40  15  87  90 104  84  79  29  93  32  83  48  45  18  61  55  31  16  39  53  23  26   6  67  33   8  28  91 102  60  34   5  19   2  85  24  71  25  59  70  11 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [49 19 73 58 13 32 44 69 12 24 77 37 26 11 17  4  3 54  7 72 45 10 36 65 56 21 59 66 57 64 38 63 35 79 33 74 78 47 15  5  2 31 52 28 40 67  1 48 27 53 71 18  9 25 20 46 30 61 41 55 70 29 76 75 23  6 62 22 50 16  8 39  0 60 34 14 42 68 51 43], a_shuffle_aclus: [ 66  26  97  79  18  43  59  91  16  31 102  51  33  15  24   6   5  71  11  95  60  14  48  86  75  28  80  87  77  85  52  84  47 104  44  98 103  62  20   8   4  40  69  35  55  89   3  63  34  70  93  25  13  32  27  61  39  82  56  72  92  38 101 100  30   9  83  29  67  23  12  53   2  81  45  19  57  90  68  58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 16  5 76 13 75 63 60 72 17 40  7 57 22 26 34 23 42 27 20 46 79 28 30 73 62 51 38 35 54 37 48 31 52 15 33 69 11  0 12 32 45 25  3 68 41  4 71 21 19 44 10 64 29 78 55  1 50 61 43 53 49 74 59 66  8 65 39  2 58  6 14 77 18 36 56 24 47 70  9], a_shuffle_aclus: [ 89  23   8 101  18 100  84  81  95  24  55  11  77  29  33  45  30  57  34  27  61 104  35  39  97  83  68  52  47  71  51  63  40  69  20  44  91  15   2  16  43  60  32   5  90  56   6  93  28  26  59  14  85  38 103  72   3  67  82  58  70  66  98  80  87  12  86  53   4  79   9  19 102  25  48  75  31  62  92  13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45 20 73 57 69 33 14 27 64 35 58 44 39 29 37 30 18 61 41 15 71 62 13 75 36 50 38  2 77 47 78 66 79 65  5  7 46 72  6 23 17 63 55 49 68 59 40 12 74 10 31 76 28 32  1 51 56 42 43 54 16 70 25 48 60 26 22 53 11 67 21  8 34 19 52  4  9  0 24  3], a_shuffle_aclus: [ 60  27  97  77  91  44  19  34  85  47  79  59  53  38  51  39  25  82  56  20  93  83  18 100  48  67  52   4 102  62 103  87 104  86   8  11  61  95   9  30  24  84  72  66  90  80  55  16  98  14  40 101  35  43   3  68  75  57  58  71  23  92  32  63  81  33  29  70  15  89  28  12  45  26  69   6  13   2  31   5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 72 16 31 50 54 44  1 45 18 35 39 27 68  8 26 64 66 29 67 15 37 40 22 28  5 51 14 10 47 49 38 52 36 58 11 48  0 34 41  6 53 63 71  7 57 32 75  3 13 78 76 42 55 12 77 70 33 21 79 62 56 30 20 59 25 43 23 60 73  9 17 74 19 69 46 61 24  2 65], a_shuffle_aclus: [  6  95  23  40  67  71  59   3  60  25  47  53  34  90  12  33  85  87  38  89  20  51  55  29  35   8  68  19  14  62  66  52  69  48  79  15  63   2  45  56   9  70  84  93  11  77  43 100   5  18 103 101  57  72  16 102  92  44  28 104  83  75  39  27  80  32  58  30  81  97  13  24  98  26  91  61  82  31   4  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [15 74 12 78 25 39 75 65 66 18 43 41 40 20 55 22 17 61 28 16  2 45 46  8 37  9 59 38 33 23  7  3 30 19 21 31 73 71 51 13 10 36 52 26 27  0 72 24 14 35 34 63  6 54 47 67 53 57  5 60 56 11 44 77 70 58  1 62 48 49 68 42 50  4 29 76 32 69 79 64], a_shuffle_aclus: [ 20  98  16 103  32  53 100  86  87  25  58  56  55  27  72  29  24  82  35  23   4  60  61  12  51  13  80  52  44  30  11   5  39  26  28  40  97  93  68  18  14  48  69  33  34   2  95  31  19  47  45  84   9  71  62  89  70  77   8  81  75  15  59 102  92  79   3  83  63  66  90  57  67   6  38 101  43  91 104  85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 57  8 26 51  6 32 38 71 34 58 76 29  4 24 31  5 60 40 56 69 45 12 41 54 42 78 15 77 53  3 49 66 11 44 28 70  7  1  0 27 65 33 16 30 74 52 63 61 22 46 59 23  2  9 25 72 20 47 73 79 48 13 37 17 39 68 43 14 35 67 36 19 50 62 10 18 64 21 55], a_shuffle_aclus: [100  77  12  33  68   9  43  52  93  45  79 101  38   6  31  40   8  81  55  75  91  60  16  56  71  57 103  20 102  70   5  66  87  15  59  35  92  11   3   2  34  86  44  23  39  98  69  84  82  29  61  80  30   4  13  32  95  27  62  97 104  63  18  51  24  53  90  58  19  47  89  48  26  67  83  14  25  85  28  72]\n",
      "a_shuffle_IDXs: [20 48 35 58 53 62 56 76 31 79 29 28 61 14 41 64 75 15 50 37 66  1 69 43  3 60  5 39 11 17 38 44 72 74 67  4  2 36 33 40  9  8 42 12  6 68  0 23 26 24 46 21 55 59 78 54 25 73 65 52 77 45 70 51 57 49 32 10 22 47  7 63 34 71 27 19 13 18 30 16], a_shuffle_aclus: [ 27  63  47  79  70  83  75 101  40 104  38  35  82  19  56  85 100  20  67  51  87   3  91  58   5  81   8  53  15  24  52  59  95  98  89   6   4  48  44  55  13  12  57  16   9  90   2  30  33  31  61  28  72  80 103  71  32  97  86  69 102  60  92  68  77  66  43  14  29  62  11  84  45  93  34  26  18  25  39  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63  1 41 43 60 22 35 74 11 51 54 42  2 46 39 21 13  3  8 26 28 17 78 76 49  4 25 59 71 14 70 52 65 29 53 32 62 12 75 79 48 31 30 55 38 57 24 16 64 37 15 18 58 56 36  5 20 27  9 40 34 47 23 10 33  0 19 73 44 66 68 72  7 67 69  6 61 77 50 45], a_shuffle_aclus: [ 84   3  56  58  81  29  47  98  15  68  71  57   4  61  53  28  18   5  12  33  35  24 103 101  66   6  32  80  93  19  92  69  86  38  70  43  83  16 100 104  63  40  39  72  52  77  31  23  85  51  20  25  79  75  48   8  27  34  13  55  45  62  30  14  44   2  26  97  59  87  90  95  11  89  91   9  82 102  67  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [54 51 79 55 12 33 34 65  1 17 62 40 75 44 59 37 24  5 38  9 71 49 13 70 21 60 73 10 53 41 29 50 77 78 14 22 16 25 63 42 28 36 76  3 32 47  6 11 64 48 72 18 52 57 61 31 35  2 69 15 27 23 74  0  7 39 46 19 20 43  8 66 58  4 67 56 45 26 68 30], a_shuffle_aclus: [ 71  68 104  72  16  44  45  86   3  24  83  55 100  59  80  51  31   8  52  13  93  66  18  92  28  81  97  14  70  56  38  67 102 103  19  29  23  32  84  57  35  48 101   5  43  62   9  15  85  63  95  25  69  77  82  40  47   4  91  20  34  30  98   2  11  53  61  26  27  58  12  87  79   6  89  75  60  33  90  39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 10 62 35  0 54 29 57 65 12 30 13 68 33 50 55 16 44 11  8 67 31 63 74 24  3 76 49 42  9 48 51 69 20 15 19  5 77 71 58 26 23 22 25  2 36 64 70 56 40 73 39 79 60 61 78  7 72 53 66 46 14 59 34 43 37 75 27 21  1 32 17  6  4 41 28 45 38 18 52], a_shuffle_aclus: [ 62  14  83  47   2  71  38  77  86  16  39  18  90  44  67  72  23  59  15  12  89  40  84  98  31   5 101  66  57  13  63  68  91  27  20  26   8 102  93  79  33  30  29  32   4  48  85  92  75  55  97  53 104  81  82 103  11  95  70  87  61  19  80  45  58  51 100  34  28   3  43  24   9   6  56  35  60  52  25  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 59 14 15 51 21 78  4 71 22 46 62 32 76 25 74 68 60 64 58  3 50 42 52 35 23 16 43 34 41 45 36  5 39  8 20 69 54 26 12 37 29 72 11 33 66  7 47  1 75 17  0  9 30 55 65 61 13 56 28 79 57 40 10 31 24 44 19 53 38 63 48 73 18 27 77 70  6 49 67], a_shuffle_aclus: [  4  80  19  20  68  28 103   6  93  29  61  83  43 101  32  98  90  81  85  79   5  67  57  69  47  30  23  58  45  56  60  48   8  53  12  27  91  71  33  16  51  38  95  15  44  87  11  62   3 100  24   2  13  39  72  86  82  18  75  35 104  77  55  14  40  31  59  26  70  52  84  63  97  25  34 102  92   9  66  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51  3 68 62 23  2 18 66 53 60 69 35 61 34 33 22  4 24 59  6 42 54 67 48 70 19 10 78  1 32 17 58 74 40 49 38 37  5 46 71 14 39 20 45 28 64 52 65 47 73 72 30 63 16 56 27 75 12  9 55 44 13 26  7 50  0 79 57 21 43 76 15 36  8 31 77 29 25 41 11], a_shuffle_aclus: [ 68   5  90  83  30   4  25  87  70  81  91  47  82  45  44  29   6  31  80   9  57  71  89  63  92  26  14 103   3  43  24  79  98  55  66  52  51   8  61  93  19  53  27  60  35  85  69  86  62  97  95  39  84  23  75  34 100  16  13  72  59  18  33  11  67   2 104  77  28  58 101  20  48  12  40 102  38  32  56  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76 26 42 12 66 41 16 78 10 69 24  2 25 73  3 72 60  8 58 77 43 71 59 50  9 74 28 65 64 20  6 29 27 34  1  5 15 39 44 45 38 11 47 46 52 75 40 61  7 57 30 32  0 51 37 14 18 21 53 17 79 55 31 19 35 33 13 49 54 68 23 62 56 22 36 48  4 63 67 70], a_shuffle_aclus: [101  33  57  16  87  56  23 103  14  91  31   4  32  97   5  95  81  12  79 102  58  93  80  67  13  98  35  86  85  27   9  38  34  45   3   8  20  53  59  60  52  15  62  61  69 100  55  82  11  77  39  43   2  68  51  19  25  28  70  24 104  72  40  26  47  44  18  66  71  90  30  83  75  29  48  63   6  84  89  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 10 24 19 49 55 30 53 32 56 52 44 33 34 13 50 74 58 42 70 64 26 28  6 14 22 57 36 18 72 16  3 46 11  0 59 12 27 39 73 65 60 31 17 77 35  8 68 69 15 40 62  4 76 29  9 41 54  2 67 66 61 71 21 78 75 43 47 45 37 23 38  1 25  5  7 48 20 51 63], a_shuffle_aclus: [104  14  31  26  66  72  39  70  43  75  69  59  44  45  18  67  98  79  57  92  85  33  35   9  19  29  77  48  25  95  23   5  61  15   2  80  16  34  53  97  86  81  40  24 102  47  12  90  91  20  55  83   6 101  38  13  56  71   4  89  87  82  93  28 103 100  58  62  60  51  30  52   3  32   8  11  63  27  68  84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [42 33 76 35 23 24  1 60 34 12 68 48 13 14 44 37 46 31 65 27 32 75 79  6 28 10 22 54  3 72 57 40 53  0 20 50 59 67 15 56  9 64 49 30  4 70 71 16 43 19 18 74  5 38  2 51 78 58 17 69 55 29 52 73 62 61 45 77 39 66  8 26 36 21 25  7 41 47 63 11], a_shuffle_aclus: [ 57  44 101  47  30  31   3  81  45  16  90  63  18  19  59  51  61  40  86  34  43 100 104   9  35  14  29  71   5  95  77  55  70   2  27  67  80  89  20  75  13  85  66  39   6  92  93  23  58  26  25  98   8  52   4  68 103  79  24  91  72  38  69  97  83  82  60 102  53  87  12  33  48  28  32  11  56  62  84  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 15 37 75 14 47 43 72 41 69 13  6 67 11 31 18 71  7 50 36 56 40  0 30 24 59 27  4 42  2 19 77 12 10 44 32 21 63 48 54 29  1 55 23 25 79 60 26  3 17 33 68 52 74 51  5 49 16 53 78 45 58 73 61 64 57 70  9 76 20 22  8 62 65 35 38 66 39 34 46], a_shuffle_aclus: [ 35  20  51 100  19  62  58  95  56  91  18   9  89  15  40  25  93  11  67  48  75  55   2  39  31  80  34   6  57   4  26 102  16  14  59  43  28  84  63  71  38   3  72  30  32 104  81  33   5  24  44  90  69  98  68   8  66  23  70 103  60  79  97  82  85  77  92  13 101  27  29  12  83  86  47  52  87  53  45  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [13 65 19 17 49 45  4 67 75 24 77 41 26 55 53 22 44 12 63 50 66 57 23 69  9 32 71 20 46 10 70 60 30  0 28  3 21 36 51  5 59 78  6 18 73 34 31 61 76 27 79 29 11 25 54  2 43 47  7 40 37 42 62 15  1 33 52 72 48 58 16 35 64 38 56 74 68  8 39 14], a_shuffle_aclus: [ 18  86  26  24  66  60   6  89 100  31 102  56  33  72  70  29  59  16  84  67  87  77  30  91  13  43  93  27  61  14  92  81  39   2  35   5  28  48  68   8  80 103   9  25  97  45  40  82 101  34 104  38  15  32  71   4  58  62  11  55  51  57  83  20   3  44  69  95  63  79  23  47  85  52  75  98  90  12  53  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [44 15  3 41 11 64 40 62 69 32 27 51 26  4  5  2 24 36 50 45 37 20 58 13 17 46 48 55 35 59 23 52 75 43 30 49 21 72 60 76  6 68 33 71  0 19 65 39 14 53 12 54 79 67 57 47 34 10  7 29 56 28 18 16 22 74 25 73 63 42  1 38 66  9 78 61  8 31 77 70], a_shuffle_aclus: [ 59  20   5  56  15  85  55  83  91  43  34  68  33   6   8   4  31  48  67  60  51  27  79  18  24  61  63  72  47  80  30  69 100  58  39  66  28  95  81 101   9  90  44  93   2  26  86  53  19  70  16  71 104  89  77  62  45  14  11  38  75  35  25  23  29  98  32  97  84  57   3  52  87  13 103  82  12  40 102  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29 16 77 21 22  3 66 35 59 50 37 55 31  2 76 56 28 49 61 32 70 19 44 48 75 62 40 60  4  8  0 47 23 20 38 53 65 74 42 33 58 52 10 63 71 57 30 25 79 43 11 13 17 68  5 73  1 24 78 69 51 46 67 64  7 72 15 45 18 12  6 26 34 27 54 14 39 36 41  9], a_shuffle_aclus: [ 38  23 102  28  29   5  87  47  80  67  51  72  40   4 101  75  35  66  82  43  92  26  59  63 100  83  55  81   6  12   2  62  30  27  52  70  86  98  57  44  79  69  14  84  93  77  39  32 104  58  15  18  24  90   8  97   3  31 103  91  68  61  89  85  11  95  20  60  25  16   9  33  45  34  71  19  53  48  56  13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [44 24  4 27 30 36 38  9 48 12 16 56 29 31 14 63 17 26  2 32 15 28 69 54 34 57 39 18 13 73 79 55 72 25 66 22 67 11 61  7 45 62 53 78 65 50  8 75 20 58 51 64 70  1 21 10  6 37 76 35 23 77 33 71 49  3 74 19  0 60 59 47 41 40 52  5 43 42 68 46], a_shuffle_aclus: [ 59  31   6  34  39  48  52  13  63  16  23  75  38  40  19  84  24  33   4  43  20  35  91  71  45  77  53  25  18  97 104  72  95  32  87  29  89  15  82  11  60  83  70 103  86  67  12 100  27  79  68  85  92   3  28  14   9  51 101  47  30 102  44  93  66   5  98  26   2  81  80  62  56  55  69   8  58  57  90  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47  1  2 79 69  7 64 56 44 31 32 53 50 11 33 67 59 38  6  9 13 29 57 62 49 78 74 72 52 35 22  3 20 16 12 42 60 24 28 76 71 63 10 21 68 73 61 36 37 48 34 27 58 41 17 45 70 77 26  5 75 54 18 40 39 51 14  4  0 23 15 30 55 66  8 65 25 19 46 43], a_shuffle_aclus: [ 62   3   4 104  91  11  85  75  59  40  43  70  67  15  44  89  80  52   9  13  18  38  77  83  66 103  98  95  69  47  29   5  27  23  16  57  81  31  35 101  93  84  14  28  90  97  82  48  51  63  45  34  79  56  24  60  92 102  33   8 100  71  25  55  53  68  19   6   2  30  20  39  72  87  12  86  32  26  61  58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 5 79 23 12 57 30 62 44 67 38 71 68 51 34  0 13 35 17 37 25 27 56  4 65 41  9 76 61 52  8 50 58 21 69 36 74 49 29 26 72 55 15 22 20 40 75 19 64 77 11 46  7 53 60 10 43 45 28 73 47 54 42 48 14 78 59 39 18  3  1 66 70 16 63  6 24 32 31 33  2], a_shuffle_aclus: [  8 104  30  16  77  39  83  59  89  52  93  90  68  45   2  18  47  24  51  32  34  75   6  86  56  13 101  82  69  12  67  79  28  91  48  98  66  38  33  95  72  20  29  27  55 100  26  85 102  15  61  11  70  81  14  58  60  35  97  62  71  57  63  19 103  80  53  25   5   3  87  92  23  84   9  31  43  40  44   4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66 32 20 12 56 48 11 65 46 34 14 37  7  3 45  8 62 54 39 60 16  4 28  5 49 15 18 51 64 30  9 72 10 74 25 75 29 52 69 58 35 36 23 59 42 76 41  6 70  0 31 27 50 63 21 47 55 33 57 13 61 79 71  2 19 38 24 73 40 43 26 44 53 77 17 22 78 68  1 67], a_shuffle_aclus: [ 87  43  27  16  75  63  15  86  61  45  19  51  11   5  60  12  83  71  53  81  23   6  35   8  66  20  25  68  85  39  13  95  14  98  32 100  38  69  91  79  47  48  30  80  57 101  56   9  92   2  40  34  67  84  28  62  72  44  77  18  82 104  93   4  26  52  31  97  55  58  33  59  70 102  24  29 103  90   3  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [43 42  6 56  3 47 72 73 25 17 52 67 41 66 78 23 48 50 24 19 51  2 30 70 39  7 58 68 53 11 55  5 12 33 64  8 34 32 38 13 76  0 57 54 35 15  4 63 16 20 49 65 69 26 31 44 22 27 62 21  1 59 74 60 79 28 71 45 10 77 75 14 36 29 18 37 40 61 46  9], a_shuffle_aclus: [ 58  57   9  75   5  62  95  97  32  24  69  89  56  87 103  30  63  67  31  26  68   4  39  92  53  11  79  90  70  15  72   8  16  44  85  12  45  43  52  18 101   2  77  71  47  20   6  84  23  27  66  86  91  33  40  59  29  34  83  28   3  80  98  81 104  35  93  60  14 102 100  19  48  38  25  51  55  82  61  13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 77 41 37 51 62 46 12 47 19 21 58 69 34 54 78 75 35 63 40 44 20 70  1 22 13 31 68 26 71 50 73 14  8 42  4 60  2  3 24 39  9 72 43 48 49 79 16  6 74 64 36 17 66 57 25 59 27 76 15 67 33 28 23 30  7  5 55 29 56 45 65 53 10 38 18  0 32 11 52], a_shuffle_aclus: [ 82 102  56  51  68  83  61  16  62  26  28  79  91  45  71 103 100  47  84  55  59  27  92   3  29  18  40  90  33  93  67  97  19  12  57   6  81   4   5  31  53  13  95  58  63  66 104  23   9  98  85  48  24  87  77  32  80  34 101  20  89  44  35  30  39  11   8  72  38  75  60  86  70  14  52  25   2  43  15  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 10  8  0 71 53  5 56 42 22 62 65 69 28 11 39 43  4 18 21 38 54 57 50 59 72 61  9 64 45 35  1 24 51 70  3 25 34 67  2 30 74 33 79 12 78 23 60 66 15 14 49 36 76  6 63 52 20 26 58 32 46 68 37 31 13 17 77 40 48 29 16 47 55 19  7 75 27 44 73], a_shuffle_aclus: [ 56  14  12   2  93  70   8  75  57  29  83  86  91  35  15  53  58   6  25  28  52  71  77  67  80  95  82  13  85  60  47   3  31  68  92   5  32  45  89   4  39  98  44 104  16 103  30  81  87  20  19  66  48 101   9  84  69  27  33  79  43  61  90  51  40  18  24 102  55  63  38  23  62  72  26  11 100  34  59  97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14 10 64 19 68 39 40 29  4 44 77 20 11  9 79 31 30 15  5 63 35 41 48 16 37 69 52 54 72 24 27 42 61 22 71 21 70  2 46 34 75 18  3  8  6 57 47 74 62 17 33 67 45 32 25 56 13 55 12 76 58 36  0 26 53 51 49 78 38 59  1 73 50  7 65 28 60 66 43 23], a_shuffle_aclus: [ 19  14  85  26  90  53  55  38   6  59 102  27  15  13 104  40  39  20   8  84  47  56  63  23  51  91  69  71  95  31  34  57  82  29  93  28  92   4  61  45 100  25   5  12   9  77  62  98  83  24  44  89  60  43  32  75  18  72  16 101  79  48   2  33  70  68  66 103  52  80   3  97  67  11  86  35  81  87  58  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 61  8  4 54 73 70 22 69 40 71 14 45 58 65 44 18 41  6 72 12 75 64 76 35  2 49 67 32 50 66 55  1 15 36 13 47 29 20 43 51 10 46 23 26  9  5 48 31 37 19  3 17 52 56 57 77 28 27 74 21 25 53 63  0 59 11 34 78 38 60 30 24 42 68 16 39 33 62  7], a_shuffle_aclus: [104  82  12   6  71  97  92  29  91  55  93  19  60  79  86  59  25  56   9  95  16 100  85 101  47   4  66  89  43  67  87  72   3  20  48  18  62  38  27  58  68  14  61  30  33  13   8  63  40  51  26   5  24  69  75  77 102  35  34  98  28  32  70  84   2  80  15  45 103  52  81  39  31  57  90  23  53  44  83  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56  4 41 28 50  6  8 70 43 69 57 36 79 65 61 22 45 34 35 16 75 30 67 17 40 33 63 53  5 66  3 62  7 23 64 52 27 25 74 31 59 37 73 49  0 68 77 46 39 10 51 72 29  2 58 48 71 26  9 78 11 12 38 55 60 42 54 76 32 18 44 24 47 21 20 15 19  1 14 13], a_shuffle_aclus: [ 75   6  56  35  67   9  12  92  58  91  77  48 104  86  82  29  60  45  47  23 100  39  89  24  55  44  84  70   8  87   5  83  11  30  85  69  34  32  98  40  80  51  97  66   2  90 102  61  53  14  68  95  38   4  79  63  93  33  13 103  15  16  52  72  81  57  71 101  43  25  59  31  62  28  27  20  26   3  19  18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 32 17 68 51 57 66 45 25  9  0  4 54  2 64 56 29 24  5 28 34 19 71 46 75 67 73 59 41 15 21  1 42 26 62 44 10 74 78 22  6 77  8 31 36 27 39  3 76 20 58 79 16 61 13 52 23 48 69 70 47 33 11 49 60 14 35 50 72 18 53 30 43 63 38 37 12 40  7 65], a_shuffle_aclus: [ 72  43  24  90  68  77  87  60  32  13   2   6  71   4  85  75  38  31   8  35  45  26  93  61 100  89  97  80  56  20  28   3  57  33  83  59  14  98 103  29   9 102  12  40  48  34  53   5 101  27  79 104  23  82  18  69  30  63  91  92  62  44  15  66  81  19  47  67  95  25  70  39  58  84  52  51  16  55  11  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [38 33 67  2 60 63  8 37  3 31  4 72 12 53 35 24 14 54 47 73 65 41 75 62 15  0 32 29 10 74 46 77  7 43 45 40 50  5 71 25 69 70 55 39 61 78 58  1 48 52  9 76 13  6 30 21 64 27 66 51 59 23 56 11 28 49 22 20 44 36 18 17 26 57 34 68 16 79 42 19], a_shuffle_aclus: [ 52  44  89   4  81  84  12  51   5  40   6  95  16  70  47  31  19  71  62  97  86  56 100  83  20   2  43  38  14  98  61 102  11  58  60  55  67   8  93  32  91  92  72  53  82 103  79   3  63  69  13 101  18   9  39  28  85  34  87  68  80  30  75  15  35  66  29  27  59  48  25  24  33  77  45  90  23 104  57  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 1 12  5 62 65 15 18 59 68 35 67 22 63 41  8  9 27 21 58 34 30 79 26 38 54 50 39 14 46 19 37 16  7 71 45  4 60  2 61 11  3 78 23 24 76 40 10 51 31 57  0  6 28 44 13 70 33 74 73 53 20 52 72 77 48 29 36 75 25 43 64 69 47 66 55 32 17 42 49 56], a_shuffle_aclus: [  3  16   8  83  86  20  25  80  90  47  89  29  84  56  12  13  34  28  79  45  39 104  33  52  71  67  53  19  61  26  51  23  11  93  60   6  81   4  82  15   5 103  30  31 101  55  14  68  40  77   2   9  35  59  18  92  44  98  97  70  27  69  95 102  63  38  48 100  32  58  85  91  62  87  72  43  24  57  66  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45  8 52 78 55 68 75 69 27 60 21 39 16 15 70  5  6 43  7 13 32 48 38 28 47 23 62 59 56 76 26 54 51  3 79 58 31 41 71  0 64 49 37 34 40 67 35 25 63 18 77 36 73 11 57 72 29 61  2 20 22 30 33  1 12  4  9 44 53 66 24 42 50 14 46 19 10 65 17 74], a_shuffle_aclus: [ 60  12  69 103  72  90 100  91  34  81  28  53  23  20  92   8   9  58  11  18  43  63  52  35  62  30  83  80  75 101  33  71  68   5 104  79  40  56  93   2  85  66  51  45  55  89  47  32  84  25 102  48  97  15  77  95  38  82   4  27  29  39  44   3  16   6  13  59  70  87  31  57  67  19  61  26  14  86  24  98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [32 42 28 48 22 14 26 33 36 45 63 27 53 62 39 51 15 72  2 71 68 66 12 64  6 79 69 10 77 16 40 29 20 46 11 56 65 73 70 38 78 74  5 41 67 75 55 30 24 57  1  0 19 31  3 44 34 58 52 59  4 37 21 49 13  7 17  9  8 43 18 61 76 47 60 50 54 23 25 35], a_shuffle_aclus: [ 43  57  35  63  29  19  33  44  48  60  84  34  70  83  53  68  20  95   4  93  90  87  16  85   9 104  91  14 102  23  55  38  27  61  15  75  86  97  92  52 103  98   8  56  89 100  72  39  31  77   3   2  26  40   5  59  45  79  69  80   6  51  28  66  18  11  24  13  12  58  25  82 101  62  81  67  71  30  32  47]\n",
      "a_shuffle_IDXs: [55 74  0 28 16 14 61  3 63 26  8 52 42  4 25 58  5 45 69 17 29 30 60 37  6  2 21 79 77 27 32 56 64 53 19 67 59 48 24 41 70 33 43 18 12 62 15  1  7 46 71 47 44 39  9 68 76 13 65 54 57 31 66 10 20 36 38 50 23 51 73 11 35 22 49 78 72 40 34 75], a_shuffle_aclus: [ 72  98   2  35  23  19  82   5  84  33  12  69  57   6  32  79   8  60  91  24  38  39  81  51   9   4  28 104 102  34  43  75  85  70  26  89  80  63  31  56  92  44  58  25  16  83  20   3  11  61  93  62  59  53  13  90 101  18  86  71  77  40  87  14  27  48  52  67  30  68  97  15  47  29  66 103  95  55  45 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55  1 74 51 14 75 21 71 28 35 18 67 11 70 59 41 50 16 56 12 39 79 23 40 29 65 57 10 68 42 19 20 76 27  7  3 64 26 78  4  6 38 45 32 54 61 44 69 30 25 31  8 66 48 63 17 53 34 37 58 77 60 13 73  9 33 72 49  5 43 22  0 52 62 46 15 36 24  2 47], a_shuffle_aclus: [ 72   3  98  68  19 100  28  93  35  47  25  89  15  92  80  56  67  23  75  16  53 104  30  55  38  86  77  14  90  57  26  27 101  34  11   5  85  33 103   6   9  52  60  43  71  82  59  91  39  32  40  12  87  63  84  24  70  45  51  79 102  81  18  97  13  44  95  66   8  58  29   2  69  83  61  20  48  31   4  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0 28 64 71 45 52  4 16 49 39 10 36 22  6 25 55  2 15  8 34 50 76  1 35 56 53 38 14 66 31 60 54  5 78 24 42 32 68 11 59  9 79 58 17 77 61 73 12 72 23 40 65 46 47 19 51 70 33 69 48 27  3 75 43 21 37 67 13 63 44  7 20 57 41 74 18 30 62 26 29], a_shuffle_aclus: [  2  35  85  93  60  69   6  23  66  53  14  48  29   9  32  72   4  20  12  45  67 101   3  47  75  70  52  19  87  40  81  71   8 103  31  57  43  90  15  80  13 104  79  24 102  82  97  16  95  30  55  86  61  62  26  68  92  44  91  63  34   5 100  58  28  51  89  18  84  59  11  27  77  56  98  25  39  83  33  38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 30  9 14 36 55 43 18 25 38 59 26 65 23 74 67 21 31  0 49 24 54 39 12 20 66 70 60 72 28 61 46 68 19 17 47 69 75 34 64 63 35 42 40 37 48 58  5  2 10 45 27 79 52 29 78 77 51 16 62 56  1 32 44  4 41 11  8 71 22  3 50 13 76 33 15  6 57 73 53], a_shuffle_aclus: [ 11  39  13  19  48  72  58  25  32  52  80  33  86  30  98  89  28  40   2  66  31  71  53  16  27  87  92  81  95  35  82  61  90  26  24  62  91 100  45  85  84  47  57  55  51  63  79   8   4  14  60  34 104  69  38 103 102  68  23  83  75   3  43  59   6  56  15  12  93  29   5  67  18 101  44  20   9  77  97  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 19 69 60 56 30 57 62 58 78 55 39 64 13 68 79  0 43 23 42 48 70 33 52 29  5  2 76 26 17 67 35 32  7 34 38 61 65 41 71 59 77 74 75  8 51 40 54 18 22 27 37 16  6 20 10 21 53 31  9 24 25 45 66 73  3 47 63 12  1 72 49 36 28 15 11 44  4 14 50], a_shuffle_aclus: [ 61  26  91  81  75  39  77  83  79 103  72  53  85  18  90 104   2  58  30  57  63  92  44  69  38   8   4 101  33  24  89  47  43  11  45  52  82  86  56  93  80 102  98 100  12  68  55  71  25  29  34  51  23   9  27  14  28  70  40  13  31  32  60  87  97   5  62  84  16   3  95  66  48  35  20  15  59   6  19  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67  4 62 45 43 29 68 39 33 41 15 65 13 51 50 44 47 32 53 64 72 10 52 30 34 23 79 57 26 25 59 12  1  0 56 70  2 27  9 63 22 17 76 77 37 66 71 28 61 18 38 19 42  8  6 31 78 24 73  3 46 69 16 49 55 75 40 14 60 35 58 11  5 54 20 74 21  7 36 48], a_shuffle_aclus: [ 89   6  83  60  58  38  90  53  44  56  20  86  18  68  67  59  62  43  70  85  95  14  69  39  45  30 104  77  33  32  80  16   3   2  75  92   4  34  13  84  29  24 101 102  51  87  93  35  82  25  52  26  57  12   9  40 103  31  97   5  61  91  23  66  72 100  55  19  81  47  79  15   8  71  27  98  28  11  48  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [62 51 79 69  4  6 39 33 21 47 61 27 22 12 55 29 72 23 13 19 30 64 35 20 60 68 78  5 57  2 73 77 74 10  0 11 54 50 53 66 65 75  8 14 16  1 63 31 70  3 42 76 71 18 43 36 59 34 41 58  9 44 45 32 28 38 25 46 52 49 67 15 37 26 40 56 48 17  7 24], a_shuffle_aclus: [ 83  68 104  91   6   9  53  44  28  62  82  34  29  16  72  38  95  30  18  26  39  85  47  27  81  90 103   8  77   4  97 102  98  14   2  15  71  67  70  87  86 100  12  19  23   3  84  40  92   5  57 101  93  25  58  48  80  45  56  79  13  59  60  43  35  52  32  61  69  66  89  20  51  33  55  75  63  24  11  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 47 34 27 75 40 77 53 59 14 28 38  2 46 66 36 70  8 52 48 45 21 25  9 65 12 63 74 18 35 54 15 11  6 10 73 31 41 72 39 60 32  3  0 68 56 50 37 76 44 42 62 67 58 69 71 64  5 57 24 22 51  1 30 43 26  7 16 61 20  4 49 79 13 17 23 29 19 33 78], a_shuffle_aclus: [ 72  62  45  34 100  55 102  70  80  19  35  52   4  61  87  48  92  12  69  63  60  28  32  13  86  16  84  98  25  47  71  20  15   9  14  97  40  56  95  53  81  43   5   2  90  75  67  51 101  59  57  83  89  79  91  93  85   8  77  31  29  68   3  39  58  33  11  23  82  27   6  66 104  18  24  30  38  26  44 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 26 29 10 51 36  0 22 72  3 32 47 78 24 28 52 75  5 50 63 49 46 44 54 27 23  8 76 25 16 13 53  6 65 57 37 12 11  7 34 70 42 33 21 30 14 69 64 55 43 71  1 79 66 74 17 67 60 35 59 19 73 39 40 31 20 45 62 68 61  2 77 56 38 15  9 58 18  4 48], a_shuffle_aclus: [ 56  33  38  14  68  48   2  29  95   5  43  62 103  31  35  69 100   8  67  84  66  61  59  71  34  30  12 101  32  23  18  70   9  86  77  51  16  15  11  45  92  57  44  28  39  19  91  85  72  58  93   3 104  87  98  24  89  81  47  80  26  97  53  55  40  27  60  83  90  82   4 102  75  52  20  13  79  25   6  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 51 34 39 12 15 47  8 42  7 55 33 27 59 28 78 69 66 14 73 46 19 67 54 60 21 76 65 18 38 35 50 31 32  3  4 72 53 16 74 58 37 56 68 62 52 23 30 25 70  6 45 24 22 29  0 75 13 43 17 57 26 48 71 41  5 10  9 36 79 20 64 49 63 61 77 40 44  1 11], a_shuffle_aclus: [  4  68  45  53  16  20  62  12  57  11  72  44  34  80  35 103  91  87  19  97  61  26  89  71  81  28 101  86  25  52  47  67  40  43   5   6  95  70  23  98  79  51  75  90  83  69  30  39  32  92   9  60  31  29  38   2 100  18  58  24  77  33  63  93  56   8  14  13  48 104  27  85  66  84  82 102  55  59   3  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22 68 71 75 17 44 65  1 39 62 55 15 59 36 33 23 73 76 70 40 54 37 35 50 42 29 52 32 12 21 30 67 72 18  8  3 34 49  6 77 61  0  9  7 43 11 19 48 56 47  5 31 41  4 57 46 74 25 20 13 10 58 79 24 66 16 63 45 28 60 69 27 14 78 26 64 51 53  2 38], a_shuffle_aclus: [ 29  90  93 100  24  59  86   3  53  83  72  20  80  48  44  30  97 101  92  55  71  51  47  67  57  38  69  43  16  28  39  89  95  25  12   5  45  66   9 102  82   2  13  11  58  15  26  63  75  62   8  40  56   6  77  61  98  32  27  18  14  79 104  31  87  23  84  60  35  81  91  34  19 103  33  85  68  70   4  52]\n",
      "a_shuffle_IDXs: [52 54 58 76 25  1 75  8 11 23 79 55 42 61 48 77 24 26 20 72  0 37 14 32 51  9 40 10 47  6 17 62 46 12  2  7 53 45 74 68 13 50 28  5 34 73 15 71 30 65  4 19 60 36 18 31 22 33 21 16 78 27 44 70 64 67 39 69 49 56 38 66 43 29 57 59 63  3 35 41], a_shuffle_aclus: [ 69  71  79 101  32   3 100  12  15  30 104  72  57  82  63 102  31  33  27  95   2  51  19  43  68  13  55  14  62   9  24  83  61  16   4  11  70  60  98  90  18  67  35   8  45  97  20  93  39  86   6  26  81  48  25  40  29  44  28  23 103  34  59  92  85  89  53  91  66  75  52  87  58  38  77  80  84   5  47  56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 29 33 32 75  9 35 78 43 25 39 59 61  3 52  6 38 79 14 45  0 16 46 12 47  5  1 57 58 73 51 49 71 17 42 19  8 20 26 21 65 34 48  7  2 15 13 27 22 44 11 66 28 55 68 70 36 18 77 50 60 62 69 41 40 72 30 54 64 31 63 24 67 76 74  4 37 23 53 10], a_shuffle_aclus: [ 75  38  44  43 100  13  47 103  58  32  53  80  82   5  69   9  52 104  19  60   2  23  61  16  62   8   3  77  79  97  68  66  93  24  57  26  12  27  33  28  86  45  63  11   4  20  18  34  29  59  15  87  35  72  90  92  48  25 102  67  81  83  91  56  55  95  39  71  85  40  84  31  89 101  98   6  51  30  70  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12 15 72 26 13 17 44 30 62 32 20 38 14 61 70 75 29 76 60 18 73 59 42 10 50 74  9  7 28  6 11 24  4 64  8 36 33 53 65 22 19 55 25 45 63 27 51 49 16 37 67 57 35 66 58 56 39  1 48 71 41 54 21 34 69 78 43 31  5 47 23  2 40  3  0 77 68 52 46 79], a_shuffle_aclus: [ 16  20  95  33  18  24  59  39  83  43  27  52  19  82  92 100  38 101  81  25  97  80  57  14  67  98  13  11  35   9  15  31   6  85  12  48  44  70  86  29  26  72  32  60  84  34  68  66  23  51  89  77  47  87  79  75  53   3  63  93  56  71  28  45  91 103  58  40   8  62  30   4  55   5   2 102  90  69  61 104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76 12 61 16 32 30 62 57 43 18 19 48 44  6 54  8  0 77 58 56 68 37 55 79  4 20 35 27  3 66 40 52 28  2 72 39 73 15 23 65 50 51 49 10 46 47 69 71 59 34 24 74 41 75 17 36  9 33 38 29 67 70 42 26 45 13 78 22 11  1  5  7 53 63 31 64 21 25 14 60], a_shuffle_aclus: [101  16  82  23  43  39  83  77  58  25  26  63  59   9  71  12   2 102  79  75  90  51  72 104   6  27  47  34   5  87  55  69  35   4  95  53  97  20  30  86  67  68  66  14  61  62  91  93  80  45  31  98  56 100  24  48  13  44  52  38  89  92  57  33  60  18 103  29  15   3   8  11  70  84  40  85  28  32  19  81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 79 63 20 23 43 36 68 16  0 29 47 18 65 15 61 11 50 14 31 53 34 35  3 49 42  7 78 37 73 59 69 33 51 41 10 74 27 64 28 60 30 22 66 67 17 39 26  9  1 57 56 24 21 77 32 62 58 48 54 19 13 75 71 55 76  5  8 38 44 25  4 45  6 46 12 52 40 72 70], a_shuffle_aclus: [  4 104  84  27  30  58  48  90  23   2  38  62  25  86  20  82  15  67  19  40  70  45  47   5  66  57  11 103  51  97  80  91  44  68  56  14  98  34  85  35  81  39  29  87  89  24  53  33  13   3  77  75  31  28 102  43  83  79  63  71  26  18 100  93  72 101   8  12  52  59  32   6  60   9  61  16  69  55  95  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [15 33 40 35 78 41 50 11 29 48 75  6 49 69 21 55 39 72 19 17 46 28 25 52 43 71 66 24 79 27 10 63  4 59 47 61 18 76 56 53 73 13 45 67  5 64 38 32 30 12 65  0  1 36 77 57  8 22 62  3 42  9 31 54 68 44 74  2  7 16 37 34 58 26 23 60 14 20 70 51], a_shuffle_aclus: [ 20  44  55  47 103  56  67  15  38  63 100   9  66  91  28  72  53  95  26  24  61  35  32  69  58  93  87  31 104  34  14  84   6  80  62  82  25 101  75  70  97  18  60  89   8  85  52  43  39  16  86   2   3  48 102  77  12  29  83   5  57  13  40  71  90  59  98   4  11  23  51  45  79  33  30  81  19  27  92  68]\n",
      "a_shuffle_IDXs: [39 77  4  7 56 54 20 55 12 24 43  9 38 64 72  5 31 60 79 40 44 76 70 37 48 58 13 18  6 61 73 17 29 19 74 16 49 36 42 53  2  0 65 63 10 45 34 23 11 62 46 66 15 75 30 52  8  3 69 67  1 33 35 28 41 47 59 27 26 14 22 68 51 71 50 57 21 25 78 32], a_shuffle_aclus: [ 53 102   6  11  75  71  27  72  16  31  58  13  52  85  95   8  40  81 104  55  59 101  92  51  63  79  18  25   9  82  97  24  38  26  98  23  66  48  57  70   4   2  86  84  14  60  45  30  15  83  61  87  20 100  39  69  12   5  91  89   3  44  47  35  56  62  80  34  33  19  29  90  68  93  67  77  28  32 103  43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 68 49 59 47 70 73  9 10 13 35 52 78 69  4 46 29  6 64  7 61 65 54 67 33 23 15 55 32 44 48 50 14 42 38 24 71 43 77  3 34 76 37 11 18 26 25 31 30 62 21 28 57 12 66 75 72  5 36 79 22 74 39 20 41  2 58 19  0 63 27 51 17 56 45 16  8  1 40 53], a_shuffle_aclus: [ 81  90  66  80  62  92  97  13  14  18  47  69 103  91   6  61  38   9  85  11  82  86  71  89  44  30  20  72  43  59  63  67  19  57  52  31  93  58 102   5  45 101  51  15  25  33  32  40  39  83  28  35  77  16  87 100  95   8  48 104  29  98  53  27  56   4  79  26   2  84  34  68  24  75  60  23  12   3  55  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [26 46 65  9 64  3 36 50 57 60 14 56 17 79 16  1 75 37 39 66 38 23 74 21 18 71 58 69 40 44 41 59 24  2 49 53 15 73 43 30 31 12 76 34 25 55 48 10 13 70  5 33  7 47  4 19 28 77 32 22  8 45 20 67 27 62 35 78 72 63  6 68 61 54 51 52  0 11 42 29], a_shuffle_aclus: [ 33  61  86  13  85   5  48  67  77  81  19  75  24 104  23   3 100  51  53  87  52  30  98  28  25  93  79  91  55  59  56  80  31   4  66  70  20  97  58  39  40  16 101  45  32  72  63  14  18  92   8  44  11  62   6  26  35 102  43  29  12  60  27  89  34  83  47 103  95  84   9  90  82  71  68  69   2  15  57  38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 34 73 68 47  5 62  2 44 76 20 58 36 17 52 57 29 66 59 79 32 60 51 14 27 38 56 12 65 45 19 11 28 24  4 30 21 78  0 69  7 67 25 18 31 23 70 77 22 75 55  3 43 42 16 26 33 54 72 53 49  6 61 41 50 39  8 71 40 74 13 46  9 37 64 15 35 10  1 48], a_shuffle_aclus: [ 84  45  97  90  62   8  83   4  59 101  27  79  48  24  69  77  38  87  80 104  43  81  68  19  34  52  75  16  86  60  26  15  35  31   6  39  28 103   2  91  11  89  32  25  40  30  92 102  29 100  72   5  58  57  23  33  44  71  95  70  66   9  82  56  67  53  12  93  55  98  18  61  13  51  85  20  47  14   3  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 23 29 37 41 60 33 34 48 52 38  8  1 20 42 28  4 26 45 17 77 14 78 40 11 65 62 18  0 56  2 19 25  5 27 67 30 13 36 73 79 31 16 12 22 54 55  6 72 51 49 71 74 15 75 46 35  7 69 59 76 53 58 61 70 68 21 66 24 44 50  9 47 43 39 64 32 57 63  3], a_shuffle_aclus: [ 14  30  38  51  56  81  44  45  63  69  52  12   3  27  57  35   6  33  60  24 102  19 103  55  15  86  83  25   2  75   4  26  32   8  34  89  39  18  48  97 104  40  23  16  29  71  72   9  95  68  66  93  98  20 100  61  47  11  91  80 101  70  79  82  92  90  28  87  31  59  67  13  62  58  53  85  43  77  84   5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 79 23 73 24 62 11  7  1  9 74 70 45 39 28 77  5 41 58 71 25 61 53  8 32 34 40 48 14 15 67 64 78 55 63 37 51 50 54 21 22 59 76 66 26 10 35 17  0 36 56 30 72 38  4 19 43 68 16 60 33 20 52 69 42 46 29 31 12 27 49 75 18 57 44  2 65 13  6  3], a_shuffle_aclus: [ 62 104  30  97  31  83  15  11   3  13  98  92  60  53  35 102   8  56  79  93  32  82  70  12  43  45  55  63  19  20  89  85 103  72  84  51  68  67  71  28  29  80 101  87  33  14  47  24   2  48  75  39  95  52   6  26  58  90  23  81  44  27  69  91  57  61  38  40  16  34  66 100  25  77  59   4  86  18   9   5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 52 25 62 47 28 32 22 63 41 66 75 70 39 40 61 43 26 30 51 57  6 54 16  0 50 27 55  8 77 21 65  1 72 53 12 29 67 18 56 68 10  5  7 13 33  3 11 64 34 36 60 14 23 42  9 19 35 44 78 20 58 48 59 49 71 76 38 79  4 73 31 24 45 74 37 15 46 17 69], a_shuffle_aclus: [  4  69  32  83  62  35  43  29  84  56  87 100  92  53  55  82  58  33  39  68  77   9  71  23   2  67  34  72  12 102  28  86   3  95  70  16  38  89  25  75  90  14   8  11  18  44   5  15  85  45  48  81  19  30  57  13  26  47  59 103  27  79  63  80  66  93 101  52 104   6  97  40  31  60  98  51  20  61  24  91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [13 28 34  5  4 51 60 69 76 52  0 65 15 35 57 39 50 32 20 22 30 31 44 63 62 53 25 33 47 48  1 19 70 49 67 36 56  6 14 18  7  8 58 23 11 72 27 78 43  9 74 45 29 79 17 59 54 61  3 68 12 73 75 16 10 71 21 42 64 24 37 26 77 38  2 40 41 55 46 66], a_shuffle_aclus: [ 18  35  45   8   6  68  81  91 101  69   2  86  20  47  77  53  67  43  27  29  39  40  59  84  83  70  32  44  62  63   3  26  92  66  89  48  75   9  19  25  11  12  79  30  15  95  34 103  58  13  98  60  38 104  24  80  71  82   5  90  16  97 100  23  14  93  28  57  85  31  51  33 102  52   4  55  56  72  61  87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [34 25 42 70 57  2 62 65 13 41 40 75 60  8 45 58  1 21 76  6 38 69 14  4 46 16 18 61 35 71 39  7 74 30 54 73 11 48 10 59 47 78 19 33 64 67 27  0 68  9 51 63 22 20 28 49 29 31 44  3 12 26 72  5 24 50 23 36 66 17 55 79 32 43 77 56 52 37 53 15], a_shuffle_aclus: [ 45  32  57  92  77   4  83  86  18  56  55 100  81  12  60  79   3  28 101   9  52  91  19   6  61  23  25  82  47  93  53  11  98  39  71  97  15  63  14  80  62 103  26  44  85  89  34   2  90  13  68  84  29  27  35  66  38  40  59   5  16  33  95   8  31  67  30  48  87  24  72 104  43  58 102  75  69  51  70  20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 63  3 25 74 35 14 34 38 24 65 62 40 39  8  5 72 79 36 10 48 37 20 21 42 17 11 27 47 68 51 43 18 73 70 41 15 50 33 71 19 54 78 30  2 13 22 26 12  6 16 61  4 77  7 64 52 57 46 59 29 23  9 32 45 53  1 56 44 69 76 75  0 49 58 28 60 66 31 67], a_shuffle_aclus: [ 72  84   5  32  98  47  19  45  52  31  86  83  55  53  12   8  95 104  48  14  63  51  27  28  57  24  15  34  62  90  68  58  25  97  92  56  20  67  44  93  26  71 103  39   4  18  29  33  16   9  23  82   6 102  11  85  69  77  61  80  38  30  13  43  60  70   3  75  59  91 101 100   2  66  79  35  81  87  40  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [30 52 24 25  9  4 17 14 50  1 33 49  5 75 19 38 60 77 54  2 29 56 63 66 51 64 53 43 47 69  0 71 68 40  6 44 79 70 21 39 31 20 23 62 65 15 55 18 61 48 27 22  7 16 11 10 32 58 74  3 72 41 35 46 45 28 76 13 34 67 78 73  8 57 26 36 42 12 37 59], a_shuffle_aclus: [ 39  69  31  32  13   6  24  19  67   3  44  66   8 100  26  52  81 102  71   4  38  75  84  87  68  85  70  58  62  91   2  93  90  55   9  59 104  92  28  53  40  27  30  83  86  20  72  25  82  63  34  29  11  23  15  14  43  79  98   5  95  56  47  61  60  35 101  18  45  89 103  97  12  77  33  48  57  16  51  80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 78 19 23 69 64 17 31 39 48 25 56  6 71 38  2 63 75 40 33 65 46 44 61 62 76 53 20 54 49 47 70 12 27 58 24  4 60 41 55 29 30  8 45 32 36 51 16 68 52 21 74 15 59  5 66 67 79 28 35 34  1 13 72 14 77 57 43 37 26 18 10  9 22 11  7 50 42 73  0], a_shuffle_aclus: [  5 103  26  30  91  85  24  40  53  63  32  75   9  93  52   4  84 100  55  44  86  61  59  82  83 101  70  27  71  66  62  92  16  34  79  31   6  81  56  72  38  39  12  60  43  48  68  23  90  69  28  98  20  80   8  87  89 104  35  47  45   3  18  95  19 102  77  58  51  33  25  14  13  29  15  11  67  57  97   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 66  9 62  4  2 68 77 33 51 10 48  7 25 59 78 46 21 76 20 39 52 63 43 56 73 57  6 50 29 23  5 79 37 28 74  8 32 49 18 34 67 11  0 12 64 16  1 14 30 36 38 31 41 15 17 54 44 26 71 60 27 65 75 53 24 58 69 19 13 70 55 45 47 35 40 22 72 42 61], a_shuffle_aclus: [  5  87  13  83   6   4  90 102  44  68  14  63  11  32  80 103  61  28 101  27  53  69  84  58  75  97  77   9  67  38  30   8 104  51  35  98  12  43  66  25  45  89  15   2  16  85  23   3  19  39  48  52  40  56  20  24  71  59  33  93  81  34  86 100  70  31  79  91  26  18  92  72  60  62  47  55  29  95  57  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 12 19 68 52 13 70 58 69 31 23 65 35 72 77 71  3  5 14 57  0 18 43 67 56 75 62 10 32 46 44 79 48 64 29 74 26 40 78 17 27 41 16 36 15 21 53 51 37 73 59 60 55  8 38 20 28 61 39  9 45 42 24  2 49 47 54 33 76 34 66 25 22  6  7 30  4 50 11  1], a_shuffle_aclus: [ 84  16  26  90  69  18  92  79  91  40  30  86  47  95 102  93   5   8  19  77   2  25  58  89  75 100  83  14  43  61  59 104  63  85  38  98  33  55 103  24  34  56  23  48  20  28  70  68  51  97  80  81  72  12  52  27  35  82  53  13  60  57  31   4  66  62  71  44 101  45  87  32  29   9  11  39   6  67  15   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [36 68 17  1  6 11 28 67  0 34 29  3 43 50  8 47 69 35 44 56 48 61 49 65  2 18 21 10 66 70 76 42 23 51 75 55 46 15 38 27 60 37 72 40  9  5 31 57 59 79 32 53 39 45 78  4 24 63 73 20 25 77 14 71 33 12 13 26 54 19 74 64  7 41 30 62 58 16 22 52], a_shuffle_aclus: [ 48  90  24   3   9  15  35  89   2  45  38   5  58  67  12  62  91  47  59  75  63  82  66  86   4  25  28  14  87  92 101  57  30  68 100  72  61  20  52  34  81  51  95  55  13   8  40  77  80 104  43  70  53  60 103   6  31  84  97  27  32 102  19  93  44  16  18  33  71  26  98  85  11  56  39  83  79  23  29  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76 44 23 26 16  9  1 39  4 46 20 62 42 27 67 70 77 28 65 71 36 38 17 24 57 11 59  8 79 45 22 43 64 33  5 49 12 40 15 14 19 74 31 61 72 18 48 29 50  2 68 34 25 47 73 21 69 30 41  0 10 58 35 75 56  7 37 53 32 13 54 55 60 63 51 78 66  3 52  6], a_shuffle_aclus: [101  59  30  33  23  13   3  53   6  61  27  83  57  34  89  92 102  35  86  93  48  52  24  31  77  15  80  12 104  60  29  58  85  44   8  66  16  55  20  19  26  98  40  82  95  25  63  38  67   4  90  45  32  62  97  28  91  39  56   2  14  79  47 100  75  11  51  70  43  18  71  72  81  84  68 103  87   5  69   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 75 45 27 37 14 43 52 32 15 29 12 74 54 48 57 78 13 49  9 38 79 31 69 34 50 16 53 35  7 26 22 68 67 44 11 33 76 10 60  4 65  6 73 72 21 59 30 61  3 66 19 40 36 41 25 23 17 18  2 39 58 71 42 55 51 24 46  0 47 56 20 77  8 70  1 63 64  5 62], a_shuffle_aclus: [ 35 100  60  34  51  19  58  69  43  20  38  16  98  71  63  77 103  18  66  13  52 104  40  91  45  67  23  70  47  11  33  29  90  89  59  15  44 101  14  81   6  86   9  97  95  28  80  39  82   5  87  26  55  48  56  32  30  24  25   4  53  79  93  57  72  68  31  61   2  62  75  27 102  12  92   3  84  85   8  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 69 33 13 19 44 22  4 45  6 51 46 61 73 70  5 29 39 74 35 68 31 43 50 62 18 15 14 64 78 76 12 49 17 41 38  8 59  0 56 40 55 16 27 77 42 37 65 11 53 72 57 52  1 24 60 63 71 23 10 54 34 48 32  7  2 30 58 67 75 26 28  9 20 79 25 36 47 21 66], a_shuffle_aclus: [  5  91  44  18  26  59  29   6  60   9  68  61  82  97  92   8  38  53  98  47  90  40  58  67  83  25  20  19  85 103 101  16  66  24  56  52  12  80   2  75  55  72  23  34 102  57  51  86  15  70  95  77  69   3  31  81  84  93  30  14  71  45  63  43  11   4  39  79  89 100  33  35  13  27 104  32  48  62  28  87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [40 70 32 62  4 30 73 23 47 46 64 15 65 37 67 14 54 50  6 24 28 25 52 27 22 53  0  2 60 45  9 68  7 35  3 59 10 11 75 77 42 34 44 36 78 29 72 20 33 21 76 18 43 26 19 66 58  1 31 61 13 71 79 51 49 39 69 48 41 17 63 56 57 12 74 38 16  5  8 55], a_shuffle_aclus: [ 55  92  43  83   6  39  97  30  62  61  85  20  86  51  89  19  71  67   9  31  35  32  69  34  29  70   2   4  81  60  13  90  11  47   5  80  14  15 100 102  57  45  59  48 103  38  95  27  44  28 101  25  58  33  26  87  79   3  40  82  18  93 104  68  66  53  91  63  56  24  84  75  77  16  98  52  23   8  12  72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 37 72 21 23 13 63 55 76 33 40 47 32 68 24 35 54 78  5  9 59 27 50 36 19 25 77 73 62 61 38 42  3  7 16 26 67 31 12 60 22 18 65 29 69  2 70 30  0  1 53 46 14 66 45 48 75 34 52  6 56 20 44 43 71 79 64 17 41 10 51  4 39 58 74 49 28 57  8 15], a_shuffle_aclus: [ 15  51  95  28  30  18  84  72 101  44  55  62  43  90  31  47  71 103   8  13  80  34  67  48  26  32 102  97  83  82  52  57   5  11  23  33  89  40  16  81  29  25  86  38  91   4  92  39   2   3  70  61  19  87  60  63 100  45  69   9  75  27  59  58  93 104  85  24  56  14  68   6  53  79  98  66  35  77  12  20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [35 13 63 77 28 43 58 22 61 66 42 56 34 45 11 73 57  2  4 44 50  3  9 25 46 29 41 32 33 69 67 19 65 74 78 49 36 47  5 37 10 17 40 75 71 24 76 70 52 14 72  8  6 23 68 12 55 21  0 48 64 15 79 27 31 38  1 26 30 16 20 59 18 62 53  7 60 51 54 39], a_shuffle_aclus: [ 47  18  84 102  35  58  79  29  82  87  57  75  45  60  15  97  77   4   6  59  67   5  13  32  61  38  56  43  44  91  89  26  86  98 103  66  48  62   8  51  14  24  55 100  93  31 101  92  69  19  95  12   9  30  90  16  72  28   2  63  85  20 104  34  40  52   3  33  39  23  27  80  25  83  70  11  81  68  71  53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 53 13  0 42 20 35 61  4 52 11  6 48 12 16 54  3 50  8 39 71 72 47 28 15 36 62 41 45 51 24 30 59 78 77 27  5 75 60 17 38  7 57 14 34  9 21 55 79 69 22 32 40 33 56 74 43 73 26 76 58 23 46 31 70 49 25 68 65 44 29 63 37 19 10 67 66 64 18  1], a_shuffle_aclus: [  4  70  18   2  57  27  47  82   6  69  15   9  63  16  23  71   5  67  12  53  93  95  62  35  20  48  83  56  60  68  31  39  80 103 102  34   8 100  81  24  52  11  77  19  45  13  28  72 104  91  29  43  55  44  75  98  58  97  33 101  79  30  61  40  92  66  32  90  86  59  38  84  51  26  14  89  87  85  25   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 29 51 12  8 76 18 55 15 50 72 71 22 57 30 70 31 75 78  6 58 46 23 16 69  5 59 32 48 41 43 77 79 68 56  3 21  7 45 61 40 14 37 52 62 19 60  2 11 65 63  4 54 39 36 34 24 13 20 28 33 10 73 38 25 67 53 64  9 44  1 47 35  0 26 74 42 27 66 49], a_shuffle_aclus: [ 24  38  68  16  12 101  25  72  20  67  95  93  29  77  39  92  40 100 103   9  79  61  30  23  91   8  80  43  63  56  58 102 104  90  75   5  28  11  60  82  55  19  51  69  83  26  81   4  15  86  84   6  71  53  48  45  31  18  27  35  44  14  97  52  32  89  70  85  13  59   3  62  47   2  33  98  57  34  87  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 48 54  7 59 70 28 33 52 30 32 23 65  3 68  4 43 40  0  5 17 22 62 56 34 12 74 13 77 51 66 26 27 76 45 36 35  2  9 47 11 10 75 21 44 20 78 55 29 46 73 39 69 42 60 53 61 14 79 16 57 19 38 64 37 72 24 63 31 71  6 49 58 25 15 41  1  8 18 50], a_shuffle_aclus: [ 89  63  71  11  80  92  35  44  69  39  43  30  86   5  90   6  58  55   2   8  24  29  83  75  45  16  98  18 102  68  87  33  34 101  60  48  47   4  13  62  15  14 100  28  59  27 103  72  38  61  97  53  91  57  81  70  82  19 104  23  77  26  52  85  51  95  31  84  40  93   9  66  79  32  20  56   3  12  25  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45 28 35  9 73 19 58 74 76  8 56 57 78 36 40 29 60 65 51 68  0  6 17 53 27 77 42 71 62 37 64 44 66 11 14 24 79 55  1 54 70 69 13 47 75 10 21 26 23  3 43 20 41 52 16 15 67 39  2 31 25 59 32 46 30 38 63 34 22  7 12 50 61 18 33  4 72  5 48 49], a_shuffle_aclus: [ 60  35  47  13  97  26  79  98 101  12  75  77 103  48  55  38  81  86  68  90   2   9  24  70  34 102  57  93  83  51  85  59  87  15  19  31 104  72   3  71  92  91  18  62 100  14  28  33  30   5  58  27  56  69  23  20  89  53   4  40  32  80  43  61  39  52  84  45  29  11  16  67  82  25  44   6  95   8  63  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [58 15 28 23 72 21 49 27 11 50 74 26 76 78 44 37  7 24  8 18 29 48 16 62  5 20 54 55 59 38 35 64 12 41 53 17 67 34 43 66 79 51 42 65 14 69 77 56 71 63  3 39 10 22  4 70  6  9 45 46 13 40 61 47 75  1 60 36 31 19  2 52 30 68 73  0 57 25 33 32], a_shuffle_aclus: [ 79  20  35  30  95  28  66  34  15  67  98  33 101 103  59  51  11  31  12  25  38  63  23  83   8  27  71  72  80  52  47  85  16  56  70  24  89  45  58  87 104  68  57  86  19  91 102  75  93  84   5  53  14  29   6  92   9  13  60  61  18  55  82  62 100   3  81  48  40  26   4  69  39  90  97   2  77  32  44  43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [19 50 71 55 47 37 20 42 17 53 70 28 29 34 35 16 44 25 43 22 54 23 38 48 14  9 62 45 68 52 10  8 64 51 18  5 40 74 46 21 56  1 32 60 72 79  3 65 69 15 78 76 41 36 39 13  6 31 11 61 63 67 66 33 58 77  7  2 24 27 49 59  0 12  4 57 30 73 26 75], a_shuffle_aclus: [ 26  67  93  72  62  51  27  57  24  70  92  35  38  45  47  23  59  32  58  29  71  30  52  63  19  13  83  60  90  69  14  12  85  68  25   8  55  98  61  28  75   3  43  81  95 104   5  86  91  20 103 101  56  48  53  18   9  40  15  82  84  89  87  44  79 102  11   4  31  34  66  80   2  16   6  77  39  97  33 100]\n",
      "a_shuffle_IDXs: [32 42 75  4 26 20  6 72  3 47 52 65 24 22 25 71 76 28 30 67 68 35 40 61 70 16 15 11 59 55 66 19 18 57 73 48 33 37 56 49 10 31 14 58 41 43 13 54 34 69 78 17 79  0 36 23 50 53  2 64 77 29 21 63 39 12 38 27  8  7  5 44 62  9 46  1 60 74 51 45], a_shuffle_aclus: [ 43  57 100   6  33  27   9  95   5  62  69  86  31  29  32  93 101  35  39  89  90  47  55  82  92  23  20  15  80  72  87  26  25  77  97  63  44  51  75  66  14  40  19  79  56  58  18  71  45  91 103  24 104   2  48  30  67  70   4  85 102  38  28  84  53  16  52  34  12  11   8  59  83  13  61   3  81  98  68  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 79 26  6 36 47 33 16 76 27 23 69 11 60 31 74 63 32 12  7 21 64  8 20 77 44  1 57 30 70 78 42 41 37 66 45 28 59 14 19  5 75  3 53 62 43 18 35 38  2  9 15  0 10 24  4 34 55 22 17 72 52 48 56 46 40 71 61 13 49 39 65 58 68 54 25 29 51 73 50], a_shuffle_aclus: [ 89 104  33   9  48  62  44  23 101  34  30  91  15  81  40  98  84  43  16  11  28  85  12  27 102  59   3  77  39  92 103  57  56  51  87  60  35  80  19  26   8 100   5  70  83  58  25  47  52   4  13  20   2  14  31   6  45  72  29  24  95  69  63  75  61  55  93  82  18  66  53  86  79  90  71  32  38  68  97  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 76 50  7 11 13 48 23 73  5 66 19  8  3 10 46  0 55 34 39 38 70 26 24 32 47 40 15 37 14 33  4 35 41 49 60  2 54 31 62 53 74 44 57 30 27 68 20 64 71 52  9 12 78 77 58 51 28 79 22 63 18 45 29 36 69 65 43 72 16 56 59 75 61 42 17 21  1  6 25], a_shuffle_aclus: [ 89 101  67  11  15  18  63  30  97   8  87  26  12   5  14  61   2  72  45  53  52  92  33  31  43  62  55  20  51  19  44   6  47  56  66  81   4  71  40  83  70  98  59  77  39  34  90  27  85  93  69  13  16 103 102  79  68  35 104  29  84  25  60  38  48  91  86  58  95  23  75  80 100  82  57  24  28   3   9  32]\n",
      "a_shuffle_IDXs: [ 7 27 74 40 79 57 71 45 58 50  2 43 38 76 47 70 29 41 24 12 39 46 18 67 11  9 78 77 30 49 21 75 20 52 54 23 22  6 48  4 59 68 65 15  0  1 42 56 66 61 33 35  3 19 17 34  8 32 13 14 36 55 72 16 64 44 31 62 69 60 53 10 28 63  5 73 26 25 51 37], a_shuffle_aclus: [ 11  34  98  55 104  77  93  60  79  67   4  58  52 101  62  92  38  56  31  16  53  61  25  89  15  13 103 102  39  66  28 100  27  69  71  30  29   9  63   6  80  90  86  20   2   3  57  75  87  82  44  47   5  26  24  45  12  43  18  19  48  72  95  23  85  59  40  83  91  81  70  14  35  84   8  97  33  32  68  51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 63  0  8 37  3 28 15 50 42 68 27 74 26 58 77  6 60 10 24 72 69 57  1 45 38 71  4 33 76 44 17 12 13 67 36 66 16 53 70  2  5 52 73 32 34 78 40 18 49 43 56 62 75 29 79 65 21  7 64 39 25 19 20 35 46  9 47 30 59 11 55 14 48 31 51 23 61 54 22], a_shuffle_aclus: [ 56  84   2  12  51   5  35  20  67  57  90  34  98  33  79 102   9  81  14  31  95  91  77   3  60  52  93   6  44 101  59  24  16  18  89  48  87  23  70  92   4   8  69  97  43  45 103  55  25  66  58  75  83 100  38 104  86  28  11  85  53  32  26  27  47  61  13  62  39  80  15  72  19  63  40  68  30  82  71  29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 27 79 42 74 48 32  1 67 17 24 43 28 25 30 69 65 62  2 29 22 76 72 16 70 61 49 39 78 53 55 13 19  5 18 12  0 57 66 40 46 20 50 63 45 60 41 26 21 14 15  9 47 51  7 31 37 71 68 54 64 56 11 10  4  6 34  8 36 23 52 58 38 33 44  3 75 59 77 35], a_shuffle_aclus: [ 97  34 104  57  98  63  43   3  89  24  31  58  35  32  39  91  86  83   4  38  29 101  95  23  92  82  66  53 103  70  72  18  26   8  25  16   2  77  87  55  61  27  67  84  60  81  56  33  28  19  20  13  62  68  11  40  51  93  90  71  85  75  15  14   6   9  45  12  48  30  69  79  52  44  59   5 100  80 102  47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [30 79 74 35 15 70 63 37 23 66 17 52 56 36 38 16 49 14 54  2 53 11 75 25 46 71 42 31 13 57 69 10 48 73 40 60 45 12  5 43  1 20 62 59 76 24 44 77 29 47  9  7 26 28 34 67 61 58 55 68 39  6 22 64 41 33 18 65 21  3 27  0 51 72 19  4 50 32 78  8], a_shuffle_aclus: [ 39 104  98  47  20  92  84  51  30  87  24  69  75  48  52  23  66  19  71   4  70  15 100  32  61  93  57  40  18  77  91  14  63  97  55  81  60  16   8  58   3  27  83  80 101  31  59 102  38  62  13  11  33  35  45  89  82  79  72  90  53   9  29  85  56  44  25  86  28   5  34   2  68  95  26   6  67  43 103  12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 39 17 21 10 33  9 15 56 53  4  3 70 35 25 23 69 42 34 52 43 78 38 13 47 57  8 19  6 36 41 60 62 45 79 77 26 30 72 49 44 76 37 71 63 14 24 67 61 31  2 22 65 32 74 46 58 75 55 40  1 18 28 27 48 73 29 12 16 54 11  7 51 66 64 20  5 68  0 59], a_shuffle_aclus: [ 67  53  24  28  14  44  13  20  75  70   6   5  92  47  32  30  91  57  45  69  58 103  52  18  62  77  12  26   9  48  56  81  83  60 104 102  33  39  95  66  59 101  51  93  84  19  31  89  82  40   4  29  86  43  98  61  79 100  72  55   3  25  35  34  63  97  38  16  23  71  15  11  68  87  85  27   8  90   2  80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75  7 32 66 61 72 14 31 34  9 76 11 68 44 16 59 27 33 50 63 43 69 12 55 60 52 35  1 21 54 70  2 25 65 48 71 56 74 73 40  4 22 36 30 41 77 57 64 26 13 51  6  8 67 62 17  0 38 49 37 46 10 39 42 28 29 47  3 58 19 79 53 18 45 78 24  5 15 20 23], a_shuffle_aclus: [100  11  43  87  82  95  19  40  45  13 101  15  90  59  23  80  34  44  67  84  58  91  16  72  81  69  47   3  28  71  92   4  32  86  63  93  75  98  97  55   6  29  48  39  56 102  77  85  33  18  68   9  12  89  83  24   2  52  66  51  61  14  53  57  35  38  62   5  79  26 104  70  25  60 103  31   8  20  27  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22 16 58 77 44 39 68 71 45 69 41 57 53  7 43 67 34 37  4 46 48 74 78 14 64 49 65 47  0 76 24  5 59 30 33 38  3  9 20 35 29 26 72 52 62 54 32 19 25 50 15  8 61 51 55 23 18 40 11 75 60 12 66 31  1 79 56 27  2 70 10 13 42 36 73 28  6 21 63 17], a_shuffle_aclus: [ 29  23  79 102  59  53  90  93  60  91  56  77  70  11  58  89  45  51   6  61  63  98 103  19  85  66  86  62   2 101  31   8  80  39  44  52   5  13  27  47  38  33  95  69  83  71  43  26  32  67  20  12  82  68  72  30  25  55  15 100  81  16  87  40   3 104  75  34   4  92  14  18  57  48  97  35   9  28  84  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 62 79 10 31 71 12 14 18 35 17  6 20 36 30 19 32 38 23 50 70 47 44 63 43 26 42 61 28 51 46  0 41 15 73  8 29 76 53 55 65 59  9 45  3 58 40 77 74 22 39  1 69 25 78  4 48 13 37 33 64 66 54 60  5  7 75 72 57 27 16 21  2 34 56 49 68 67 52 24], a_shuffle_aclus: [ 15  83 104  14  40  93  16  19  25  47  24   9  27  48  39  26  43  52  30  67  92  62  59  84  58  33  57  82  35  68  61   2  56  20  97  12  38 101  70  72  86  80  13  60   5  79  55 102  98  29  53   3  91  32 103   6  63  18  51  44  85  87  71  81   8  11 100  95  77  34  23  28   4  45  75  66  90  89  69  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [77  1 52 54 36 10 46 48 27  8 73 69 29 23 64  4 50 21 75 56 38  6 65 51 68 22 72 40 70 47 66 17  3 60 67  2 20 37 61 49 12 43 26 41 45  7 55 18 59 15 39 71 14  9 34 74 11 13 19 28 24  5 42 58 53 31 33 35 25 62 57 79 78 32 63 44 30 76  0 16], a_shuffle_aclus: [102   3  69  71  48  14  61  63  34  12  97  91  38  30  85   6  67  28 100  75  52   9  86  68  90  29  95  55  92  62  87  24   5  81  89   4  27  51  82  66  16  58  33  56  60  11  72  25  80  20  53  93  19  13  45  98  15  18  26  35  31   8  57  79  70  40  44  47  32  83  77 104 103  43  84  59  39 101   2  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 66 69 15 18 49 11 79 75 12  5 32 48 20 35 38 27 33 74 77  1 51  4 44 65 46 41 40 34 62 52 42 54 22 72  8  9 56 26 23  0 68 58 53 70 55 30 10  6 14 28  3 19 43 13 25 47 57 50 76 71 37 78 16 59 73  7 45 60 39 36  2 24 64 63 21 17 29 31 67], a_shuffle_aclus: [ 82  87  91  20  25  66  15 104 100  16   8  43  63  27  47  52  34  44  98 102   3  68   6  59  86  61  56  55  45  83  69  57  71  29  95  12  13  75  33  30   2  90  79  70  92  72  39  14   9  19  35   5  26  58  18  32  62  77  67 101  93  51 103  23  80  97  11  60  81  53  48   4  31  85  84  28  24  38  40  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [52 18 75  6 63  1 27  8 77 37 30 28 23 10 12 53 68  3 56 34 60 44 54 14 69 79 24 48 22 71 42 73 50  7 74  9 70 33 40 31 32 66 57 13 41  0  5 72 17 29 59 51 36  4 67 38 49 65 19 58 16 26 62 76 43 78 55 35 45 21 25 15 11 46 39  2 20 64 61 47], a_shuffle_aclus: [ 69  25 100   9  84   3  34  12 102  51  39  35  30  14  16  70  90   5  75  45  81  59  71  19  91 104  31  63  29  93  57  97  67  11  98  13  92  44  55  40  43  87  77  18  56   2   8  95  24  38  80  68  48   6  89  52  66  86  26  79  23  33  83 101  58 103  72  47  60  28  32  20  15  61  53   4  27  85  82  62]\n",
      "a_shuffle_IDXs: [12 29 13 66 23 74  7  4 44 24 22 38 42 45 37 47  2 53 69 67 26 14 71 30 28 55 33 60 72 70 78 43 57 58 52 79  8 10 68 50 49 19 41 35 76 16 48  0 39 75 46 27 63  3 65  1 31 34 20 54 32 11 56 15  6 17 51 73 18 62 61 77 59  9 36 21 40 64 25  5], a_shuffle_aclus: [ 16  38  18  87  30  98  11   6  59  31  29  52  57  60  51  62   4  70  91  89  33  19  93  39  35  72  44  81  95  92 103  58  77  79  69 104  12  14  90  67  66  26  56  47 101  23  63   2  53 100  61  34  84   5  86   3  40  45  27  71  43  15  75  20   9  24  68  97  25  83  82 102  80  13  48  28  55  85  32   8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 47 67 44  4  8 60 58 30 73 48 19 46 51 33 54 14 66 70 55 75 68 13 78 17 24 15 65 32 12  6 79  0 42  1 23 22 57  9 56 38 31 59 69 53 26 43 74 39 34 63 45 37 52 77 36 21 10 72 61 62 71 29 11  5 41 35  7 49 25 20 16 27  3 40 76  2 50 18 28], a_shuffle_aclus: [ 85  62  89  59   6  12  81  79  39  97  63  26  61  68  44  71  19  87  92  72 100  90  18 103  24  31  20  86  43  16   9 104   2  57   3  30  29  77  13  75  52  40  80  91  70  33  58  98  53  45  84  60  51  69 102  48  28  14  95  82  83  93  38  15   8  56  47  11  66  32  27  23  34   5  55 101   4  67  25  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 53 52 17 31 29  9  6 34 50 60 44 15  4 75 71 35 32 70 25 43 73 47 19 41 14 72 39 76  3 78 13 54 33 12 79 16 11 27 22  7 63 51 10 30 58 21 23 18 68 64 49 61  2  0 24 40 45 26 48 55 46 66 20 67  5 69  8 36 59 65 42 38 28 37 62 57  1 56 77], a_shuffle_aclus: [ 98  70  69  24  40  38  13   9  45  67  81  59  20   6 100  93  47  43  92  32  58  97  62  26  56  19  95  53 101   5 103  18  71  44  16 104  23  15  34  29  11  84  68  14  39  79  28  30  25  90  85  66  82   4   2  31  55  60  33  63  72  61  87  27  89   8  91  12  48  80  86  57  52  35  51  83  77   3  75 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [58 40 64 77 19 49 35 11 15 46 61 12 33 29 68 18 62 79 69  7 71 14 60  4 42 37 56 17 65 63  6 66 41 54 38 76  3 34 47 48 74 73  2 57 50 23  9 36  1 78 67 72 22 43  0 45 70 59 39  5 26 75 20 13 31 10 52 28 51 16 53 55 30 21  8 32 27 25 44 24], a_shuffle_aclus: [ 79  55  85 102  26  66  47  15  20  61  82  16  44  38  90  25  83 104  91  11  93  19  81   6  57  51  75  24  86  84   9  87  56  71  52 101   5  45  62  63  98  97   4  77  67  30  13  48   3 103  89  95  29  58   2  60  92  80  53   8  33 100  27  18  40  14  69  35  68  23  70  72  39  28  12  43  34  32  59  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33 59 24 72 25 48 13 57 79 73 62 70 76 38  3 21 18 23  6 74 63 16 31 44 58 61 27 67 75 46 20 78 39 60 68 36 45 69 34 42 50 55  2 49 65 11 19 29 17 66 14 47 28 10  7  5 12 30  1 51 54 53 43 35 71 41 26  9 77 56 37  0 32 15 52 22 64 40  4  8], a_shuffle_aclus: [ 44  80  31  95  32  63  18  77 104  97  83  92 101  52   5  28  25  30   9  98  84  23  40  59  79  82  34  89 100  61  27 103  53  81  90  48  60  91  45  57  67  72   4  66  86  15  26  38  24  87  19  62  35  14  11   8  16  39   3  68  71  70  58  47  93  56  33  13 102  75  51   2  43  20  69  29  85  55   6  12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [58 21 24 17 53 73 30 77 18 79 48  0 36 34 51 15 50 63 39 75 59 20 61 57 22 44 45 28 49 41 27  1 10 31 43 12 13 74 55 78 35 26 16  9 76 38  3 65  7 69 14 52 67 29 40 46 64 32 19 23 11  4  5 33 62 54 25  8 56 37 70 60 71 66 68  2 42 72  6 47], a_shuffle_aclus: [ 79  28  31  24  70  97  39 102  25 104  63   2  48  45  68  20  67  84  53 100  80  27  82  77  29  59  60  35  66  56  34   3  14  40  58  16  18  98  72 103  47  33  23  13 101  52   5  86  11  91  19  69  89  38  55  61  85  43  26  30  15   6   8  44  83  71  32  12  75  51  92  81  93  87  90   4  57  95   9  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78 35 28 43 25  9 63 15 74 39 60 77 14 19 26 33 40  3 57 59 46 34 68 51 30 10  2 75  4 53 21  8 11 29 73 27 67 52  1 32 56 20 50 37 49 48 66 70 12 24 55 54 76  0 31  5 71 62 45 38 22  6 72 69 44 61 17  7 13 36 64 23 65 16 41 42 47 18 79 58], a_shuffle_aclus: [103  47  35  58  32  13  84  20  98  53  81 102  19  26  33  44  55   5  77  80  61  45  90  68  39  14   4 100   6  70  28  12  15  38  97  34  89  69   3  43  75  27  67  51  66  63  87  92  16  31  72  71 101   2  40   8  93  83  60  52  29   9  95  91  59  82  24  11  18  48  85  30  86  23  56  57  62  25 104  79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7  5 26 30 64 44 33  2 34 42 14 12 62 60 78 45 31 71 38  3 55 40 11 69 35 39 52 29 18 19 22 54  6 61 77 57  8 15 72 73 32 27 47 67 59 24 50  4 58 16 75  1 25 17 65  9 37 46 43 76 49 48 74 20 63 51 56 53  0 68 41 10 36 13 21 66 28 70 79 23], a_shuffle_aclus: [ 11   8  33  39  85  59  44   4  45  57  19  16  83  81 103  60  40  93  52   5  72  55  15  91  47  53  69  38  25  26  29  71   9  82 102  77  12  20  95  97  43  34  62  89  80  31  67   6  79  23 100   3  32  24  86  13  51  61  58 101  66  63  98  27  84  68  75  70   2  90  56  14  48  18  28  87  35  92 104  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78 18 28 58  8 43  6  3 35 71 57  9 14  0 49 63 25 59 68 17 40 23 44 77 69 72 19 29 61 34 12 60 37 75 24  5 52 54 42 22 13 62 31 48 76 51 47 50 56 65 73  4  2 27 21  7 45 33 66 36 53 67 74 16 10  1 15 38 79 64 11 32 41 70 30 39 55 26 46 20], a_shuffle_aclus: [103  25  35  79  12  58   9   5  47  93  77  13  19   2  66  84  32  80  90  24  55  30  59 102  91  95  26  38  82  45  16  81  51 100  31   8  69  71  57  29  18  83  40  63 101  68  62  67  75  86  97   6   4  34  28  11  60  44  87  48  70  89  98  23  14   3  20  52 104  85  15  43  56  92  39  53  72  33  61  27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 76 21 26 78  6 61 16 22 37 19 62 47 72 77 41 71 58 23 40 75 73  7 34 31  0  1 48 66  9 12 55 67 51  3 13 27 56 32 28 42 50 64 10 17  8 63 33 59 74 39 68 69 14  2 49 18 45 60 36 29 30 15 43 25 79 38 54 53 35 65  4 57  5 20 70 11 24 44 52], a_shuffle_aclus: [ 61 101  28  33 103   9  82  23  29  51  26  83  62  95 102  56  93  79  30  55 100  97  11  45  40   2   3  63  87  13  16  72  89  68   5  18  34  75  43  35  57  67  85  14  24  12  84  44  80  98  53  90  91  19   4  66  25  60  81  48  38  39  20  58  32 104  52  71  70  47  86   6  77   8  27  92  15  31  59  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63  7 26  3 72 49 14  6  2 59 10 60 71 57 58  0 13 44 15 17 62 75 33 35 73 79 51 69 43 76 64 46  5 16 70 12 29 77 55 27  4 18 78 48 45 37 30 23 66 65  8 32 28 20 67 19 11 52 56  9 39 42 22 47 25 38 41 21 50 34 24 61 31 53 68  1 40 74 54 36], a_shuffle_aclus: [ 84  11  33   5  95  66  19   9   4  80  14  81  93  77  79   2  18  59  20  24  83 100  44  47  97 104  68  91  58 101  85  61   8  23  92  16  38 102  72  34   6  25 103  63  60  51  39  30  87  86  12  43  35  27  89  26  15  69  75  13  53  57  29  62  32  52  56  28  67  45  31  82  40  70  90   3  55  98  71  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 34 39 60 16  8 10 41 53  6  7 62  3 23 27 31 25 28 68 24 76 43 11 35 74 14 78 36 72 29 66  0 69 30 37 54  5  1 52 40 61 57 17 38 65 47 63 49 20 58 77 44 55 70 46 33  2 48 12 56 13 19  4  9 59 26 73 51 22 18 71 45 67 15 50 21 75 64 42 32], a_shuffle_aclus: [104  45  53  81  23  12  14  56  70   9  11  83   5  30  34  40  32  35  90  31 101  58  15  47  98  19 103  48  95  38  87   2  91  39  51  71   8   3  69  55  82  77  24  52  86  62  84  66  27  79 102  59  72  92  61  44   4  63  16  75  18  26   6  13  80  33  97  68  29  25  93  60  89  20  67  28 100  85  57  43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 26 28  3 25 74 70 68 41 45 66 79 62 18 53 13 61 58 14 57 64  7 55 16 29  1 33 12 31 56 30 42 48 39 22  8 23 52 37 76 21 54 20 38 10  6  9 15 47 65 19 73 43 49 51 32 77  5 75 71 35  2 40 60  4 59 72 78  0 63 34 11 44 69 36 50 27 67 17 24], a_shuffle_aclus: [ 61  33  35   5  32  98  92  90  56  60  87 104  83  25  70  18  82  79  19  77  85  11  72  23  38   3  44  16  40  75  39  57  63  53  29  12  30  69  51 101  28  71  27  52  14   9  13  20  62  86  26  97  58  66  68  43 102   8 100  93  47   4  55  81   6  80  95 103   2  84  45  15  59  91  48  67  34  89  24  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 64 49 47 25 17  1 13 24  0 56  4  8 27 76 63 22 58 54 35 18 36 21 33 28 68 79 29 23 72 59 51 26 57  5 30 53 16 70 39 66 32  6 65 60 14 48 46 69 37  2  9 74 40 77 12 78 38 11 10 20 55 52 62 31 71 15 44 61  3 73 34 45 42 50 43 75  7 41 19], a_shuffle_aclus: [ 89  85  66  62  32  24   3  18  31   2  75   6  12  34 101  84  29  79  71  47  25  48  28  44  35  90 104  38  30  95  80  68  33  77   8  39  70  23  92  53  87  43   9  86  81  19  63  61  91  51   4  13  98  55 102  16 103  52  15  14  27  72  69  83  40  93  20  59  82   5  97  45  60  57  67  58 100  11  56  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 23 64 33 47 60 71 25 56 46 50 62 20 67 52 36 74  9 42 75  4 66  2 79 38 10 76 58 54  8 39 73 41 49 43  7 27 34 22 14 65 72 37 48 19 29 40 45 53 32 18 44 61 16 78 51  5 68 26 77 24 12  1  3 15 11 70 63 35 69 17  6 13  0 21 59 28 30 31 55], a_shuffle_aclus: [ 77  30  85  44  62  81  93  32  75  61  67  83  27  89  69  48  98  13  57 100   6  87   4 104  52  14 101  79  71  12  53  97  56  66  58  11  34  45  29  19  86  95  51  63  26  38  55  60  70  43  25  59  82  23 103  68   8  90  33 102  31  16   3   5  20  15  92  84  47  91  24   9  18   2  28  80  35  39  40  72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 42 79 77  0 55 10 40 45 66 37 69 53 60 67 78 29 64  6  4 44 68 71 17 36 59 14 50  7 23 21 76  1 31 16 11  3 46 22 32 26 62 20  5 39 28 12 27 18 63 49 75 30 61 38  2 41 15 54 52 34 47 57 72 58 65 13 48 25  9 73 43 33 35 19 74 70  8 56 51], a_shuffle_aclus: [ 31  57 104 102   2  72  14  55  60  87  51  91  70  81  89 103  38  85   9   6  59  90  93  24  48  80  19  67  11  30  28 101   3  40  23  15   5  61  29  43  33  83  27   8  53  35  16  34  25  84  66 100  39  82  52   4  56  20  71  69  45  62  77  95  79  86  18  63  32  13  97  58  44  47  26  98  92  12  75  68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [53 49 31 59 62 18 51 72 63 17 12 16 55 10  9 27 57 41 48 13  1 74 14  8 50 22 33 64 38 69 52 43 44 46 28 42 21 67 29 32 15 40 76 34  2 35 79  4 24 78 61 58 19 25  6  7 45 77 73 39 11  5 36  0 65 54 26 47 20 70 68 66 71  3 56 75 60 30 37 23], a_shuffle_aclus: [ 70  66  40  80  83  25  68  95  84  24  16  23  72  14  13  34  77  56  63  18   3  98  19  12  67  29  44  85  52  91  69  58  59  61  35  57  28  89  38  43  20  55 101  45   4  47 104   6  31 103  82  79  26  32   9  11  60 102  97  53  15   8  48   2  86  71  33  62  27  92  90  87  93   5  75 100  81  39  51  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 19 63 65 41 70 73 36 51 17 14 22 46 48 11 74  1 39  2 15 32 72 31 43 47 33  3 52 79 64 29 58 57 24 49 12  5 18 16 10 75  0  6 54 53 62 42  7 77 13 40 37 30 38  4 76 34 28 69 78 27 71 25 60 66 68 50 61  8 20 45 55  9 23 44 26 59 67 56 35], a_shuffle_aclus: [ 28  26  84  86  56  92  97  48  68  24  19  29  61  63  15  98   3  53   4  20  43  95  40  58  62  44   5  69 104  85  38  79  77  31  66  16   8  25  23  14 100   2   9  71  70  83  57  11 102  18  55  51  39  52   6 101  45  35  91 103  34  93  32  81  87  90  67  82  12  27  60  72  13  30  59  33  80  89  75  47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 70 49 41  2 73 68 65 60 16 25  1 44 31 50  3 48 69 76 43 26 61 54 13  7 24 62 30 52 28 45 19 38  0 46 36 37  4 40 63 35  9 27 78 53 23 17 42 75  6 33 58 57 55 67 15 51 22 32 59 72 29 21 34 14 66 79 12  5 11 71 47  8 18 74 20 39 77 10 56], a_shuffle_aclus: [ 85  92  66  56   4  97  90  86  81  23  32   3  59  40  67   5  63  91 101  58  33  82  71  18  11  31  83  39  69  35  60  26  52   2  61  48  51   6  55  84  47  13  34 103  70  30  24  57 100   9  44  79  77  72  89  20  68  29  43  80  95  38  28  45  19  87 104  16   8  15  93  62  12  25  98  27  53 102  14  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 66 13 75 67 69 62 60 15 38  8  7 55  6 27  4 71 43 54 35 44 34 17 37 11  9  0 70 20  2 16 29 73 59 33 58 76 63 10 23 65 51  1 56 36 12 40 49 30 18 25 42 48 22 28 46 14 74 61 45 78 52 64 50 72 77 47 39  3 24 31 57 79 41  5 26 68 53 19 32], a_shuffle_aclus: [ 28  87  18 100  89  91  83  81  20  52  12  11  72   9  34   6  93  58  71  47  59  45  24  51  15  13   2  92  27   4  23  38  97  80  44  79 101  84  14  30  86  68   3  75  48  16  55  66  39  25  32  57  63  29  35  61  19  98  82  60 103  69  85  67  95 102  62  53   5  31  40  77 104  56   8  33  90  70  26  43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 64 44 75  7 20 43 37  9 14 77 25 59 52 34 45 21 32 72 15 76 35 26  3 39 29  8 17 36 41  2 22 13 60 69 61 10 24 55 40  0 28 68 38  5 62 12 27 50 46 79 65 74 47 23 53 63 11 33 67  1 78 57 16  4 56 31 70 19 42 30 73 66 49 18 54 48 51 71 58], a_shuffle_aclus: [  9  85  59 100  11  27  58  51  13  19 102  32  80  69  45  60  28  43  95  20 101  47  33   5  53  38  12  24  48  56   4  29  18  81  91  82  14  31  72  55   2  35  90  52   8  83  16  34  67  61 104  86  98  62  30  70  84  15  44  89   3 103  77  23   6  75  40  92  26  57  39  97  87  66  25  71  63  68  93  79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [15 17 76 72 40 36 26 68 34 61 50 44 49 66 28  7 20 79 14 30 37 29  2 63 78 33 55 13 71 27 70 11 54 56 10 74  0 59 25 77  1 24 52  8 12 32  6 18 45 16 53 42 73 60 22 67  4  3 41 19 64 38 23 47 48 35 65 39 57 75 46  9 62  5 69 51 58 21 43 31], a_shuffle_aclus: [ 20  24 101  95  55  48  33  90  45  82  67  59  66  87  35  11  27 104  19  39  51  38   4  84 103  44  72  18  93  34  92  15  71  75  14  98   2  80  32 102   3  31  69  12  16  43   9  25  60  23  70  57  97  81  29  89   6   5  56  26  85  52  30  62  63  47  86  53  77 100  61  13  83   8  91  68  79  28  58  40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 79 26 25  2 54 55 20 50  5 52 31 51 64  3  6 66 43 48 56 24 13 19 77 78 38 23 72 42 30 46 40 47 14 71  8 73 68 53 28 36 57 11 75 12 45 33 76  0 22 62 74 27 29 60 35 63 16 49  1 70 37 58 18 32  7 65 17 59 15 21 10 69 44 67 41 34  9 39 61], a_shuffle_aclus: [  6 104  33  32   4  71  72  27  67   8  69  40  68  85   5   9  87  58  63  75  31  18  26 102 103  52  30  95  57  39  61  55  62  19  93  12  97  90  70  35  48  77  15 100  16  60  44 101   2  29  83  98  34  38  81  47  84  23  66   3  92  51  79  25  43  11  86  24  80  20  28  14  91  59  89  56  45  13  53  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 61 36  7 44 74 27 29 72 62 11 12 18 78 16  9  6 38 77 31 70 52 60 54 21 45 57 32 24  2 73 35 75 41 15 34 43 22 47 63 69 76 71 51 33  8 79 30 42  3 39 19 65 37 17 48 13 58 46 50 53 40 68  4 14 67 10 49 26 20 66 28  0  1 64 25 59  5 55 23], a_shuffle_aclus: [ 75  82  48  11  59  98  34  38  95  83  15  16  25 103  23  13   9  52 102  40  92  69  81  71  28  60  77  43  31   4  97  47 100  56  20  45  58  29  62  84  91 101  93  68  44  12 104  39  57   5  53  26  86  51  24  63  18  79  61  67  70  55  90   6  19  89  14  66  33  27  87  35   2   3  85  32  80   8  72  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12 74 70  8 36  2 26 57 11 29 17 34 42 61 45 14 78 69 62 79 37 55 51 13 33  7 21  6 50 59 30 73 40 41 31 28 47 25 53 68  5 64 46 35 66 15  9 54 49 76  4 77 10 65 20 18 23 71 72 58 16 32 48 52 24  3 44  1  0 38 67 75 22 43 39 63 19 56 60 27], a_shuffle_aclus: [ 16  98  92  12  48   4  33  77  15  38  24  45  57  82  60  19 103  91  83 104  51  72  68  18  44  11  28   9  67  80  39  97  55  56  40  35  62  32  70  90   8  85  61  47  87  20  13  71  66 101   6 102  14  86  27  25  30  93  95  79  23  43  63  69  31   5  59   3   2  52  89 100  29  58  53  84  26  75  81  34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 14  7 44 57 18 74 59 68  2 40  4 62 19 45 23 73 26 71 53 15 39 63 48 69 42 36 49 10 55 22 35 24 41 47 51 54 30 20  8 38 56 58 76 21 70 79  0  3 31 61 65 50 78 13  1 25 75  5 37 29 64  6 34 52 17 43 60 33 28 32 66 72 77 11 12  9 46 67 16], a_shuffle_aclus: [ 34  19  11  59  77  25  98  80  90   4  55   6  83  26  60  30  97  33  93  70  20  53  84  63  91  57  48  66  14  72  29  47  31  56  62  68  71  39  27  12  52  75  79 101  28  92 104   2   5  40  82  86  67 103  18   3  32 100   8  51  38  85   9  45  69  24  58  81  44  35  43  87  95 102  15  16  13  61  89  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0 60 77 67  2 53 62 26 68 73 65 71 14 29 47 66 10 78 33 52 63 34 11 19 16 28 50 40 42  3 51 48 24 35 59 20  1 79 41  8 38 61 76 45 46 44 22 17 39 64  6 43 18  4 30 54 37 70 49 15  7 13  5 25 36 32 21 72 31 75 23 69 55  9 12 58 56 74 27 57], a_shuffle_aclus: [  2  81 102  89   4  70  83  33  90  97  86  93  19  38  62  87  14 103  44  69  84  45  15  26  23  35  67  55  57   5  68  63  31  47  80  27   3 104  56  12  52  82 101  60  61  59  29  24  53  85   9  58  25   6  39  71  51  92  66  20  11  18   8  32  48  43  28  95  40 100  30  91  72  13  16  79  75  98  34  77]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 36 50 47 69 31 68 40 24 10 17 45 43 33 78 75 13  1  8 64 41  7 21 19 72 12 30 38 14 16 59 71 15  5 74  0 49 58 67 79 54 55 57 34 73 76 48 46 25 70 26 27 65 61 32 23 20  3  2 52  4 35 56 37 53 62 63 77 60  9 18 11 44 39 42 28 66 51 29 22], a_shuffle_aclus: [  9  48  67  62  91  40  90  55  31  14  24  60  58  44 103 100  18   3  12  85  56  11  28  26  95  16  39  52  19  23  80  93  20   8  98   2  66  79  89 104  71  72  77  45  97 101  63  61  32  92  33  34  86  82  43  30  27   5   4  69   6  47  75  51  70  83  84 102  81  13  25  15  59  53  57  35  87  68  38  29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [53 62 58  2 56 74 59 52 47 16 50 71 13 25 73 48 76 68 28  8 14 34 41 63 72 27 11 36  7 49 69 12  4 19 39 45 23  5 33 21 37 18 75 66 38 35  1 54 26  6 43 64 24 70 15 17 31 32 79 20  3 77  0 61 67 65 40 55 57 22 44  9 60 42 51 78 10 29 46 30], a_shuffle_aclus: [ 70  83  79   4  75  98  80  69  62  23  67  93  18  32  97  63 101  90  35  12  19  45  56  84  95  34  15  48  11  66  91  16   6  26  53  60  30   8  44  28  51  25 100  87  52  47   3  71  33   9  58  85  31  92  20  24  40  43 104  27   5 102   2  82  89  86  55  72  77  29  59  13  81  57  68 103  14  38  61  39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [25 29 12 72 23 52 30 46 61 50 51 19 47 64 14 44 58 41  4 37  6 11 62 73 70 78 40 34 24 35 31  9 69 17 16  2 55 36 56  8 15 71 39 67 21 28 65 53 42 48  1 33 43 77  7 22  3 60 63 27 26 54 74 75 10 68 38 49 66 76 32 13 45  5 79  0 57 20 18 59], a_shuffle_aclus: [ 32  38  16  95  30  69  39  61  82  67  68  26  62  85  19  59  79  56   6  51   9  15  83  97  92 103  55  45  31  47  40  13  91  24  23   4  72  48  75  12  20  93  53  89  28  35  86  70  57  63   3  44  58 102  11  29   5  81  84  34  33  71  98 100  14  90  52  66  87 101  43  18  60   8 104   2  77  27  25  80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 26  9 17 44 35 47 33 28 74 51  6 53 58 60 71 20 67 43 45 54 25 49 59  2 18 13 10  8 65 62  0 40 79  3 24 16 70 38 46 37 73  5 21 34 42 12 75 48 27 31  4 68 41 22 39 77 69 32 14 55  7 78 76 52 29 23 72 36 57 63 56  1 66 11 64 30 15 50 19], a_shuffle_aclus: [ 82  33  13  24  59  47  62  44  35  98  68   9  70  79  81  93  27  89  58  60  71  32  66  80   4  25  18  14  12  86  83   2  55 104   5  31  23  92  52  61  51  97   8  28  45  57  16 100  63  34  40   6  90  56  29  53 102  91  43  19  72  11 103 101  69  38  30  95  48  77  84  75   3  87  15  85  39  20  67  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [18  7 20 47 37 15 78 59 74 39 75 76 77 43 53 44 49 60 29 62 64 36 16 72 46 32  9 45 51 21 10 48 63 38 26 23  1 56 50 54 24 11 27 70 57 67 19  3 22 17 65 73 14 31 61 25 69  6 68  4 41 12  5  8 66  2 34 28 35 42  0 71 33 30 13 55 79 40 52 58], a_shuffle_aclus: [ 25  11  27  62  51  20 103  80  98  53 100 101 102  58  70  59  66  81  38  83  85  48  23  95  61  43  13  60  68  28  14  63  84  52  33  30   3  75  67  71  31  15  34  92  77  89  26   5  29  24  86  97  19  40  82  32  91   9  90   6  56  16   8  12  87   4  45  35  47  57   2  93  44  39  18  72 104  55  69  79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [36 71 67 55 79  2 59 29 32 45  6 65 63 69  1 33  9 20 46 14 39  5 53 57 28 66 42 10  0 43 50 40 75 44 16 22 64 48 18 68 17  8 60  7 25 74 12 35 34 54 62 31 78 24  3 38 70 58 27 13 37 52 21 23  4 51 73 56 72 19 49 11 26 15 76 77 30 41 61 47], a_shuffle_aclus: [ 48  93  89  72 104   4  80  38  43  60   9  86  84  91   3  44  13  27  61  19  53   8  70  77  35  87  57  14   2  58  67  55 100  59  23  29  85  63  25  90  24  12  81  11  32  98  16  47  45  71  83  40 103  31   5  52  92  79  34  18  51  69  28  30   6  68  97  75  95  26  66  15  33  20 101 102  39  56  82  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 26 37 39 32 51 24 42 30 27 61 63 43  7  0 45 74 28 25 13  8 72 58  5 56 44 69  3 64  9 35 59 16 12  1 48 78 68 49 75 10 21 15  2 54 55 46 41 73 50 22 67  4 19 57 31 52 18 70 76 14 20 40 17 60 36 71 38 11 65 66 29 34  6 79 33 77 53 62 23], a_shuffle_aclus: [ 62  33  51  53  43  68  31  57  39  34  82  84  58  11   2  60  98  35  32  18  12  95  79   8  75  59  91   5  85  13  47  80  23  16   3  63 103  90  66 100  14  28  20   4  71  72  61  56  97  67  29  89   6  26  77  40  69  25  92 101  19  27  55  24  81  48  93  52  15  86  87  38  45   9 104  44 102  70  83  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14 77 65 10  6 53 51 17 41 22 62 25 21 23  1 42 35 13 79 27 28 11 36 43 69 32 48  0 16  5 68  9 75 50 29 71 63 60 59 18 58 19 76 15 37 39 46 26 40 74  7 73 20 24 64 52 70 30  4 45 78  2 31 47 72 56 34 55  8 61 57 44 12  3 67 38 54 33 66 49], a_shuffle_aclus: [ 19 102  86  14   9  70  68  24  56  29  83  32  28  30   3  57  47  18 104  34  35  15  48  58  91  43  63   2  23   8  90  13 100  67  38  93  84  81  80  25  79  26 101  20  51  53  61  33  55  98  11  97  27  31  85  69  92  39   6  60 103   4  40  62  95  75  45  72  12  82  77  59  16   5  89  52  71  44  87  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 16 35 10 20  1 23  6  2 72 50 68 64 17 27 48 30 13 45 63 43 66 69 59 54 26 60 49 12 24 71 56  5 76 53 47 34 28 15 37 46 67 21 73 11 57 79 25 31 42 19 44 36 41 65 70 75 55 62 74 52 18 14  8 22 32 78 38  9 77  0 51 40 33 58 39 29  7  4 61], a_shuffle_aclus: [  5  23  47  14  27   3  30   9   4  95  67  90  85  24  34  63  39  18  60  84  58  87  91  80  71  33  81  66  16  31  93  75   8 101  70  62  45  35  20  51  61  89  28  97  15  77 104  32  40  57  26  59  48  56  86  92 100  72  83  98  69  25  19  12  29  43 103  52  13 102   2  68  55  44  79  53  38  11   6  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [18 70 40 14 30 68 48 27  3 45 28 25 75 19 47 67  8 38 57 16 63  5  7 31 56 15 79 39 42 10 74  4 13  2 36 69 33 66 17 62  1 78 22 12 23 37 72 20 50 41 21 29 52 53 77 55 54  9 11 43 34  6 61 35 32 44 26 59 58 60 46 73  0 76 71 64 49 51 65 24], a_shuffle_aclus: [ 25  92  55  19  39  90  63  34   5  60  35  32 100  26  62  89  12  52  77  23  84   8  11  40  75  20 104  53  57  14  98   6  18   4  48  91  44  87  24  83   3 103  29  16  30  51  95  27  67  56  28  38  69  70 102  72  71  13  15  58  45   9  82  47  43  59  33  80  79  81  61  97   2 101  93  85  66  68  86  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 26  3 39  6 47 22 58 57 69 19 66 68 62 45 75 13 77 29 11 12 63 72 32 38 74 50 18 51 44  4  2 35 59 27 37 25 21 15 17 41 61 53 10  8 64  5 43 65 30 71 31 56 16 70 20 46 73 49 42 79 54  1 28  0  9 14 33 24 48 55 34 52 60  7 36 40 23 76 78], a_shuffle_aclus: [ 89  33   5  53   9  62  29  79  77  91  26  87  90  83  60 100  18 102  38  15  16  84  95  43  52  98  67  25  68  59   6   4  47  80  34  51  32  28  20  24  56  82  70  14  12  85   8  58  86  39  93  40  75  23  92  27  61  97  66  57 104  71   3  35   2  13  19  44  31  63  72  45  69  81  11  48  55  30 101 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [77 69 40 14 12 60 73 62 31 79 71  9 48  5 34  1 23 46  4 25 24 26 27 41 70 49 19  8 67 30 32 44 42  7 20 54 53 56 52 15  3 78 33 61 66 10 29  6 51 50 18 22 38 47 72 68  0 17 58 21 13 63 45 76 37 74 64 75 39 35 59 65 57 55 11 36 28  2 43 16], a_shuffle_aclus: [102  91  55  19  16  81  97  83  40 104  93  13  63   8  45   3  30  61   6  32  31  33  34  56  92  66  26  12  89  39  43  59  57  11  27  71  70  75  69  20   5 103  44  82  87  14  38   9  68  67  25  29  52  62  95  90   2  24  79  28  18  84  60 101  51  98  85 100  53  47  80  86  77  72  15  48  35   4  58  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 57 45 20 52 19 64 41 11 43 67 25 14 27  4 77  3 24 26 51 59 17 37  5 23  7 46 33 42 70 48 30 60 47 62 13 15 39 10 18 34 21 53 69  0 63 22 56 16 35  2 38 29 74 31 73 12 44 65  8 36 76 54 71 40 58 28 79 55 78 61 50 72  1 32  6 66 49 68 75], a_shuffle_aclus: [ 13  77  60  27  69  26  85  56  15  58  89  32  19  34   6 102   5  31  33  68  80  24  51   8  30  11  61  44  57  92  63  39  81  62  83  18  20  53  14  25  45  28  70  91   2  84  29  75  23  47   4  52  38  98  40  97  16  59  86  12  48 101  71  93  55  79  35 104  72 103  82  67  95   3  43   9  87  66  90 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 27 52 49 50 61 76 63 12 23 14 38  6 43 73 65 30 68 55  8 72 77 59 74 67 26 64  9 16 10 36 57 33 11 25 20 60 78 17  2 53 31 40 32 45 51 47 66 21  4 79 22 18 75 28 35  5 29 70 69 15  3  0 37 19 71 13 62 41 34 24 39 42 44  7 54 58 48 46  1], a_shuffle_aclus: [ 75  34  69  66  67  82 101  84  16  30  19  52   9  58  97  86  39  90  72  12  95 102  80  98  89  33  85  13  23  14  48  77  44  15  32  27  81 103  24   4  70  40  55  43  60  68  62  87  28   6 104  29  25 100  35  47   8  38  92  91  20   5   2  51  26  93  18  83  56  45  31  53  57  59  11  71  79  63  61   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [42 50 74 55 79 40 14 48 22 29 27 64 66 19 33 67  0 24 63 41  7 51 43 15 52 53 18 47 11  9 58 12 45 78 26 46 59 28 71 77 72 73 39 20 70  1 69 34 76 37 16 54  2  3 31 44 60 32 17 23 49 65 35 57  5  4 36 75 56 21 61 38 13  8 25 10 62  6 68 30], a_shuffle_aclus: [ 57  67  98  72 104  55  19  63  29  38  34  85  87  26  44  89   2  31  84  56  11  68  58  20  69  70  25  62  15  13  79  16  60 103  33  61  80  35  93 102  95  97  53  27  92   3  91  45 101  51  23  71   4   5  40  59  81  43  24  30  66  86  47  77   8   6  48 100  75  28  82  52  18  12  32  14  83   9  90  39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 17 72 15 61 31 51 33 46  3  1 71 29 56 34  0 18 12 57 54 42 63 68 14  7 58 66 37 41 70 62 48 47 69  6 30 26  5 79 24 45 59 77 44 43 39 74 13 36 75 53 10 22 25 76 49 28 20 23 67 27 65 40 32 11  4 38 52 35 16 64 73 21 50  2  8 19 78  9 55], a_shuffle_aclus: [ 81  24  95  20  82  40  68  44  61   5   3  93  38  75  45   2  25  16  77  71  57  84  90  19  11  79  87  51  56  92  83  63  62  91   9  39  33   8 104  31  60  80 102  59  58  53  98  18  48 100  70  14  29  32 101  66  35  27  30  89  34  86  55  43  15   6  52  69  47  23  85  97  28  67   4  12  26 103  13  72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 33 66  7 41 79 35  5 13 68 42 71 50 52  9  2  8 10 56  4 78 73 20 22 51 65 62 46 48 61 24 16 19 17 30 12 39 36 37 57 76  0  1 43 38 58 77 14 59 47 63 15 23 44 45 67 74  3 49 29 28 53 40 25 69 34 21 75 27  6 55 60 26 31 32 70 72 54 64 18], a_shuffle_aclus: [ 15  44  87  11  56 104  47   8  18  90  57  93  67  69  13   4  12  14  75   6 103  97  27  29  68  86  83  61  63  82  31  23  26  24  39  16  53  48  51  77 101   2   3  58  52  79 102  19  80  62  84  20  30  59  60  89  98   5  66  38  35  70  55  32  91  45  28 100  34   9  72  81  33  40  43  92  95  71  85  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22 47  8 73 78 60 37 56 13 28 12 55 70 49 65 23 75 50 15 62  1 43 58 64 18 14 68 53 16 30 29 44 41 52  0 57 66 61 79 20 11 46  9 42  6 19 24 71 40 74 54  7 39 69 33  5 51  3 48 36 38  4 31 32 72 63 21 10 35 59 27 25 45 76 17 67 26 77  2 34], a_shuffle_aclus: [ 29  62  12  97 103  81  51  75  18  35  16  72  92  66  86  30 100  67  20  83   3  58  79  85  25  19  90  70  23  39  38  59  56  69   2  77  87  82 104  27  15  61  13  57   9  26  31  93  55  98  71  11  53  91  44   8  68   5  63  48  52   6  40  43  95  84  28  14  47  80  34  32  60 101  24  89  33 102   4  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 20 79 24 64  5 71  1 17 43 68 15 59 46 16 54 11  8 44 14 19 75  7 47 35 13 37 51 62 22 53 42 26 58 78 63 56 29 25 52 49 69 77 10 74  9 61 73  3 72 67  2 12 39 66 36 28 23 48 34 65 31 41  6 76 70 45 55 40 38 60  4 30 33 32 57 21 27  0 18], a_shuffle_aclus: [ 67  27 104  31  85   8  93   3  24  58  90  20  80  61  23  71  15  12  59  19  26 100  11  62  47  18  51  68  83  29  70  57  33  79 103  84  75  38  32  69  66  91 102  14  98  13  82  97   5  95  89   4  16  53  87  48  35  30  63  45  86  40  56   9 101  92  60  72  55  52  81   6  39  44  43  77  28  34   2  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [38 35 19 37 78 20 60 25 28  0 63 54 13 27 15  1 53 66  8  5  3 45 41 56 49 73 29  6 30 77 50 70 46 22 16 67 74 44 62 75  9 64  7 21 26 69 39 48 43 72 57 34 14 18 32 24  2  4 65 79 36 10 42 33 12 17 11 68 47 58 59 51 55 52 40 23 31 61 76 71], a_shuffle_aclus: [ 52  47  26  51 103  27  81  32  35   2  84  71  18  34  20   3  70  87  12   8   5  60  56  75  66  97  38   9  39 102  67  92  61  29  23  89  98  59  83 100  13  85  11  28  33  91  53  63  58  95  77  45  19  25  43  31   4   6  86 104  48  14  57  44  16  24  15  90  62  79  80  68  72  69  55  30  40  82 101  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12  9 37 42 61 11 10 55 57 44 60 26 18 59  4 66 38 17 52 40 35 64 43 69 53 68  8 21 14 54 25 47 33  2 67  3 73 58 51 27 65 13 23 50 16 49  5  7 19 15  6 75 72 70 29 71 20 34 78 74 62 45 56 28 36  1 39 77 31 32 76  0 22 24 63 48 46 41 30 79], a_shuffle_aclus: [ 16  13  51  57  82  15  14  72  77  59  81  33  25  80   6  87  52  24  69  55  47  85  58  91  70  90  12  28  19  71  32  62  44   4  89   5  97  79  68  34  86  18  30  67  23  66   8  11  26  20   9 100  95  92  38  93  27  45 103  98  83  60  75  35  48   3  53 102  40  43 101   2  29  31  84  63  61  56  39 104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 65 64 47 36 37 77 30 62 38 78 42 25 67 24 74 76 54 60 46 34 48 35 31 10  2 58 19 28  5 68 20 39 66 32  1  9  8 63  7 71 75 53 59 73 79 50 56 41 22 17 21 69 72  0 13 45 33 12 29 40 52 44 51 57 23 26 18  4  3 15 49 16  6 11 14 70 43 27 55], a_shuffle_aclus: [ 82  86  85  62  48  51 102  39  83  52 103  57  32  89  31  98 101  71  81  61  45  63  47  40  14   4  79  26  35   8  90  27  53  87  43   3  13  12  84  11  93 100  70  80  97 104  67  75  56  29  24  28  91  95   2  18  60  44  16  38  55  69  59  68  77  30  33  25   6   5  20  66  23   9  15  19  92  58  34  72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 15 49 63  5 11 10 40 41 65  2 71  4 60 72 56 67 20 51 24 36 28 29 43 16  0 44 42 17 77 19 34 55  1 13 68 35 23 66 79 48 73 61 46  8 69 64 12 38 76 37 25 32 45 75 54 26 50 31  9 21 47 62 14 70 18 53 57  6 58 39  3 33 27 78 74 59 22 52 30], a_shuffle_aclus: [ 11  20  66  84   8  15  14  55  56  86   4  93   6  81  95  75  89  27  68  31  48  35  38  58  23   2  59  57  24 102  26  45  72   3  18  90  47  30  87 104  63  97  82  61  12  91  85  16  52 101  51  32  43  60 100  71  33  67  40  13  28  62  83  19  92  25  70  77   9  79  53   5  44  34 103  98  80  29  69  39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 34 47 68 33  0 18 11 27  5  6 56  1 30 65 54 59 66 25 72 75 21  3 78 24 74 29 17 48  2 49 43 13 67 50 79 58 31 39 23 16 60 10 20 52 32 73 14 41 77 61 15  7  8 35 19 28 22 69 45 12 53 70 46 40 26 76 62 42 44 36 55 37  9 63  4 51 38 57 71], a_shuffle_aclus: [ 85  45  62  90  44   2  25  15  34   8   9  75   3  39  86  71  80  87  32  95 100  28   5 103  31  98  38  24  63   4  66  58  18  89  67 104  79  40  53  30  23  81  14  27  69  43  97  19  56 102  82  20  11  12  47  26  35  29  91  60  16  70  92  61  55  33 101  83  57  59  48  72  51  13  84   6  68  52  77  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 18 33 49  8 19 43 62 70 34 21 58 36 44 24 27 46 64 69 17 54 68  5 72 47 31 32 41 73 67  2 14 40 23 45 15 20 10 28 42 12  6 30  9 55 51 13 61 35 52 56  4 63 76  0  3 38 60  7 77 78 59 22 11 79 29 48 25 39 66 53 71  1 74 26 50 57 16 37 75], a_shuffle_aclus: [ 86  25  44  66  12  26  58  83  92  45  28  79  48  59  31  34  61  85  91  24  71  90   8  95  62  40  43  56  97  89   4  19  55  30  60  20  27  14  35  57  16   9  39  13  72  68  18  82  47  69  75   6  84 101   2   5  52  81  11 102 103  80  29  15 104  38  63  32  53  87  70  93   3  98  33  67  77  23  51 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [16  6 76 21 69 52 62 18 45  8  1 51 44 70 46 49 37 26 19 40 53 13 41 59 55 60 68 27 61 10 31 56 34 78 64 33 71  5 67 63 72 77 65 22 47  0 42 25  4 75 24 30 79 43 20 11 48 66  7 54 39  2 28  3 58 15 12 23 17 35 73 50  9 32 74 14 29 38 36 57], a_shuffle_aclus: [ 23   9 101  28  91  69  83  25  60  12   3  68  59  92  61  66  51  33  26  55  70  18  56  80  72  81  90  34  82  14  40  75  45 103  85  44  93   8  89  84  95 102  86  29  62   2  57  32   6 100  31  39 104  58  27  15  63  87  11  71  53   4  35   5  79  20  16  30  24  47  97  67  13  43  98  19  38  52  48  77]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 68  6 77 40 54 25 26 70 14 59 49 61 60  1  5  3 24 16 17 56 34 31 66 53  8 20 37 58 39 73 19 22 30 35 44 36 65  0 78 38 18 23 72 47  7 51 12 28 64 62 71 27 41 43 50 52 69 32 15 67 29 48 33 46 55 11 42 10 57 63 13 74 45 21 75  2 76 79  4], a_shuffle_aclus: [ 13  90   9 102  55  71  32  33  92  19  80  66  82  81   3   8   5  31  23  24  75  45  40  87  70  12  27  51  79  53  97  26  29  39  47  59  48  86   2 103  52  25  30  95  62  11  68  16  35  85  83  93  34  56  58  67  69  91  43  20  89  38  63  44  61  72  15  57  14  77  84  18  98  60  28 100   4 101 104   6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 36 17 60 49 52 76 23 35 45 47 34 28  0 22 37 58 48 14 42 64  3  1 29  5  7 55 13 38  9 54 67 73 78 63  6 11 41 19 44 66 31 26 61 70 30 56 59 53 40 33 72 62  4  8 68 43 18 57 51 12  2 75 46 65 27 74 24 39 20 15 79 69 32 25 50 77 16 10 71], a_shuffle_aclus: [ 28  48  24  81  66  69 101  30  47  60  62  45  35   2  29  51  79  63  19  57  85   5   3  38   8  11  72  18  52  13  71  89  97 103  84   9  15  56  26  59  87  40  33  82  92  39  75  80  70  55  44  95  83   6  12  90  58  25  77  68  16   4 100  61  86  34  98  31  53  27  20 104  91  43  32  67 102  23  14  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 64 50 66 14 47 29 49 51  2 60 26 38 79 58 16 27 67  5 54 20 39 17 52 76 40  8 12  6 57 23  1 45 42  7 13 31 70 22 25 74 61 59 48 34 77 10 75  4 71 56 43  9 69 35 36 21 44 37  0 28 41 68 33 63 73 46 32 24 55 78 19 30 11 15 18 65 62 72 53], a_shuffle_aclus: [  5  85  67  87  19  62  38  66  68   4  81  33  52 104  79  23  34  89   8  71  27  53  24  69 101  55  12  16   9  77  30   3  60  57  11  18  40  92  29  32  98  82  80  63  45 102  14 100   6  93  75  58  13  91  47  48  28  59  51   2  35  56  90  44  84  97  61  43  31  72 103  26  39  15  20  25  86  83  95  70]\n",
      "a_shuffle_IDXs: [ 1 70 60 46 27 33 72 22 54 68 47 25 14 42 73 23 50 45  3 57 34 41 56  8 62 58 48 16 20 15 32  9 35 67  2 63 30 53 74 78 10 61 44 52  5 69  0 59 79 38  4 21 17 39 12 49 77 37 18 28 75  6 36 66 40 13 65 51 71 76 64 24 55 31  7 19 29 26 43 11], a_shuffle_aclus: [  3  92  81  61  34  44  95  29  71  90  62  32  19  57  97  30  67  60   5  77  45  56  75  12  83  79  63  23  27  20  43  13  47  89   4  84  39  70  98 103  14  82  59  69   8  91   2  80 104  52   6  28  24  53  16  66 102  51  25  35 100   9  48  87  55  18  86  68  93 101  85  31  72  40  11  26  38  33  58  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [18 10  0 22 50 32  7  6 39 14 31 19 45 12 53 51  5 46 65 79 35 70 78 74 48 56 64 59 25 42 27 34 61 26 16 37 44 15  9 63 40 54 71 55 66 76 28  3 69 13  1 60 20 33 38 23 11  2 57 17 36  4 72 29 77 52 47 75 73 21 43  8 62 49 68 58 24 30 41 67], a_shuffle_aclus: [ 25  14   2  29  67  43  11   9  53  19  40  26  60  16  70  68   8  61  86 104  47  92 103  98  63  75  85  80  32  57  34  45  82  33  23  51  59  20  13  84  55  71  93  72  87 101  35   5  91  18   3  81  27  44  52  30  15   4  77  24  48   6  95  38 102  69  62 100  97  28  58  12  83  66  90  79  31  39  56  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 50 28 39 76 49 13 73 54 25  1 11 36 26 42  8 57 17 72 19 79 32 52 67 71 21  3 68 27 66 20 75 65 34 31 70 47 18 78 33 41 23 74 22 69  6 16 53 40 77 61 24 29 44 37  2 59 15 64 43  4 45 38 48 56 62  0 51 12  5 14  9 63  7 60 55 30 35 10 58], a_shuffle_aclus: [ 61  67  35  53 101  66  18  97  71  32   3  15  48  33  57  12  77  24  95  26 104  43  69  89  93  28   5  90  34  87  27 100  86  45  40  92  62  25 103  44  56  30  98  29  91   9  23  70  55 102  82  31  38  59  51   4  80  20  85  58   6  60  52  63  75  83   2  68  16   8  19  13  84  11  81  72  39  47  14  79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51 16  8  5 69 75  7 21 18 65 23 26 67 73 63 54 17 44 68 33 72 10 13 45 31 27 41 60 46  2  9 35  0 74 62 39  3 76 43 66 28  4 19 25 12 38 15 40 58 49 30 52 20 55 50 24 48 78 70 14 59 34 22 79 71 36 64 53 32  6 11 61 29 56 42 77 37  1 57 47], a_shuffle_aclus: [ 68  23  12   8  91 100  11  28  25  86  30  33  89  97  84  71  24  59  90  44  95  14  18  60  40  34  56  81  61   4  13  47   2  98  83  53   5 101  58  87  35   6  26  32  16  52  20  55  79  66  39  69  27  72  67  31  63 103  92  19  80  45  29 104  93  48  85  70  43   9  15  82  38  75  57 102  51   3  77  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 43 24 14 63 41  1 52 12 64 57 21 25  9 10 26 78 40  3  6  7 20  0 73 61 72 53 32 13 62 39 48 18 74 34  8 33 38 31 67 70 59 46 22 54 42  5 19 47 45 17 49 56 75 37 65 55 77 15 11 66 76 44 50  2 68 27 30 58 69 29 71 60 79 35 23 16 28 51 36], a_shuffle_aclus: [  6  58  31  19  84  56   3  69  16  85  77  28  32  13  14  33 103  55   5   9  11  27   2  97  82  95  70  43  18  83  53  63  25  98  45  12  44  52  40  89  92  80  61  29  71  57   8  26  62  60  24  66  75 100  51  86  72 102  20  15  87 101  59  67   4  90  34  39  79  91  38  93  81 104  47  30  23  35  68  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 40 20 62 75 55 56 48 11 33 17 45 64 12 13 51  4 38  7 37 60 73 72 31 35 44 27 69 76 63 59 18 65  9 42 79 21 10 34 39 74 58  2  0 32 67 52 15 50 28 24 71 57 29 61 30  1 23 78 41 68  3 49  8 46 26 22 25 19 54 66 47 14 70 77 53 36 16  5 43], a_shuffle_aclus: [  9  55  27  83 100  72  75  63  15  44  24  60  85  16  18  68   6  52  11  51  81  97  95  40  47  59  34  91 101  84  80  25  86  13  57 104  28  14  45  53  98  79   4   2  43  89  69  20  67  35  31  93  77  38  82  39   3  30 103  56  90   5  66  12  61  33  29  32  26  71  87  62  19  92 102  70  48  23   8  58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0 40 23 37 11 35 15 12 65 52  8 77 42 48 76 39 61 10 69 34 53 74 67 17 22 45  7 49  2 19 13 30 70 72 60 79 14 38 16 59 68 75 54 66 62 20 33 44 26 46  5 73  4 43  9 29 31 24 58 25 47 57 78 50 63 41 21 28 51 71  6 18  3 32  1 64 27 56 36 55], a_shuffle_aclus: [  2  55  30  51  15  47  20  16  86  69  12 102  57  63 101  53  82  14  91  45  70  98  89  24  29  60  11  66   4  26  18  39  92  95  81 104  19  52  23  80  90 100  71  87  83  27  44  59  33  61   8  97   6  58  13  38  40  31  79  32  62  77 103  67  84  56  28  35  68  93   9  25   5  43   3  85  34  75  48  72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 72 67 35 28 45 62 19 58 61  4 64 75 71 68 31  0 41 21 39 57 17 69 50 32 24 43 51 22 66 33 44 29 11 27  7 38 76 60 70 16 74 18 73 26  2 53 13 78 48  6 63 14  8 56 10 65 20 49 77 52 36  3 47  1  5 54 23 25 40 42 30 46  9 37 59 34 55 15 12], a_shuffle_aclus: [104  95  89  47  35  60  83  26  79  82   6  85 100  93  90  40   2  56  28  53  77  24  91  67  43  31  58  68  29  87  44  59  38  15  34  11  52 101  81  92  23  98  25  97  33   4  70  18 103  63   9  84  19  12  75  14  86  27  66 102  69  48   5  62   3   8  71  30  32  55  57  39  61  13  51  80  45  72  20  16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [23 43 35 61  6 28 72 77 60 42 32 74 38  0 18 50  3 49 40 25 46 31  5 26 65 79 58 20 37 73 10 39 66 11 51 69 30  1 54 70 59 34 12 55 22 71 48 27 29 24  9 14 19 56 45 78 21 16 68 44  2 15 64 57 13 53 76 67  4 33 62 47  7 52 75 41 63 36 17  8], a_shuffle_aclus: [ 30  58  47  82   9  35  95 102  81  57  43  98  52   2  25  67   5  66  55  32  61  40   8  33  86 104  79  27  51  97  14  53  87  15  68  91  39   3  71  92  80  45  16  72  29  93  63  34  38  31  13  19  26  75  60 103  28  23  90  59   4  20  85  77  18  70 101  89   6  44  83  62  11  69 100  56  84  48  24  12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33  3 70 48 44 37 35 77 53 52 76 64 45 56 28 17 46 24 60 68  4 15 36 61 47  6 54 41 34 78 55 25 72 22 26  0  7 39 42 31 19 13 43 10 79  9 63 58 75 73 51 11 40 14 27 69 71 32 29 57  5 65 38 20  2 62 12 30 67  1 74  8 50 59 66 16 21 49 23 18], a_shuffle_aclus: [ 44   5  92  63  59  51  47 102  70  69 101  85  60  75  35  24  61  31  81  90   6  20  48  82  62   9  71  56  45 103  72  32  95  29  33   2  11  53  57  40  26  18  58  14 104  13  84  79 100  97  68  15  55  19  34  91  93  43  38  77   8  86  52  27   4  83  16  39  89   3  98  12  67  80  87  23  28  66  30  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45 24 18 40 47 74 23 15 71 25 59 55 77 57 61 76 66 14 48 42 70 38 17 11 56  8 32 49  6 21 62 13 19  3 44 58 69 67 27  0 10 16  4 31 72 28 33 53 65 43 51 75 68 39 20 63 50 60 34 73 22 26 64 41 36  9 12  5 30 35 37 46 78  7 54  2 79 52  1 29], a_shuffle_aclus: [ 60  31  25  55  62  98  30  20  93  32  80  72 102  77  82 101  87  19  63  57  92  52  24  15  75  12  43  66   9  28  83  18  26   5  59  79  91  89  34   2  14  23   6  40  95  35  44  70  86  58  68 100  90  53  27  84  67  81  45  97  29  33  85  56  48  13  16   8  39  47  51  61 103  11  71   4 104  69   3  38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 75 31 43 53 77 56 13 19 32 68 49  2 44 18 28  9  3 40 34 47 72 30 66 22 27 51 26 15 16 60  7 20  0 25 23 54 79 10 37 39 76 48 11 46 33 45 67  5 78 73 58 69 59 14 41 64 61 50 70 52 38 29  8  4 63 17 55 74 65 57  1 71  6 62 12 42 24 35 36], a_shuffle_aclus: [ 28 100  40  58  70 102  75  18  26  43  90  66   4  59  25  35  13   5  55  45  62  95  39  87  29  34  68  33  20  23  81  11  27   2  32  30  71 104  14  51  53 101  63  15  61  44  60  89   8 103  97  79  91  80  19  56  85  82  67  92  69  52  38  12   6  84  24  72  98  86  77   3  93   9  83  16  57  31  47  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76 75 53  6 22 33 48 54 57 73 72 11 26 43 23 21  4 18 62  8 34 67 39 56 30  9  2 37 38 71 63  0 35  3 40 17 27 41 16 74 52 28 47 77 60 50 69 65 45 36 24 61 12 59 70 13 58 20 15 79 29  5 32 19 42 66 51 25 55  1 78 10  7 14 49 31 64 46 68 44], a_shuffle_aclus: [101 100  70   9  29  44  63  71  77  97  95  15  33  58  30  28   6  25  83  12  45  89  53  75  39  13   4  51  52  93  84   2  47   5  55  24  34  56  23  98  69  35  62 102  81  67  91  86  60  48  31  82  16  80  92  18  79  27  20 104  38   8  43  26  57  87  68  32  72   3 103  14  11  19  66  40  85  61  90  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [59 70 21  0 50 12 17 52 34 30 45 42 79  4 75 26 15 51 46 54 56 25 60  3 10  1 47 78 29 57 22 76  8 65 68 39 24 33  6 62  9 16 72 66 23 28 35 31 69 13 38  5  2 58 11 44 64 49 19 40 20 37 77 48 63 53 61 18 41 55 74 36 71 27 43 73 14 32  7 67], a_shuffle_aclus: [ 80  92  28   2  67  16  24  69  45  39  60  57 104   6 100  33  20  68  61  71  75  32  81   5  14   3  62 103  38  77  29 101  12  86  90  53  31  44   9  83  13  23  95  87  30  35  47  40  91  18  52   8   4  79  15  59  85  66  26  55  27  51 102  63  84  70  82  25  56  72  98  48  93  34  58  97  19  43  11  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [54 50  9 21 78 34 69 38 67 51 26  5 48  2 75 20 12 29 31 76 59 28 65 22 13 37 44 64 68 32 47 61 66 30 40 73 77  0 24 27  3 79 19  4 74 41 58 46 18 42 70 25 56  1 55 45 71 15 10 35 60 72 62 14 11 43  8 36 57 63 16 39 52 49  6 33  7 17 23 53], a_shuffle_aclus: [ 71  67  13  28 103  45  91  52  89  68  33   8  63   4 100  27  16  38  40 101  80  35  86  29  18  51  59  85  90  43  62  82  87  39  55  97 102   2  31  34   5 104  26   6  98  56  79  61  25  57  92  32  75   3  72  60  93  20  14  47  81  95  83  19  15  58  12  48  77  84  23  53  69  66   9  44  11  24  30  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 26 29 13 74 32  3  9 52  7 12 10 76 19 16 39 36 45  0 49 65 67 35 54 78 33  4 31 68 42  5 50 57 47  8 27 59  6 34 44 43 28 20 66 61 30  1 25 70 55 18 21 41 51 17 69 77 63 11 71 58 38 60 73 24  2 15 62 22 64 46 72 40 48 37 79 14 53 56 23], a_shuffle_aclus: [100  33  38  18  98  43   5  13  69  11  16  14 101  26  23  53  48  60   2  66  86  89  47  71 103  44   6  40  90  57   8  67  77  62  12  34  80   9  45  59  58  35  27  87  82  39   3  32  92  72  25  28  56  68  24  91 102  84  15  93  79  52  81  97  31   4  20  83  29  85  61  95  55  63  51 104  19  70  75  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 12  1 65  5  3 30 54 26 66 15 69 31 64 70 42 21 35 62  0 68 40 22 53 60 11  2 17 44 38 57 50 76  4 45 51 75 47 13 29 58 67 36  9 39 14 41 49 10 79 24 27  7 37 55 34 78 74 73  6 52 33 32 77 16 71 20 48 25 23 18 61 63  8 56 72 43 46 59 19], a_shuffle_aclus: [ 35  16   3  86   8   5  39  71  33  87  20  91  40  85  92  57  28  47  83   2  90  55  29  70  81  15   4  24  59  52  77  67 101   6  60  68 100  62  18  38  79  89  48  13  53  19  56  66  14 104  31  34  11  51  72  45 103  98  97   9  69  44  43 102  23  93  27  63  32  30  25  82  84  12  75  95  58  61  80  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60  3 47 45 36 73 34 23 41 65 37  5 46 50 14  9 15 27 43 31 26 10 77 54 25  0 32 21 71 11 53 63 24 19 39  4 74  7 33 28 13 78 18 51 68 52  2 55 69 48 66  6 58 22 20 40 61 35 17 56 64 29 38  8 70 42 44 75 12  1 67 72 30 49 76 57 62 59 79 16], a_shuffle_aclus: [ 81   5  62  60  48  97  45  30  56  86  51   8  61  67  19  13  20  34  58  40  33  14 102  71  32   2  43  28  93  15  70  84  31  26  53   6  98  11  44  35  18 103  25  68  90  69   4  72  91  63  87   9  79  29  27  55  82  47  24  75  85  38  52  12  92  57  59 100  16   3  89  95  39  66 101  77  83  80 104  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [42 63 11 72 61 39 20 47 33 44  4 14 13 55 73 24  5 35 31  0 49 71 59  3 57 68 52 28 41 65 77 22 12  6 62 50 15 23  8 74 16 75 32 30 21 37 18 36 67 25 27 48  7 46 40 56 26 69 10  2 38 29 60  1 64 43 51 34 70 78  9 19 17 45 54 76 79 53 66 58], a_shuffle_aclus: [ 57  84  15  95  82  53  27  62  44  59   6  19  18  72  97  31   8  47  40   2  66  93  80   5  77  90  69  35  56  86 102  29  16   9  83  67  20  30  12  98  23 100  43  39  28  51  25  48  89  32  34  63  11  61  55  75  33  91  14   4  52  38  81   3  85  58  68  45  92 103  13  26  24  60  71 101 104  70  87  79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [23 18 56 38 66 12 62  7 35 77 74 63 79 53 34 78  8 69  4 42 22 40 54 31 70 15  2  3  0  5 44 52 16 10 29 45 73 67 46 48 58 11 64 30 71 13 32 24 55  6 47 68 36 17  1 19 27 50 51 75 59 76 72 60  9 65 25 28 21 33 39 37 20 61 43 49 57 26 41 14], a_shuffle_aclus: [ 30  25  75  52  87  16  83  11  47 102  98  84 104  70  45 103  12  91   6  57  29  55  71  40  92  20   4   5   2   8  59  69  23  14  38  60  97  89  61  63  79  15  85  39  93  18  43  31  72   9  62  90  48  24   3  26  34  67  68 100  80 101  95  81  13  86  32  35  28  44  53  51  27  82  58  66  77  33  56  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [44 33 17 38 19 42 41 57 50 11 65 55 10 79 43 75 36 63 28 20 25  9 14 61 49 68 37 15 32 30 34  6  2  7 53 78 52 12 54  8  5 29 51 23  1 45 77 40 46 47 24 66 13 18  4 16  3  0 56 48 60 26 70 72 73 22 76 59 58 71 27 64 69 39 31 62 21 35 74 67], a_shuffle_aclus: [ 59  44  24  52  26  57  56  77  67  15  86  72  14 104  58 100  48  84  35  27  32  13  19  82  66  90  51  20  43  39  45   9   4  11  70 103  69  16  71  12   8  38  68  30   3  60 102  55  61  62  31  87  18  25   6  23   5   2  75  63  81  33  92  95  97  29 101  80  79  93  34  85  91  53  40  83  28  47  98  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [62 49 31 30  1 60 70 34 42 53 28 12 43 72 67 33 66 65  0 46 73 64 27 32 41 36 29 76 40 71 77 54 23 45  2 59 57 55 61 10 25 11  6 14 52 68  4 51 18 74 22 47  8 35 44 58 69 56 15 13  9 79 38 37 21 19 50 20 24 17 39  5 26 16 78 63 48  3  7 75], a_shuffle_aclus: [ 83  66  40  39   3  81  92  45  57  70  35  16  58  95  89  44  87  86   2  61  97  85  34  43  56  48  38 101  55  93 102  71  30  60   4  80  77  72  82  14  32  15   9  19  69  90   6  68  25  98  29  62  12  47  59  79  91  75  20  18  13 104  52  51  28  26  67  27  31  24  53   8  33  23 103  84  63   5  11 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 14 41 77 75 45 39 55 28 10 62 68 73 63 52 72 42 25 66 29 64  7 71  6 78  2 50  8 67  3 26 69  0 27 18 24 60 57  4 59 30 11 12 19 20 34 36 40 38 33 22 46 43 56 16 13 53 51  1  5 70 44 31 79 35 32 49 76 37 23 54  9 17 21 65 58 47 48 15 61], a_shuffle_aclus: [ 98  19  56 102 100  60  53  72  35  14  83  90  97  84  69  95  57  32  87  38  85  11  93   9 103   4  67  12  89   5  33  91   2  34  25  31  81  77   6  80  39  15  16  26  27  45  48  55  52  44  29  61  58  75  23  18  70  68   3   8  92  59  40 104  47  43  66 101  51  30  71  13  24  28  86  79  62  63  20  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 52 69 75 19 36 33 34 15 32 77 61 53 45 26 58 73 35 29  4 46 30 72 25 60 43 49 31 14 37 12 63 20 55 42 27 23  2 64 18 68 28 40 79 66 38 44 76 70 56 54 59  0 22 39 51 57 74 10  1  9 16 78  8 65 17 21  7  5 48 47 62  6 41 24 67  3 71 50 13], a_shuffle_aclus: [ 15  69  91 100  26  48  44  45  20  43 102  82  70  60  33  79  97  47  38   6  61  39  95  32  81  58  66  40  19  51  16  84  27  72  57  34  30   4  85  25  90  35  55 104  87  52  59 101  92  75  71  80   2  29  53  68  77  98  14   3  13  23 103  12  86  24  28  11   8  63  62  83   9  56  31  89   5  93  67  18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 28 44 53 48 77 10 16 69 59 74 62 18 37 46 55 50 30 43 57 25 51  9 23 63 27 71 38 58  3 76 45 70 32 61 65 54 49 35 68  4 29 40 19 79  2 67 75  8 20 13 24 42 12 33 64 17  6 26 56 31  5 78 21 39 22 72 52 11  1 73 36 66 15 41  7 14 60  0 34], a_shuffle_aclus: [ 62  35  59  70  63 102  14  23  91  80  98  83  25  51  61  72  67  39  58  77  32  68  13  30  84  34  93  52  79   5 101  60  92  43  82  86  71  66  47  90   6  38  55  26 104   4  89 100  12  27  18  31  57  16  44  85  24   9  33  75  40   8 103  28  53  29  95  69  15   3  97  48  87  20  56  11  19  81   2  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 60 35 45 44 31 13 53 73  2 48 34 69 71 65 43 51 66 18 57 19 36 56 50 63 46  5 41  0  1 79 27 33 52 28 32 15 40 64 23 47  8 39 42 38  3 20 14 70  7 26 37 10 62 77 49 17 75 68 55  4  9 78 12 58 22 29 25 16 61 54 30  6 74 24 76 11 59 72 67], a_shuffle_aclus: [ 28  81  47  60  59  40  18  70  97   4  63  45  91  93  86  58  68  87  25  77  26  48  75  67  84  61   8  56   2   3 104  34  44  69  35  43  20  55  85  30  62  12  53  57  52   5  27  19  92  11  33  51  14  83 102  66  24 100  90  72   6  13 103  16  79  29  38  32  23  82  71  39   9  98  31 101  15  80  95  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [19 52 17 41  7 45 78 25 13  3 24 32  5 46 12  8 54 60 55 53 43  6 62 65 56 37  1 36 35 74 71 38 72 18 29 16 63 44  0 39 22 42 14 57 48 50 77 79 68  4 40 66 11 58 76 51 59 31 21 33 30 23 70 28 69 49 67  2 20 27 15 34 75 10 26 61 47  9 64 73], a_shuffle_aclus: [ 26  69  24  56  11  60 103  32  18   5  31  43   8  61  16  12  71  81  72  70  58   9  83  86  75  51   3  48  47  98  93  52  95  25  38  23  84  59   2  53  29  57  19  77  63  67 102 104  90   6  55  87  15  79 101  68  80  40  28  44  39  30  92  35  91  66  89   4  27  34  20  45 100  14  33  82  62  13  85  97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 15 42 58 11 65 43 23 56 12 78 47 48 61 14  6 27 29 55 66  0 63 54 52 30  9 18 19  3 36  4 37 25 28 39 51 60 32 59 76  2 34 73 35 44 45 40 16 46 41 20 38 13 69 79 64 74 72  1 57 50 70 26 67 17  7 49 77 71 53 31 21 10  5 68 75 22 33 62 24], a_shuffle_aclus: [ 12  20  57  79  15  86  58  30  75  16 103  62  63  82  19   9  34  38  72  87   2  84  71  69  39  13  25  26   5  48   6  51  32  35  53  68  81  43  80 101   4  45  97  47  59  60  55  23  61  56  27  52  18  91 104  85  98  95   3  77  67  92  33  89  24  11  66 102  93  70  40  28  14   8  90 100  29  44  83  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [71 37 50 26  4 57 19 74 68 22 47 52 54 63 28 62 27 16 43 56 53 51 76 39 32 49 40 35 10 58 23 38  0 79 33 42  2 24 61 65 29 70 31 34 15 20 14 11 41 17 59 73 18 69 13 45 67 60 77 44 25 66  8 46 48 78 55  1 21 75 12  3 36  6 72  7 64  5  9 30], a_shuffle_aclus: [ 93  51  67  33   6  77  26  98  90  29  62  69  71  84  35  83  34  23  58  75  70  68 101  53  43  66  55  47  14  79  30  52   2 104  44  57   4  31  82  86  38  92  40  45  20  27  19  15  56  24  80  97  25  91  18  60  89  81 102  59  32  87  12  61  63 103  72   3  28 100  16   5  48   9  95  11  85   8  13  39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [15 63 26 65 40  0  4 48 51 30 24 54 32 75 16 21 68 53 14 77 62 31  5  8 72 13 43 45 46 47 18 70 41 17 25 57  2 44 52 22 55 37 69 58 74 34 28 38  6 10 19 71  3 78 20 60 42 59 66 73 76 64 23 33 50 29 12 11  9 27 61 35  1 79 67 36 49 56  7 39], a_shuffle_aclus: [ 20  84  33  86  55   2   6  63  68  39  31  71  43 100  23  28  90  70  19 102  83  40   8  12  95  18  58  60  61  62  25  92  56  24  32  77   4  59  69  29  72  51  91  79  98  45  35  52   9  14  26  93   5 103  27  81  57  80  87  97 101  85  30  44  67  38  16  15  13  34  82  47   3 104  89  48  66  75  11  53]\n",
      "a_shuffle_IDXs: [27 41 59 21 11 69  9 14 17 67 36  4 56 28  7 66 71 34 51 49 30 12 52 76 44 62 33 37 43 19 20 63 32 57 18 54 74 10  2  0 64 46 58 53 40 24 13 26 31 42 16 70  5 55 45 61 60 73 23  6 29 25 15 39  8 65 35 77 72  1 68 22 47  3 50 78 75 38 48 79], a_shuffle_aclus: [ 34  56  80  28  15  91  13  19  24  89  48   6  75  35  11  87  93  45  68  66  39  16  69 101  59  83  44  51  58  26  27  84  43  77  25  71  98  14   4   2  85  61  79  70  55  31  18  33  40  57  23  92   8  72  60  82  81  97  30   9  38  32  20  53  12  86  47 102  95   3  90  29  62   5  67 103 100  52  63 104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_completed_shuffles: 1200\n"
     ]
    }
   ],
   "source": [
    "wcorr_ripple_shuffle.compute_shuffles(num_shuffles=200, curr_active_pipeline=curr_active_pipeline)\n",
    "n_completed_shuffles: int = wcorr_ripple_shuffle.n_completed_shuffles\n",
    "print(f'n_completed_shuffles: {n_completed_shuffles}')\n",
    "\n",
    "# 7m - 200 shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3961cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shuffles_wcorr_array = deepcopy(wcorr_ripple_shuffle.all_shuffles_wcorr_array)\n",
    "all_shuffles_wcorr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_ripple_shuffle.n_completed_shuffles\n",
    "(_out_p, _out_p_dict), (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT), (total_n_shuffles_more_extreme_than_real_df, total_n_shuffles_more_extreme_than_real_dict), all_shuffles_wcorr_array = wcorr_ripple_shuffle.post_compute()\n",
    "## OUTPUTS: all_shuffles_wcorr_array, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de422514",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply Heuristic scores to the shuffles too:\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring, HeuristicScoresTuple\n",
    "\n",
    "# ## Single Shuffle:\n",
    "# a_decoded_filter_epochs_decoder_result_dict = wcorr_ripple_shuffle.output_all_shuffles_decoded_results_list[0] ## get first result\n",
    "# a_decoded_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=deepcopy(wcorr_ripple_shuffle.track_templates), a_decoded_filter_epochs_decoder_result_dict=a_decoded_filter_epochs_decoder_result_dict) # 4.0 s\n",
    "\n",
    "## REAL:\n",
    "active_spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "real_directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = deepcopy(curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders'])\n",
    "# real_output_alt_directional_merged_decoders_result, (real_decoder_laps_filter_epochs_decoder_result_dict, real_decoder_ripple_filter_epochs_decoder_result_dict) = _try_all_templates_decode(spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df), a_directional_merged_decoders_result=real_directional_merged_decoders_result, shuffled_decoders_dict=real_directional_merged_decoders_result.all_directional_decoder_dict, **a_sweep_dict)\n",
    "real_output_alt_directional_merged_decoders_result, (real_decoder_laps_filter_epochs_decoder_result_dict, real_decoder_ripple_filter_epochs_decoder_result_dict) = WCorrShuffle._try_all_templates_decode(spikes_df=active_spikes_df, a_directional_merged_decoders_result=real_directional_merged_decoders_result,\n",
    "                                                                                                                                                                                                           shuffled_decoders_dict=deepcopy(wcorr_ripple_shuffle.track_templates).get_decoders_dict(), \n",
    "                                                                                                                                                                                                            skip_merged_decoding=True, **deepcopy(wcorr_ripple_shuffle.all_templates_decode_kwargs))\n",
    "# real_decoder_ripple_filter_epochs_decoder_result_dict\n",
    "## OUTPUTS: real_decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "real_decoder_ripple_filter_epochs_decoder_result_dict, _real_new_heuristic_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=deepcopy(wcorr_ripple_shuffle.track_templates), a_decoded_filter_epochs_decoder_result_dict=real_decoder_ripple_filter_epochs_decoder_result_dict) # 4.0 s\n",
    "real_decoder_ripple_filter_epochs_decoder_result_dict\n",
    "## OUTPUTS: real_decoder_ripple_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_real_new_heuristic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c56f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at a single decoder:\n",
    "decoder_name: types.DecoderName = 'long_LR'\n",
    "a_decoder_real_heuristic_scores: pd.DataFrame = _real_new_heuristic_scores[decoder_name]\n",
    "a_decoder_real_heuristic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c076d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_score_name: str = 'longest_sequence_length'\n",
    "a_score_name: str = 'integral_second_derivative'\n",
    "\n",
    "a_real_scores = a_decoder_real_heuristic_scores[a_score_name].to_numpy()\n",
    "\n",
    "_shuffle_scores_stack = []\n",
    "\n",
    "for a_decoded_filter_epochs_decoder_result_dict in wcorr_ripple_shuffle.output_all_shuffles_decoded_results_list:\n",
    "    ## get first result\n",
    "    a_shuffle_scores = a_decoded_filter_epochs_decoder_result_dict[decoder_name].filter_epochs[a_score_name].to_numpy() # decoded_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=deepcopy(wcorr_ripple_shuffle.track_templates), a_decoded_filter_epochs_decoder_result_dict=a_decoded_filter_epochs_decoder_result_dict) # 4.0 s\n",
    "    _shuffle_scores_stack.append(a_shuffle_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffles:\n",
    "_heuristic_stack = []\n",
    "for a_decoded_filter_epochs_decoder_result_dict in wcorr_ripple_shuffle.output_all_shuffles_decoded_results_list:\n",
    "    ## get first result\n",
    "    a_decoded_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=deepcopy(wcorr_ripple_shuffle.track_templates), a_decoded_filter_epochs_decoder_result_dict=a_decoded_filter_epochs_decoder_result_dict) # 4.0 s\n",
    "    _heuristic_stack.append(_out_new_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d161f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_filter_epochs_decoder_result_dict\n",
    "a_decoded_filter_epochs_decoder_result_dict[a_decoder_name]\n",
    "p_x_given_n_list\n",
    "spkcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09e9f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to \"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-05-30_0755PM_standalone_wcorr_ripple_shuffle_data_only_1200.pkl\"...\n",
      "total_n_shuffles: 1200\n",
      "Saving (file mode 'w+b') session pickle file results :... saved session pickle file\n"
     ]
    }
   ],
   "source": [
    "# standalone save\n",
    "standalone_filename: str = f'{get_now_rounded_time_str()}_standalone_wcorr_ripple_shuffle_data_only_{wcorr_ripple_shuffle.n_completed_shuffles}.pkl' \n",
    "standalone_filepath = curr_active_pipeline.get_output_path().joinpath(standalone_filename).resolve() # Path(\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\output\\2024-05-30_0925AM_standalone_wcorr_ripple_shuffle_data_only_1100.pkl\")\n",
    "print(f'saving to \"{standalone_filepath}\"...')\n",
    "wcorr_ripple_shuffle.save_data(standalone_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441a089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5966cf23",
   "metadata": {},
   "source": [
    "### Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e823ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "\n",
    "clicked_epoch = np.array([[149.95935746072792, 150.25439218967222]]).T\n",
    "clicked_epoch_start_times = clicked_epoch[:, 0]\n",
    "# clicked_epoch_start_times\n",
    "\n",
    "selected_epoch_indicies = find_data_indicies_from_epoch_times(wcorr_ripple_shuffle.filtered_epochs_df, epoch_times=np.squeeze(clicked_epoch_start_times), t_column_names=['start',], atol=0.001, not_found_action='skip_index', debug_print=False)\n",
    "selected_epoch_indicies\n",
    "\n",
    "assert len(selected_epoch_indicies) > 0\n",
    "\n",
    "selected_epoch_index: int = selected_epoch_indicies[0]\n",
    "print(f'selected_epoch_index: {selected_epoch_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting complete results:\n",
    "a_shuffle_outcome: Dict[types.DecoderName, DecodedFilterEpochsResult] = output_all_shuffles_decoded_results_list[-1]\n",
    "a_shuffle_wcorrs = output_extracted_result_wcorrs_list[-1]\n",
    "\n",
    "## start with one decoder:\n",
    "a_decoder_name: types.DecoderName = 'long_LR'\n",
    "a_decoder_idx: int = 0\n",
    "a_shuffle_decoder_result: DecodedFilterEpochsResult = a_shuffle_outcome[a_decoder_name]\n",
    "# a_shuffle_decoder_result.filter_epochs\n",
    "\n",
    "a_shuffle_decoder_wcorr_result: NDArray = np.squeeze(a_shuffle_wcorrs[a_decoder_name].to_numpy()) # (n_epochs, )\n",
    "\n",
    "# a_shufle_wcorrs\n",
    "a_shuffle_decoder_wcorr_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4846e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: a_decoder_name\n",
    "_out_shuffle_wcorr_arr = np.stack([np.squeeze(v[a_decoder_name].to_numpy()) for v in output_extracted_result_wcorrs_list]) # .shape ## (n_shuffles, n_epochs) \n",
    "print(f'_out_shuffle_wcorr_arr.shape: {np.shape(_out_shuffle_wcorr_arr)}') # _out_shuffle_wcorr_arr.shape: (n_shuffles, n_epochs)\n",
    "n_shuffles: int = np.shape(_out_shuffle_wcorr_arr)[0]\n",
    "print(f'n_shuffles: {n_shuffles}')\n",
    "n_epochs: int = np.shape(_out_shuffle_wcorr_arr)[1]\n",
    "print(f'n_epochs: {n_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: `wcorr_ripple_shuffle`\n",
    "wcorr_ripple_shuffle.n_completed_shuffles\n",
    "(_out_p, _out_p_dict), (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT), (total_n_shuffles_more_extreme_than_real_df, total_n_shuffles_more_extreme_than_real_dict), all_shuffles_wcorr_array = wcorr_ripple_shuffle.post_compute(curr_active_pipeline=curr_active_pipeline)\n",
    "## OUTPUTS: all_shuffles_wcorr_array, \n",
    "all_shuffles_wcorr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c91a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all shuffles for a single epoch:\n",
    "## INPUTS: all_shuffles_wcorr_array, a_decoder_idx, selected_epoch_index\n",
    "\n",
    "## start with one decoder:\n",
    "a_decoder_name: types.DecoderName = 'long_LR'\n",
    "a_decoder_idx: int = 0\n",
    "selected_epoch_index: int = 0\n",
    "\n",
    "# _single_epoch_all_shuffles_wcorr_arr = _out_shuffle_wcorr_arr[:, selected_epoch_index]\n",
    "_single_epoch_all_shuffles_wcorr_arr = wcorr_ripple_shuffle.all_shuffles_wcorr_array[:, selected_epoch_index, a_decoder_idx]\n",
    "print(f'np.shape(_single_epoch_all_shuffles_wcorr_arr): {np.shape(_single_epoch_all_shuffles_wcorr_arr)}') # (n_shuffles, )\n",
    "_single_epoch_all_shuffles_wcorr_arr\n",
    "\n",
    "_single_epoch_real_wcorr: float = wcorr_ripple_shuffle.real_decoder_ripple_weighted_corr_arr[selected_epoch_index, a_decoder_idx]\n",
    "_single_epoch_real_wcorr # -0.35003949543741564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2030bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_single_decoder_epoch_z_scored_values: NDArray = wcorr_ripple_shuffle.compute_z_transformed_scores(_single_epoch_all_shuffles_wcorr_arr)\n",
    "a_single_decoder_epoch_z_scored_values\n",
    "a_single_decoder_epoch_z_score: float = wcorr_ripple_shuffle.compute_z_score(_single_epoch_all_shuffles_wcorr_arr, _single_epoch_real_wcorr)\n",
    "print(f'a_single_decoder_epoch_z_score: {a_single_decoder_epoch_z_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import plot_histogram_for_z_scores\n",
    "\n",
    "# List of z-scored values\n",
    "z_scores = a_single_decoder_epoch_z_scored_values\n",
    "plot_histogram_for_z_scores(z_scores)\n",
    "plt.axvline(a_single_decoder_epoch_z_score, color='red', linestyle='--', linewidth=2, label='Actual Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import pho_jointplot, plot_histograms\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "\n",
    "(_out_p, _out_p_dict), (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT), (total_n_shuffles_more_extreme_than_real_df, total_n_shuffles_more_extreme_than_real_dict), all_shuffles_wcorr_array = wcorr_ripple_shuffle.post_compute(curr_active_pipeline=curr_active_pipeline)\n",
    "\n",
    "## INPUTS: _out_wcorr_ZScore_LR_dict\n",
    "\n",
    "\n",
    "# total_n_shuffles_more_extreme_than_real_df.plot.scatter(x=np.arange(wcorr_ripple_shuffle.n_epochs), y\n",
    "x = np.arange(wcorr_ripple_shuffle.n_epochs)\n",
    "epoch_start_t = filtered_epochs_df['start'].to_numpy() # ripple start time\n",
    "\n",
    "# plt.scatter(x, total_n_shuffles_more_extreme_than_real[:,0], label='LongLR')\n",
    "\n",
    "\n",
    "## OUTPUTS: _out_shuffle_wcorr_arr_ZScores_LONG, _out_shuffle_wcorr_arr_ZScores_SHORT\n",
    "# _out_shuffle_wcorr_arr_ZScores_LR_dict = dict(zip(['long', 'short'], (_out_shuffle_wcorr_arr_ZScores_LONG, _out_shuffle_wcorr_arr_ZScores_SHORT)))\n",
    "\n",
    "\n",
    "## OUTPUTS: _out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT\n",
    "_out_wcorr_ZScore_LR_dict = dict(zip(['long', 'short'], (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT)))\n",
    "\n",
    "# pd.DataFrame(_out_shuffle_wcorr_Zscore_val, columns=['LongLR', 'LongRL', 'ShortLR', 'ShortRL'])\n",
    "\n",
    "# _out_wcorr_ZScore_LR_dict = dict(zip(['LongLR', 'LongRL', 'ShortLR', 'ShortRL'], [v for v in _out_shuffle_wcorr_Zscore_val.T]))\n",
    "# _out_wcorr_ZScore_LR_dict\n",
    "\n",
    "figure_identifier: str = f\"wcorr (best dir selected for each event) fig\"\n",
    "fig = plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "# for time_bin_size in time_bin_sizes:\n",
    "#     df_tbs = pre_delta_df[pre_delta_df['time_bin_size']==time_bin_size]\n",
    "#     df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "\n",
    "n_shuffles: int = wcorr_ripple_shuffle.n_completed_shuffles\n",
    "print(f'n_shuffles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f492678",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_dfs = []\n",
    "# for i, (name, v) in enumerate(total_n_shuffles_more_extreme_than_real_dict.items()):\n",
    "# for i, (name, v) in enumerate(_out_p_dict.items()):\n",
    "# for i, (name, v) in enumerate(_out_shuffle_wcorr_arr_ZScores_LR_dict.items()):\n",
    "for i, (name, v) in enumerate(_out_wcorr_ZScore_LR_dict.items()):\n",
    "\n",
    "    # if i == 0:\n",
    "    assert np.shape(epoch_start_t) == np.shape(_out_shuffle_wcorr_ZScore_LONG)\n",
    "    assert np.shape(epoch_start_t) == np.shape(_out_shuffle_wcorr_ZScore_SHORT)\n",
    "    # curr_is_valid_epoch_shuffle_indicies = np.any(valid_shuffle_indicies[:,:,i]) # (n_shuffles, n_epochs)\n",
    "    # curr_t = np.squeeze(epoch_start_t[curr_is_valid_epoch_shuffle_indicies])\n",
    "    curr_t = np.squeeze(epoch_start_t)\n",
    "    print(f'np.shape(curr_t): {np.shape(curr_t)}')\n",
    "    print(f'np.shape(v): {np.shape(v)}')\n",
    "    curr_shuffle_wcorr_arr = wcorr_ripple_shuffle.all_shuffles_wcorr_array[:,:,i] # (n_shuffles, n_epochs)\n",
    "\n",
    "\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([curr_shuffle_wcorr_arr, np.repeat(i, n_shuffles)])) # , columns=[]\n",
    "\n",
    "    n_shuffles, n_epochs = np.shape(curr_shuffle_wcorr_arr)\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(curr_shuffle_wcorr_arr) # , columns=[]\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([np.atleast_2d(np.repeat(i, n_shuffles)).T, curr_shuffle_wcorr_arr]), columns=(['epoch_idx'] + [f'{int(i)}' for i in np.arange(n_epochs)]))\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([np.atleast_2d(np.repeat(i, n_shuffles)).T, np.atleast_2d(np.arange(n_shuffles)).T, curr_shuffle_wcorr_arr]), columns=(['epoch_idx', 'shuffle_idx'] + [f'{int(i)}' for i in np.arange(n_epochs)]))\n",
    "\n",
    "    curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([np.atleast_2d(np.repeat(i, n_shuffles)).T, np.atleast_2d(np.arange(n_shuffles)).T]), columns=(['epoch_idx', 'shuffle_idx']))\n",
    "\n",
    "    curr_shuffle_wcorr_arr\n",
    "    [f'{int(i)}' for i in np.arange(n_epochs)]\n",
    "\n",
    "    curr_shuffle_wcorr_df['epoch_idx'] = curr_shuffle_wcorr_df['epoch_idx'].astype(int)\n",
    "    curr_shuffle_wcorr_df['shuffle_idx'] = curr_shuffle_wcorr_df['shuffle_idx'].astype(int)\n",
    "    curr_shuffle_wcorr_df['track_id'] = name\n",
    "    # curr_shuffle_wcorr_df\n",
    "\n",
    "    _out_dfs.append(curr_shuffle_wcorr_df)\n",
    "    \n",
    "    \n",
    "    # _flat_v = np.nanmean(v, axis=0)\n",
    "    # _flat_v = np.abs(v)\n",
    "    # _flat_v = v\n",
    "    # _temp_df = pd.DataFrame({'epoch_start_t': curr_t, 'p': v})\n",
    "    # _temp_df = pd.DataFrame({'epoch_start_t': curr_t, 'flat_Z_corr': _flat_v})\n",
    "    # plt.scatter(np.arange(np.shape(v)[0]), v, label=name)\n",
    "    # plt.scatter(epoch_start_t, _flat_v, label=name)\n",
    "\n",
    "    # _temp_df.hist(column='flat_Z_corr', alpha=0.5, label=str(name)) \n",
    "\n",
    "    # plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "\n",
    "\n",
    "all_shuffles_wcorr_df: pd.DataFrame = pd.concat(_out_dfs)\n",
    "all_shuffles_wcorr_df\n",
    "\n",
    "# plt.title(f'{figure_identifier}')\n",
    "# plt.xlabel('time')\n",
    "# plt.ylabel('Z-scored Wcorr')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c401f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would I represent the results of 1000 shuffles for each epoch. (n_shuffles, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "mpg = sns.load_dataset(\"mpg\")\n",
    "sns.catplot(\n",
    "    data=all_shuffles_wcorr_df, x=\"cylinders\", y=\"acceleration\", hue=\"weight\",\n",
    "    native_scale=True, zorder=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d395c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_shuffle_wcorr_df.co\n",
    "\n",
    "curr_shuffle_wcorr_df.columns[-1] = 'epoch_idx'\n",
    "# curr_shuffle_wcorr_df.set_axis(labels=['n_shuffles', 'n_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_shuffle_wcorr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_wcorr_ZScore_LR_df: pd.DataFrame = pd.DataFrame({'start': epoch_start_t, **_out_wcorr_ZScore_LR_dict})\n",
    "_out_wcorr_ZScore_LR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aed53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# \"Melt\" the dataset to \"long-form\" or \"tidy\" representation\n",
    "iris = iris.melt(id_vars=\"species\", var_name=\"measurement\")\n",
    "\n",
    "# Initialize the figure\n",
    "f, ax = plt.subplots()\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(\n",
    "    data=iris, x=\"value\", y=\"measurement\", hue=\"species\",\n",
    "    dodge=True, alpha=.25, zorder=1, legend=False,\n",
    ")\n",
    "\n",
    "# Show the conditional means, aligning each pointplot in the\n",
    "# center of the strips by adjusting the width allotted to each\n",
    "# category (.8 by default) by the number of hue levels\n",
    "sns.pointplot(\n",
    "    data=iris, x=\"value\", y=\"measurement\", hue=\"species\",\n",
    "    dodge=.8 - .8 / 3, palette=\"dark\", errorbar=None,\n",
    "    markers=\"d\", markersize=4, linestyle=\"none\",\n",
    ")\n",
    "\n",
    "# Improve the legend\n",
    "sns.move_legend(\n",
    "    ax, loc=\"lower right\", ncol=3, frameon=True, columnspacing=1, handletextpad=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35184d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_shuffle_wcorr_ZScore_LONG\n",
    "_out_shuffle_wcorr_ZScore_SHORT\n",
    "epoch_start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b378dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_shuffle_wcorr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoder_ripple_weighted_corr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_shuffle_wcorr_arr[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.logical_not(a_decoder_ripple_weighted_corr_df.notna()))\n",
    "\n",
    "a_shuffle_is_more_extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c84388",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_ripple_weighted_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49403abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decoder_ripple_weighted_corr_df_dicts = np.hstack([output_extracted_result_tuples[i][-1] for i, v in enumerate(output_extracted_result_tuples)])\n",
    "output_decoder_ripple_weighted_corr_df_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f731403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import compute_weighted_correlations\n",
    "\n",
    "\n",
    "## Weighted Correlation\n",
    "decoder_laps_weighted_corr_df_dict = compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(decoder_laps_filter_epochs_decoder_result_dict))\n",
    "decoder_ripple_weighted_corr_df_dict = compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(decoder_ripple_filter_epochs_decoder_result_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict[types.DecoderName, DecodedFilterEpochsResult]\n",
    "\n",
    "decoder_ripple_filter_epochs_decoder_result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27eb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _subfn_compute_complete_df_metrics(directional_merged_decoders_result, track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                                            decoder_laps_df_dict=deepcopy(decoder_laps_weighted_corr_df_dict), decoder_ripple_df_dict=deepcopy(decoder_ripple_weighted_corr_df_dict), active_df_columns = ['wcorr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04485453",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12500b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a82089d",
   "metadata": {},
   "source": [
    "#  2024-05-29 - Trial-by-Trial Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "# # spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "# rank_order_results = global_computation_results.computed_data['RankOrder'] # : \"RankOrderComputationsContainer\"\n",
    "# minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "# # included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "# directional_laps_results: DirectionalLapsResult = global_computation_results.computed_data['DirectionalLaps']\n",
    "# track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# # long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# # Unpack all directional variables:\n",
    "# ## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "# long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# # Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "# long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "\n",
    "## INPUTS: curr_active_pipeline, track_templates, global_epoch_name, (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)\n",
    "any_decoder_neuron_IDs: NDArray = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "# long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "# ## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "    # if `KeyError: 'pf1D_dt'` recompute\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "# active_pf_2D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = active_pf_1D_dt\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "## OUTPUTS: directional_active_lap_pf_results_dicts\n",
    "a_trial_by_trial_result: TrialByTrialActivityResult = TrialByTrialActivityResult(any_decoder_neuron_IDs=any_decoder_neuron_IDs,\n",
    "                                                                                active_pf_dt=active_pf_dt,\n",
    "                                                                                directional_lap_epochs_dict=directional_lap_epochs_dict,\n",
    "                                                                                directional_active_lap_pf_results_dicts=directional_active_lap_pf_results_dicts,\n",
    "                                                                                is_global=True)  # type: Tuple[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, BasePositionDecoder], Any]\n",
    "\n",
    "a_trial_by_trial_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71633da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_trial_by_trial_result.directional_active_lap_pf_results_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62264383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_trial_by_trial_result.active_pf_dt.plot_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTbyT:TrialByTrialActivity = a_trial_by_trial_result.directional_active_lap_pf_results_dicts['long_LR']\n",
    "aTbyT.C_trial_by_trial_correlation_matrix.shape # (40, 21, 21)\n",
    "aTbyT.z_scored_tuning_map_matrix.shape # (21, 40, 57) (n_epochs, n_neurons, n_pos_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTbyT.neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_xbins = len(active_pf_dt.xbin_centers)\n",
    "n_xbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTbyT.z_scored_tuning_map_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b29c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_trial_by_trial_result.active_pf_dt.historical_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308530d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pf2D_dt.historical_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81edc831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "saveData('test_a_trial_by_trial_result_data.pkl', a_trial_by_trial_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_a_trial_by_trial_result = loadData('test_a_trial_by_trial_result_data.pkl')\n",
    "loaded_a_trial_by_trial_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e425b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b1025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa979505",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_a_trial_by_trial_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523071",
   "metadata": {},
   "source": [
    "# 2024-05-30 - Continuous decoded posterior output videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "spikes_df = directional_decoders_decode_result.spikes_df\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "pseudo2D_decoder_continuously_decoded_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('long_LR', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.media_output_helpers import save_array_as_video\n",
    "\n",
    "\n",
    "def save_posterior_to_video(a_decoder_continuously_decoded_result: DecodedFilterEpochsResult, result_name: str='a_decoder_continuously_decoded_result'):\n",
    "    a_p_x_given_n = deepcopy(a_decoder_continuously_decoded_result.p_x_given_n_list[0]) # (57, 4, 83755) (n_x_bins, n_decoders, n_time_bins)\n",
    "    if np.ndim(a_p_x_given_n) > 2:\n",
    "        n_x_bins, n_decoders, n_time_bins = np.shape(a_p_x_given_n)\n",
    "        transpose_axes_tuple = (2, 1, 0,)\n",
    "    else:\n",
    "        assert np.ndim(a_p_x_given_n) == 2, f\"np.ndim(a_p_x_given_n): {np.ndim(a_p_x_given_n)}\"\n",
    "        n_x_bins, n_time_bins = np.shape(a_p_x_given_n)\n",
    "        a_p_x_given_n = a_p_x_given_n[:, np.newaxis, :]\n",
    "        assert np.ndim(a_p_x_given_n) == 3, f\"np.ndim(a_p_x_given_n): {np.ndim(a_p_x_given_n)}\"\n",
    "        # transpose_axes_tuple = (1, 0,)\n",
    "        transpose_axes_tuple = (2, 1, 0,)\n",
    "    \n",
    "        a_p_x_given_n = np.tile(a_p_x_given_n, (1, 8, 1,))\n",
    "        # display(a_p_x_given_n)\n",
    "    # time_window_centers = deepcopy(a_decoder_continuously_decoded_result.time_window_centers[0])\n",
    "\n",
    "    ## get tiny portion just to test\n",
    "    # a_p_x_given_n = a_p_x_given_n[:, :, :2000]\n",
    "    # a_p_x_given_n\n",
    "\n",
    "    # a_p_x_given_n = np.reshape(a_p_x_given_n, (n_time_bins, n_decoders, n_x_bins))\n",
    "    a_p_x_given_n = np.transpose(a_p_x_given_n, transpose_axes_tuple)\n",
    "    # display(a_p_x_given_n)\n",
    "\n",
    "    decoding_realtime_FPS: float = 1.0 / float(a_decoder_continuously_decoded_result.decoding_time_bin_size)\n",
    "    print(f'decoding_realtime_FPS: {decoding_realtime_FPS}')\n",
    "    ## save video\n",
    "    video_out_path = save_array_as_video(array=a_p_x_given_n, video_filename=f'output/videos/{result_name}.avi', isColor=False, fps=decoding_realtime_FPS)\n",
    "    print(f'video_out_path: {video_out_path}')\n",
    "    # reveal_in_system_file_manager(video_out_path)\n",
    "    return video_out_path\n",
    "\n",
    "\n",
    "a_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('long_LR', None)\n",
    "save_posterior_to_video(a_decoder_continuously_decoded_result=a_decoder_continuously_decoded_result, result_name='continuous_long_LR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_posterior_to_video(a_decoder_continuously_decoded_result=pseudo2D_decoder_continuously_decoded_result, result_name='continuous_pseudo2D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f828c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: global_results, global_epoch_name\n",
    "\n",
    "# Get the decoders from the computation result:\n",
    "active_one_step_decoder = global_results['pf2D_Decoder']\n",
    "active_two_step_decoder = global_results.get('pf2D_TwoStepDecoder', None)\n",
    "if active_two_step_decoder is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['position_decoding_two_step'], computation_kwargs_list=[{}], enabled_filter_names=[global_epoch_name, global_LR_name, global_RL_name], fail_on_exception=True, debug_print=False)\n",
    "    active_two_step_decoder = global_results.get('pf2D_TwoStepDecoder', None)\n",
    "    assert active_two_step_decoder is not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image\n",
    "from pyphocorehelpers.plotting.media_output_helpers import save_array_as_video\n",
    "\n",
    "def colormap_and_save_as_video(array, video_filename='output.avi', fps=30.0, colormap=cv2.COLORMAP_VIRIDIS):\n",
    "    # array = ((array - array.min()) / (array.max() - array.min()) * 255).astype(np.uint8)\n",
    "    # color_array = cv2.applyColorMap(array, colormap)\n",
    "    return save_array_as_video(array, video_filename=video_filename, fps=fps, isColor=True, colormap=colormap)\n",
    "\n",
    "# image = get_array_as_image(img_data, desired_height=100, desired_width=None, skip_img_normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_input_posterior = deepcopy(active_two_step_decoder.p_x_given_n_and_x_prev)\n",
    "result_name: str = f'two_step_maze_all'\n",
    "\n",
    "# an_input_posterior = deepcopy(active_one_step_decoder.p_x_given_n)\n",
    "# result_name: str = f'one_step_2D_maze_all'\n",
    "\n",
    "\n",
    "n_x_bins, n_y_bins, n_time_bins = np.shape(an_input_posterior)\n",
    "transpose_axes_tuple = (2, 1, 0,)\n",
    "an_input_posterior = np.transpose(an_input_posterior, transpose_axes_tuple)\n",
    "decoding_realtime_FPS: float = 1.0 / float(active_one_step_decoder.time_bin_size)\n",
    "print(f'decoding_realtime_FPS: {decoding_realtime_FPS}')\n",
    "## save video\n",
    "video_out_path = save_array_as_video(array=an_input_posterior, video_filename=f'output/videos/{result_name}.avi', isColor=True, fps=decoding_realtime_FPS, colormap=cv2.COLORMAP_VIRIDIS)\n",
    "# video_out_path = colormap_and_save_as_video(array=an_input_posterior, video_filename=f'output/videos/{result_name}.avi', fps=decoding_realtime_FPS)\n",
    "\n",
    "print(f'video_out_path: {video_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['position_decoding_two_step'], computation_kwargs_list=[{}], enabled_filter_names=[global_epoch_name, global_LR_name, global_RL_name], fail_on_exception=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96151def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_binned_position_df: pd.DataFrame = global_results.get('extended_stats', {}).get('time_binned_position_df', None)\n",
    "time_binned_position_df\n",
    "# active_measured_positions = computation_result.sess.position.to_dataframe()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_black",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
