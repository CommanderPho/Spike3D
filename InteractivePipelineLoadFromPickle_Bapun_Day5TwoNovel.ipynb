{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45f3f6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ InteractivePipelineLoadFromPickle (Independent Load-only Visualization Notebook) - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:34:16.396240Z",
     "start_time": "2025-01-07T11:34:09.057603Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "pho-run-2024",
     "run-load",
     "run-main",
     "run-fresh-load"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "doc_output_parent_folder: C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\EXTERNAL\\DEVELOPER_NOTES\\DataStructureDocumentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\hdf5storage\\utilities.py:44: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field.name: \"merged_directional_placefields\", variable_name: \"merged_directional_placefields\"\n",
      "field.name: \"rank_order_shuffle_analysis\", variable_name: \"rank_order_shuffle_analysis\"\n",
      "field.name: \"directional_decoders_decode_continuous\", variable_name: \"directional_decoders_decode_continuous\"\n",
      "field.name: \"directional_decoders_evaluate_epochs\", variable_name: \"directional_decoders_evaluate_epochs\"\n",
      "field.name: \"directional_decoders_epoch_heuristic_scoring\", variable_name: \"directional_decoders_epoch_heuristic_scoring\"\n",
      "field.name: \"directional_train_test_split\", variable_name: \"directional_train_test_split\"\n",
      "field.name: \"long_short_decoding_analyses\", variable_name: \"long_short_decoding_analyses\"\n",
      "field.name: \"long_short_rate_remapping\", variable_name: \"long_short_rate_remapping\"\n",
      "field.name: \"long_short_inst_spike_rate_groups\", variable_name: \"long_short_inst_spike_rate_groups\"\n",
      "field.name: \"wcorr_shuffle_analysis\", variable_name: \"wcorr_shuffle_analysis\"\n",
      "field.name: \"non_pbe_epochs_results\", variable_name: \"non_pbe_epochs_results\"\n",
      "field.name: \"position_decoding\", variable_name: \"position_decoding\"\n",
      "field.name: \"perform_specific_epochs_decoding\", variable_name: \"perform_specific_epochs_decoding\"\n",
      "field.name: \"DEP_ratemap_peaks\", variable_name: \"DEP_ratemap_peaks\"\n",
      "field.name: \"ratemap_peaks_prominence2d\", variable_name: \"ratemap_peaks_prominence2d\"\n",
      "DAY_DATE_STR: 2025-09-23, DAY_DATE_TO_USE: 2025-09-23\n",
      "NOW_DATETIME: 2025-09-23_0603PM, NOW_DATETIME_TO_USE: 2025-09-23_0603PM\n",
      "global_data_root_parent_path changed to H:\\Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ba8168797949d5ab821773d15887e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Data Root:', layout=Layout(width='auto'), options=(WindowsPath('H:/Data'), WindowsPath('W:/Data')), style=ToggleButtonsStyle(button_width='max-content'), tooltip='global_data_root_parent_path', value=WindowsPath('H:/Data'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "# # !pip install viztracer\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "\n",
    "# %load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# # Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "from pyphocorehelpers.ipython_helpers import CustomFormatterMagics\n",
    "\n",
    "# Register the magic\n",
    "get_ipython().register_magics(CustomFormatterMagics)\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "from pyphocorehelpers.indexing_helpers import get_dict_subset\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import HardcodedProcessingParameters\n",
    "# from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import metadata_attributes\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "## Pho Programming Helpers:\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.notebook_helpers import NotebookCellExecutionLogger\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "_notebook_path:Path = Path(IPythonHelpers.try_find_notebook_filepath(IPython.extract_module_locals())).resolve() # Finds the path of THIS notebook\n",
    "# _notebook_execution_logger: NotebookCellExecutionLogger = NotebookCellExecutionLogger(notebook_path=_notebook_path, enable_logging_to_file=False) # Builds a logger that records info about this notebook\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme # used in perform_pipeline_save\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions,  RankOrderComputationsContainer, RankOrderResult, RankOrderAnalyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "from neuropy.utils.mixins.binning_helpers import transition_matrix\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.transition_matrix import TransitionMatrixComputations\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates, get_proper_global_spikes_df\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "def get_global_variable(var_name):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    return globals()[var_name]\n",
    "    \n",
    "def update_global_variable(var_name, value):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    globals()[var_name] = value\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/home/halechr/FastData'), Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/media/halechr/MAX/Data'), Path(r'H:\\Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/Users/pho/cloud/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "    global global_data_root_parent_path\n",
    "    new_global_data_root_parent_path = new_path.resolve()\n",
    "    global_data_root_parent_path = new_global_data_root_parent_path\n",
    "    print(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "    assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "            \n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load",
     "run-main",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# BAPUN data format                                                                                                    #\n",
    "# ==================================================================================================================== #\n",
    "active_data_mode_name = 'bapun'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('Bapun')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='bapun',animal='RatN', session_name='Day4OpenField') # ,exper_name='one' -- This is the one with the 2 behaviors ('roam', 'sprinkle') in the same open field box\n",
    "curr_context = IdentifyingContext(format_name='bapun',animal='RatS', session_name='Day5TwoNovel') # ,exper_name='one' -- this one has the 2 maze shapes: [\"N\", \"U\"]\n",
    "# Create a dictionary with the parameters to override\n",
    "override_parameters = {\n",
    "    'preprocessing.laps.use_direction_dependent_laps': False\n",
    "}\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name) #.resolve()\n",
    "\n",
    "# basedir: Path = Path('/media/halechr/MAX/Data/Rachel/cho/cho_241117_2_merged') # DO NOT `.resolve()``\n",
    "# basedir: Path = Path(r'H:\\Data\\Bapun\\RatS\\Day5TwoNovel')\n",
    "print(f'basedir: {str(basedir)}')\n",
    "Assert.path_exists(basedir)\n",
    "\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist = ['pf_computation', 'pfdt_computation', 'position_decoding']\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "selector, on_value_change = PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(update_global_variable_fn=update_global_variable, debug_print=False, enable_full_view=True)\n",
    "# selector.value = 'clean_run'\n",
    "selector.value = 'continued_run'\n",
    "# selector.value = 'final_run'\n",
    "on_value_change(dict(new=selector.value)) ## do update manually so the workspace variables reflect the set values\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "print(f\"saving_mode: {saving_mode}, force_reload: {force_reload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33267230",
   "metadata": {},
   "source": [
    "# Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21a4df",
   "metadata": {
    "tags": [
     "run-group-0",
     "run-load",
     "run-main",
     "active-2025-01-16",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases, PipelinePickleFileSelectorWidget\n",
    "\n",
    "# ## INPUTS: basedir\n",
    "# active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir)\n",
    "\n",
    "extended_computations_include_includelist_phase_dict: Dict[str, CustomProcessingPhases] = CustomProcessingPhases.get_extended_computations_include_includelist_phase_dict()\n",
    "\n",
    "current_phase: CustomProcessingPhases = CustomProcessingPhases[selector.value]  # Assuming selector.value is an instance of CustomProcessingPhases\n",
    "extended_computations_include_includelist: List[str] = [key for key, value in extended_computations_include_includelist_phase_dict.items() if value <= current_phase]\n",
    "display(extended_computations_include_includelist)\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'] # \n",
    "\n",
    "# ## INPUTS: basedir\n",
    "active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir, on_update_global_variable_callback=update_global_variable, on_get_global_variable_callback=get_global_variable)\n",
    "\n",
    "_subfn_load, _subfn_save, _subfn_compute, _subfn_compute_new = active_session_pickle_file_widget._build_load_save_callbacks(global_data_root_parent_path=global_data_root_parent_path, active_data_mode_name=active_data_mode_name, basedir=basedir, saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                                             extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist)\n",
    "\n",
    "\n",
    "# Display the widget\n",
    "display(active_session_pickle_file_widget.servable())\n",
    "# active_session_pickle_file_widget.local_file_browser_widget.servable()\n",
    "# active_session_pickle_file_widget.global_file_browser_widget.servable()\n",
    "# display(active_session_pickle_file_widget.local_file_browser_widget.servable())\n",
    "# display(active_session_pickle_file_widget.global_file_browser_widget.servable())\n",
    "\n",
    "# OUTPUTS: active_session_pickle_file_widget, widget.active_local_pkl, widget.active_global_pkl\n",
    "\n",
    "if selector.value == 'clean_run':\n",
    "    ## handle a clean run specially, this will create the pkls and not load them\n",
    "    print(f'clean run!')\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    # active_session_pickle_file_widget.is_compute_button_disabled = False # enable the compute button always during a clean run\n",
    "    # active_session_pickle_file_widget.is_load_button_disabled = True\n",
    "    \n",
    "    new_default_local_pkl_file: Path = active_session_pickle_file_widget.directory.joinpath(default_selected_local_file_name).resolve()\n",
    "    print(f'new_default_local_pkl_file: {new_default_local_pkl_file}')\n",
    "\n",
    "    active_session_pickle_file_widget.selected_local_pkl_files = [new_default_local_pkl_file]\n",
    "    active_session_pickle_file_widget.selected_global_pkl_files = []\n",
    "    active_session_pickle_file_widget._update_load_save_button_disabled_state()\n",
    "    print(f'active_session_pickle_file_widget.is_load_button_disabled: {active_session_pickle_file_widget.is_load_button_disabled}')\n",
    "    print(f'active_session_pickle_file_widget.is_compute_button_disabled: {active_session_pickle_file_widget.is_compute_button_disabled}')\n",
    "    print(f'active_local_pkl: \"{active_session_pickle_file_widget.active_local_pkl}\"')\n",
    "    print(f'active_global_pkl: \"{active_session_pickle_file_widget.active_global_pkl}\"')\n",
    "    active_session_pickle_file_widget.load_button.disabled = False\n",
    "    active_session_pickle_file_widget.compute_button.disabled = False\n",
    "else:\n",
    "    # not `clean_run` mode, continuing processing which might include loading from pickles\n",
    "    ## try selecting the first\n",
    "    did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "\n",
    "    ## Set default local comp pkl:\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    if not active_session_pickle_file_widget.is_local_file_names_list_empty:\n",
    "        default_local_section_indicies = [active_session_pickle_file_widget.local_file_browser_widget._data['File Name'].tolist().index(default_selected_local_file_name)]\n",
    "        active_session_pickle_file_widget.local_file_browser_widget.selection = default_local_section_indicies\n",
    "\n",
    "    ## Set default global computation pkl:\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    if not active_session_pickle_file_widget.is_global_file_names_list_empty:\n",
    "        default_global_section_indicies = [active_session_pickle_file_widget.global_file_browser_widget._data['File Name'].tolist().index(default_selected_global_file_name)]\n",
    "        active_session_pickle_file_widget.global_file_browser_widget.selection = default_global_section_indicies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b1147",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "did_find_valid_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "if did_find_valid_selection:\n",
    "    _subfn_load()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669bb69c",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "if did_find_valid_selection:\n",
    "    curr_active_pipeline, custom_suffix, proposed_load_pkl_path = active_session_pickle_file_widget.on_load_local(global_data_root_parent_path=global_data_root_parent_path, active_data_mode_name=active_data_mode_name, basedir=basedir, saving_mode=saving_mode, force_reload=force_reload)\n",
    "    print(f'on_load_local(...) complete. workspace variables updated: curr_active_pipeline, custom_suffix, proposed_load_pkl_path')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if did_find_valid_selection:\n",
    "    skip_global_load = False\n",
    "    curr_active_pipeline = active_session_pickle_file_widget.on_load_global(curr_active_pipeline=curr_active_pipeline, basedir=basedir, extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "                                skip_global_load=skip_global_load, force_reload=False, override_global_computation_results_pickle_path=active_session_pickle_file_widget.active_global_pkl)\n",
    "    # Update the global variable after loading global\n",
    "    print(f'on_load_global(...) complete. workspace variables updated: curr_active_pipeline, custom_suffix, proposed_load_pkl_path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d280d3",
   "metadata": {},
   "source": [
    "## Manual Recomputations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd6416",
   "metadata": {
    "tags": [
     "run-main",
     "active-2025-09-21",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "### Bapun Open-Field Experiment (2022-08-09 Analysis)\n",
    "from neuropy.core.session.SessionSelectionAndFiltering import build_custom_epochs_filters # used particularly to build Bapun-style filters\n",
    "\n",
    "active_data_mode_name = 'bapun'\n",
    "# active_data_mode_name = 'rachel'\n",
    "print(f'active_data_session_types_registered_classes_dict: {active_data_session_types_registered_classes_dict}')\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "\n",
    "# basedir = Path('/media/halechr/MAX/Data/Rachel/Cho_241117_Session2').resolve()\n",
    "## INPUTS: basedir \n",
    "\n",
    "force_reload = force_reload #True\n",
    "print(f'force_reload: {force_reload}')\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties, override_basepath=Path(basedir), force_reload=force_reload) # , override_parameters_flat_keypaths_dict=override_parameters\n",
    "\n",
    "# _test_session = RachelDataSessionFormat.build_session(Path(r'R:\\data\\Rachel\\merged_M1_20211123_raw_phy'))\n",
    "# _test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "# _test_session\n",
    "\n",
    "## ~20m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42da889",
   "metadata": {},
   "source": [
    "##### Old not needed anymore manual comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621607ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch_names: List[str] = curr_active_pipeline.sess.epochs.to_dataframe()['label'].to_list()\n",
    "print(f'curr_epoch_names: {curr_epoch_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686d53e",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.session.SessionSelectionAndFiltering import build_custom_epochs_filters\n",
    "\n",
    "# epoch_name_includelist = ['pre', 'maze1', 'post1', 'maze2', 'post2']\n",
    "# epoch_name_includelist = ['pre', 'roam', 'sprinkle', 'post']\n",
    "# epoch_name_includelist = ['roam', 'sprinkle']\n",
    "\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['pre', 'maze1', 'post1', 'maze2', 'post2']) ## ALL possible epochs\n",
    "\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze1', 'maze2', 'maze_GLOBAL']) ## ALL possible epochs\n",
    "active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze1', 'maze2', 'maze_GLOBAL']) ## ALL possible epochs\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess)\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['pre', 'roam', 'maze', 'sprinkle', 'post']) ## ALL possible epochs\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['pre', 'roam', 'sprinkle', 'post']) ## ALL possible epochs\n",
    "\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze', 'sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['roam', 'sprinkle']) # , 'maze'\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_filters_pyramidal_epochs(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "# active_session_filter_configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04689045",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.filter_sessions(active_session_filter_configurations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d068e",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "active_session_computation_configs = active_data_mode_registered_class.build_active_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=0.5)\n",
    "active_session_computation_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.computation_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcaaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_bin_bounds=(((-83.33747881216672, 110.15967332926644), (-94.89955475226206, 97.07387994733473)))\n",
    "\n",
    "\n",
    "bapun_open_field_grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0)))\n",
    "curr_active_pipeline.get_all_parameters()\n",
    "# curr_active_pipeline.update_parameters(grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0))))\n",
    "curr_active_pipeline.sess.config.grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0)))\n",
    "\n",
    "\n",
    "# override_parameters_flat_keypaths_dict = {'grid_bin_bounds': (((-120.0, 120.0), (-120.0, 120.0))), # 'rank_order_shuffle_analysis.minimum_inclusion_fr_Hz': minimum_inclusion_fr_Hz,\n",
    "# \t\t\t\t\t\t\t\t\t\t#   'sess.config.preprocessing_parameters.laps.use_direction_dependent_laps': False, # lap_estimation_parameters\n",
    "#                                         }\n",
    "\n",
    "# curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76553c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, EpochsAccessor, ensure_dataframe, ensure_Epoch\n",
    "\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# Update computation_epochs to be only the maze ones                                                                                                                                                                                                                                   #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "\n",
    "## activity_only_epochs_df:\n",
    "epochs_df = ensure_dataframe(deepcopy(curr_active_pipeline.sess.epochs))\n",
    "# activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(['maze1', 'maze2', 'maze_GLOBAL'])]\n",
    "\n",
    "activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(['maze1', 'maze2'])].epochs.get_non_overlapping_df()\n",
    "activity_only_epochs: Epoch = ensure_Epoch(activity_only_epochs_df, metadata=curr_active_pipeline.sess.epochs.metadata)\n",
    "\n",
    "## GLobal only ('maze_GLOBAL')\n",
    "epochs_df = ensure_dataframe(deepcopy(curr_active_pipeline.sess.epochs))\n",
    "global_activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(['maze_GLOBAL'])].epochs.get_non_overlapping_df()\n",
    "global_activity_only_epoch: Epoch = ensure_Epoch(global_activity_only_epochs_df, metadata=curr_active_pipeline.sess.epochs.metadata)\n",
    "\n",
    "## OUTPUTS: activity_only_epochs, global_activity_only_epoch\n",
    "\n",
    "## OUTPUTS: activity_only_epoch\n",
    "\n",
    "\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.filtered_sessions['maze'].epochs)\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.sess.epochs)\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.sess.epochs) ## prev\n",
    "active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(activity_only_epochs)\n",
    "\n",
    "global_only_sess_comp_config = deepcopy(active_session_computation_configs[0])\n",
    "global_only_sess_comp_config.pf_params.computation_epochs = deepcopy(global_activity_only_epoch)\n",
    "if len(active_session_computation_configs) < 2:\n",
    "    active_session_computation_configs.append(global_only_sess_comp_config)\n",
    "else:\n",
    "    active_session_computation_configs[1] = global_only_sess_comp_config\n",
    "\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(bapun_epochs)\n",
    "# active_session_computation_configs[1].pf_params.computation_epochs = deepcopy(curr_active_pipeline.filtered_sessions['maze'].epochs.to_dataframe())\n",
    "active_session_computation_configs\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs\n",
    "\n",
    "#    start   stop     label  duration\n",
    "# 0      0   7407       pre      7407\n",
    "# 1   7423  11483      maze      4060\n",
    "# 3  10186  11483  sprinkle      1297\n",
    "# 2  11497  25987      post     14490\n",
    "\n",
    "# [4 rows x 4 columns]\n",
    "\n",
    "## UPDATES: active_session_computation_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d8497",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.linearization_method = \"umap\"\n",
    "\n",
    "for an_epoch_name, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    ## forcibly compute the linearized position so it doesn't fallback to \"isomap\" method which eats all the memory\n",
    "    a_pos_df: pd.DataFrame = a_sess.position.compute_linearized_position(method='umap')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb73824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_only_epoch_names: List[str] = ['maze1', 'maze2', 'maze_GLOBAL']\n",
    "# active_computation_functions_name_includelist\n",
    "activity_only_epoch_names: List[str] = active_session_computation_configs[0].pf_params.computation_epochs.labels.tolist() ## should be same as config\n",
    "activity_only_epoch_names\n",
    "\n",
    "# # Create non-overlapping version\n",
    "# non_overlapping_epochs = ensure_Epoch(active_session_computation_configs[0].pf_params.computation_epochs.epochs.get_non_overlapping_df())\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = non_overlapping_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "    \n",
    "active_computation_functions_name_includelist = ['pf_computation',\n",
    "                                                'pfdt_computation',\n",
    "                                                'position_decoding',\n",
    "                                                #  'position_decoding_two_step',\n",
    "                                                #  'extended_pf_peak_information',\n",
    "                                                ] # 'ratemap_peaks_prominence2d'\n",
    "\n",
    "\n",
    "## Loops through all configs\n",
    "for i, a_config in enumerate(active_session_computation_configs):\n",
    "    active_epoch_names: List[str] = a_config.pf_params.computation_epochs.labels.tolist() ## should be same as config\n",
    "    print(f'i: {i}, active_epoch_names: {active_epoch_names}') # (activity_only_epoch_names)\n",
    "\n",
    "    # curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation', '_perform_velocity_vs_pf_density_computation', '_perform_velocity_vs_pf_simplified_count_density_computation']) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "    # curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_includelist=active_computation_functions_name_includelist, enabled_filter_names=activity_only_epoch_names, overwrite_extant_results=True, fail_on_exception=False, debug_print=True) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "    curr_active_pipeline.perform_computations(a_config, computation_functions_name_includelist=active_computation_functions_name_includelist, enabled_filter_names=active_epoch_names, overwrite_extant_results=False, fail_on_exception=False, debug_print=True) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.active_completed_computation_result_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_includelist=active_computation_functions_name_includelist, enabled_filter_names=['maze1', 'maze2'], overwrite_extant_results=False, fail_on_exception=False, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491af66",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# curr_active_pipeline.computation_results['maze'].accumulated_errors\n",
    "curr_active_pipeline.clear_all_failed_computations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831df11",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display(root_output_dir=r'Output', should_smooth_maze=True) # TODO: pass a display config\n",
    "# curr_active_pipeline.prepare_for_display(root_output_dir=r'W:\\Data\\Output', should_smooth_maze=True) # TODO: pass a display config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e45325",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.pickle_path\n",
    "curr_active_pipeline.global_computation_results_pickle_path\n",
    "curr_active_pipeline.get_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09507ee4",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)\n",
    "# _out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE)\n",
    "# _out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-26.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4608e0",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_global_computation_results()#save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-27.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797a9de",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# include_includelist = ['pre', 'maze1', 'post1', 'maze2', 'post2', 'maze',]\n",
    "include_includelist = ['roam', 'sprinkle']\n",
    "# include_includelist = curr_active_pipeline.filtered_session_names\n",
    "include_includelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_session_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_includelist = curr_active_pipeline.filtered_session_names\n",
    "print(f'include_includelist: {include_includelist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d93f",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "## Setup Computation Functions to be executed:\n",
    "# includelist Mode:\n",
    "computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                '_perform_position_decoding_computation', \n",
    "                                '_perform_firing_rate_trends_computation',\n",
    "                                '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                '_perform_two_step_position_decoding_computation',\n",
    "                                # '_perform_recursive_latent_placefield_decoding'\n",
    "                            ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "computation_functions_name_excludelist=None\n",
    "\n",
    "batch_extended_computations(curr_active_pipeline, included_computation_filter_names=computation_functions_name_includelist, include_includelist=include_includelist,\n",
    "                            include_global_functions=True, fail_on_exception=False, progress_print=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845bf16",
   "metadata": {},
   "source": [
    "## NEW BATCH COMPUTE ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c088e73",
   "metadata": {
    "tags": [
     "run-main",
     "run-load",
     "run-fresh-load",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import final_process_bapun_all_comps\n",
    "\n",
    "# try:\n",
    "curr_active_pipeline = final_process_bapun_all_comps(curr_active_pipeline=curr_active_pipeline, posthoc_save=False)\n",
    "# except Exception as e:\n",
    "#     print(f'exception: {e}')\n",
    "#     # raise e\n",
    "#     pass    \n",
    "\n",
    "## 9m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.rerun_failed_computations()\n",
    "curr_active_pipeline.get_failed_computations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7a902",
   "metadata": {},
   "source": [
    "# 2025-09-08 - Concatenate Movement Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a30303",
   "metadata": {},
   "source": [
    "## Decode across both movement sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(curr_active_pipeline.filtered_sessions.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf3625",
   "metadata": {
    "tags": [
     "run-fresh-load",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.epoch import Epoch, ensure_dataframe, ensure_Epoch, EpochsAccessor\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_contextual_pf2D_decoder, decode_using_contextual_pf2D_decoder\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import decode_using_contextual_pf2D_decoder\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.context_dependent import GenericDecoderDictDecodedEpochsDictResult\n",
    "\n",
    "## Build the merged decoder `contextual_pf2D`\n",
    "# ACTUALLY BUILD THE PSEUDO 2D: ______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "hardcoded_params: HardcodedProcessingParameters = BapunDataSessionFormatRegisteredClass._get_session_specific_parameters(session_context=curr_active_pipeline.get_session_context())\n",
    "epochs_to_create_global_from_names = hardcoded_params.non_global_activity_session_names # ['maze1', 'maze2'] or ['roam', 'sprinkle']\n",
    "pf2D_Decoder_dict, contextual_pf2D, contextual_pf2D_Decoder = build_contextual_pf2D_decoder(curr_active_pipeline, epochs_to_create_global_from_names=epochs_to_create_global_from_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77caca1",
   "metadata": {
    "tags": [
     "run-fresh-load",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "# active_laps_decoding_time_bin_size: float = 0.25\n",
    "active_laps_decoding_time_bin_size: float = 1.0\n",
    "\n",
    "all_context_filter_epochs_decoder_result, global_only_epoch = decode_using_contextual_pf2D_decoder(curr_active_pipeline, contextual_pf2D_Decoder=contextual_pf2D_Decoder, active_laps_decoding_time_bin_size=active_laps_decoding_time_bin_size)\n",
    "# 10m 2s\n",
    "\n",
    "## 32m at 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e279cb",
   "metadata": {},
   "source": [
    "#### Build `DirectionalDecodersContinuouslyDecodedResult` object to hold all the decoded epochs/decoders/etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f50cf",
   "metadata": {
    "tags": [
     "run-fresh-load",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "global_spikes_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = DirectionalDecodersContinuouslyDecodedResult(pf1D_Decoder_dict=pf2D_Decoder_dict, pseudo2D_decoder=contextual_pf2D_Decoder, spikes_df=global_spikes_df, continuously_decoded_result_cache_dict={\n",
    "        active_laps_decoding_time_bin_size: dict(pseudo2D = all_context_filter_epochs_decoder_result,\n",
    "                                        #     **individual_decoder_decoding_results,\n",
    "                                            ),\n",
    "\t})\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded'] = directional_decoders_decode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5105bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_decode_result_pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-09-23_directional_decoders_decode_result.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from pickle\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = DirectionalDecodersContinuouslyDecodedResult.from_file(pkl_path=directional_decoders_decode_result_pkl_output_path)\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded'] = directional_decoders_decode_result ## assign to curr_active_pipeline's global result\n",
    "directional_decoders_decode_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ed642",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_decode_result_pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-09-23_directional_decoders_decode_result.pkl')\n",
    "directional_decoders_decode_result.save(pkl_output_path=directional_decoders_decode_result_pkl_output_path)\n",
    "print(f'directional_decoders_decode_result_pkl_output_path: \"{directional_decoders_decode_result_pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3af70",
   "metadata": {},
   "source": [
    "## PENDING: Detection of laps/run sprints from position data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import Position\n",
    "import ruptures as rpt  # our package\n",
    "\n",
    "def lap_detect(lin_pos, sigma = 5, jump=10, model = \"l2\"):\n",
    "    \"\"\" \n",
    "        jump=10 # speeds things up at the cost of quality\n",
    "        sigma = 5 # standard dev.\n",
    "    \"\"\"\n",
    "    # change point detection\n",
    "    model = \"l2\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",...\n",
    "    algo = rpt.Binseg(model=model, jump=jump).fit(lin_pos)\n",
    "    # algo = rpt.Dynp(model=\"l2\").fit(lin_pos)\n",
    "    num_samples: int = len(lin_pos)\n",
    "\n",
    "    n_dim: int = np.ndim(lin_pos)\n",
    "    pen: float = np.log(num_samples) * n_dim * sigma**2 # In the situation in which the number of change points is unknown, one can specify a penalty\n",
    "    my_bkps = algo.predict(pen=pen)\n",
    "    return my_bkps\n",
    "\n",
    "\n",
    "\n",
    "a_sess = curr_active_pipeline.filtered_sessions['maze1']\n",
    "a_pos: Position = a_sess.position\n",
    "a_pos_df: pd.DataFrame = a_pos.adding_approx_head_dir_columns()\n",
    "a_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_detection_columns_list = [\n",
    "    ['lin_pos', 'speed'],\n",
    "    ['lin_pos', 'approx_head_dir_degrees'],\n",
    "    ['lin_pos', 'speed', 'approx_head_dir_degrees'],\n",
    "    ['x', 'y', 'velocity_x_smooth', 'velocity_y_smooth', 'approx_head_dir_degrees'],\n",
    "    ['x_smooth', 'y_smooth', 'velocity_x_smooth', 'velocity_y_smooth', 'approx_head_dir_degrees'],\n",
    "]\n",
    "\n",
    "lap_detection_dict = {\n",
    "\t\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184291cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_columns = lap_detection_columns_list[-1]\n",
    "\n",
    "pos_arr = a_pos_df[pos_columns].to_numpy()\n",
    "bkpts = lap_detect(pos_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_detection_dict[tuple(pos_columns)] = bkpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "rpt.show.display(pos_arr, bkpts, figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ccc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_pos = (a_pos_df['lin_pos', 'approx_head_dir_degrees'].to_numpy())\n",
    "lap_detection_1D = lap_detect(lin_pos)\n",
    "lap_detection_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lin_pos = (a_pos_df['lin_pos'].to_numpy())\n",
    "lap_detection_1D = lap_detect(lin_pos)\n",
    "lap_detection_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fad2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_xy = (a_pos_df[['x', 'y', 'velocity_x_smooth', 'velocity_y_smooth', 'approx_head_dir_degrees']].to_numpy())\n",
    "lap_detection_2D = lap_detect(pos_xy)\n",
    "lap_detection_2D\n",
    "# 38 sec\n",
    "# 2m 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "rpt.show.display(pos_xy, lap_detection_2D, figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6457b6",
   "metadata": {},
   "source": [
    "## STEP 2025-09-23 - Add detected laps, and re-run main computations. Many things that failed before occured due to lack of laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac02548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.laps import estimate_session_laps\n",
    "\n",
    "sess = estimate_session_laps(curr_active_pipeline.sess, should_plot_laps_2d=False) ## unfiltered session \n",
    "\n",
    "for k, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    a_sess = estimate_session_laps(a_sess, should_plot_laps_2d=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c5f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09505c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.laps import estimate_session_laps\n",
    "\n",
    "a_sess = estimate_session_laps(a_sess, should_plot_laps_2d=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num='lap detect', clear=True)\n",
    "plt.stem(rising_edges, label='rising')\n",
    "plt.stem(falling_edges, label='falling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num='lap detect')\n",
    "plt.stem(hs_diff)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cbb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rising_edge_change_points = np.where(is_high_speed_period.diff() > 0)[0]\n",
    "falling_edge_change_points = np.where(is_high_speed_period.diff() < 0)[0]\n",
    "\n",
    "rising_edge_change_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d06c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "falling_edge_change_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81295b7",
   "metadata": {},
   "source": [
    "## STEP 2025-09-23 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12a5a1",
   "metadata": {},
   "source": [
    "## STEP 2025-09-23 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bfd33",
   "metadata": {},
   "source": [
    "## STEP 2025-09-23 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2eb44",
   "metadata": {},
   "source": [
    "## PENDING: Proper, fully general result saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import EpochsAccessor, Epoch, ensure_dataframe, ensure_Epoch\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.EpochComputationFunctions import EpochComputationFunctions, EpochComputationsComputationsContainer, DecodingResultND, Compute_NonPBE_Epochs, KnownFilterEpochs\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.EpochComputationFunctions import GeneralDecoderDictDecodedEpochsDictResult, GenericResultTupleIndexType, KnownNamedDecodingEpochsType, MaskedTimeBinFillType\n",
    "\n",
    "curr_active_pipeline.perform_computation('perform_generalized_specific_epochs_decoding')\t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "directional_trial_by_trial_activity_result: TrialByTrialActivityResult = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None)\n",
    "if directional_trial_by_trial_activity_result is None:\n",
    "    # if `KeyError: 'TrialByTrialActivity'` recompute\n",
    "    print(f'TrialByTrialActivity is not computed, computing it...')\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "    directional_trial_by_trial_activity_result = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None) ## try again to get the result\n",
    "    assert directional_trial_by_trial_activity_result is not None, f\"directional_trial_by_trial_activity_result is None even after forcing recomputation!!\"\n",
    "    print(f'\\t done.')\n",
    "\n",
    "## unpack either way:\n",
    "any_decoder_neuron_IDs = directional_trial_by_trial_activity_result.any_decoder_neuron_IDs\n",
    "active_pf_dt: PfND_TimeDependent = directional_trial_by_trial_activity_result.active_pf_dt\n",
    "directional_lap_epochs_dict: Dict[str, Epoch] = directional_trial_by_trial_activity_result.directional_lap_epochs_dict\n",
    "directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts\n",
    "## OUTPUTS: directional_trial_by_trial_activity_result, directional_active_lap_pf_results_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeda349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign new empty computation result:\n",
    "curr_active_pipeline.global_computation_results.computed_data['EpochComputations'] = EpochComputationsComputationsContainer(training_data_portion=None, epochs_decoding_time_bin_size=None, frame_divide_bin_size=None,\n",
    "                                                                                                    a_new_NonPBE_Epochs_obj=None, results1D=None, results2D=None, is_global=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_maze_epoch_names = deepcopy(hardcoded_params.non_global_activity_session_names)\n",
    "active_maze_epochs_df: pd.DataFrame = curr_active_pipeline.sess.paradigm.to_dataframe() # ['label']\n",
    "active_maze_epochs_df = active_maze_epochs_df[active_maze_epochs_df['label'].isin(active_maze_epoch_names)]\n",
    "active_maze_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb13fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2366ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.session.dataSession import Laps\n",
    "from neuropy.core.epoch import EpochsAccessor, Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5892fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t_start, t_delta, t_end = owning_pipeline_reference.find_LongShortDelta_times()\n",
    "laps_obj: Laps = curr_active_pipeline.sess.laps\n",
    "laps_df: pd.DataFrame = laps_obj.to_dataframe()\n",
    "# laps_df = laps_df.epochs.adding_maze_id_if_needed(t_start=t_start, t_delta=t_delta, t_end=t_end)\n",
    "laps_df.epochs.adding_maze_id_if_needed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dab602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "laps_df['maze_id'] = ''\n",
    "\n",
    "\n",
    "maze_dfs = {}\n",
    "for a_row in active_maze_epochs_df.itertuples():\n",
    "    # maze_dfs[a_row.label] = laps_df.epochs.time_slice(t_start=a_row.start, t_stop=a_row.stop)\n",
    "    is_epoch_on_maze = np.logical_and((a_row.start < laps_df['start']), (laps_df['stop'] < a_row.stop))\n",
    "    # is_epoch_on_maze: NDArray = find_epochs_overlapping_other_epochs(epochs_df=laps_df, epochs_df_required_to_overlap=deepcopy(active_maze_epochs_df[active_maze_epochs_df['label'] == a_row.label]))\n",
    "    maze_dfs[a_row.label] = laps_df[is_epoch_on_maze]\n",
    "    laps_df.loc[maze_dfs[a_row.label].index, 'maze_id'] = a_row.label\n",
    "    \n",
    "# maze_dfs\n",
    "laps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.time_slicing import add_fully_overlapping_epochs_id_identity_to_epochs\n",
    "\n",
    "active_maze_epoch_names = deepcopy(hardcoded_params.non_global_activity_session_names)\n",
    "active_maze_epochs_df: pd.DataFrame = curr_active_pipeline.sess.paradigm.to_dataframe() # ['label']\n",
    "active_maze_epochs_df = active_maze_epochs_df[active_maze_epochs_df['label'].isin(active_maze_epoch_names)]\n",
    "laps_df = add_fully_overlapping_epochs_id_identity_to_epochs(query_child_epochs = laps_df, potential_fully_enclosing_epochs_df = active_maze_epochs_df, epoch_id_key_name = 'maze_id')\n",
    "laps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "laps_df = laps_df.sort_values('start', ascending=True)\n",
    "\n",
    "laps_df # ['maze_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fae001",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_time_binned_computation_epochs_df\n",
    "continuous_time_binned_computation_epochs_included_only_df = continuous_time_binned_computation_epochs_df[continuous_time_binned_computation_epochs_df['is_in_laps']].drop(columns=['is_in_laps'])\n",
    "continuous_time_binned_computation_epochs_included_only_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recompute_global = False\n",
    "extended_computations_include_includelist=[\n",
    "\t# 'lap_direction_determination',\n",
    "\t#  'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    # 'extended_stats',\n",
    "    # 'spike_burst_detection',\n",
    "    # 'split_to_directional_laps',\n",
    "    # 'merged_directional_placefields',\n",
    "    # 'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs',\n",
    "    # 'directional_decoders_epoch_heuristic_scoring',\n",
    "\t'non_PBE_epochs_results',\n",
    "\t'perform_generalized_specific_epochs_decoding',\n",
    "] # do only specified\n",
    "\n",
    "# ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous']\n",
    "\n",
    "# force_recompute_override_computations_includelist = [\n",
    "#     'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring',\n",
    "#     'split_to_directional_laps', 'lap_direction_determination', 'DirectionalLaps',\n",
    "#     'merged_directional_placefields',\n",
    "#     'directional_decoders_decode_continuous',\n",
    "# ]\n",
    "force_recompute_override_computations_includelist = None\n",
    "\n",
    "epochs_decoding_time_bin_size = 1.0\n",
    "computation_kwargs_dict = {\n",
    "\t'non_PBE_epochs_results': dict(epochs_decoding_time_bin_size=epochs_decoding_time_bin_size, drop_previous_result_and_compute_fresh=False, compute_2D=False, IGNORE_MEMORY_ERROR_FOR_DEBUGGING = True),\n",
    "\t'perform_generalized_specific_epochs_decoding': dict(),\n",
    "}\n",
    "\n",
    "\n",
    "# computation_kwargs_dict = {'non_PBE_epochs_results': dict(epochs_decoding_time_bin_size=epochs_decoding_time_bin_size, drop_previous_result_and_compute_fresh=False, compute_2D=False, IGNORE_MEMORY_ERROR_FOR_DEBUGGING=True), }\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, computation_kwargs_dict=computation_kwargs_dict, debug_print=False)\n",
    "newly_computed_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c24c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extended_computations_include_includelist=['ratemap_peaks_prominence2d', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring',] # do only specified\n",
    "extended_computations_include_includelist=['rank_order_shuffle_analysis', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs' ] # do only specified\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c82a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "newly_computed_values = curr_active_pipeline.batch_extended_computations(include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = curr_active_pipeline.batch_evaluate_required_computations(include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb051a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96853688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "newly_computed_values = curr_active_pipeline.batch_extended_computations(include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = curr_active_pipeline.batch_evaluate_required_computations(include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e05ceb",
   "metadata": {},
   "source": [
    "## Pickle the decoded PBEs output too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380171fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUTS: pbes, global_spikes_df, pbe_decoder_result\n",
    "pbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45afca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2461f",
   "metadata": {},
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bae072",
   "metadata": {},
   "source": [
    "#### Don't need this in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef930b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "# active_laps_decoding_time_bin_size = 1.0\n",
    "\n",
    "## INPUTS: pf2D_Decoder_dict, \n",
    "desired_global_created_epoch_name: str = 'maze_GLOBAL'\n",
    "epochs_to_decode_dict = {desired_global_created_epoch_name: deepcopy(global_only_epoch)}\n",
    "individual_decoder_decoding_results: Dict[str, SingleEpochDecodedResult] = {}\n",
    "\n",
    "for a_decoder_name, a_decoder in pf2D_Decoder_dict.items():    \n",
    "    global_spikes_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "    individual_decoder_decoding_results[a_decoder_name] = a_decoder.decode_specific_epochs(spikes_df=deepcopy(global_spikes_df), filter_epochs=ensure_dataframe(epochs_to_decode_dict[desired_global_created_epoch_name]), decoding_time_bin_size=active_laps_decoding_time_bin_size, debug_print=False)    \n",
    "    individual_decoder_decoding_results[a_decoder_name].marginal_z_list = [None]\n",
    "    \n",
    "    # ## ASSIGN IT to .marginal_z\n",
    "    # individual_decoder_decoding_results[a_decoder_name].marginal_z_list = [] ## empty\n",
    "    # for p_x_given_n in individual_decoder_decoding_results[a_decoder_name].p_x_given_n_list:\n",
    "    #     n_x_bins, n_y_bins, n_contexts, n_t_bins = np.shape(p_x_given_n) # (41, 63, 2, 51974)\n",
    "    #     marginal_z = np.nansum(p_x_given_n, axis=(0, 1)) \n",
    "    #     marginal_z = marginal_z / np.sum(marginal_z, axis=0, keepdims=True) # sum over all directions for each time_bin (so there's a normalized distribution at each timestep)\n",
    "    #     individual_decoder_decoding_results[a_decoder_name].marginal_z_list.append(DynamicContainer(p_x_given_n=marginal_z, most_likely_positions_2D=None))\n",
    "\n",
    "    # try:\n",
    "    #     individual_decoder_decoding_results[a_decoder_name] = individual_decoder_decoding_results[a_decoder_name].get_result_for_epoch(0)\n",
    "    # except Exception as e:\n",
    "    #     print(f'encountered exception: {e}')\n",
    "    #     pass\n",
    "    #     # raise e\n",
    "\n",
    "#.get_result_for_epoch(0)\n",
    "    \n",
    "## 4m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-hoc (one time hopefully) fixup:\n",
    "for a_decoder_name, a_decoder_result in individual_decoder_decoding_results.items():    \n",
    "\n",
    "    ## ASSIGN IT to .marginal_z\n",
    "    a_decoder_result.marginal_z_list = [] ## empty\n",
    "    for p_x_given_n in a_decoder_result.p_x_given_n_list:\n",
    "        n_x_bins, n_y_bins, n_contexts, n_t_bins = np.shape(p_x_given_n) # (41, 63, 2, 51974)\n",
    "        marginal_z = np.nansum(p_x_given_n, axis=(0, 1)) \n",
    "        marginal_z = marginal_z / np.sum(marginal_z, axis=0, keepdims=True) # sum over all directions for each time_bin (so there's a normalized distribution at each timestep)\n",
    "        a_decoder_result.marginal_z_list.append(DynamicContainer(p_x_given_n=marginal_z, most_likely_positions_2D=None))\n",
    "\n",
    "    individual_decoder_decoding_results[a_decoder_name] = a_decoder_result.get_result_for_epoch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd42ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CORRECT CONTEXT DECODING MARGINALS:\n",
    "\n",
    "## INPUTS: all_context_filter_epochs_decoder_result\n",
    "# all_context_filter_epochs_decoder_result.marginal_z\n",
    "p_x_given_n = all_context_filter_epochs_decoder_result.p_x_given_n\n",
    "n_x_bins, n_y_bins, n_contexts, n_t_bins = np.shape(p_x_given_n) # (41, 63, 2, 51974)\n",
    "\n",
    "marginal_z = np.nansum(p_x_given_n, axis=(0, 1)) \n",
    "marginal_z = marginal_z / np.sum(marginal_z, axis=0, keepdims=True) # sum over all directions for each time_bin (so there's a normalized distribution at each timestep)\n",
    "# marginal_z\n",
    "print(f'marginal_z.shape: {np.shape(marginal_z)}')\n",
    "\n",
    "# most_likely_context_idx = np.argmax(marginal_z, axis=0)\n",
    "# n_x_bins, n_y_bins, n_contexts, n_t_bins = np.shape(p_x_given_n) # (41, 63, 2, 51974)\n",
    "\n",
    "# most_liekly_context_only_p_x_given_n = np.array([np.squeeze(p_x_given_n[:, :, a_most_likely_ctxt, t]) for t, a_most_likely_ctxt in enumerate(most_likely_context_idx)])\n",
    "# np.shape(most_liekly_context_only_p_x_given_n)\n",
    "# most_liekly_context_only_p_x_given_n = np.where(most_likely_context_idx, np.squeeze(p_x_given_n[:, :, 1, :]), np.squeeze(p_x_given_n[:, :, 0, :]))\n",
    "# np.shape(most_liekly_context_only_p_x_given_n) # (41, 63, 51974)\n",
    "\n",
    "## ASSIGN IT to .marginal_z\n",
    "all_context_filter_epochs_decoder_result.marginal_z = DynamicContainer(p_x_given_n=marginal_z, most_likely_positions_2D=None)\n",
    "# all_context_filter_epochs_decoder_result.marginal_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c84981",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_z: NDArray = all_context_filter_epochs_decoder_result.marginal_z.p_x_given_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428fc3",
   "metadata": {},
   "source": [
    "### Unpacking a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "\n",
    "## Uses the `global_computation_results.computed_data['DirectionalDecodersDecoded']`\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "# all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo3D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "# all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "continuously_decoded_pseudo2D_decoder_dict = directional_decoders_decode_result.continuously_decoded_pseudo2D_decoder_dict\n",
    "# continuously_decoded_result_cache_dict\n",
    "\n",
    "## Unpacking a result:\n",
    "all_context_filter_epochs_decoder_result: SingleEpochDecodedResult = list(continuously_decoded_pseudo2D_decoder_dict.values())[-1]\n",
    "marginal_z: NDArray = all_context_filter_epochs_decoder_result.marginal_z.p_x_given_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b454125",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT goinng to work at all for this\n",
    "# epochs_decoding_time_bin_size: float = 0.50 # half second\n",
    "# a_new_fully_generic_result: GenericDecoderDictDecodedEpochsDictResult = GenericDecoderDictDecodedEpochsDictResult.batch_user_compute_fn(curr_active_pipeline=curr_active_pipeline, force_recompute=True, time_bin_size=epochs_decoding_time_bin_size, debug_print=True)\n",
    "# # valid_EpochComputations_result.a_generic_decoder_dict_decoded_epochs_dict_result = a_new_fully_generic_result\n",
    "\n",
    "## OUTPUTS: contextual_pf2D_dict, contextual_pf2D, contextual_pf2D_Decoder, all_context_filter_epochs_decoder_result, global_only_epoch\n",
    "all_context_filter_epochs_decoder_result.marginal_x = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aa0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = dict()\n",
    "_out['_display_spike_rasters_window'] = curr_active_pipeline.display(display_function='_display_spike_rasters_window', active_session_configuration_context=curr_active_pipeline.get_session_context().adding_context_if_missing(filter_name='maze_GLOBAL')) # _display_spike_rasters_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec68454",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Day4OpenField type sessions\n",
    "epochs_df = ensure_dataframe(curr_active_pipeline.sess.epochs)\n",
    "epochs_df = epochs_df.epochs.adding_concatenated_epoch(epochs_to_create_global_from_names=['pre', 'roam', 'sprinkle', 'post'], created_epoch_name='maze_any')\n",
    "global_only_epoch: Epoch = ensure_Epoch(epochs_df[(epochs_df['label'] == 'maze_any')])\n",
    "# global_only_epoch\n",
    "\n",
    "\n",
    "active_laps_decoding_time_bin_size = 0.250\n",
    "contextual_pf2D_dict, contextual_pf2D, contextual_pf2D_Decoder, all_context_filter_epochs_decoder_result = build_contextual_pf2D_decoder(curr_active_pipeline, epochs_to_create_global_from_names = ['roam', 'sprinkle'], active_laps_decoding_time_bin_size=0.75)\n",
    "## OUTPUTS: contextual_pf2D_Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3304c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: contextual_pf2D_Decoder\n",
    "global_decoded_cache = {}\n",
    "# active_laps_decoding_time_bin_sizes = [0.050, 0.1, 0.25, 1.0]\n",
    "# active_laps_decoding_time_bin_sizes = [0.1, 1.0]\n",
    "active_laps_decoding_time_bin_sizes = [0.1,]\n",
    "for active_time_bin_size in active_laps_decoding_time_bin_sizes:\n",
    "    an_all_context_filter_epochs_decoder_result, global_only_epoch = decode_using_contextual_pf2D_decoder(curr_active_pipeline, contextual_pf2D_Decoder=contextual_pf2D_Decoder, active_laps_decoding_time_bin_size=active_time_bin_size)\t\n",
    "    global_decoded_cache[active_time_bin_size] = deepcopy(an_all_context_filter_epochs_decoder_result)\n",
    "\n",
    "## OUTPUTS: global_only_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fcab4",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import DecodeSpecificEpochsResultWithDecodingInfo\n",
    "\n",
    "## INPUTS: contextual_pf2D_Decoder, curr_active_pipeline\n",
    "## Decode PBEs please\n",
    "pbes = deepcopy(curr_active_pipeline.sess.pbe)\n",
    "# ripple_decoding_time_bin_size: float = 0.025 # 25ms\n",
    "ripple_decoding_time_bin_size: float = 0.060 # 25ms\n",
    "global_spikes_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "pbes_full_result: DecodeSpecificEpochsResultWithDecodingInfo = DecodeSpecificEpochsResultWithDecodingInfo.init_by_decoding(decoding_context=IdentifyingContext(epoch_name='pbe'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   decoder=contextual_pf2D_Decoder, spikes_df=deepcopy(global_spikes_df), filter_epochs=ensure_dataframe(pbes), decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "\n",
    "\n",
    "## 18m at 60ms\n",
    "\n",
    "## OUTPUTS: pbes, ripple_decoding_time_bin_size,  global_spikes_df, pbe_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ae5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbes_full_result_pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-09-23_pbes_full_result.pkl')\n",
    "pbes_full_result.save(pkl_output_path=pbes_full_result_pkl_output_path)\n",
    "print(f'pbes_full_result_pkl_output_path: \"{pbes_full_result_pkl_output_path.as_posix()}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbes_full_result_h5_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-09-23_pbes_full_result.h5')\n",
    "# pbes_full_result.to_hdf(file_path=pbes_full_result_h5_output_path, key='pbes_full_result') # , enable_hdf_testing_mode=True, OVERRIDE_ALLOW_GLOBAL_NESTED_EXPANSION=True\n",
    "# print(f'pbes_full_result_h5_output_path: \"{pbes_full_result_h5_output_path.as_posix()}\"')\n",
    "# # _ALLOW_GLOBAL_NESTED_EXPANSION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28421f3",
   "metadata": {},
   "source": [
    "# 💾 Save Export Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d035840",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_complete_session_context()\n",
    "custom_save_filepaths, custom_save_filenames, custom_suffix = curr_active_pipeline.get_custom_pipeline_filenames_from_parameters()\n",
    "custom_save_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filenames['pipeline_pkl']\n",
    "custom_save_filenames['global_computation_pkl']\n",
    "\n",
    "pickle_path = 'loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl'\n",
    "global_computation_pkl = 'global_computation_results_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20555817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename=curr_active_pipeline.pickle_path.name) #active_pickle_filename=\n",
    "# curr_active_pipeline.save_global_computation_results(override_global_pickle_path=curr_active_pipeline.global_computation_results_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74080dd",
   "metadata": {
    "tags": [
     "run-perform-save-pkl"
    ]
   },
   "outputs": [],
   "source": [
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "if curr_active_pipeline.pickle_path is None:\n",
    "    active_pickle_filename = 'loadedSessPickle.pkl'\n",
    "else:\n",
    "    active_pickle_filename = curr_active_pipeline.pickle_path.name\n",
    "    \n",
    "print(f'active_pickle_filename: {active_pickle_filename}')\n",
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename=active_pickle_filename) #active_pickle_filename=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520284fb",
   "metadata": {
    "tags": [
     "run-perform-save-pkl"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results(override_global_pickle_path=curr_active_pipeline.global_computation_results_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1d1c4",
   "metadata": {},
   "source": [
    "#### Manual Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22341765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename='loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl') #active_pickle_filename=\n",
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl') #active_pickle_filename="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849549fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results(override_global_pickle_filename='global_computation_results_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import PipelineWithInputStage, PipelineWithLoadableStage, loadData, saveData\n",
    "\n",
    "## Custom Save\n",
    "curr_sess_pkl_path = basedir.joinpath('loadedSessPickle_2025-09-08.pkl')\n",
    "print(f'saving out to modern pickle: \"{curr_sess_pkl_path}\"')\n",
    "saveData(curr_sess_pkl_path, db=curr_active_pipeline, safe_save=True) # (v_dict, str(curr_item_type.__module__), str(curr_item_type.__name__)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52cc45",
   "metadata": {},
   "source": [
    "### 2024-06-25 - Load from saved custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337ddc6",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "# Loads custom pipeline pickles that were saved out via `custom_save_filepaths['pipeline_pkl'] = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename=custom_save_filenames['pipeline_pkl'])`\n",
    "\n",
    "## INPUTS: global_data_root_parent_path, active_data_mode_name, basedir, saving_mode, force_reload, custom_save_filenames\n",
    "# custom_suffix: str = '_withNewKamranExportedReplays'\n",
    "\n",
    "# custom_suffix: str = '_withNewComputedReplays'\n",
    "# custom_suffix: str = '_withNewComputedReplays-qclu_[1, 2]-frateThresh_5.0'\n",
    "\n",
    "# custom_save_filenames = {\n",
    "#     'pipeline_pkl':f'loadedSessPickle{custom_suffix}.pkl',\n",
    "#     'global_computation_pkl':f\"global_computation_results{custom_suffix}.pkl\",\n",
    "#     'pipeline_h5':f'pipeline{custom_suffix}.h5',\n",
    "# }\n",
    "# print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "# custom_save_filepaths = {k:v for k, v in custom_save_filenames.items()}\n",
    "\n",
    "# # ==================================================================================================================== #\n",
    "# # PIPELINE LOADING                                                                                                     #\n",
    "# # ==================================================================================================================== #\n",
    "# # load the custom saved outputs\n",
    "# active_pickle_filename = custom_save_filenames['pipeline_pkl'] # 'loadedSessPickle_withParameters.pkl'\n",
    "# print(f'active_pickle_filename: \"{active_pickle_filename}\"')\n",
    "# # assert active_pickle_filename.exists()\n",
    "# active_session_h5_filename = custom_save_filenames['pipeline_h5'] # 'pipeline_withParameters.h5'\n",
    "# print(f'active_session_h5_filename: \"{active_session_h5_filename}\"')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "## DO NOT allow recompute if the file doesn't exist!!\n",
    "# Computing loaded session pickle file results : \"W:/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_withNewComputedReplays.pkl\"... done.\n",
    "# Failure loading W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle_withNewComputedReplays.pkl.\n",
    "# proposed_load_pkl_path = basedir.joinpath(active_pickle_filename).resolve()\n",
    "\n",
    "## INPUTS: widget.active_global_pkl, widget.active_global_pkl\n",
    "\n",
    "if active_session_pickle_file_widget.active_global_pkl is None:\n",
    "    skip_global_load: bool = True\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skip_global_load: {skip_global_load}')\n",
    "else:\n",
    "    skip_global_load: bool = False\n",
    "    override_global_computation_results_pickle_path = active_session_pickle_file_widget.active_global_pkl.resolve()\n",
    "    Assert.path_exists(override_global_computation_results_pickle_path)\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "proposed_load_pkl_path = active_session_pickle_file_widget.active_local_pkl.resolve()\n",
    "Assert.path_exists(proposed_load_pkl_path)\n",
    "proposed_load_pkl_path\n",
    "\n",
    "custom_suffix: str = active_session_pickle_file_widget.try_extract_custom_suffix()\n",
    "print(f'custom_suffix: \"{custom_suffix}\"')\n",
    "\n",
    "## OUTPUTS: custom_suffix, proposed_load_pkl_path, (override_global_computation_results_pickle_path, skip_global_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_name_includelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eef26a",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: proposed_load_pkl_path\n",
    "# assert proposed_load_pkl_path.exists(), f\"for a saved custom the file must exist, but proposed_load_pkl_path: '{proposed_load_pkl_path}' does not!\"\n",
    "\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "\n",
    "with set_posix_windows():\n",
    "    curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                            computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                            saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                            skip_extended_batch_computations=True, debug_print=False, fail_on_exception=False, active_pickle_filename=proposed_load_pkl_path, \n",
    "                                            override_parameters_flat_keypaths_dict=override_parameters) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "print(f'Pipeline loaded from custom pickle!!')\n",
    "## OUTPUT: curr_active_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_session_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61969df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data_mode_name = 'bapun'\n",
    "\n",
    "# curr_active_pipeline.get_session_additional_parameters_context()\n",
    "# curr_active_pipeline.session_data_type\n",
    "\n",
    "DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()[active_data_mode_name]\n",
    "DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()[active_data_mode_name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488363cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.BaseDataSessionFormats import HardcodedProcessingParameters\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "\n",
    "hardcoded_params: HardcodedProcessingParameters = BapunDataSessionFormatRegisteredClass._get_session_specific_parameters(session_context=curr_active_pipeline.get_session_context())\n",
    "hardcoded_params\n",
    "\n",
    "hardcoded_params.decoder_building_session_names\n",
    "hardcoded_params.non_global_activity_session_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@define(slots=False, eq=False, repr=False)\n",
    "class position_decoding_Parameters(HDF_SerializationMixin, AttrsBasedClassHelperMixin, BaseGlobalComputationParameters):\n",
    "    \"\"\" Docstring for position_decoding_Parameters. \n",
    "    \"\"\"\n",
    "    override_decoding_time_bin_size: Optional[float] = serialized_attribute_field(default=None)\n",
    "    ## PARAMS - these are class properties\n",
    "    override_decoding_time_bin_size_PARAM = param.Number(default=None, doc='override_decoding_time_bin_size param', label='override_decoding_time_bin_size')\n",
    "    # HDFMixin Conformances ______________________________________________________________________________________________ #\n",
    "    def to_hdf(self, file_path, key: str, **kwargs):\n",
    "        \"\"\" Saves the object to key in the hdf5 file specified by file_path\"\"\"\n",
    "        super().to_hdf(file_path, key=key, **kwargs)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "session_specific_parameters: Dict[IdentifyingContext, HardcodedProcessingParameters] = {\n",
    "\n",
    "    IdentifyingContext(format_name= 'bapun', animal= 'RatN', session_name= 'Day4OpenField'): HardcodedProcessingParameters(decoder_building_session_names=['roam', 'sprinkle', 'maze_GLOBAL'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   ),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "    IdentifyingContext(format_name= 'bapun', animal= 'RatN', session_name= 'Day5TwoNovel'): HardcodedProcessingParameters(decoder_building_session_names=['maze1', 'maze2', 'maze_GLOBAL'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   ),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.PipelineParameterClassTemplating import GlobalComputationParametersAttrsClassTemplating\n",
    "\n",
    "registered_merged_computation_function_default_kwargs_dict, code_str, nested_classes_dict, (imports_dict, imports_list, imports_string) = GlobalComputationParametersAttrsClassTemplating.main_generate_params_classes(curr_active_pipeline=curr_active_pipeline)\n",
    "\n",
    "code_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e10fea",
   "metadata": {},
   "source": [
    "## from `batch_load_session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From `pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing.batch_load_session` 2025-02-26 09:09 \n",
    "kwargs = {}\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "override_parameters_flat_keypaths_dict = override_parameters\n",
    "active_pickle_filename = proposed_load_pkl_path\n",
    "active_session_computation_configs = None\n",
    "fail_on_exception: bool = False\n",
    "\n",
    "saving_mode = PipelineSavingScheme.init(saving_mode)\n",
    "epoch_name_includelist = kwargs.get('epoch_name_includelist', ['maze1','maze2','maze'])\n",
    "# epoch_name_includelist = ['maze', 'sprinkle']\n",
    "# epoch_name_includelist = ['pre', 'maze', 'sprinkle', 'post']\n",
    "\n",
    "\n",
    "debug_print = kwargs.get('debug_print', False)\n",
    "assert 'skip_save' not in kwargs, f\"use saving_mode=PipelineSavingScheme.SKIP_SAVING instead\"\n",
    "# skip_save = kwargs.get('skip_save', False)\n",
    "# active_pickle_filename = kwargs.get('active_pickle_filename', 'loadedSessPickle.pkl')\n",
    "\n",
    "# active_session_computation_configs = kwargs.get('active_session_computation_configs', None)\n",
    "# computation_functions_name_includelist = kwargs.get('computation_functions_name_includelist', None)\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "## Begin main run of the pipeline (load or execute):\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties,\n",
    "    override_basepath=Path(basedir), force_reload=force_reload, active_pickle_filename=active_pickle_filename, skip_save_on_initial_load=True, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n",
    "\n",
    "was_loaded_from_file: bool =  curr_active_pipeline.has_associated_pickle # True if pipeline was loaded from an existing file, False if it was created fresh\n",
    "\n",
    "# Get the previous configs:\n",
    "# curr_active_pipeline.filtered_sessions\n",
    "# ['filtered_session_names', 'filtered_contexts', 'filtered_epochs', 'filtered_sessions']\n",
    "# loaded_session_filter_configurations = {k:v.filter_config['filter_function'] for k,v in curr_active_pipeline.active_configs.items()}\n",
    "# loaded_pipeline_computation_configs = {k:v.computation_config for k,v in curr_active_pipeline.active_configs.items()}\n",
    "\n",
    "\n",
    "## Build updated ones from the current configs:\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "if debug_print:\n",
    "    print(f'active_session_filter_configurations: {active_session_filter_configurations}')\n",
    "\n",
    "## Skip the filtering, it used to be performed bere but NOT NOW\n",
    "\n",
    "## TODO 2023-05-16 - set `curr_active_pipeline.active_configs[a_name].computation_config.pf_params.computation_epochs = curr_laps_obj` equivalent\n",
    "## TODO 2023-05-16 - determine appropriate binning from `compute_short_long_constrained_decoders` so it's automatically from the long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafaf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_session_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6081a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_name_includelist = kwargs.get('epoch_name_includelist', ['maze1','maze2','maze'])\n",
    "# epoch_name_includelist = ['roam', 'sprinkle']\n",
    "epoch_name_includelist = ['pre', 'roam', 'sprinkle', 'post']\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef85735",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_on_exception: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43096f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16a638",
   "metadata": {
    "tags": [
     "collect-2025-09-19"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "if active_session_computation_configs is None:\n",
    "    \"\"\"\n",
    "    If there are is provided computation config, get the default:\n",
    "    \"\"\"\n",
    "    # ## Compute shared grid_bin_bounds for all epochs from the global positions:\n",
    "    # global_unfiltered_session = curr_active_pipeline.sess\n",
    "    # # ((22.736279243974774, 261.696733348342), (49.989466271998936, 151.2870218547401))\n",
    "    # first_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[0]]\n",
    "    # # ((22.736279243974774, 261.696733348342), (125.5644705153173, 151.21507349463707))\n",
    "    # second_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[1]]\n",
    "    # # ((71.67666779621361, 224.37820920766043), (110.51617463644946, 151.2870218547401))\n",
    "\n",
    "    # grid_bin_bounding_session = first_filtered_session\n",
    "    # grid_bin_bounds = PlacefieldComputationParameters.compute_grid_bin_bounds(grid_bin_bounding_session.position.x, grid_bin_bounding_session.position.y)\n",
    "\n",
    "    ## OR use no grid_bin_bounds meaning they will be determined dynamically for each epoch:\n",
    "    # grid_bin_bounds = None\n",
    "    # time_bin_size = 0.03333 #1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = 0.1 # 10 fps\n",
    "    time_bin_size = kwargs.get('time_bin_size', 0.03333) # 0.03333 = 1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = kwargs.get('time_bin_size', 0.1) # 10 fps\n",
    "\n",
    "    # lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "    # assert lap_estimation_parameters is not None\n",
    "    active_session_computation_configs: List[DynamicContainer] = active_data_mode_registered_class.build_active_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=time_bin_size, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # , grid_bin_bounds=grid_bin_bounds\n",
    "\n",
    "else:\n",
    "    # Use the provided `active_session_computation_configs`:\n",
    "    assert 'time_bin_size' not in kwargs, f\"time_bin_size kwarg provided but will not be used because a custom active_session_computation_configs was provided as well.\"\n",
    "\n",
    "active_session_computation_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation_functions_name_includelist = []\n",
    "computation_functions_name_includelist = ['pf_computation','firing_rate_trends', 'position_decoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Setup Computation Functions to be executed:\n",
    "if computation_functions_name_includelist is None:\n",
    "    # includelist Mode:\n",
    "    computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        # '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "    computation_functions_name_excludelist=None\n",
    "else:\n",
    "    print(f'using provided computation_functions_name_includelist: {computation_functions_name_includelist}')\n",
    "    computation_functions_name_excludelist=None\n",
    "\n",
    "## For every computation config we build a fake (duplicate) filter config).\n",
    "# OVERRIDE WITH TRUE:\n",
    "# curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps['use_direction_dependent_laps'] = True # override with True\n",
    "lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "assert lap_estimation_parameters is not None\n",
    "use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', False) # whether to split the laps into left and right directions\n",
    "# use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', True) # whether to split the laps into left and right directions\n",
    "\n",
    "if (use_direction_dependent_laps or (len(active_session_computation_configs) > 3)):\n",
    "    lap_direction_suffix_list = ['_odd', '_even', '_any'] # ['maze1_odd', 'maze1_even', 'maze1_any', 'maze2_odd', 'maze2_even', 'maze2_any', 'maze_odd', 'maze_even', 'maze_any']\n",
    "    # lap_direction_suffix_list = ['_odd', '_even', ''] # no '_any' prefix, instead reuses the existing names\n",
    "    # assert len(lap_direction_suffix_list) == len(active_session_computation_configs), f\"len(lap_direction_suffix_list): {len(lap_direction_suffix_list)}, len(active_session_computation_configs): {len(active_session_computation_configs)}, \"\n",
    "else:\n",
    "    print(f'not using direction-dependent laps.')\n",
    "    lap_direction_suffix_list = ['']\n",
    "\n",
    "# active_session_computation_configs: this should contain three configs, one for each Epoch    \n",
    "active_session_computation_configs = [deepcopy(a_config) for a_config in active_session_computation_configs]\n",
    "\n",
    "#TODO 2024-10-30 13:22: - [ ] This is where we should override the params using `override_parameters_flat_keypaths_dict`\n",
    "# if override_parameters_flat_keypaths_dict is not None:\n",
    "# \tfor a_config in active_session_computation_configs:\n",
    "# \t\tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\ta_config.set_by_keypath(k, deepcopy(v))\n",
    "# \t\t\texcept Exception as e:\n",
    "# \t\t\t\t# raise e\n",
    "# \t\t\t\tprint(f'cannot set_by_keypath: {k} -- error: {e}. Skipping for now.')\n",
    "\n",
    "assert len(lap_direction_suffix_list) == len(active_session_computation_configs)\n",
    "updated_active_session_pseudo_filter_configs = {} # empty list, woot!\n",
    "\n",
    "\n",
    "for a_computation_suffix_name, a_computation_config in zip(lap_direction_suffix_list, active_session_computation_configs): # these should NOT be the same length: lap_direction_suffix_list: ['_odd', '_even', '_any']\n",
    "    # We need to filter and then compute with the appropriate config iteratively.\n",
    "    for a_filter_config_name, a_filter_config_fn in active_session_filter_configurations.items():\n",
    "        # TODO: Build a context:\n",
    "        a_combined_name: str = f'{a_filter_config_name}{a_computation_suffix_name}'\n",
    "        # if a_computation_suffix_name != '':\n",
    "        updated_active_session_pseudo_filter_configs[a_combined_name] = deepcopy(a_filter_config_fn) # this copy is just so that the values are recomputed with the appropriate config. This is a HACK\n",
    "    # end for filter_configs\n",
    "\n",
    "    ## Actually do the filtering now. We have \n",
    "    curr_active_pipeline.filter_sessions(updated_active_session_pseudo_filter_configs, changed_filters_ignore_list=['maze1','maze2','maze'], debug_print=False)\n",
    "\n",
    "    ## TODO 2023-01-15 - perform_computations for all configs!!\n",
    "    #TODO 2024-10-30 13:22: - [ ] This is where we should override the params\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "    # \t\ta_filter_config_fn.set_by_keypath(k, deepcopy(v))\n",
    "\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tcurr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n",
    "    #TODO 2023-10-31 14:58: - [ ] This is where the computations are being done multiple times!\n",
    "    #TODO 2023-11-13 14:23: - [ ] With this approach, we can't actually properly filter the computation_configs for the relevant sessions ahead of time because they are calculated for a single computation config but across all sessions at once.\n",
    "    curr_active_pipeline.perform_computations(a_computation_config, computation_functions_name_includelist=computation_functions_name_includelist, computation_functions_name_excludelist=computation_functions_name_excludelist, fail_on_exception=fail_on_exception, debug_print=debug_print) #, overwrite_extant_results=False  ], fail_on_exception=True, debug_print=False)\n",
    "\n",
    "    if override_parameters_flat_keypaths_dict is not None:\n",
    "        curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_extended_batch_computations = False\n",
    "fail_on_exception = True\n",
    "if not skip_extended_batch_computations:\n",
    "    batch_extended_computations(curr_active_pipeline, include_global_functions=False, fail_on_exception=fail_on_exception, progress_print=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation'], debug_print=False, fail_on_exception=False) # includelist: ['_perform_baseline_placefield_computation']\n",
    "\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.prepare_for_display(root_output_dir=global_data_root_parent_path.joinpath('Output'), should_smooth_maze=True) # TODO: pass a display config\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to do `curr_active_pipeline.prepare_for_display(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "if not saving_mode.shouldSave:\n",
    "    print(f'saving_mode.shouldSave == False, so not saving at the end of batch_load_session')\n",
    "\n",
    "## Load pickled global computations:\n",
    "# If previously pickled global results were saved, they will typically no longer be relevent if the pipeline was recomputed. We need a system of invalidating/versioning the global results when the other computations they depend on change.\n",
    "# Maybe move into `batch_extended_computations(...)` or integrate with that somehow\n",
    "# curr_active_pipeline.load_pickled_global_computation_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e374c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "skip_global_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cb3e1",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Global computations loading:                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "# Loads saved global computations that were saved out via: `custom_save_filepaths['global_computation_pkl'] = curr_active_pipeline.save_global_computation_results(override_global_pickle_filename=custom_save_filenames['global_computation_pkl'])`\n",
    "## INPUTS: custom_save_filenames\n",
    "## INPUTS: curr_active_pipeline, (override_global_computation_results_pickle_path, skip_global_load), extended_computations_include_includelist\n",
    "\n",
    "if skip_global_load:\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skipping global load because skip_global_load==True')\n",
    "else:\n",
    "    # override_global_computation_results_pickle_path = custom_save_filenames['global_computation_pkl']\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "# Pre-load ___________________________________________________________________________________________________________ #\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list\n",
    "\n",
    "# Try Unpickling Global Computations to update pipeline ______________________________________________________________ #\n",
    "if (not force_reload) and (not skip_global_load): # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # INPUTS: override_global_computation_results_pickle_path\n",
    "        with set_posix_windows():\n",
    "            sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(override_global_computation_results_pickle_path=override_global_computation_results_pickle_path,\n",
    "                                                                                            allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist, ) # is new\n",
    "            print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "            did_any_paths_change: bool = curr_active_pipeline.post_load_fixup_sess_basedirs(updated_session_basepath=deepcopy(basedir)) ## use INPUT: basedir\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'force_reload: {force_reload}, saving_mode: {saving_mode}')\n",
    "force_reload\n",
    "saving_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e54ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: curr_active_pipeline.global_computation_results_pickle_path, skip_global_load\n",
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "print(f'override_pickle_path = \"{curr_active_pipeline.pickle_path}\",\\nactive_pickle_filename = \"{curr_active_pipeline.pickle_path.name}\"')\n",
    "print(f'override_global_pickle_path = \"{curr_active_pipeline.global_computation_results_pickle_path}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9674fd9",
   "metadata": {},
   "source": [
    "## OUTPUTS: `curr_active_pipeline`  0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣0️⃣ RESUME Normal Pipeline Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f755b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 0️⃣ Shared Post-Pipeline load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load",
     "run-main",
     "end-run"
    ]
   },
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Lab'\n",
    " \n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        BATCH_DATE_TO_USE = f'{BATCH_DATE_TO_USE}{custom_suffix}'\n",
    "        print(f'Adding custom suffix: \"{custom_suffix}\" - BATCH_DATE_TO_USE: \"{BATCH_DATE_TO_USE}\"')\n",
    "except NameError as err:\n",
    "    custom_suffix = None\n",
    "    print(f'NO CUSTOM SUFFIX.')\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                           r\"K:\\scratch\\collected_outputs\",\n",
    "                                                           '/Users/pho/data/collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "all",
     "run-load",
     "run-main",
     "end-run",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607a444",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# / 🛑 End Run Section 🛑\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🎨 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "all",
     "run-group-display",
     "run-spike_raster_window_test",
     "end-run",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5423e",
   "metadata": {},
   "source": [
    "## `LauncherWidget`: GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df7ce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Display import DisplayFunctionItem\n",
    "from pyphocorehelpers.gui.Qt.tree_helpers import find_tree_item_by_text\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_str: str = curr_active_pipeline.get_complete_session_identifier_string()\n",
    "widget.setWindowTitle(f'Spike3D Launcher: {session_id_str}')\n",
    "treeWidget.root\n",
    "# curr_active_pipeline.get_session_additional_parameters_context()\n",
    "# curr_active_pipeline.get_complete_session_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5dc3b3",
   "metadata": {},
   "source": [
    "## ✅ 2025-09-19 - Clean programmmatic figure outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c3103",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.figure_management import PhoActiveFigureManager2D, capture_new_figures_decorator\n",
    "fig_man = PhoActiveFigureManager2D(name=f'fig_man') # Initialize a new figure manager\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import programmatic_render_to_file, programmatic_display_to_PDF, extract_figures_from_display_function_output\n",
    "\n",
    "fig_man.close_all()\n",
    "\n",
    "# subset_includelist = ['maze1', 'maze2', 'maze_GLOBAL'] # Day5TwoNovel\n",
    "# subset_includelist = ['roam', 'sprinkle'] # Day4\n",
    "\n",
    "subset_includelist = hardcoded_params.decoder_building_session_names\n",
    "print(f'subset_includelist: {subset_includelist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e400f3",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "# display_fn_kwargs = dict(subplots=(None, 9))\n",
    "display_fn_kwargs = dict(subplots=(None, 5))\n",
    "\n",
    "# _out = dict()\n",
    "# _out['_display_2d_placefield_result_plot_ratemaps_2D'] = curr_active_pipeline.display(display_function='_display_2d_placefield_result_plot_ratemaps_2D', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatS',session_name='Day5TwoNovel',filter_name='maze1'), **display_fn_kwargs) # _display_2d_placefield_result_plot_ratemaps_2D\n",
    "# _out['_display_2d_placefield_result_plot_ratemaps_2D'] = curr_active_pipeline.display(display_function='_display_2d_placefield_result_plot_ratemaps_2D', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatS',session_name='Day5TwoNovel',filter_name='maze2'), **display_fn_kwargs) # _display_2d_placefield_result_plot_ratemaps_2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a03d6",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "_out_list = programmatic_render_to_file(curr_active_pipeline=curr_active_pipeline, curr_display_function_name='_display_2d_placefield_result_plot_ratemaps_2D', subset_includelist=subset_includelist, \n",
    "                                        write_vector_format=True, write_png=True, debug_print=True, **display_fn_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a3369",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "_out_list = programmatic_render_to_file(curr_active_pipeline=curr_active_pipeline, curr_display_function_name='_display_2d_placefield_occupancy', subset_includelist=subset_includelist, \n",
    "                                        write_vector_format=True, write_png=True, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a240b9",
   "metadata": {},
   "source": [
    "## `Spike3DRasterWindowWidget` Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "from neuropy.core.flattened_spiketrains import SpikesAccessor\n",
    "\n",
    "active_spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "\n",
    "# INLINEING `build_spikes_data_values_from_df`: ______________________________________________________________________ #\n",
    "# curr_spike_x, curr_spike_y, curr_spike_pens, all_scatterplot_tooltips_kwargs, all_spots, curr_n = cls.build_spikes_data_values_from_df(spikes_df, config_fragile_linear_neuron_IDX_map, is_spike_included=is_spike_included, should_return_data_tooltips_kwargs=should_return_data_tooltips_kwargs, **kwargs)\n",
    "# All units at once approach:\n",
    "active_time_variable_name = active_spikes_df.spikes.time_variable_name\n",
    "print(f'active_time_variable_name: {active_time_variable_name}')\n",
    "if active_time_variable_name != 't': \n",
    "    active_spikes_df = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(active_spikes_df, required_columns_synonym_dict={\"t\":{active_time_variable_name,'t_rel_seconds', 't_seconds'}})\n",
    "    active_spikes_df = active_spikes_df.drop(columns=[active_time_variable_name], inplace=False) ## drop the old column    \n",
    "    active_time_variable_name = 't' ## get the new one\n",
    "    active_spikes_df.spikes.set_time_variable_name('t')\n",
    "    # default_datapoint_column_names = [active_spikes_df.spikes.time_variable_name, 'aclu', 'fragile_linear_neuron_IDX']\n",
    "    # active_datapoint_column_names = default_datapoint_column_names\n",
    "    active_spikes_df\n",
    "    \n",
    "# active_spikes_df.spikes.time_variable_name\n",
    "# active_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy only the relevent columns so filtering is easier:\n",
    "filtered_spikes_df = active_spikes_df[[active_time_variable_name, 'visualization_raster_y_location',  'visualization_raster_emphasis_state', 'aclu', 'fragile_linear_neuron_IDX']].copy()\n",
    "\n",
    "# active_spikes_df.spikes.time_variable_name,\n",
    "# 't_seconds'\n",
    "\n",
    "filtered_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eea58",
   "metadata": {
    "tags": [
     "all",
     "spike_raster_window",
     "display",
     "gui",
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.ScrollableRasterViewOwnerMixin import ScrollableRasterViewOwnerMixin\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _setup_spike_raster_window_for_debugging\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import ScatterItemData # used in `NewSimpleRaster`\n",
    "\n",
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
    "spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170607c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview_overview_scatter_plot: pg.ScatterPlotItem  = active_2d_plot.plots.preview_overview_scatter_plot # ScatterPlotItem \n",
    "# preview_overview_scatter_plot.setDownsampling(auto=True, method='subsample', dsRate=10)\n",
    "main_graphics_layout_widget: pg.GraphicsLayoutWidget = active_2d_plot.ui.main_graphics_layout_widget\n",
    "wrapper_layout: pg.QtWidgets.QVBoxLayout = active_2d_plot.ui.wrapper_layout\n",
    "main_content_splitter = active_2d_plot.ui.main_content_splitter # QSplitter\n",
    "layout = active_2d_plot.ui.layout\n",
    "background_static_scroll_window_plot = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "active_window_container_layout = active_2d_plot.ui.active_window_container_layout # GraphicsLayout, first item of `main_graphics_layout_widget` -- just the active raster window I think, there is a strange black space above it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c979d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('active_2d_plot', active_2d_plot, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(active_2d_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b71f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('active_2d_plot.ui', active_2d_plot.ui, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spike_raster_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_window.ui.wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.add_docked_marginal_track(curr_active_pipeline.sess.epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.list_all_rendered_intervals()\n",
    "active_2d_plot.add_laps_intervals(curr_active_pipeline.sess)\n",
    "\n",
    "active_2d_plot.add_rendered_intervals()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.sess.epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56888f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import SessionEpochs2DRenderTimeEpochs\n",
    "from neuropy.core.epoch import ensure_dataframe, ensure_Epoch, Epoch, EpochsAccessor\n",
    "# SessionEpochs2DRenderTimeEpochs.add_render_time_epochs(curr_sess=curr_active_pipeline.sess.epochs, destination_plot=active_2d_plot)\n",
    "\n",
    "active_ds = ensure_dataframe(curr_active_pipeline.sess.epochs)\n",
    "active_ds\n",
    "\n",
    "_out = active_2d_plot.add_rendered_intervals(active_ds, 'SessionEpochs')\n",
    "\n",
    "# num_epochs = len(curr_active_pipeline.sess.epochs)\n",
    "\n",
    "# pen_colors = {'pre': pg.mkColor('purple'), 'roam': pg.mkColor('red'), 'sprinkle': pg.mkColor('red'), 'post': pg.mkColor('purple')}\n",
    "# brush_colors = {'pre': pg.mkColor('purple'), 'roam': pg.mkColor('red'), 'sprinkle': pg.mkColor('red'), 'post': pg.mkColor('purple')}\n",
    "\n",
    "# pen_color = list(pen_colors.values())\n",
    "# brush_color = list(brush_colors.values())\n",
    "# for a_pen_color in pen_color:\n",
    "#     a_pen_color.setAlphaF(0.8)\n",
    "\n",
    "# for a_brush_color in brush_color:\n",
    "#     a_brush_color.setAlphaF(0.5)\n",
    "\n",
    "\n",
    "# active_df = cls._update_df_visualization_columns(active_df, y_location, height, pen_color, brush_color, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.rendered_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571754a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ds = ensure_dataframe(curr_active_pipeline.sess.epochs)\n",
    "global_epoch_only = ensure_Epoch(active_ds[active_ds['label'] == 'maze_GLOBAL'])\n",
    "global_epoch_only\n",
    "\n",
    "curr_active_pipeline.sess\n",
    "sess.position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359264c",
   "metadata": {},
   "source": [
    "## 2025-09-19 - Add Session Paradigm Epochs with a different color for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e07415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.EpochsEditorItem import EpochsEditor # perform_plot_laps_diagnoser\n",
    "import matplotlib.pyplot as plt\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers, ColorFormatConverter\n",
    "\n",
    "def generate_colors(n_epoch):\n",
    "    cmap = plt.get_cmap('tab20', n_epoch)\n",
    "    return [plt.matplotlib.colors.rgb2hex(cmap(i)) for i in range(n_epoch)]\n",
    "\n",
    "\n",
    "\n",
    "sess = curr_active_pipeline.sess # global_session\n",
    "\n",
    "# pos_df = sess.compute_position_laps() # ensures the laps are computed if they need to be:\n",
    "position_obj = deepcopy(sess.position)\n",
    "position_obj.compute_higher_order_derivatives()\n",
    "pos_df = position_obj.compute_smoothed_position_info(N=20) ## Smooth the velocity curve to apply meaningful logic to it\n",
    "pos_df = position_obj.to_dataframe()\n",
    "# Drop rows with missing data in columns: 't', 'velocity_x_smooth' and 2 other columns. This occurs from smoothing\n",
    "pos_df = pos_df.dropna(subset=['t', 'x_smooth', 'velocity_x_smooth', 'acceleration_x_smooth']).reset_index(drop=True)\n",
    "# curr_laps_df = sess.laps.to_dataframe()\n",
    "\n",
    "curr_paradigm_df = ensure_dataframe(sess.paradigm)\n",
    "curr_paradigm_df = curr_paradigm_df[np.logical_not(np.isin(curr_paradigm_df['label'], ['maze_GLOBAL', 'maze']))] ## exclude the global epoch\n",
    "n_epochs: int = len(curr_paradigm_df)\n",
    "# epoch_color_strs: List[str] = generate_colors(n_epochs)\n",
    "epoch_color_strs: List[str] = [ColorFormatConverter.qColor_to_hexstring(v, include_alpha=False) for v in ColormapHelpers.mpl_to_pg_colormap(mpl_cmap_name='tab20', resolution=n_epochs).getColors(mode='qcolor')]\n",
    "curr_paradigm_df['lap_color'] = \"#10FF44\"\n",
    "curr_paradigm_df['lap_color'] = epoch_color_strs\n",
    "curr_paradigm_df['lap_accent_color'] = '#FFFFFF'\n",
    "curr_paradigm_df\n",
    "## Create a new window:\n",
    "custom_epoch_label_kwargs = dict(epoch_label_position=0.05, epoch_label_rotateAxis=(0,0), epoch_label_anchor=(0.0, 1.0))\n",
    "epochs_editor = EpochsEditor.init_laps_diagnoser(pos_df, curr_paradigm_df, include_velocity=False, include_accel=False, span=(0.05, 0.95), movable=False, **custom_epoch_label_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_editor.plots\n",
    "# epochs_editor.rebuild_epoch_regions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.plots.main_plot_widget\n",
    "\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "main_plot_widget.setMinimumHeight(20.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout\n",
    "# main_graphics_layout_widget.ci # GraphicsLayout\n",
    "main_graphics_layout_widget.ci.childItems()\n",
    "# main_graphics_layout_widget.setHidden(True) ## hides too much\n",
    "main_graphics_layout_widget.setHidden(False)\n",
    "\n",
    "# main_graphics_layout_widget\n",
    "\n",
    "active_window_container_layout.setBorder(pg.mkPen('yellow', width=4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout.allChildItems()\n",
    "active_window_container_layout.setPreferredHeight(200.0)\n",
    "active_window_container_layout.setMaximumHeight(800.0)\n",
    "active_window_container_layout.setSpacing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stretch factors to control priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(0, 400)  # Plot1: lowest priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(1, 2)  # Plot2: mid priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(2, 2)  # Plot3: highest priority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ParameterTreeWidget import create_parameter_tree_widget\n",
    "# win, param_tree = create_pipeline_filter_parameter_tree()\n",
    "win, param_tree = create_parameter_tree_widget(curr_active_pipeline.get_all_parameters())\n",
    "win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f61dd",
   "metadata": {
    "tags": [
     "_perform_plot_multi_decoder_meas_pred_position_track",
     "active-2025-01-16"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DecoderIdentityColors\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_plot_multi_decoder_meas_pred_position_track\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "## Build the new dock track:\n",
    "dock_identifier: str = 'Continuous Decoding Performance'\n",
    "ts_widget, fig, ax_list, dDisplayItem = active_2d_plot.add_new_matplotlib_render_plot_widget(name=dock_identifier)\n",
    "## Get the needed data:\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n",
    "\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "all_directional_continuously_decoded_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {k:v for k, v in (continuously_decoded_dict or {}).items() if k in TrackTemplates.get_decoder_names()} ## what is plotted in the `f'{a_decoder_name}_ContinuousDecode'` rows by `AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand`\n",
    "## OUT: all_directional_continuously_decoded_dict\n",
    "## Draw the position meas/decoded on the plot widget\n",
    "## INPUT: fig, ax_list, all_directional_continuously_decoded_dict, track_templates\n",
    "\n",
    "_out_artists =  _perform_plot_multi_decoder_meas_pred_position_track(curr_active_pipeline, fig, ax_list, desired_time_bin_size=0.058, enable_flat_line_drawing=True)\n",
    "\n",
    "\n",
    "## sync up the widgets\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget(dock_identifier, sync_mode=SynchronizedPlotMode.TO_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38325e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['truth_decoder_name'] = pos_df['truth_decoder_name'].fillna('')\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_color_dict: Dict[types.DecoderName, str] = DecoderIdentityColors.build_decoder_color_dict()\n",
    "\n",
    "decoded_pos_line_kwargs = dict(lw=1.0, color='gray', alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "inactive_decoded_pos_line_kwargs = dict(lw=0.3, alpha=0.2, marker='.', markersize=2, animated=False)\n",
    "active_decoded_pos_line_kwargs = dict(lw=1.0, alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "\n",
    "\n",
    "_out_data = {}\n",
    "_out_data_plot_kwargs = {}\n",
    "# curr_active_pipeline.global_computation_results.t\n",
    "for a_decoder_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    a_continuously_decoded_result = all_directional_continuously_decoded_dict[a_decoder_name]\n",
    "    a_decoder_color = decoder_color_dict[a_decoder_name]\n",
    "    \n",
    "    assert len(a_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = a_continuously_decoded_result.time_bin_containers[0]\n",
    "    time_window_centers = time_bin_containers.centers\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "    a_marginal_x = a_continuously_decoded_result.marginal_x_list[0]\n",
    "    # active_time_window_variable = a_decoder.active_time_window_centers\n",
    "    active_time_window_variable = time_window_centers\n",
    "    active_most_likely_positions_x = a_marginal_x['most_likely_positions_1D'] # a_decoder.most_likely_positions[:,0].T\n",
    "    _out_data[a_decoder_name] = pd.DataFrame({'t': time_window_centers, 'x': active_most_likely_positions_x, 'binned_time': np.arange(len(time_window_centers))})\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "    _out_data[a_decoder_name]['is_active_decoder_time'] = (_out_data[a_decoder_name]['truth_decoder_name'].fillna('', inplace=False) == a_decoder_name)\n",
    "\n",
    "    # is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "    active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "    active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "    ## could fill y with np.nan instead of getting shorter?\n",
    "    _out_data_plot_kwargs[a_decoder_name] = (dict(x=active_decoder_time_points, y=active_decoder_most_likely_positions_x, color=a_decoder_color, **active_decoded_pos_line_kwargs), dict(x=active_decoder_inactive_time_points, y=active_decoder_inactive_most_likely_positions_x, color=a_decoder_color, **inactive_decoded_pos_line_kwargs))\n",
    "\n",
    "_out_data_plot_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8972db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "\n",
    "# is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "\n",
    "_out_data[a_decoder_name] = ((active_decoder_time_points, active_decoder_most_likely_positions_x), (active_decoder_inactive_time_points, active_decoder_inactive_most_likely_positions_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_dfs = partition_df_dict(pos_df, partitionColumn='truth_decoder_name')\n",
    "\n",
    "a_decoder_name: str = 'short_LR'\n",
    "a_binned_time_grouped_df = partitioned_dfs[a_decoder_name].groupby('binned_time', axis='index', dropna=True)\n",
    "a_binned_time_grouped_df = a_binned_time_grouped_df.median().dropna(axis='index', subset=['x']) ## without the `.dropna(axis='index', subset=['x'])` part it gets an exhaustive df for all possible values of 'binned_time', even those not listed\n",
    "\n",
    "a_matching_binned_times = a_binned_time_grouped_df.reset_index(drop=False)['binned_time']\n",
    "a_matching_binned_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into two dfs for each decoder -- the supported and the unsupported\n",
    "partition\n",
    "\n",
    "PandasHelpers.safe_pandas_get_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.dropna(axis='index', subset=['lap', 'truth_decoder_name'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df: pd.DataFrame = global_laps_obj.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import find_epochs_overlapping_other_epochs\n",
    "\n",
    "## INPUTS: global_laps\n",
    "_out_split_pseudo2D_posteriors_dict = {}\n",
    "_out_split_pseudo2D_out_dict = {}\n",
    "pre_filtered_col_names = ['pre_filtered_most_likely_position_indicies', 'pre_filtered_most_likely_position'] # 'pre_filtered_time_bin_containers', 'pre_filtered_p_x_given_n', \n",
    "post_filtered_col_names = [a_col_name.removeprefix('pre_filtered_') for a_col_name in pre_filtered_col_names] # ['time_bin_containers', 'most_likely_position_indicies', 'most_likely_position']\n",
    "print(post_filtered_col_names)\n",
    "for a_time_bin_size, pseudo2D_decoder_continuously_decoded_result in continuously_decoded_pseudo2D_decoder_dict.items():\n",
    "    print(f'a_time_bin_size: {a_time_bin_size}')\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size] = {'pre_filtered_p_x_given_n': None, 'pre_filtered_time_bin_containers': None, 'pre_filtered_most_likely_position_indicies': None, 'pre_filtered_most_likely_position': None, \n",
    "                                                     'is_timebin_included': None, 'p_x_given_n': None} # , 'time_window_centers': None\n",
    "    # pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "    assert len(pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = pseudo2D_decoder_continuously_decoded_result.time_bin_containers[0]\n",
    "    # time_window_centers = time_bin_containers.centers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_position_indicies_list[0])\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_positions_list[0])\n",
    "    ## INPUTS: time_bin_containers, global_laps\n",
    "    left_edges = deepcopy(time_bin_containers.left_edges)\n",
    "    right_edges = deepcopy(time_bin_containers.right_edges)\n",
    "    continuous_time_binned_computation_epochs_df: pd.DataFrame = pd.DataFrame({'start': left_edges, 'stop': right_edges, 'label': np.arange(len(left_edges))})\n",
    "    is_timebin_included: NDArray = find_epochs_overlapping_other_epochs(epochs_df=continuous_time_binned_computation_epochs_df, epochs_df_required_to_overlap=deepcopy(global_laps))\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_p_x_given_n'] = p_x_given_n\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_time_bin_containers'] = time_bin_containers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['is_timebin_included'] = is_timebin_included\n",
    "    # continuous_time_binned_computation_epochs_df['is_in_laps'] = is_timebin_included\n",
    "    ## filter by whether it's included or not:\n",
    "    p_x_given_n = p_x_given_n[:, :, is_timebin_included]\n",
    "    # time_window_centers = \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['p_x_given_n'] = p_x_given_n\n",
    "    # _out_split_pseudo2D_out_dict[a_time_bin_size]['time_window_centers'] = time_window_centers[is_timebin_included]\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "\n",
    "    ## Split across the 2nd axis to make 1D posteriors that can be displayed in separate dock rows:\n",
    "    assert p_x_given_n.shape[1] == 4, f\"expected the 4 pseudo-y bins for the decoder in p_x_given_n.shape[1]. but found p_x_given_n.shape: {p_x_given_n.shape}\"\n",
    "    # split_pseudo2D_posteriors_dict = {k:np.squeeze(p_x_given_n[:, i, :]) for i, k in enumerate(('long_LR', 'long_RL', 'short_LR', 'short_RL'))}\n",
    "    _out_split_pseudo2D_posteriors_dict[a_time_bin_size] = deepcopy(p_x_given_n)\n",
    "    \n",
    "    # for a_col_name in pre_filtered_col_names:\n",
    "    #     filtered_col_name = a_col_name.removeprefix('pre_filtered_')\n",
    "    #     print(f'a_col_name: {a_col_name}, filtered_col_name: {filtered_col_name}, shape: {np.shape(_out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name])}')\n",
    "    #     _out_split_pseudo2D_out_dict[a_time_bin_size][filtered_col_name] = _out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name][is_timebin_included, :]\n",
    "        \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position_indicies'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'][:, is_timebin_included]\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'][is_timebin_included, :]\n",
    "    \n",
    "\n",
    "p_x_given_n.shape # (n_position_bins, n_decoding_models, n_time_bins) - (57, 4, 29951)\n",
    "\n",
    "## OUTPUTS: _out_split_pseudo2D_posteriors_dict, _out_split_pseudo2D_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_most_likely_position_comparsions\n",
    "\n",
    "# fig, axs = plot_most_likely_position_comparsions(pho_custom_decoder, axs=ax, sess.position.to_dataframe())\n",
    "fig, axs = plot_most_likely_position_comparsions(computation_result.computed_data['pf2D_Decoder'], computation_result.sess.position.to_dataframe(), **overriding_dict_with(lhs_dict={'show_posterior':True, 'show_one_step_most_likely_positions_plots':True}, **kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4129c9",
   "metadata": {},
   "source": [
    "# 🖼️⚓❎ Time Synchronized Plotting with position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f834b",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.Mixins.AnimalTrajectoryPlottingMixin import AnimalTrajectoryPlottingMixin\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPositionDecoderPlotter import TimeSynchronizedPositionDecoderPlotter\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_combined_time_synchronized_Bapun_decoders_window\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster, SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.SpecificDockWidgetManipulatingMixin import SpecificDockWidgetManipulatingMixin\n",
    "\n",
    "hardcoded_params: HardcodedProcessingParameters = BapunDataSessionFormatRegisteredClass._get_session_specific_parameters(session_context=curr_active_pipeline.get_session_context())\n",
    "# hardcoded_params.decoder_building_session_names\n",
    "# hardcoded_params.non_global_activity_session_names\n",
    "\n",
    "# pg.setConfigOptions(useOpenGL=True)  # do this BEFORE creating plots/widgets\n",
    "\n",
    "## Uses the `global_computation_results.computed_data['DirectionalDecodersDecoded']`\n",
    "# directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "\n",
    "# _out_container: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=hardcoded_params.non_global_activity_session_names, fixed_window_duration = 3.0)\n",
    "_out_container_new: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=hardcoded_params.non_global_activity_session_names, fixed_window_duration = 3.0,\n",
    "    directional_decoders_decode_result=directional_decoders_decode_result,\n",
    ")\n",
    "\n",
    "active_2d_plot: Spike2DRaster = _out_container_new.ui.controlling_widget\n",
    "sync_plotters: Dict[str, TimeSynchronizedPositionDecoderPlotter] = _out_container_new.ui.sync_plotters\n",
    "win: PhoDockAreaContainingWindow = _out_container_new.ui.root_dockAreaWindow\n",
    "# a_sync_plotter: TimeSynchronizedPositionDecoderPlotter = sync_plotters['roam']\n",
    "# a_sync_plotter.curr_position\n",
    "\n",
    "# ## Disable debug print to speed up animation\n",
    "# for a_plotter_name, a_plotter in sync_plotters.items():\n",
    "#     a_plotter.params.debug_print = False\n",
    "\n",
    "\n",
    "## INPUTS: _out_container, active_2d_plot, _out_container, sync_plotters, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad9ee",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "_out_pbe_tracks, _out_pbe_overview_tracks = _out_container_new.add_pbes_full_result_marginals(pbes_full_result=pbes_full_result)\n",
    "\n",
    "# grouped_dock_items_dict = _out_container_new.build_overview_and_windowed_dockgroups() ## Not quite ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241f945",
   "metadata": {},
   "source": [
    "## Reorder the dock items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e167155",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(active_2d_plot.dock_manager_widget.dynamic_display_dict.keys())\n",
    "# original_identifier_order = ['interval_overview', 'intervals', 'rasters[raster_overview]', 'rasters[raster_window]', 'global context', 'global context (overview)', 'pbe[0.06]', 'pbe[0.06] (Overview)']\n",
    "desired_identifier_order = ['interval_overview', 'rasters[raster_overview]', 'global context (overview)', 'pbe[0.06] (Overview)', 'rasters[raster_window]', 'intervals', 'global context', 'pbe[0.06]'] ## #TODO 2025-09-21 15:41: - [ ] Enforce this order programmatically?!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.dock_manager_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ac9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: active_2d_plot\n",
    "grouped_dock_items_dict = active_2d_plot.ui.dynamic_docked_widget_container.get_dockGroup_dock_dict()\n",
    "nested_dock_items = {}\n",
    "nested_dynamic_docked_widget_container_widgets = {}\n",
    "for dock_group_name, flat_group_dockitems_list in grouped_dock_items_dict.items():\n",
    "    dDisplayItem, nested_dynamic_docked_widget_container = active_2d_plot.ui.dynamic_docked_widget_container.build_wrapping_nested_dock_area(flat_group_dockitems_list, dock_group_name=dock_group_name)\n",
    "    nested_dock_items[dock_group_name] = dDisplayItem\n",
    "    nested_dynamic_docked_widget_container_widgets[dock_group_name] = nested_dynamic_docked_widget_container\n",
    "\n",
    "## OUTPUTS: nested_dock_items, nested_dynamic_docked_widget_container_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dock_items_dict = active_2d_plot.ui.dynamic_docked_widget_container.get_dockGroup_dock_dict()\n",
    "grouped_dock_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00426b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "grouped_dock_items_dict = build_overview_and_windowed_dockgroups(active_2d_plot)\n",
    "grouped_dock_items_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e67b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = active_2d_plot.dock_manager_widget.layout_dockGroups()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635e921",
   "metadata": {},
   "source": [
    "### 📈🔃❎ Exporting as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyqtgraph.exporters import ImageExporter\n",
    "from PIL import Image\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.CustomGraphicsLayoutWidget import CustomGraphicsLayoutWidget\n",
    "\n",
    "## \"playback\" refers to output video:\n",
    "desired_playback_duration: float = 8 * 60.0 # 8m\n",
    "\n",
    "# \"session\" refers to the actual recording session:\n",
    "desired_session_time_range_duration: float = (16.0 * 60.0) # 1m\n",
    "# session_start_t: float = 23170.37047485091 #20740.63578640229 # 11631.186907154472 # 11451.186907154472 # 11391.186907154472 # 7665.232126354053 # active_2d_plot.total_data_start_time + 4.0 * 60.0 # 4 minutes into start of recording ## Day 5\n",
    "session_start_t: float = 11023.018433333335 # 10843.018433333334 # active_2d_plot.total_data_start_time + 4.0 * 60.0 # 4 minutes into start of recording\n",
    "# session_start_t: float = 23170.37047485091 #20740.63578640229 # 11631.186907154472 # 11451.186907154472 # 11391.186907154472 # active_2d_plot.total_data_start_time + 4.0 * 60.0 # 4 minutes into start of recording ## day 4\n",
    "desired_session_time_range: Tuple[float, float] = (session_start_t, (session_start_t + desired_session_time_range_duration))\n",
    "\n",
    "playback_speed_factor: float = (desired_playback_duration / desired_session_time_range_duration)\n",
    "\n",
    "print(f'playback_speed_factor: {playback_speed_factor}')\n",
    "time_window_duration: float = active_2d_plot.active_window_duration\n",
    "print(f'time_window_duration: {time_window_duration}')\n",
    "\n",
    "## INPUTS: _out_container, active_2d_plot, _out_container, sync_plotters, \n",
    "desired_framerate: float = 2.0\n",
    "desired_frame_duration_sec: float = 1.0/desired_framerate\n",
    "print(f'desired_frame_duration_sec: {desired_frame_duration_sec}')\n",
    "\n",
    "# ## All Frames from entire recording (too long)\n",
    "# total_duration: float = active_2d_plot.total_data_duration\n",
    "# desired_num_total_frames: int = int(np.ceil((total_duration * desired_framerate)))\n",
    "# frame_start_indicies = np.linspace(active_2d_plot.total_data_start_time,  active_2d_plot.total_data_end_time, num=desired_num_total_frames)\n",
    "\n",
    "## Plot only for the range of interest:\n",
    "desired_num_total_frames: int = int(np.ceil((desired_session_time_range_duration * desired_framerate)))\n",
    "frame_start_indicies = np.linspace(desired_session_time_range[0], desired_session_time_range[1], num=desired_num_total_frames)\n",
    "frame_end_indices = frame_start_indicies + desired_frame_duration_sec\n",
    "\n",
    "print(f'desired_num_total_frames: {desired_num_total_frames}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Disable debug print to speed up animation\n",
    "for a_plotter_name, a_plotter in sync_plotters.items():\n",
    "    a_plotter.params.debug_print = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a285a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.active_window_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f338de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_end_timestamp = next_start_timestamp + self.animation_active_time_window.window_duration\n",
    "\n",
    "def _frame_update(frame_start_t, frame_end_t):\n",
    "    active_2d_plot.update_scroll_window_region(frame_start_t, frame_end_t, block_signals=True)\n",
    "    active_2d_plot.window_scrolled.emit(frame_start_t, frame_end_t)\n",
    "    QtWidgets.QApplication.processEvents()\n",
    "    win.repaint()\n",
    "\n",
    "\n",
    "_frame_update(desired_session_time_range[0], (desired_session_time_range[0]+time_window_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (frame_start_t, frame_end_t) in enumerate(zip(frame_start_indicies, frame_end_indices)):\n",
    "    print(f'frame[{i}]: ({frame_start_t}, {frame_end_t}):')\n",
    "    # active_2d_plot.on_window_changed(frame_start_t, frame_end_t)\n",
    "    # active_2d_plot.update_scroll_window_region(frame_start_t, frame_end_t, block_signals=True)\n",
    "    # active_2d_plot.window_scrolled.emit(frame_start_t, frame_end_t)\n",
    "    # pg.SignalProxy(driver.window_scrolled, delay=0.2, rateLimit=60, slot=drivable.on_window_changed_rate_limited)\n",
    "    # QtWidgets.QApplication.processEvents()\n",
    "    # win.repaint()\n",
    "    # _frame_update(frame_start_t, frame_end_t)\n",
    "    _frame_update(frame_start_t, (frame_start_t + time_window_duration))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3238e",
   "metadata": {},
   "source": [
    "##### NOT WORKING YET: Export both to a multi-page PDF or video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureToImageHelpers\n",
    "\n",
    "all_tracks_export_path = curr_active_pipeline.get_output_path().joinpath('2025-09-19_910pm_time_synch_dual_plotters.pdf')\n",
    "\n",
    "## INPUTS: \n",
    "_out_container: GenericPyQtGraphContainer\n",
    "active_2d_plot: Spike2DRaster = _out_container.ui.controlling_widget\n",
    "sync_plotters = _out_container.ui.sync_plotters\n",
    "\n",
    "saved_output_pdf_path = FigureToImageHelpers.export_wrapped_tracks_to_paged_df(active_2d_plot, output_pdf_path=all_tracks_export_path)\n",
    "print(f'saved_output_pdf_path: {saved_output_pdf_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59004b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import _temp_debug_two_step_plots_animated_pyqtgraph\n",
    "\n",
    "# Simple plot type 1:\n",
    "# plotted_variable_name = kwargs.get('variable_name', 'p_x_given_n') # Tries to get the user-provided variable name, otherwise defaults to 'p_x_given_n'\n",
    "plotted_variable_name = 'p_x_given_n' # Tries to get the user-provided variable name, otherwise defaults to 'p_x_given_n'\n",
    "_temp_debug_two_step_plots_animated_pyqtgraph(active_one_step_decoder, active_two_step_decoder, active_computed_data.extended_stats.time_binned_position_df, variable_name=plotted_variable_name) # Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0ef60",
   "metadata": {},
   "source": [
    "### OLD EXPLORE ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.params.trajectory_path_marker_max_fill_opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.params.recent_position_trajectory_max_seconds_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.curr_recent_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3266ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_plotter.ui.im\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "# from PyQt5 import QtGui\n",
    "# import pyphoplacecellanalysis.External.pyqtgraph.Qt as pg\n",
    "\n",
    "from PyQt5.QtGui import QPainter\n",
    "\n",
    "\n",
    "traj_curve_item: pg.PlotDataItem = a_plotter.ui.trajectory_curve\n",
    "# traj_curve_item.show()\n",
    "\n",
    "\n",
    "\n",
    "traj_curve_item.setAlpha(1.0, auto=False)\n",
    "traj_curve_item.setSymbolBrush((50, 50, 50, 25))\n",
    "\n",
    "\n",
    "# from QtGui.QPainter import \n",
    "# pg.QtGui.QPainter\n",
    "\n",
    "# from pg import QtGui\n",
    "# from PyQt5.QtGui.QPainter import CompositionMode\n",
    "\n",
    "# from pg.QtGui.QPainter import CompositionMode\n",
    "# QPainter.CompositionMode.CompositionMode_Overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ab866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# a_plotter.ui.imv.setCompositionMode(QPainter.CompositionMode_Multiply)\n",
    "# a_plotter.ui.imv.setCompositionMode(QPainter.CompositionMode_Overlay)\n",
    "a_plotter.ui.imv.setCompositionMode(QPainter.CompositionMode_Plus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d13ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.active_one_step_decoder.time_window_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_plotter.ui.trajectory_curve.\n",
    "a_plotter.params.recent_position_trajectory_path_shadow_pen\n",
    "a_plotter.params.recent_position_trajectory_path_pen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d90663",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.params.recent_position_trajectory_symbol_pen\n",
    "\n",
    "pg.mkPen(a_plotter.params.recent_position_trajectory_symbol_pen.color(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.params.recent_position_trajectory_symbol_pen.color().setAlphaF(0.1)\n",
    "\n",
    "pg.mkBrush("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0272b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plotter.ui.trajectory_curve.setPen(a_plotter.params.recent_position_trajectory_symbol_pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_plotters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b5ab2",
   "metadata": {},
   "source": [
    "### Build Marginals over track context and plot them on the timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _add_context_marginal_to_timeline, _add_context_decoded_epoch_marginals_to_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decoded_epochs_track_name: str = f'{epochs_name}[{decoding_time_bin_size}]'\n",
    "\n",
    "\n",
    "_out = _add_context_marginal_to_timeline(active_2d_plot, a_filter_epochs_decoded_result=all_context_filter_epochs_decoder_result, name='global context')\n",
    "_out_pbe_tracks = _add_context_decoded_epoch_marginals_to_timeline(active_2d_plot=active_2d_plot, decoded_epochs_result=pbe_decoder_result, name=f\"pbe[{decoding_time_bin_size}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "_out_new = _add_context_marginal_to_timeline(active_2d_plot, a_filter_epochs_decoded_result=all_context_filter_epochs_decoder_result, name='global context (overview)')\n",
    "_out_new\n",
    "\n",
    "identifier_name, widget, matplotlib_fig, matplotlib_fig_axes, dock_item = _out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c818bc",
   "metadata": {},
   "source": [
    "## 🟢🎨 Add the Session Paradigm/PBEs Epoch Intervals, then color and position them on the timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5b78e",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import SessionEpochs2DRenderTimeEpochs\n",
    "from neuropy.core.epoch import ensure_dataframe, ensure_Epoch, Epoch, EpochsAccessor\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.EpochRenderingMixin import EpochRenderingMixin\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import General2DRenderTimeEpochs, Ripples_2DRenderTimeEpochs\n",
    "\n",
    "\n",
    "    # _out_pbe_tracks, _out_pbe_overview_tracks = add_pbes_full_result_marginals(active_2d_plot, pbes_full_result)\n",
    "\n",
    "_out_pbe_tracks, _out_pbe_overview_tracks = _out_container.add_pbes_full_result_marginals(pbes_full_result=pbes_full_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7aff5",
   "metadata": {},
   "source": [
    "#### Overflow unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT NEEDED:\n",
    "# active_df = cls._update_df_visualization_columns(active_df, y_location, height, pen_color, brush_color, **kwargs)\n",
    "\n",
    "rendered_interval_keys = ['_', 'SessionEpochs', '_', 'PBEs'] # '_' indicates a vertical spacer\n",
    "\n",
    "# desired_interval_height_ratios = [2.0, 2.0, 1.0, 0.1, 1.0, 1.0, 1.0] # ratio of heights to each interval\n",
    "desired_interval_height_ratios = [1.0, 2.0, 0.1, 0.5] # ratio of heights to each interval\n",
    "\n",
    "required_vertical_offsets, required_interval_heights = EpochRenderingMixin.build_stacked_epoch_layout(desired_interval_height_ratios, epoch_render_stack_height=20.0, interval_stack_location='below')\n",
    "stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(rendered_interval_keys, required_vertical_offsets, required_interval_heights)}\n",
    "\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties({'SessionEpochs': dict()})\n",
    "\n",
    "a_ds = active_2d_plot.interval_datasources.SessionEpochs\n",
    "a_ds.update_visualization_properties(_updated_custom_interval_dataframe_visualization_columns_general_epoch)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.clear_all_rendered_intervals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ds = active_2d_plot.interval_datasources.SessionEpochs\n",
    "a_ds.update_visualization_properties(_updated_custom_interval_dataframe_visualization_columns_general_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf595977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SessionEpochs2DRenderTimeEpochs.add_render_time_epochs(curr_sess=curr_active_pipeline.sess.epochs, destination_plot=active_2d_plot)\n",
    "\n",
    "active_pbe_ds = ensure_dataframe(curr_active_pipeline.sess.pbe)\n",
    "_out_pbes = active_2d_plot.add_rendered_intervals(active_pbe_ds, 'PBEs')\n",
    "_out_pbes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.interval_datasources.PBEs.update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, y_location=-12.0, height=1.5, pen_color=inline_mkColor('cyan', 0.8), brush_color=inline_mkColor('cyan', 0.5), **kwargs)) ## Fully inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afa225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _out\n",
    "\n",
    "active_2d_plot.get_all_rendered_intervals_dict()\n",
    "\n",
    "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.EpochRenderingMixin import EpochRenderingMixin\n",
    "\n",
    "# rendered_interval_keys = ['_', 'SessionEpochs', 'Laps', '_', 'PBEs', 'Ripples', 'Replays'] # '_' indicates a vertical spacer\n",
    "\n",
    "# desired_interval_height_ratios = [2.0, 2.0, 1.0, 0.1, 1.0, 1.0, 1.0] # ratio of heights to each interval\n",
    "# required_vertical_offsets, required_interval_heights = EpochRenderingMixin.build_stacked_epoch_layout(desired_interval_height_ratios, epoch_render_stack_height=20.0, interval_stack_location='below')\n",
    "# stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(rendered_interval_keys, required_vertical_offsets, required_interval_heights)}\n",
    "\n",
    "# num_epochs = len(curr_active_pipeline.sess.epochs)\n",
    "\n",
    "# pen_colors = {'pre': pg.mkColor('purple'), 'roam': pg.mkColor('red'), 'sprinkle': pg.mkColor('red'), 'post': pg.mkColor('purple')}\n",
    "# brush_colors = {'pre': pg.mkColor('purple'), 'roam': pg.mkColor('red'), 'sprinkle': pg.mkColor('red'), 'post': pg.mkColor('purple')}\n",
    "\n",
    "# pen_color = list(pen_colors.values())\n",
    "# brush_color = list(brush_colors.values())\n",
    "# for a_pen_color in pen_color:\n",
    "#     a_pen_color.setAlphaF(0.8)\n",
    "\n",
    "# for a_brush_color in brush_color:\n",
    "#     a_brush_color.setAlphaF(0.5)\n",
    "\n",
    "\n",
    "# active_df = cls._update_df_visualization_columns(active_df, y_location, height, pen_color, brush_color, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import General2DRenderTimeEpochs, inline_mkColor\n",
    "\n",
    "active_2d_plot.get_all_rendered_intervals_dict()\n",
    "active_2d_plot.interval_datasources.SessionEpochs.update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, y_location=-12.0, height=1.5, pen_color=inline_mkColor('cyan', 0.8), brush_color=inline_mkColor('cyan', 0.5), **kwargs)) ## Fully inline\n",
    "\n",
    "# active_2d_plot.add_rendered_intervals(\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb42b75",
   "metadata": {},
   "source": [
    "#### (OTHER) Independent Epochs with `EpochsEditor` which looks much nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.EpochsEditorItem import EpochsEditor # perform_plot_laps_diagnoser\n",
    "import matplotlib.pyplot as plt\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers, ColorFormatConverter\n",
    "\n",
    "def generate_colors(n_epoch):\n",
    "    cmap = plt.get_cmap('tab20', n_epoch)\n",
    "    return [plt.matplotlib.colors.rgb2hex(cmap(i)) for i in range(n_epoch)]\n",
    "\n",
    "\n",
    "\n",
    "sess = curr_active_pipeline.sess # global_session\n",
    "\n",
    "# pos_df = sess.compute_position_laps() # ensures the laps are computed if they need to be:\n",
    "position_obj = deepcopy(sess.position)\n",
    "position_obj.compute_higher_order_derivatives()\n",
    "pos_df = position_obj.compute_smoothed_position_info(N=20) ## Smooth the velocity curve to apply meaningful logic to it\n",
    "pos_df = position_obj.to_dataframe()\n",
    "# Drop rows with missing data in columns: 't', 'velocity_x_smooth' and 2 other columns. This occurs from smoothing\n",
    "pos_df = pos_df.dropna(subset=['t', 'x_smooth', 'velocity_x_smooth', 'acceleration_x_smooth']).reset_index(drop=True)\n",
    "# curr_laps_df = sess.laps.to_dataframe()\n",
    "\n",
    "curr_paradigm_df = ensure_dataframe(sess.paradigm)\n",
    "curr_paradigm_df = curr_paradigm_df[np.logical_not(np.isin(curr_paradigm_df['label'], ['maze_GLOBAL', 'maze']))] ## exclude the global epoch\n",
    "n_epochs: int = len(curr_paradigm_df)\n",
    "# epoch_color_strs: List[str] = generate_colors(n_epochs)\n",
    "epoch_color_strs: List[str] = [ColorFormatConverter.qColor_to_hexstring(v, include_alpha=False) for v in ColormapHelpers.mpl_to_pg_colormap(mpl_cmap_name='tab20', resolution=n_epochs).getColors(mode='qcolor')]\n",
    "curr_paradigm_df['lap_color'] = \"#10FF44\"\n",
    "curr_paradigm_df['lap_color'] = epoch_color_strs\n",
    "curr_paradigm_df['lap_accent_color'] = '#FFFFFF'\n",
    "curr_paradigm_df\n",
    "## Create a new window:\n",
    "custom_epoch_label_kwargs = dict(epoch_label_position=0.05, epoch_label_rotateAxis=(0,0), epoch_label_anchor=(0.0, 1.0))\n",
    "epochs_editor = EpochsEditor.init_laps_diagnoser(pos_df, curr_paradigm_df, include_velocity=False, include_accel=False, span=(0.05, 0.95), movable=False, **custom_epoch_label_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.Mixins.AnimalTrajectoryPlottingMixin import AnimalTrajectoryPlottingMixin\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPositionDecoderPlotter import TimeSynchronizedPositionDecoderPlotter\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_combined_time_synchronized_Bapun_decoders_window\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode, Spike2DRaster\n",
    "\n",
    "# _out_container: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=['roam', 'sprinkle'], fixed_window_duration = 15.0)\n",
    "# # _out_container: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=curr_active_pipeline.active_completed_computation_result_names, fixed_window_duration = 15.0)\n",
    "# # _out_container: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=['maze1', 'maze2', 'maze'], fixed_window_duration = 15.0)\n",
    "# active_2d_plot: Spike2DRaster = _out_container.ui.controlling_widget\n",
    "# sync_plotters = _out_container.ui.sync_plotters\n",
    "# # a_sync_plotter: TimeSynchronizedPositionDecoderPlotter = sync_plotters['roam']\n",
    "# # a_sync_plotter.curr_position\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965feb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import DummyOneStepDecoder\n",
    "\n",
    "\n",
    "pf2D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo3D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "# all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "continuously_decoded_pseudo2D_decoder_dict = directional_decoders_decode_result.continuously_decoded_pseudo2D_decoder_dict\n",
    "## Unpacking a result:\n",
    "a_time_bin_size: float = list(continuously_decoded_pseudo2D_decoder_dict.keys())[-1] ## ALWAYS GET THE MOST RECENT\n",
    "all_context_filter_epochs_decoder_result: SingleEpochDecodedResult = continuously_decoded_pseudo2D_decoder_dict[a_time_bin_size] ## ALWAYS GET THE MOST RECENT\n",
    "marginal_z: NDArray = all_context_filter_epochs_decoder_result.marginal_z.p_x_given_n\n",
    "\n",
    "# individual_decoder_decoding_results = {k:v for k, v in directional_decoders_decode_result.continuously_decoded_result_cache_dict[1.0].items() if k != 'pseudo2D'}\n",
    "\n",
    "# one_step_decoder_dummy_dict = {}\n",
    "# for k, v in individual_decoder_decoding_results.items():\n",
    "#     one_step_decoder_dummy_dict[k] = DummyOneStepDecoder(xbin=deepcopy(pseudo3D_decoder.xbin), ybin=deepcopy(pseudo3D_decoder.ybin), time_window_centers=deepcopy(all_context_filter_epochs_decoder_result.time_bin_container.centers), p_x_given_n=deepcopy(v.p_x_given_n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ad4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_position_decoder_plotter.on_window_changed(5.0, 15.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
