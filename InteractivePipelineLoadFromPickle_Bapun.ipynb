{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45f3f6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ InteractivePipelineLoadFromPickle (Independent Load-only Visualization Notebook) - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc319c3",
   "metadata": {},
   "source": [
    "#### Completed `2025-02-27`, got working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:34:16.396240Z",
     "start_time": "2025-01-07T11:34:09.057603Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "pho-run-2024",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "# !pip install viztracer\n",
    "%load_ext viztracer\n",
    "from viztracer import VizTracer\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# # Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "from pyphocorehelpers.ipython_helpers import CustomFormatterMagics\n",
    "\n",
    "# Register the magic\n",
    "get_ipython().register_magics(CustomFormatterMagics)\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "from pyphocorehelpers.indexing_helpers import get_dict_subset\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import metadata_attributes\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "## Pho Programming Helpers:\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.notebook_helpers import NotebookCellExecutionLogger\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "_notebook_path:Path = Path(IPythonHelpers.try_find_notebook_filepath(IPython.extract_module_locals())).resolve() # Finds the path of THIS notebook\n",
    "# _notebook_execution_logger: NotebookCellExecutionLogger = NotebookCellExecutionLogger(notebook_path=_notebook_path, enable_logging_to_file=False) # Builds a logger that records info about this notebook\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme # used in perform_pipeline_save\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions,  RankOrderComputationsContainer, RankOrderResult, RankOrderAnalyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "from neuropy.utils.mixins.binning_helpers import transition_matrix\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.transition_matrix import TransitionMatrixComputations\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates, get_proper_global_spikes_df\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "def get_global_variable(var_name):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    return globals()[var_name]\n",
    "    \n",
    "def update_global_variable(var_name, value):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    globals()[var_name] = value\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/home/halechr/FastData'), Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/media/halechr/MAX/Data'), Path(r'H:\\Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/Users/pho/cloud/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "    global global_data_root_parent_path\n",
    "    new_global_data_root_parent_path = new_path.resolve()\n",
    "    global_data_root_parent_path = new_global_data_root_parent_path\n",
    "    print(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "    assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "            \n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load",
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "# active_data_mode_name = 'kdiba'\n",
    "# local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "# local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "# override_parameters = {\n",
    "#     'preprocessing.laps.use_direction_dependent_laps': True\n",
    "# }\n",
    "\n",
    "# # [*] - indicates bad or session with a problem\n",
    "# # 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # Recomputed 2024-12-16 18:51 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # Recomputed 2025-01-15 18:52 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # Recomputed 2025-01-16 03:21 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # Recomputed 2025-01-07 13:31 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46') # Recomputed 2024-12-16 19:23 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30') ## BLOCKING ERROR with pf2D computation (empty) for 5Hz 2024-12-02 15:24 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50') # Recomputed 2024-12-16 19:45 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54') # Recomputed 2024-12-16 19:29 -- about 3 good replays\n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3') # Recomputed 2024-12-16 19:32 \n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # Recomputed 2024-12-16 19:33 -- about 5 good replays\n",
    "# # curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # Recomputed 2024-12-16 19:36 -- TONS of good replays, 10+ pages of them \n",
    "# local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# BAPUN data format                                                                                                    #\n",
    "# ==================================================================================================================== #\n",
    "active_data_mode_name = 'bapun'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('Bapun')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='bapun',animal='RatN', session_name='Day4OpenField') # ,exper_name='one'\n",
    "curr_context = IdentifyingContext(format_name='bapun',animal='RatS', session_name='Day5TwoNovel') # ,exper_name='one'\n",
    "# Create a dictionary with the parameters to override\n",
    "override_parameters = {\n",
    "    'preprocessing.laps.use_direction_dependent_laps': False\n",
    "}\n",
    "\n",
    "\n",
    "# # ==================================================================================================================== #\n",
    "# # RACHEL Data Format 2025-02-27 11:36                                                                                  #\n",
    "# # ==================================================================================================================== #\n",
    "# active_data_mode_name = 'rachel'\n",
    "# local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "# local_session_root_parent_path = global_data_root_parent_path.joinpath('Rachel')\n",
    "\n",
    "# # [*] - indicates bad or session with a problem\n",
    "# # 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='rachel', animal='cho', session_name='cho_241117_2_merged') # ,exper_name='one'\n",
    "# # Create a dictionary with the parameters to override\n",
    "# override_parameters = {\n",
    "#     'preprocessing.laps.use_direction_dependent_laps': False\n",
    "# }\n",
    "\n",
    "\n",
    "# Path('/media/halechr/MAX/Data/Rachel/cho_241117_2_merged')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name) #.resolve()\n",
    "\n",
    "# basedir: Path = Path('/media/halechr/MAX/Data/Rachel/cho/cho_241117_2_merged') # DO NOT `.resolve()``\n",
    "# basedir: Path = Path(r'H:\\Data\\Bapun\\RatS\\Day5TwoNovel')\n",
    "print(f'basedir: {str(basedir)}')\n",
    "Assert.path_exists(basedir)\n",
    "\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist = ['pf_computation', 'pfdt_computation', 'position_decoding']\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "selector, on_value_change = PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(update_global_variable_fn=update_global_variable, debug_print=False, enable_full_view=True)\n",
    "# selector.value = 'clean_run'\n",
    "selector.value = 'continued_run'\n",
    "# selector.value = 'final_run'\n",
    "on_value_change(dict(new=selector.value)) ## do update manually so the workspace variables reflect the set values\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "print(f\"saving_mode: {saving_mode}, force_reload: {force_reload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ca602",
   "metadata": {},
   "source": [
    "# Fresh direct unpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276d9d3",
   "metadata": {},
   "source": [
    "# Rachel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497468e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core import Neurons, NeuronType\n",
    "\n",
    "print(f'basedir: \"{basedir}\"')\n",
    "\n",
    "# neurons = Neurons.from_file(basedir.joinpath('cho_241117_2_merged.paradigm.npy'))\n",
    "neurons_path = basedir.joinpath('cho_241117_2_merged.paradigm.npy_bak')\n",
    "neurons_path.exists()\n",
    "neurons = Neurons.from_file(neurons_path)\n",
    "assert neurons is not None\n",
    "\n",
    "# cho_241117_2_merged.neurons.npy_bak\n",
    "\n",
    "# neuronIDs = pd.read_csv(basedir.joinpath('cluster_q.tsv'))\n",
    "# neurons = Neurons(spiketrains=phydata.spiketrains, t_stop=2*3600, sampling_rate=30000, neuron_ids = {1:'pyr1',2:'pyr2',3:'pyr3',4:'int1',5:'int2',6:'int3',7:\"mua1\",8:'mua2',9:'mua3'})\n",
    "# neurons.filename = folder.joinpath(f'{filename}.neurons.npy')\n",
    "# neurons.save()\n",
    "neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7877ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _test_session = RachelDataSessionFormat.build_session(Path(r'R:\\data\\Rachel\\merged_M1_20211123_raw_phy'))\n",
    "_test_session = RachelDataSessionFormat.build_session(basedir)\n",
    "_test_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe104601",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "_test_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Direct unpickle\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import PipelineWithInputStage, PipelineWithLoadableStage, loadData, saveData\n",
    "\n",
    "curr_sess_pkl_path = basedir.joinpath('sess.pkl')\n",
    "Assert.path_exists(curr_sess_pkl_path)\n",
    "_test_session = loadData(pkl_path=curr_sess_pkl_path)\n",
    "assert _test_session is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(_test_session)\n",
    "\n",
    "spikes_df: pd.DataFrame = deepcopy(_test_session.spikes_df)\n",
    "spikes_df\n",
    "spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = _test_session.position.to_dataframe()\n",
    "pos_df = pos_df.dropna()\n",
    "pos_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e18696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Rachel's overriding\n",
    "# epochs_df = _test_session.epochs.to_dataframe().to_numpy()\n",
    "# epochs_df[1, 2] = pos_df.t.max() # 3320.0 Fixup max time\n",
    "# epochs_df[1, 3] = pos_df.t.max() - epochs_df[1, 1] # fixup duration\n",
    "# epochs_df = pd.DataFrame(epochs_df, columns=['label', 'start', 'stop', 'duration'])\n",
    "# epochs_df[['start', 'stop', 'duration']] = epochs_df[['start', 'stop', 'duration']].astype(float)\n",
    "# epochs_df\n",
    "# _test_session.epochs = Epoch(epochs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe98de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _test_session.position =_test_session.position.time_slice(pos_df.t.min(), pos_df.t.max())\n",
    "# _test_session.position.df['y_bak'] = deepcopy(_test_session.position.df['y'])\n",
    "# _test_session.position.df['y'] = _test_session.position.df['z']\n",
    "# _test_session.position.df['z'] = _test_session.position.df['y_bak']\n",
    "# _test_session.position.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_epochs = Epoch(deepcopy(_test_session.epochs.epochs.time_slice(pos_df.t.min(), pos_df.t.max())))\n",
    "computation_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822592bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session.neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session.spikes_df['t_rel_seconds'] = _test_session.spikes_df['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import PlacefieldComputationParameters, perform_compute_placefields\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from neuropy.core.position import Position, PositionAccessor\n",
    "\n",
    "pf_computation_config = PlacefieldComputationParameters(computation_epochs=computation_epochs)\n",
    "active_session = _test_session\n",
    "\n",
    "# pf1D, pf2D = perform_compute_placefields(active_session_spikes_df=active_session.spikes_df, active_pos=active_session.position, computation_config=pf_computation_config, active_epoch_placefields1D=None, active_epoch_placefields2D=None, included_epochs=pf_computation_config.computation_epochs, should_force_recompute_placefields=True)\n",
    "# active_session.position.compute_higher_order_derivatives().position.compute_smoothed_position_info()\n",
    "\n",
    "# active_pos = deepcopy(active_session.position).to_dataframe().position.compute_higher_order_derivatives().position.compute_smoothed_position_info() #.position.com\n",
    "\n",
    "active_pos: Position = deepcopy(active_session.position)\n",
    "# active_pos = Position(lin_pos_df, metadata=None) ## build position object out of the dataframe\n",
    "active_pos.compute_higher_order_derivatives()\n",
    "active_pos.compute_smoothed_position_info()\n",
    "active_pos.speed; # ensure speed is calculated for the new object\n",
    "active_pos\n",
    "included_epochs = deepcopy(computation_epochs)\n",
    "computation_config = deepcopy(pf_computation_config)\n",
    "active_session_spikes_df = deepcopy(active_session.spikes_df)\n",
    "spikes_df = deepcopy(active_session_spikes_df).spikes.sliced_by_neuron_type('PYRAMIDAL') # Only use PYRAMIDAL neurons\n",
    "pf2D = PfND.from_config_values(spikes_df, deepcopy(active_pos), epochs=included_epochs,\n",
    "                                    speed_thresh=computation_config.speed_thresh, frate_thresh=computation_config.frate_thresh,\n",
    "                                    grid_bin=computation_config.grid_bin, grid_bin_bounds=computation_config.grid_bin_bounds, smooth=computation_config.smooth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cfc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf2D.plot_occupancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf2D.plot_ratemaps_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.dynamic_parameters import DynamicParameters\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputationResult\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.mixins.member_enumerating import AllFunctionEnumeratingMixin\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, BayesianPlacemapPositionDecoder, DecodedFilterEpochsResult, Zhang_Two_Step\n",
    "\n",
    "# placefield_computation_config = computation_result.computation_config.pf_params # should be a PlacefieldComputationParameters\n",
    "# if override_decoding_time_bin_size is not None:\n",
    "#     old_time_bin_size = computation_result.computation_config.pf_params.time_bin_size\n",
    "#     print(f'changing computation_result.computation_config.pf_params.time_bin_size from {old_time_bin_size} -> {override_decoding_time_bin_size}')\n",
    "#     did_change: bool = (override_decoding_time_bin_size != old_time_bin_size)\n",
    "#     computation_result.computation_config.pf_params.time_bin_size = override_decoding_time_bin_size\n",
    "#     print(f'\\tdid_change: {did_change}')\n",
    "    \n",
    "# ## filtered_spikes_df version:\n",
    "# computation_result.computed_data['pf1D_Decoder'] = BayesianPlacemapPositionDecoder(time_bin_size=placefield_computation_config.time_bin_size, pf=computation_result.computed_data['pf1D'], spikes_df=computation_result.computed_data['pf1D'].filtered_spikes_df.copy(), debug_print=False)\n",
    "# assert (len(computation_result.computed_data['pf1D_Decoder'].is_non_firing_time_bin) == computation_result.computed_data['pf1D_Decoder'].num_time_windows), f\"len(self.is_non_firing_time_bin): {len(computation_result.computed_data['pf1D_Decoder'].is_non_firing_time_bin)}, self.num_time_windows: {computation_result.computed_data['pf1D_Decoder'].num_time_windows}\"\n",
    "# computation_result.computed_data['pf1D_Decoder'].compute_all() # this is what breaks it\n",
    "\n",
    "pf2D_Decoder = BayesianPlacemapPositionDecoder(time_bin_size=0.5, pf=pf2D, spikes_df=pf2D.filtered_spikes_df.copy(), debug_print=False)\n",
    "pf2D_Decoder.compute_all() # Changing to fIXED grid_bin_bounds ===> MUCH (10x?) slower than before\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5318801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_most_likely_position_comparsions\n",
    "\n",
    "\n",
    "fig, axs = plot_most_likely_position_comparsions(pf2D_Decoder, _test_session.position.to_dataframe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(num='decoding_test')\n",
    "\n",
    "pf2D_Decoder.most_likely_positions.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51458391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import safeSaveData\n",
    "\n",
    "# pkl_path = _test_session.basepath.joinpath('2025-02-27_pho_session.pkl')\n",
    "pkl_path = _test_session.basepath.joinpath('2025-07-01_pho_session.pkl')\n",
    "\n",
    "safeSaveData(pkl_path, _test_session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dccb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_pkl_path = _test_session.basepath.joinpath('2025-07-01_pho_computed_outputs.pkl')\n",
    "safeSaveData(decoded_pkl_path, (pf2D_Decoder, pf2D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59175f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm_df = pd.DataFrame(dict(label=['Pre','Maze'],\n",
    "    start = [0.0, 1208.01],\n",
    "    stop = [1208.0, np.inf],\n",
    "))\n",
    "\n",
    "paradigmdf = pd.DataFrame(data=paradigm_df)\n",
    "paradigm = Epoch(paradigmdf)\n",
    "paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paradigm.filename = Path('/home/wahlberg/Exp_Data/M1_Nov2021/20211123/merged_M1_20211123_raw/merged_M1_20211123_raw_phy/merged_M1_20211123_raw.paradigm.npy')\n",
    "paradigm.filename = basedir.joinpath(f'{curr_context.session_name}.paradigm.npy')\n",
    "paradigm.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33267230",
   "metadata": {},
   "source": [
    "# Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21a4df",
   "metadata": {
    "tags": [
     "run-group-0",
     "run-load",
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases, PipelinePickleFileSelectorWidget\n",
    "\n",
    "# ## INPUTS: basedir\n",
    "# active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir)\n",
    "\n",
    "extended_computations_include_includelist_phase_dict: Dict[str, CustomProcessingPhases] = CustomProcessingPhases.get_extended_computations_include_includelist_phase_dict()\n",
    "\n",
    "current_phase: CustomProcessingPhases = CustomProcessingPhases[selector.value]  # Assuming selector.value is an instance of CustomProcessingPhases\n",
    "extended_computations_include_includelist: List[str] = [key for key, value in extended_computations_include_includelist_phase_dict.items() if value <= current_phase]\n",
    "display(extended_computations_include_includelist)\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'] # \n",
    "\n",
    "# ## INPUTS: basedir\n",
    "active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir, on_update_global_variable_callback=update_global_variable, on_get_global_variable_callback=get_global_variable)\n",
    "\n",
    "_subfn_load, _subfn_save, _subfn_compute, _subfn_compute_new = active_session_pickle_file_widget._build_load_save_callbacks(global_data_root_parent_path=global_data_root_parent_path, active_data_mode_name=active_data_mode_name, basedir=basedir, saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                                             extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist)\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n",
    "\n",
    "# Display the widget\n",
    "display(active_session_pickle_file_widget.servable())\n",
    "# active_session_pickle_file_widget.local_file_browser_widget.servable()\n",
    "# active_session_pickle_file_widget.global_file_browser_widget.servable()\n",
    "# display(active_session_pickle_file_widget.local_file_browser_widget.servable())\n",
    "# display(active_session_pickle_file_widget.global_file_browser_widget.servable())\n",
    "\n",
    "# OUTPUTS: active_session_pickle_file_widget, widget.active_local_pkl, widget.active_global_pkl\n",
    "\n",
    "if selector.value == 'clean_run':\n",
    "\t## handle a clean run specially, this will create the pkls and not load them\n",
    "    print(f'clean run!')\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    # active_session_pickle_file_widget.is_compute_button_disabled = False # enable the compute button always during a clean run\n",
    "    # active_session_pickle_file_widget.is_load_button_disabled = True\n",
    "    \n",
    "    new_default_local_pkl_file: Path = active_session_pickle_file_widget.directory.joinpath(default_selected_local_file_name).resolve()\n",
    "    print(f'new_default_local_pkl_file: {new_default_local_pkl_file}')\n",
    "\n",
    "    active_session_pickle_file_widget.selected_local_pkl_files = [new_default_local_pkl_file]\n",
    "    active_session_pickle_file_widget.selected_global_pkl_files = []\n",
    "    active_session_pickle_file_widget._update_load_save_button_disabled_state()\n",
    "    print(f'active_session_pickle_file_widget.is_load_button_disabled: {active_session_pickle_file_widget.is_load_button_disabled}')\n",
    "    print(f'active_session_pickle_file_widget.is_compute_button_disabled: {active_session_pickle_file_widget.is_compute_button_disabled}')\n",
    "    print(f'active_local_pkl: \"{active_session_pickle_file_widget.active_local_pkl}\"')\n",
    "    print(f'active_global_pkl: \"{active_session_pickle_file_widget.active_global_pkl}\"')\n",
    "    active_session_pickle_file_widget.load_button.disabled = False\n",
    "    active_session_pickle_file_widget.compute_button.disabled = False\n",
    "else:\n",
    "    # not `clean_run` mode, continuing processing which might include loading from pickles\n",
    "    ## try selecting the first\n",
    "    did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "\n",
    "    ## Set default local comp pkl:\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    if not active_session_pickle_file_widget.is_local_file_names_list_empty:\n",
    "        default_local_section_indicies = [active_session_pickle_file_widget.local_file_browser_widget._data['File Name'].tolist().index(default_selected_local_file_name)]\n",
    "        active_session_pickle_file_widget.local_file_browser_widget.selection = default_local_section_indicies\n",
    "\n",
    "    ## Set default global computation pkl:\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    if not active_session_pickle_file_widget.is_global_file_names_list_empty:\n",
    "        default_global_section_indicies = [active_session_pickle_file_widget.global_file_browser_widget._data['File Name'].tolist().index(default_selected_global_file_name)]\n",
    "        active_session_pickle_file_widget.global_file_browser_widget.selection = default_global_section_indicies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b1147",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "did_find_valid_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b07aa",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# active_session_pickle_file_widget.on_load_callback()\n",
    "# curr_active_pipeline.sess.paradigm.adding_global_epoch_row()\n",
    "# include_includelist = curr_active_pipeline.active_completed_computation_result_names\n",
    "\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "\n",
    "bapun_epochs = deepcopy(curr_active_pipeline.sess.epochs)\n",
    "bapun_epochs = BapunDataSessionFormatRegisteredClass._bapun_session_fixup_epochs_to_be_non_overlapping(bapun_epochs=bapun_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if did_find_valid_selection:\n",
    "\t_subfn_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d280d3",
   "metadata": {},
   "source": [
    "## From `test_non_interactive_crash.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd6416",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "### Bapun Open-Field Experiment (2022-08-09 Analysis)\n",
    "from neuropy.core.session.SessionSelectionAndFiltering import build_custom_epochs_filters # used particularly to build Bapun-style filters\n",
    "\n",
    "active_data_mode_name = 'bapun'\n",
    "# active_data_mode_name = 'rachel'\n",
    "print(f'active_data_session_types_registered_classes_dict: {active_data_session_types_registered_classes_dict}')\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "# basedir = Path('/media/halechr/MAX/Data/Rachel/Cho_241117_Session2').resolve()\n",
    "## INPUTS: basedir \n",
    "\n",
    "force_reload = force_reload #True\n",
    "print(f'force_reload: {force_reload}')\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties, override_basepath=Path(basedir), force_reload=force_reload) # , override_parameters_flat_keypaths_dict=override_parameters\n",
    "\n",
    "# _test_session = RachelDataSessionFormat.build_session(Path(r'R:\\data\\Rachel\\merged_M1_20211123_raw_phy'))\n",
    "# _test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "# _test_session\n",
    "\n",
    "## ~20m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not hasattr(curr_active_pipeline.sess, 'epochs_bak')):\n",
    "    print(f'fixing up Bapun computation epochs...')\n",
    "    bapun_epochs: Epoch = deepcopy(curr_active_pipeline.sess.epochs)\n",
    "    bapun_epochs = BapunDataSessionFormatRegisteredClass._bapun_session_fixup_epochs_to_be_non_overlapping(bapun_epochs=bapun_epochs, enable_global_epoch=False)\n",
    "    curr_active_pipeline.sess.epochs_bak = deepcopy(curr_active_pipeline.sess.epochs) ## backup the bad ones\n",
    "    curr_active_pipeline.sess.epochs = bapun_epochs\n",
    "    print(f'\\tdone. new epochs: \\n{bapun_epochs}\\n')\n",
    "else:\n",
    "    print(f'WARN: already fixedup Bapun epochs.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686d53e",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_name_includelist = ['pre', 'maze1', 'post1', 'maze2', 'post2']\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze', 'sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['roam', 'sprinkle']) # , 'maze'\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_filters_pyramidal_epochs(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "active_session_computation_configs = active_data_mode_registered_class.build_default_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=0.5)\n",
    "curr_active_pipeline.filter_sessions(active_session_filter_configurations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89594afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.filtered_epochs['maze']\n",
    "\n",
    "# curr_active_pipeline.filtered_sessions['maze'].epochs\n",
    "\n",
    "# curr_active_pipeline.filtered_sessions\n",
    "curr_active_pipeline.sess.epochs.to_dataframe()['label'].to_list()\n",
    "\n",
    "# curr_active_pipeline.save_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd030ec6",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import PipelineWithInputStage, PipelineWithLoadableStage, loadData, saveData\n",
    "\n",
    "## Custom Save\n",
    "curr_sess_pkl_path = basedir.joinpath('loadedSessPickle_2025-09-04.pkl')\n",
    "print(f'saving out to modern pickle: \"{curr_sess_pkl_path}\"')\n",
    "saveData(curr_sess_pkl_path, db=curr_active_pipeline, safe_save=True) # (v_dict, str(curr_item_type.__module__), str(curr_item_type.__name__)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_filter_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac019e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_active_pipeline.filtered_sessions['sprinkle'].epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcaaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_bin_bounds=(((-83.33747881216672, 110.15967332926644), (-94.89955475226206, 97.07387994733473)))\n",
    "\n",
    "\n",
    "bapun_open_field_grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_all_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.update_parameters(grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0))))\n",
    "curr_active_pipeline.sess.config.grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ce2d4",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "override_parameters_flat_keypaths_dict = {'sess.config.grid_bin_bounds': (((-120.0, 120.0), (-120.0, 120.0))), # 'rank_order_shuffle_analysis.minimum_inclusion_fr_Hz': minimum_inclusion_fr_Hz,\n",
    "\t\t\t\t\t\t\t\t\t\t  'sess.config.preprocessing_parameters.laps.use_direction_dependent_laps': False, # lap_estimation_parameters\n",
    "                                        }\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e139c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.filtered_sessions['maze'].epochs)\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.sess.epochs)\n",
    "active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(bapun_epochs)\n",
    "# active_session_computation_configs[1].pf_params.computation_epochs = deepcopy(curr_active_pipeline.filtered_sessions['maze'].epochs.to_dataframe())\n",
    "active_session_computation_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.computation_epochs\n",
    "\n",
    "#    start   stop     label  duration\n",
    "# 0      0   7407       pre      7407\n",
    "# 1   7423  11483      maze      4060\n",
    "# 3  10186  11483  sprinkle      1297\n",
    "# 2  11497  25987      post     14490\n",
    "\n",
    "# [4 rows x 4 columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a058e33",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfbf48",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "active_computation_functions_name_includelist = ['pf_computation', 'pfdt_computation', 'position_decoding', 'extended_pf_peak_information'] # 'ratemap_peaks_prominence2d', 'position_decoding_two_step'\n",
    "\n",
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation', '_perform_velocity_vs_pf_density_computation', '_perform_velocity_vs_pf_simplified_count_density_computation']) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "\n",
    "curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_includelist=active_computation_functions_name_includelist, overwrite_extant_results=True, fail_on_exception=False, debug_print=True) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.computation_results['maze'].accumulated_errors\n",
    "curr_active_pipeline.clear_all_failed_computations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831df11",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display(root_output_dir=r'Output', should_smooth_maze=True) # TODO: pass a display config\n",
    "# curr_active_pipeline.prepare_for_display(root_output_dir=r'W:\\Data\\Output', should_smooth_maze=True) # TODO: pass a display config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09507ee4",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)\n",
    "# _out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-26.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f36c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-27.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4608e0",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_global_computation_results()#save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-27.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d93f",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "## Setup Computation Functions to be executed:\n",
    "# includelist Mode:\n",
    "computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                '_perform_position_decoding_computation', \n",
    "                                '_perform_firing_rate_trends_computation',\n",
    "                                '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                # '_perform_two_step_position_decoding_computation',\n",
    "                                # '_perform_recursive_latent_placefield_decoding'\n",
    "                            ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "computation_functions_name_excludelist=None\n",
    "\n",
    "batch_extended_computations(curr_active_pipeline, included_computation_filter_names=computation_functions_name_includelist, include_includelist=['maze', 'sprinkle'],\n",
    "                            include_global_functions=True, fail_on_exception=False, progress_print=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52cc45",
   "metadata": {},
   "source": [
    "## 2024-06-25 - Load from saved custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337ddc6",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "# Loads custom pipeline pickles that were saved out via `custom_save_filepaths['pipeline_pkl'] = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename=custom_save_filenames['pipeline_pkl'])`\n",
    "\n",
    "## INPUTS: global_data_root_parent_path, active_data_mode_name, basedir, saving_mode, force_reload, custom_save_filenames\n",
    "# custom_suffix: str = '_withNewKamranExportedReplays'\n",
    "\n",
    "# custom_suffix: str = '_withNewComputedReplays'\n",
    "# custom_suffix: str = '_withNewComputedReplays-qclu_[1, 2]-frateThresh_5.0'\n",
    "\n",
    "# custom_save_filenames = {\n",
    "#     'pipeline_pkl':f'loadedSessPickle{custom_suffix}.pkl',\n",
    "#     'global_computation_pkl':f\"global_computation_results{custom_suffix}.pkl\",\n",
    "#     'pipeline_h5':f'pipeline{custom_suffix}.h5',\n",
    "# }\n",
    "# print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "# custom_save_filepaths = {k:v for k, v in custom_save_filenames.items()}\n",
    "\n",
    "# # ==================================================================================================================== #\n",
    "# # PIPELINE LOADING                                                                                                     #\n",
    "# # ==================================================================================================================== #\n",
    "# # load the custom saved outputs\n",
    "# active_pickle_filename = custom_save_filenames['pipeline_pkl'] # 'loadedSessPickle_withParameters.pkl'\n",
    "# print(f'active_pickle_filename: \"{active_pickle_filename}\"')\n",
    "# # assert active_pickle_filename.exists()\n",
    "# active_session_h5_filename = custom_save_filenames['pipeline_h5'] # 'pipeline_withParameters.h5'\n",
    "# print(f'active_session_h5_filename: \"{active_session_h5_filename}\"')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "## DO NOT allow recompute if the file doesn't exist!!\n",
    "# Computing loaded session pickle file results : \"W:/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_withNewComputedReplays.pkl\"... done.\n",
    "# Failure loading W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle_withNewComputedReplays.pkl.\n",
    "# proposed_load_pkl_path = basedir.joinpath(active_pickle_filename).resolve()\n",
    "\n",
    "## INPUTS: widget.active_global_pkl, widget.active_global_pkl\n",
    "\n",
    "if active_session_pickle_file_widget.active_global_pkl is None:\n",
    "    skip_global_load: bool = True\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skip_global_load: {skip_global_load}')\n",
    "else:\n",
    "    skip_global_load: bool = False\n",
    "    override_global_computation_results_pickle_path = active_session_pickle_file_widget.active_global_pkl.resolve()\n",
    "    Assert.path_exists(override_global_computation_results_pickle_path)\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "proposed_load_pkl_path = active_session_pickle_file_widget.active_local_pkl.resolve()\n",
    "Assert.path_exists(proposed_load_pkl_path)\n",
    "proposed_load_pkl_path\n",
    "\n",
    "custom_suffix: str = active_session_pickle_file_widget.try_extract_custom_suffix()\n",
    "print(f'custom_suffix: \"{custom_suffix}\"')\n",
    "\n",
    "## OUTPUTS: custom_suffix, proposed_load_pkl_path, (override_global_computation_results_pickle_path, skip_global_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_name_includelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eef26a",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: proposed_load_pkl_path\n",
    "# assert proposed_load_pkl_path.exists(), f\"for a saved custom the file must exist, but proposed_load_pkl_path: '{proposed_load_pkl_path}' does not!\"\n",
    "\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "\n",
    "with set_posix_windows():\n",
    "    curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                            computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                            saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                            skip_extended_batch_computations=True, debug_print=False, fail_on_exception=False, active_pickle_filename=proposed_load_pkl_path, \n",
    "                                            override_parameters_flat_keypaths_dict=override_parameters) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "print(f'Pipeline loaded from custom pickle!!')\n",
    "## OUTPUT: curr_active_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_load_pkl_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e10fea",
   "metadata": {},
   "source": [
    "## from `batch_load_session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From `pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing.batch_load_session` 2025-02-26 09:09 \n",
    "kwargs = {}\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "override_parameters_flat_keypaths_dict = override_parameters\n",
    "active_pickle_filename = proposed_load_pkl_path\n",
    "active_session_computation_configs = None\n",
    "fail_on_exception: bool = False\n",
    "\n",
    "saving_mode = PipelineSavingScheme.init(saving_mode)\n",
    "epoch_name_includelist = kwargs.get('epoch_name_includelist', ['maze1','maze2','maze'])\n",
    "debug_print = kwargs.get('debug_print', False)\n",
    "assert 'skip_save' not in kwargs, f\"use saving_mode=PipelineSavingScheme.SKIP_SAVING instead\"\n",
    "# skip_save = kwargs.get('skip_save', False)\n",
    "# active_pickle_filename = kwargs.get('active_pickle_filename', 'loadedSessPickle.pkl')\n",
    "\n",
    "# active_session_computation_configs = kwargs.get('active_session_computation_configs', None)\n",
    "# computation_functions_name_includelist = kwargs.get('computation_functions_name_includelist', None)\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "## Begin main run of the pipeline (load or execute):\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties,\n",
    "    override_basepath=Path(basedir), force_reload=force_reload, active_pickle_filename=active_pickle_filename, skip_save_on_initial_load=True, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n",
    "\n",
    "was_loaded_from_file: bool =  curr_active_pipeline.has_associated_pickle # True if pipeline was loaded from an existing file, False if it was created fresh\n",
    "\n",
    "# Get the previous configs:\n",
    "# curr_active_pipeline.filtered_sessions\n",
    "# ['filtered_session_names', 'filtered_contexts', 'filtered_epochs', 'filtered_sessions']\n",
    "# loaded_session_filter_configurations = {k:v.filter_config['filter_function'] for k,v in curr_active_pipeline.active_configs.items()}\n",
    "# loaded_pipeline_computation_configs = {k:v.computation_config for k,v in curr_active_pipeline.active_configs.items()}\n",
    "\n",
    "\n",
    "## Build updated ones from the current configs:\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "if debug_print:\n",
    "    print(f'active_session_filter_configurations: {active_session_filter_configurations}')\n",
    "\n",
    "## Skip the filtering, it used to be performed bere but NOT NOW\n",
    "\n",
    "## TODO 2023-05-16 - set `curr_active_pipeline.active_configs[a_name].computation_config.pf_params.computation_epochs = curr_laps_obj` equivalent\n",
    "## TODO 2023-05-16 - determine appropriate binning from `compute_short_long_constrained_decoders` so it's automatically from the long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef85735",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_on_exception: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43096f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if active_session_computation_configs is None:\n",
    "    \"\"\"\n",
    "    If there are is provided computation config, get the default:\n",
    "    \"\"\"\n",
    "    # ## Compute shared grid_bin_bounds for all epochs from the global positions:\n",
    "    # global_unfiltered_session = curr_active_pipeline.sess\n",
    "    # # ((22.736279243974774, 261.696733348342), (49.989466271998936, 151.2870218547401))\n",
    "    # first_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[0]]\n",
    "    # # ((22.736279243974774, 261.696733348342), (125.5644705153173, 151.21507349463707))\n",
    "    # second_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[1]]\n",
    "    # # ((71.67666779621361, 224.37820920766043), (110.51617463644946, 151.2870218547401))\n",
    "\n",
    "    # grid_bin_bounding_session = first_filtered_session\n",
    "    # grid_bin_bounds = PlacefieldComputationParameters.compute_grid_bin_bounds(grid_bin_bounding_session.position.x, grid_bin_bounding_session.position.y)\n",
    "\n",
    "    ## OR use no grid_bin_bounds meaning they will be determined dynamically for each epoch:\n",
    "    # grid_bin_bounds = None\n",
    "    # time_bin_size = 0.03333 #1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = 0.1 # 10 fps\n",
    "    time_bin_size = kwargs.get('time_bin_size', 0.03333) # 0.03333 = 1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = kwargs.get('time_bin_size', 0.1) # 10 fps\n",
    "\n",
    "    # lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "    # assert lap_estimation_parameters is not None\n",
    "    active_session_computation_configs: List[DynamicContainer] = active_data_mode_registered_class.build_active_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=time_bin_size, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # , grid_bin_bounds=grid_bin_bounds\n",
    "\n",
    "else:\n",
    "    # Use the provided `active_session_computation_configs`:\n",
    "    assert 'time_bin_size' not in kwargs, f\"time_bin_size kwarg provided but will not be used because a custom active_session_computation_configs was provided as well.\"\n",
    "\n",
    "active_session_computation_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation_functions_name_includelist = []\n",
    "computation_functions_name_includelist = ['pf_computation','firing_rate_trends', 'position_decoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Setup Computation Functions to be executed:\n",
    "if computation_functions_name_includelist is None:\n",
    "    # includelist Mode:\n",
    "    computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        # '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "    computation_functions_name_excludelist=None\n",
    "else:\n",
    "    print(f'using provided computation_functions_name_includelist: {computation_functions_name_includelist}')\n",
    "    computation_functions_name_excludelist=None\n",
    "\n",
    "## For every computation config we build a fake (duplicate) filter config).\n",
    "# OVERRIDE WITH TRUE:\n",
    "# curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps['use_direction_dependent_laps'] = True # override with True\n",
    "lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "assert lap_estimation_parameters is not None\n",
    "use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', False) # whether to split the laps into left and right directions\n",
    "# use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', True) # whether to split the laps into left and right directions\n",
    "\n",
    "if (use_direction_dependent_laps or (len(active_session_computation_configs) > 3)):\n",
    "    lap_direction_suffix_list = ['_odd', '_even', '_any'] # ['maze1_odd', 'maze1_even', 'maze1_any', 'maze2_odd', 'maze2_even', 'maze2_any', 'maze_odd', 'maze_even', 'maze_any']\n",
    "    # lap_direction_suffix_list = ['_odd', '_even', ''] # no '_any' prefix, instead reuses the existing names\n",
    "    # assert len(lap_direction_suffix_list) == len(active_session_computation_configs), f\"len(lap_direction_suffix_list): {len(lap_direction_suffix_list)}, len(active_session_computation_configs): {len(active_session_computation_configs)}, \"\n",
    "else:\n",
    "    print(f'not using direction-dependent laps.')\n",
    "    lap_direction_suffix_list = ['']\n",
    "\n",
    "# active_session_computation_configs: this should contain three configs, one for each Epoch    \n",
    "active_session_computation_configs = [deepcopy(a_config) for a_config in active_session_computation_configs]\n",
    "\n",
    "#TODO 2024-10-30 13:22: - [ ] This is where we should override the params using `override_parameters_flat_keypaths_dict`\n",
    "# if override_parameters_flat_keypaths_dict is not None:\n",
    "# \tfor a_config in active_session_computation_configs:\n",
    "# \t\tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\ta_config.set_by_keypath(k, deepcopy(v))\n",
    "# \t\t\texcept Exception as e:\n",
    "# \t\t\t\t# raise e\n",
    "# \t\t\t\tprint(f'cannot set_by_keypath: {k} -- error: {e}. Skipping for now.')\n",
    "\n",
    "assert len(lap_direction_suffix_list) == len(active_session_computation_configs)\n",
    "updated_active_session_pseudo_filter_configs = {} # empty list, woot!\n",
    "\n",
    "\n",
    "for a_computation_suffix_name, a_computation_config in zip(lap_direction_suffix_list, active_session_computation_configs): # these should NOT be the same length: lap_direction_suffix_list: ['_odd', '_even', '_any']\n",
    "    # We need to filter and then compute with the appropriate config iteratively.\n",
    "    for a_filter_config_name, a_filter_config_fn in active_session_filter_configurations.items():\n",
    "        # TODO: Build a context:\n",
    "        a_combined_name: str = f'{a_filter_config_name}{a_computation_suffix_name}'\n",
    "        # if a_computation_suffix_name != '':\n",
    "        updated_active_session_pseudo_filter_configs[a_combined_name] = deepcopy(a_filter_config_fn) # this copy is just so that the values are recomputed with the appropriate config. This is a HACK\n",
    "    # end for filter_configs\n",
    "\n",
    "    ## Actually do the filtering now. We have \n",
    "    curr_active_pipeline.filter_sessions(updated_active_session_pseudo_filter_configs, changed_filters_ignore_list=['maze1','maze2','maze'], debug_print=False)\n",
    "\n",
    "    ## TODO 2023-01-15 - perform_computations for all configs!!\n",
    "    #TODO 2024-10-30 13:22: - [ ] This is where we should override the params\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "    # \t\ta_filter_config_fn.set_by_keypath(k, deepcopy(v))\n",
    "\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tcurr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n",
    "    #TODO 2023-10-31 14:58: - [ ] This is where the computations are being done multiple times!\n",
    "    #TODO 2023-11-13 14:23: - [ ] With this approach, we can't actually properly filter the computation_configs for the relevant sessions ahead of time because they are calculated for a single computation config but across all sessions at once.\n",
    "    curr_active_pipeline.perform_computations(a_computation_config, computation_functions_name_includelist=computation_functions_name_includelist, computation_functions_name_excludelist=computation_functions_name_excludelist, fail_on_exception=fail_on_exception, debug_print=debug_print) #, overwrite_extant_results=False  ], fail_on_exception=True, debug_print=False)\n",
    "\n",
    "    if override_parameters_flat_keypaths_dict is not None:\n",
    "        curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not skip_extended_batch_computations:\n",
    "    batch_extended_computations(curr_active_pipeline, include_global_functions=False, fail_on_exception=fail_on_exception, progress_print=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation'], debug_print=False, fail_on_exception=False) # includelist: ['_perform_baseline_placefield_computation']\n",
    "\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.prepare_for_display(root_output_dir=global_data_root_parent_path.joinpath('Output'), should_smooth_maze=True) # TODO: pass a display config\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to do `curr_active_pipeline.prepare_for_display(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "if not saving_mode.shouldSave:\n",
    "    print(f'saving_mode.shouldSave == False, so not saving at the end of batch_load_session')\n",
    "\n",
    "## Load pickled global computations:\n",
    "# If previously pickled global results were saved, they will typically no longer be relevent if the pipeline was recomputed. We need a system of invalidating/versioning the global results when the other computations they depend on change.\n",
    "# Maybe move into `batch_extended_computations(...)` or integrate with that somehow\n",
    "# curr_active_pipeline.load_pickled_global_computation_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e374c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "skip_global_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cb3e1",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Global computations loading:                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "# Loads saved global computations that were saved out via: `custom_save_filepaths['global_computation_pkl'] = curr_active_pipeline.save_global_computation_results(override_global_pickle_filename=custom_save_filenames['global_computation_pkl'])`\n",
    "## INPUTS: custom_save_filenames\n",
    "## INPUTS: curr_active_pipeline, (override_global_computation_results_pickle_path, skip_global_load), extended_computations_include_includelist\n",
    "\n",
    "if skip_global_load:\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skipping global load because skip_global_load==True')\n",
    "else:\n",
    "    # override_global_computation_results_pickle_path = custom_save_filenames['global_computation_pkl']\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "# Pre-load ___________________________________________________________________________________________________________ #\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list\n",
    "\n",
    "# Try Unpickling Global Computations to update pipeline ______________________________________________________________ #\n",
    "if (not force_reload) and (not skip_global_load): # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # INPUTS: override_global_computation_results_pickle_path\n",
    "        with set_posix_windows():\n",
    "            sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(override_global_computation_results_pickle_path=override_global_computation_results_pickle_path,\n",
    "                                                                                            allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist, ) # is new\n",
    "            print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "            did_any_paths_change: bool = curr_active_pipeline.post_load_fixup_sess_basedirs(updated_session_basepath=deepcopy(basedir)) ## use INPUT: basedir\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'force_reload: {force_reload}, saving_mode: {saving_mode}')\n",
    "force_reload\n",
    "saving_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e54ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: curr_active_pipeline.global_computation_results_pickle_path, skip_global_load\n",
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "print(f'override_pickle_path = \"{curr_active_pipeline.pickle_path}\",\\nactive_pickle_filename = \"{curr_active_pipeline.pickle_path.name}\"')\n",
    "print(f'override_global_pickle_path = \"{curr_active_pipeline.global_computation_results_pickle_path}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9674fd9",
   "metadata": {},
   "source": [
    "## OUTPUTS: `curr_active_pipeline`  0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣0️⃣ RESUME Normal Pipeline Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f755b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 0️⃣ Shared Post-Pipeline load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Lab'\n",
    " \n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        BATCH_DATE_TO_USE = f'{BATCH_DATE_TO_USE}{custom_suffix}'\n",
    "        print(f'Adding custom suffix: \"{custom_suffix}\" - BATCH_DATE_TO_USE: \"{BATCH_DATE_TO_USE}\"')\n",
    "except NameError as err:\n",
    "    custom_suffix = None\n",
    "    print(f'NO CUSTOM SUFFIX.')\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                           r\"K:\\scratch\\collected_outputs\",\n",
    "                                                           '/Users/pho/data/collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "all",
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607a444",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# / 🛑 End Run Section 🛑\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🎨 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "all",
     "run-group-display",
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5423e",
   "metadata": {},
   "source": [
    "## `LauncherWidget`: GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df7ce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Display import DisplayFunctionItem\n",
    "from pyphocorehelpers.gui.Qt.tree_helpers import find_tree_item_by_text\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_str: str = curr_active_pipeline.get_complete_session_identifier_string()\n",
    "widget.setWindowTitle(f'Spike3D Launcher: {session_id_str}')\n",
    "treeWidget.root\n",
    "# curr_active_pipeline.get_session_additional_parameters_context()\n",
    "# curr_active_pipeline.get_complete_session_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3ebce",
   "metadata": {},
   "source": [
    "## non-interactive batch plotting via `batch_perform_all_plots(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83293544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "\n",
    "_out = batch_perform_all_plots(curr_active_pipeline, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a240b9",
   "metadata": {},
   "source": [
    "## `Spike3DRasterWindowWidget` Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eea58",
   "metadata": {
    "tags": [
     "all",
     "spike_raster_window",
     "display",
     "gui",
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _setup_spike_raster_window_for_debugging\n",
    "\n",
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True)\n",
    "\n",
    "all_global_menus_actionsDict, global_flat_action_dict = _setup_spike_raster_window_for_debugging(spike_raster_window)\n",
    "\n",
    "# preview_overview_scatter_plot: pg.ScatterPlotItem  = active_2d_plot.plots.preview_overview_scatter_plot # ScatterPlotItem \n",
    "# preview_overview_scatter_plot.setDownsampling(auto=True, method='subsample', dsRate=10)\n",
    "main_graphics_layout_widget: pg.GraphicsLayoutWidget = active_2d_plot.ui.main_graphics_layout_widget\n",
    "wrapper_layout: pg.QtWidgets.QVBoxLayout = active_2d_plot.ui.wrapper_layout\n",
    "main_content_splitter = active_2d_plot.ui.main_content_splitter # QSplitter\n",
    "layout = active_2d_plot.ui.layout\n",
    "background_static_scroll_window_plot = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "active_window_container_layout = active_2d_plot.ui.active_window_container_layout # GraphicsLayout, first item of `main_graphics_layout_widget` -- just the active raster window I think, there is a strange black space above it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.plots.main_plot_widget\n",
    "\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "main_plot_widget.setMinimumHeight(20.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout\n",
    "# main_graphics_layout_widget.ci # GraphicsLayout\n",
    "main_graphics_layout_widget.ci.childItems()\n",
    "# main_graphics_layout_widget.setHidden(True) ## hides too much\n",
    "main_graphics_layout_widget.setHidden(False)\n",
    "\n",
    "# main_graphics_layout_widget\n",
    "\n",
    "active_window_container_layout.setBorder(pg.mkPen('yellow', width=4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout.allChildItems()\n",
    "active_window_container_layout.setPreferredHeight(200.0)\n",
    "active_window_container_layout.setMaximumHeight(800.0)\n",
    "active_window_container_layout.setSpacing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stretch factors to control priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(0, 400)  # Plot1: lowest priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(1, 2)  # Plot2: mid priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(2, 2)  # Plot3: highest priority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ParameterTreeWidget import create_parameter_tree_widget\n",
    "# win, param_tree = create_pipeline_filter_parameter_tree()\n",
    "win, param_tree = create_parameter_tree_widget(curr_active_pipeline.get_all_parameters())\n",
    "win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f61dd",
   "metadata": {
    "tags": [
     "_perform_plot_multi_decoder_meas_pred_position_track",
     "active-2025-01-16"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DecoderIdentityColors\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_plot_multi_decoder_meas_pred_position_track\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "## Build the new dock track:\n",
    "dock_identifier: str = 'Continuous Decoding Performance'\n",
    "ts_widget, fig, ax_list = active_2d_plot.add_new_matplotlib_render_plot_widget(name=dock_identifier)\n",
    "## Get the needed data:\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n",
    "\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "all_directional_continuously_decoded_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {k:v for k, v in (continuously_decoded_dict or {}).items() if k in TrackTemplates.get_decoder_names()} ## what is plotted in the `f'{a_decoder_name}_ContinuousDecode'` rows by `AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand`\n",
    "## OUT: all_directional_continuously_decoded_dict\n",
    "## Draw the position meas/decoded on the plot widget\n",
    "## INPUT: fig, ax_list, all_directional_continuously_decoded_dict, track_templates\n",
    "\n",
    "_out_artists =  _perform_plot_multi_decoder_meas_pred_position_track(curr_active_pipeline, fig, ax_list, desired_time_bin_size=0.058, enable_flat_line_drawing=True)\n",
    "\n",
    "\n",
    "## sync up the widgets\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget(dock_identifier, sync_mode=SynchronizedPlotMode.TO_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38325e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['truth_decoder_name'] = pos_df['truth_decoder_name'].fillna('')\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_color_dict: Dict[types.DecoderName, str] = DecoderIdentityColors.build_decoder_color_dict()\n",
    "\n",
    "decoded_pos_line_kwargs = dict(lw=1.0, color='gray', alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "inactive_decoded_pos_line_kwargs = dict(lw=0.3, alpha=0.2, marker='.', markersize=2, animated=False)\n",
    "active_decoded_pos_line_kwargs = dict(lw=1.0, alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "\n",
    "\n",
    "_out_data = {}\n",
    "_out_data_plot_kwargs = {}\n",
    "# curr_active_pipeline.global_computation_results.t\n",
    "for a_decoder_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    a_continuously_decoded_result = all_directional_continuously_decoded_dict[a_decoder_name]\n",
    "    a_decoder_color = decoder_color_dict[a_decoder_name]\n",
    "    \n",
    "    assert len(a_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = a_continuously_decoded_result.time_bin_containers[0]\n",
    "    time_window_centers = time_bin_containers.centers\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "    a_marginal_x = a_continuously_decoded_result.marginal_x_list[0]\n",
    "    # active_time_window_variable = a_decoder.active_time_window_centers\n",
    "    active_time_window_variable = time_window_centers\n",
    "    active_most_likely_positions_x = a_marginal_x['most_likely_positions_1D'] # a_decoder.most_likely_positions[:,0].T\n",
    "    _out_data[a_decoder_name] = pd.DataFrame({'t': time_window_centers, 'x': active_most_likely_positions_x, 'binned_time': np.arange(len(time_window_centers))})\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "    _out_data[a_decoder_name]['is_active_decoder_time'] = (_out_data[a_decoder_name]['truth_decoder_name'].fillna('', inplace=False) == a_decoder_name)\n",
    "\n",
    "    # is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "    active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "    active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "    ## could fill y with np.nan instead of getting shorter?\n",
    "    _out_data_plot_kwargs[a_decoder_name] = (dict(x=active_decoder_time_points, y=active_decoder_most_likely_positions_x, color=a_decoder_color, **active_decoded_pos_line_kwargs), dict(x=active_decoder_inactive_time_points, y=active_decoder_inactive_most_likely_positions_x, color=a_decoder_color, **inactive_decoded_pos_line_kwargs))\n",
    "\n",
    "_out_data_plot_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8972db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "\n",
    "# is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "\n",
    "_out_data[a_decoder_name] = ((active_decoder_time_points, active_decoder_most_likely_positions_x), (active_decoder_inactive_time_points, active_decoder_inactive_most_likely_positions_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_dfs = partition_df_dict(pos_df, partitionColumn='truth_decoder_name')\n",
    "\n",
    "a_decoder_name: str = 'short_LR'\n",
    "a_binned_time_grouped_df = partitioned_dfs[a_decoder_name].groupby('binned_time', axis='index', dropna=True)\n",
    "a_binned_time_grouped_df = a_binned_time_grouped_df.median().dropna(axis='index', subset=['x']) ## without the `.dropna(axis='index', subset=['x'])` part it gets an exhaustive df for all possible values of 'binned_time', even those not listed\n",
    "\n",
    "a_matching_binned_times = a_binned_time_grouped_df.reset_index(drop=False)['binned_time']\n",
    "a_matching_binned_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into two dfs for each decoder -- the supported and the unsupported\n",
    "partition\n",
    "\n",
    "PandasHelpers.safe_pandas_get_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.dropna(axis='index', subset=['lap', 'truth_decoder_name'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df: pd.DataFrame = global_laps_obj.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import find_epochs_overlapping_other_epochs\n",
    "\n",
    "## INPUTS: global_laps\n",
    "_out_split_pseudo2D_posteriors_dict = {}\n",
    "_out_split_pseudo2D_out_dict = {}\n",
    "pre_filtered_col_names = ['pre_filtered_most_likely_position_indicies', 'pre_filtered_most_likely_position'] # 'pre_filtered_time_bin_containers', 'pre_filtered_p_x_given_n', \n",
    "post_filtered_col_names = [a_col_name.removeprefix('pre_filtered_') for a_col_name in pre_filtered_col_names] # ['time_bin_containers', 'most_likely_position_indicies', 'most_likely_position']\n",
    "print(post_filtered_col_names)\n",
    "for a_time_bin_size, pseudo2D_decoder_continuously_decoded_result in continuously_decoded_pseudo2D_decoder_dict.items():\n",
    "    print(f'a_time_bin_size: {a_time_bin_size}')\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size] = {'pre_filtered_p_x_given_n': None, 'pre_filtered_time_bin_containers': None, 'pre_filtered_most_likely_position_indicies': None, 'pre_filtered_most_likely_position': None, \n",
    "                                                     'is_timebin_included': None, 'p_x_given_n': None} # , 'time_window_centers': None\n",
    "    # pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "    assert len(pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = pseudo2D_decoder_continuously_decoded_result.time_bin_containers[0]\n",
    "    # time_window_centers = time_bin_containers.centers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_position_indicies_list[0])\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_positions_list[0])\n",
    "    ## INPUTS: time_bin_containers, global_laps\n",
    "    left_edges = deepcopy(time_bin_containers.left_edges)\n",
    "    right_edges = deepcopy(time_bin_containers.right_edges)\n",
    "    continuous_time_binned_computation_epochs_df: pd.DataFrame = pd.DataFrame({'start': left_edges, 'stop': right_edges, 'label': np.arange(len(left_edges))})\n",
    "    is_timebin_included: NDArray = find_epochs_overlapping_other_epochs(epochs_df=continuous_time_binned_computation_epochs_df, epochs_df_required_to_overlap=deepcopy(global_laps))\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_p_x_given_n'] = p_x_given_n\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_time_bin_containers'] = time_bin_containers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['is_timebin_included'] = is_timebin_included\n",
    "    # continuous_time_binned_computation_epochs_df['is_in_laps'] = is_timebin_included\n",
    "    ## filter by whether it's included or not:\n",
    "    p_x_given_n = p_x_given_n[:, :, is_timebin_included]\n",
    "    # time_window_centers = \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['p_x_given_n'] = p_x_given_n\n",
    "    # _out_split_pseudo2D_out_dict[a_time_bin_size]['time_window_centers'] = time_window_centers[is_timebin_included]\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "\n",
    "    ## Split across the 2nd axis to make 1D posteriors that can be displayed in separate dock rows:\n",
    "    assert p_x_given_n.shape[1] == 4, f\"expected the 4 pseudo-y bins for the decoder in p_x_given_n.shape[1]. but found p_x_given_n.shape: {p_x_given_n.shape}\"\n",
    "    # split_pseudo2D_posteriors_dict = {k:np.squeeze(p_x_given_n[:, i, :]) for i, k in enumerate(('long_LR', 'long_RL', 'short_LR', 'short_RL'))}\n",
    "    _out_split_pseudo2D_posteriors_dict[a_time_bin_size] = deepcopy(p_x_given_n)\n",
    "    \n",
    "    # for a_col_name in pre_filtered_col_names:\n",
    "    #     filtered_col_name = a_col_name.removeprefix('pre_filtered_')\n",
    "    #     print(f'a_col_name: {a_col_name}, filtered_col_name: {filtered_col_name}, shape: {np.shape(_out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name])}')\n",
    "    #     _out_split_pseudo2D_out_dict[a_time_bin_size][filtered_col_name] = _out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name][is_timebin_included, :]\n",
    "        \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position_indicies'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'][:, is_timebin_included]\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'][is_timebin_included, :]\n",
    "    \n",
    "\n",
    "p_x_given_n.shape # (n_position_bins, n_decoding_models, n_time_bins) - (57, 4, 29951)\n",
    "\n",
    "## OUTPUTS: _out_split_pseudo2D_posteriors_dict, _out_split_pseudo2D_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_most_likely_position_comparsions\n",
    "\n",
    "# fig, axs = plot_most_likely_position_comparsions(pho_custom_decoder, axs=ax, sess.position.to_dataframe())\n",
    "fig, axs = plot_most_likely_position_comparsions(computation_result.computed_data['pf2D_Decoder'], computation_result.sess.position.to_dataframe(), **overriding_dict_with(lhs_dict={'show_posterior':True, 'show_one_step_most_likely_positions_plots':True}, **kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f34d9",
   "metadata": {},
   "source": [
    "## Overflow/Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f49198",
   "metadata": {},
   "source": [
    "# 💾 Save Export Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd78276",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_complete_session_context()\n",
    "custom_save_filepaths, custom_save_filenames, custom_suffix = curr_active_pipeline.get_custom_pipeline_filenames_from_parameters()\n",
    "custom_save_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3258421",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filenames['pipeline_pkl']\n",
    "custom_save_filenames['global_computation_pkl']\n",
    "\n",
    "pickle_path = 'loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl'\n",
    "global_computation_pkl = 'global_computation_results_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename=curr_active_pipeline.pickle_path.name) #active_pickle_filename=\n",
    "# curr_active_pipeline.save_global_computation_results(override_global_pickle_path=curr_active_pipeline.global_computation_results_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename='loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl') #active_pickle_filename=\n",
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl') #active_pickle_filename="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4860781",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results(override_global_pickle_filename='global_computation_results_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300323f",
   "metadata": {},
   "source": [
    "### Custom Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46794d1b",
   "metadata": {},
   "source": [
    "# 2025-02-18 - Napari "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb91c76",
   "metadata": {},
   "source": [
    "# 2025-01-30 - Rat Heading-Angle from Position Change Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a true global session encompassing all epochs\n",
    "# curr_active_pipeline.sess.epochs\n",
    "global_session = deepcopy(curr_active_pipeline.filtered_sessions['roam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS: global_session \n",
    "from neuropy.analyses.placefields import Position\n",
    "\n",
    "\n",
    "global_pos_obj: Position = deepcopy(global_session.position)\n",
    "global_pos_df: pd.DataFrame = global_pos_obj.compute_higher_order_derivatives().position.compute_smoothed_position_info(N=15)\n",
    "global_pos_df['approx_head_dir_degrees'] = ((np.rad2deg(np.arctan2(global_pos_df['velocity_y_smooth'], global_pos_df['velocity_x_smooth'])) + 360) % 360) # arctan2 is required to get the angle right\n",
    "global_pos_df = global_pos_df.dropna(axis='index', subset=['approx_head_dir_degrees'])\n",
    "global_pos_df\n",
    "# Convert angles to radians\n",
    "angles = np.deg2rad(global_pos_df['approx_head_dir_degrees'])\n",
    "\n",
    "# Create circular histogram\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "ax.hist(angles, bins=36, density=True, alpha=0.70)\n",
    "\n",
    "# Set labels\n",
    "ax.set_title(\"Circular Histogram of Head Direction\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f802e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given a pd.DataFrame with columns ['x', 'y', 'approx_head_dir_degrees'], compute binned versions of these variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7da0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deepcopy(global_pos_df)\n",
    "\n",
    "# Normalize time to use as radius\n",
    "radii = (df['t'] - df['t'].min()) / (df['t'].max() - df['t'].min())\n",
    "\n",
    "# Create circular scatter plot\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "ax.scatter(angles, radii, alpha=0.20, s=1)\n",
    "# ax.plot(angles, radii, alpha=0.20, s=1)\n",
    "# Set labels\n",
    "ax.set_title(\"Circular Scatter Plot of Head Direction Over Time\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607647d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create circular scatter plot with line connecting points\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(angles, radii, alpha=0.75, s=5)  # Smaller point size\n",
    "\n",
    "# Connect points with a line\n",
    "ax.plot(angles, radii, alpha=0.5, linewidth=1)\n",
    "\n",
    "# Set labels\n",
    "ax.set_title(\"Circular Scatter Plot of Head Direction Over Time\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891f4a3",
   "metadata": {},
   "source": [
    "## Binning Position, Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13388664",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbin_edges = global_pf2D.xbin\n",
    "ybin_edges = global_pf2D.ybin\n",
    "# Create evenly spaced bin edges from 0 to 360\n",
    "n_dir_bins: int = 8\n",
    "angle_dir_bin_edges = np.linspace(0, 360, n_dir_bins + 1)\n",
    "\n",
    "n_xbins: int = len(xbin_edges) - 1\n",
    "n_ybins: int = len(ybin_edges) - 1\n",
    "n_dir_bins: int = len(angle_dir_bin_edges) - 1\n",
    "\n",
    "print(f'n_xbins: {n_xbins}, n_ybins: {n_ybins}, n_dir_bins: {n_dir_bins}')\n",
    "\n",
    "# Use pd.cut with the explicit bin edges\n",
    "global_pos_df['head_dir_angle_binned'] = pd.cut(global_pos_df['approx_head_dir_degrees'], bins=angle_dir_bin_edges, labels=False, include_lowest=True)\n",
    "global_pos_df = global_pos_df.position.adding_binned_position_columns(xbin_edges=xbin_edges, ybin_edges=ybin_edges)\n",
    "global_pos_df = global_pos_df.dropna(axis='index', subset=['binned_x', 'binned_y', 'head_dir_angle_binned'])\n",
    "global_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_attributes(short_name=None, tags=['working', 'angular'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2025-02-21 00:48', related_items=[])\n",
    "def compute_3d_occupancy_map(df, n_x_bins=50, n_y_bins=50, n_dir_bins=8):\n",
    "    \"\"\"Creates a 3D occupancy map with fixed dimensions regardless of observed data\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with binned columns\n",
    "        n_x_bins (int): Number of x position bins\n",
    "        n_y_bins (int): Number of y position bins\n",
    "        n_dir_bins (int): Number of head direction bins\n",
    "    \"\"\"\n",
    "    # Create all possible combinations\n",
    "    x_bins = range(n_x_bins)\n",
    "    y_bins = range(n_y_bins)\n",
    "    dir_bins = range(n_dir_bins)\n",
    "    \n",
    "    # Use crosstab with specific bins to force output size\n",
    "    occupancy_map = pd.crosstab(\n",
    "        index=[df['binned_x'], df['binned_y']], \n",
    "        columns=df['head_dir_angle_binned'],\n",
    "        dropna=False  # Keep all combinations\n",
    "    ).reindex(\n",
    "        index=pd.MultiIndex.from_product([x_bins, y_bins]),\n",
    "        columns=dir_bins,\n",
    "        fill_value=0  # Fill missing combinations with 0\n",
    "    ).values.reshape(n_x_bins, n_y_bins, n_dir_bins)\n",
    "    \n",
    "    return occupancy_map, {'x': n_x_bins, 'y': n_y_bins, 'dir': n_dir_bins}\n",
    "\n",
    "\n",
    "# 1. Compute the 3D occupancy map\n",
    "# occupancy_map, bin_counts = compute_3d_occupancy_map(global_pos_df)\n",
    "\n",
    "occupancy_map, bin_counts = compute_3d_occupancy_map(global_pos_df, n_x_bins=n_xbins, n_y_bins=n_ybins, n_dir_bins=n_dir_bins)\n",
    "\n",
    "# Print the shape and counts\n",
    "print(f\"Occupancy map shape: {occupancy_map.shape}\")\n",
    "print(f\"Unique bins per dimension: {bin_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.Qt import QtCore, QtGui\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow\n",
    "\n",
    "class CircularBinnedImageRenderingWindow(BasicBinnedImageRenderingWindow):\n",
    "    \"\"\"Renders circular/angular heatmaps within each spatial bin\"\"\"\n",
    "    \n",
    "    def __init__(self, angular_matrix, xbins=None, ybins=None, n_angle_bins: int=None, **kwargs):\n",
    "        \"\"\"\n",
    "        angular_matrix: shape (n_x_bins, n_y_bins, n_angle_bins) containing angular distribution data\n",
    "        \"\"\"\n",
    "        assert n_angle_bins is not None\n",
    "        pos_only_mat = np.sum(deepcopy(angular_matrix), axis=-1)\n",
    "        super().__init__(matrix=pos_only_mat, xbins=xbins, ybins=ybins, **kwargs)\n",
    "        self.n_angle_bins = n_angle_bins\n",
    "        self.angular_data = deepcopy(angular_matrix)\n",
    "        \n",
    "    def add_circular_heatmap(self, bin_x: int, bin_y: int, angular_data: np.ndarray) -> None:\n",
    "        \"\"\"Adds a circular heatmap to a specific spatial bin\"\"\"\n",
    "        # Create circular representation\n",
    "        theta = np.linspace(0, 2*np.pi, self.n_angle_bins+1)[:-1]  # Angular positions\n",
    "        r = angular_data  # Radial values from angular distribution\n",
    "        \n",
    "        # Convert to cartesian coordinates\n",
    "        x = r * np.cos(theta)\n",
    "        y = r * np.sin(theta)\n",
    "        \n",
    "        # Create polygon for the circular heatmap\n",
    "        polygon = QtGui.QPolygonF()\n",
    "        for px, py in zip(x, y):\n",
    "            polygon.append(QtCore.QPointF(px + bin_x + 0.5, py + bin_y + 0.5))\n",
    "            \n",
    "        # Create path for smooth rendering\n",
    "        path = QtGui.QPainterPath()\n",
    "        path.addPolygon(polygon)\n",
    "        path.closeSubpath()\n",
    "        \n",
    "        # Create graphics item\n",
    "        item = pg.QtGui.QGraphicsPathItem(path)\n",
    "        \n",
    "        # Set color based on distribution intensity\n",
    "        color = pg.mkColor('w')  # Base color\n",
    "        color.setAlphaF(0.7)     # Semi-transparent\n",
    "        item.setBrush(pg.mkBrush(color))\n",
    "        item.setPen(pg.mkPen(None))  # No border\n",
    "        \n",
    "        # Add to plot\n",
    "        assert len(window.plot_names) > 0 is not None # 'angular_distribution'\n",
    "        plot_name: str = window.plot_names[0]\n",
    "        assert plot_name in self.plots, f\"plot_name: {plot_name} not in self.plots\"\n",
    "        self.plots[plot_name].mainPlotItem.addItem(item)\n",
    "        \n",
    "\n",
    "    def render_all_circular_heatmaps(self):\n",
    "        \"\"\"Renders circular heatmaps for all spatial bins\"\"\"\n",
    "        n_x, n_y, _ = self.angular_data.shape\n",
    "        \n",
    "        for x in range(n_x):\n",
    "            for y in range(n_y):\n",
    "                angular_dist = self.angular_data[x, y]\n",
    "                # Normalize the distribution\n",
    "                if np.sum(angular_dist) > 0:\n",
    "                    angular_dist = angular_dist / np.max(angular_dist)\n",
    "                    self.add_circular_heatmap(x, y, angular_dist)\n",
    "\n",
    "    def init_UI(self):\n",
    "        \"\"\"Initialize the UI and render circular heatmaps\"\"\"\n",
    "        super().init_UI()\n",
    "        # self.render_all_circular_heatmaps()\n",
    "\n",
    "\n",
    "\n",
    "# ## INPUTS: occupancy_map, n_xbins, n_ybins n_x_bins=n_xbins, n_y_bins=n_ybins, n_dir_bins=n_dir_bins\n",
    "# Create sample angular distribution data\n",
    "# n_x_bins, n_y_bins = 10, 10\n",
    "# n_angle_bins = 36\n",
    "# angular_matrix = np.random.rand(n_x_bins, n_y_bins, n_angle_bins)\n",
    "angular_matrix = deepcopy(occupancy_map)\n",
    "\n",
    "# Create window\n",
    "window = CircularBinnedImageRenderingWindow(\n",
    "    angular_matrix=angular_matrix,\n",
    "    xbins=np.arange(n_xbins),\n",
    "    ybins=np.arange(n_ybins),\n",
    "    n_angle_bins=n_dir_bins,\n",
    "    name='angular_distribution',\n",
    "    title='Angular Distribution per Position Bin'\n",
    ")\n",
    "\n",
    "window.render_all_circular_heatmaps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spatial_angular_distributions(occupancy_map, subsample_factor=5):\n",
    "    \"\"\"Plot radar charts of angular distributions across spatial positions\n",
    "    \n",
    "    Args:\n",
    "        occupancy_map (np.ndarray): 3D array (x_bins, y_bins, direction_bins)\n",
    "        subsample_factor (int): Plot every Nth spatial bin to avoid overcrowding\n",
    "\n",
    "\n",
    "    Usage:    \n",
    "        fig, ax = plot_spatial_angular_distributions(occupancy_map, subsample_factor=4)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    n_x, n_y, n_angles = occupancy_map.shape\n",
    "    \n",
    "    # Create main figure\n",
    "    fig, ax = plt.subplots(figsize=(25, 15), clear=True, num='test')\n",
    "    \n",
    "    # Calculate angles for radar plot (in radians)\n",
    "    # theta = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n",
    "    # Calculate bin edges for rose plot\n",
    "    bins = np.linspace(0, 2*np.pi, n_angles+1)    \n",
    "\n",
    "    \n",
    "    # Plot radar at each subsampled position\n",
    "    for i in range(0, n_x, subsample_factor):\n",
    "        for j in range(0, n_y, subsample_factor):\n",
    "            # Get angular distribution at this position\n",
    "            values = occupancy_map[i,j,:]\n",
    "            \n",
    "            # Create small axes for this position\n",
    "            # radar_ax = fig.add_axes([i/n_x, j/n_y, 1/n_x, 1/n_y], projection='polar')\n",
    "            # radar_ax.plot(theta, values)\n",
    "            # radar_ax.fill(theta, values, alpha=0.25)\n",
    "            \n",
    "            # Create small axes for this position\n",
    "            _new_radial_ax = fig.add_axes([i/n_x, j/n_y, 1/n_x, 1/n_y], projection='polar')\n",
    "            \n",
    "            # Create rose plot using hist\n",
    "            _new_radial_ax.hist(bins[:-1], bins=bins, weights=values, density=False, histtype='stepfilled')\n",
    "\n",
    "            # pc = _new_radial_ax.pcolormesh(A, R, hist.T, cmap=\"magma_r\")\n",
    "            # fig.colorbar(pc)\n",
    "            # _new_radial_ax.grid(True)\n",
    "\n",
    "            _new_radial_ax.set_xticks([])\n",
    "            _new_radial_ax.set_yticks([])\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# angles = np.deg2rad(global_pos_df['approx_head_dir_degrees'])\n",
    "# # Create circular histogram\n",
    "# fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "# ax.hist(angles, bins=36, density=True, alpha=0.70)\n",
    "# # Set labels\n",
    "# ax.set_title(\"Circular Histogram of Head Direction\")\n",
    "\n",
    "\n",
    "fig, ax = plot_spatial_angular_distributions(occupancy_map, subsample_factor=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24471be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def radial_histogram(data, bins=12, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='polar')\n",
    "    counts, edges = np.histogram(data, bins=bins, range=(0, 2*np.pi))\n",
    "    widths = np.diff(edges)\n",
    "    ax.bar(edges[:-1], counts, width=widths, bottom=0, align='edge', color='blue', alpha=0.5)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax\n",
    "\n",
    "def plot_spatial_angular_distributions(occupancy_map, subsample_factor=5):\n",
    "    n_x, n_y, n_angles = occupancy_map.shape\n",
    "    fig, ax = plt.subplots(figsize=(25, 15), clear=True, num='test')\n",
    "\n",
    "    # Draw grid boxes for each x/y bin\n",
    "    for i in range(n_x):\n",
    "        for j in range(n_y):\n",
    "            x0 = i / n_x\n",
    "            y0 = j / n_y\n",
    "            w_ = 1 / n_x\n",
    "            h_ = 1 / n_y\n",
    "            rect = plt.Rectangle((x0, y0), w_, h_, fill=False, color='black', lw=1, transform=fig.transFigure)\n",
    "            fig.add_artist(rect)\n",
    "\n",
    "    # Size of each small polar subplot\n",
    "    w = 0.6 * (subsample_factor / n_x)\n",
    "    h = 0.6 * (subsample_factor / n_y)\n",
    "\n",
    "    for i in range(0, n_x, subsample_factor):\n",
    "        for j in range(0, n_y, subsample_factor):\n",
    "            counts = occupancy_map[i, j, :]\n",
    "            angles = np.hstack([np.full(int(counts[k]), (2*np.pi*(k + 0.5)) / n_angles) for k in range(n_angles)])\n",
    "            pos_x = i / n_x\n",
    "            pos_y = j / n_y\n",
    "            ax_sub = fig.add_axes([pos_x, pos_y, w, h], projection='polar')\n",
    "            radial_histogram(angles, bins=n_angles, ax=ax_sub)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "fig, ax = plot_spatial_angular_distributions(occupancy_map, subsample_factor=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_radial_lines(rect_width, rect_height, n_bins):\n",
    "    \"\"\"Draws lines from rectangle center to perimeter, creating equal angular divisions\n",
    "    \n",
    "    Args:\n",
    "        rect_width (float): Width of rectangle\n",
    "        rect_height (float): Height of rectangle\n",
    "        n_bins (int): Number of angular divisions desired\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: [(x1,y1,x2,y2)] coordinates for each line\n",
    "    \"\"\"\n",
    "    # Calculate center point\n",
    "    center_x = rect_width / 2\n",
    "    center_y = rect_height / 2\n",
    "    \n",
    "    # Calculate angles for each division\n",
    "    angles = np.linspace(0, 2*np.pi, n_bins, endpoint=False)\n",
    "    \n",
    "    lines = []\n",
    "    for angle in angles:\n",
    "        # Calculate direction vector\n",
    "        dx = np.cos(angle)\n",
    "        dy = np.sin(angle)\n",
    "        \n",
    "        # Find intersection with rectangle boundary\n",
    "        # Scale factor t = min positive value that hits boundary\n",
    "        t_values = []\n",
    "        \n",
    "        # Check horizontal boundaries\n",
    "        if dx != 0:\n",
    "            t_values.extend([\n",
    "                (0 - center_x) / dx,  # Left boundary\n",
    "                (rect_width - center_x) / dx  # Right boundary\n",
    "            ])\n",
    "            \n",
    "        # Check vertical boundaries\n",
    "        if dy != 0:\n",
    "            t_values.extend([\n",
    "                (0 - center_y) / dy,  # Bottom boundary\n",
    "                (rect_height - center_y) / dy  # Top boundary\n",
    "            ])\n",
    "            \n",
    "        # Get smallest positive t value\n",
    "        t = min(t for t in t_values if t > 0)\n",
    "        \n",
    "        # Calculate endpoint\n",
    "        end_x = center_x + t * dx\n",
    "        end_y = center_y + t * dy\n",
    "        \n",
    "        lines.append((center_x, center_y, end_x, end_y))\n",
    "    \n",
    "    return lines\n",
    "\n",
    "plt.figure(num='box_line_test',clear=True)\n",
    "# Draw 8 radial divisions in a 100x80 rectangle\n",
    "lines = draw_radial_lines(100, 80, 8)\n",
    "\n",
    "# Plot the lines\n",
    "for x1,y1,x2,y2 in lines:\n",
    "    plt.plot([x1,x2], [y1,y2], 'k-')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_directional_occupancy(occupancy_map, direction_bin):\n",
    "    \"\"\"Plot 2D heatmap for a specific head direction bin\"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(occupancy_map[:,:,direction_bin], origin='lower')\n",
    "    plt.colorbar(label='Count')\n",
    "    plt.title(f'Occupancy for Direction Bin {direction_bin}')\n",
    "    plt.xlabel('X bin')\n",
    "    plt.ylabel('Y bin')\n",
    "\n",
    "\n",
    "# 2. Visualize a single direction slice\n",
    "direction_bin = 1  # Example: looking at 180 degrees if using 36 bins\n",
    "plot_directional_occupancy(occupancy_map, direction_bin)\n",
    "\n",
    "# 3. Get total occupancy across all directions\n",
    "total_spatial_occupancy = np.sum(occupancy_map, axis=2)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(total_spatial_occupancy, origin='lower')\n",
    "plt.colorbar(label='Total Count')\n",
    "plt.title('Total Spatial Occupancy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spatial_angular_distributions(occupancy_map, subsample_factor=5):\n",
    "    \"\"\"Plot radar charts of angular distributions across spatial positions\n",
    "    \n",
    "    Args:\n",
    "        occupancy_map (np.ndarray): 3D array (x_bins, y_bins, direction_bins)\n",
    "        subsample_factor (int): Plot every Nth spatial bin to avoid overcrowding\n",
    "    \"\"\"\n",
    "    n_x, n_y, n_angles = occupancy_map.shape\n",
    "    \n",
    "    # Create main figure\n",
    "    fig, ax = plt.subplots(figsize=(25, 15), clear=True, num='test')\n",
    "    \n",
    "    # Calculate angles for radar plot (in radians)\n",
    "    # theta = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n",
    "    # Calculate bin edges for rose plot\n",
    "    bins = np.linspace(0, 2*np.pi, n_angles+1)    \n",
    "\n",
    "    \n",
    "    # Plot radar at each subsampled position\n",
    "    for i in range(0, n_x, subsample_factor):\n",
    "        for j in range(0, n_y, subsample_factor):\n",
    "            # Get angular distribution at this position\n",
    "            values = occupancy_map[i,j,:]\n",
    "            \n",
    "            # Create small axes for this position\n",
    "            # radar_ax = fig.add_axes([i/n_x, j/n_y, 1/n_x, 1/n_y], projection='polar')\n",
    "            # radar_ax.plot(theta, values)\n",
    "            # radar_ax.fill(theta, values, alpha=0.25)\n",
    "            \n",
    "            # Create small axes for this position\n",
    "            _new_radial_ax = fig.add_axes([i/n_x, j/n_y, 1/n_x, 1/n_y], projection='polar')\n",
    "            \n",
    "            # Create rose plot using hist\n",
    "            _new_radial_ax.hist(bins[:-1], bins=bins, weights=values, density=False, histtype='stepfilled')\n",
    "\n",
    "            # pc = _new_radial_ax.pcolormesh(A, R, hist.T, cmap=\"magma_r\")\n",
    "            # fig.colorbar(pc)\n",
    "            # _new_radial_ax.grid(True)\n",
    "\n",
    "            _new_radial_ax.set_xticks([])\n",
    "            _new_radial_ax.set_yticks([])\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# angles = np.deg2rad(global_pos_df['approx_head_dir_degrees'])\n",
    "# # Create circular histogram\n",
    "# fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "# ax.hist(angles, bins=36, density=True, alpha=0.70)\n",
    "# # Set labels\n",
    "# ax.set_title(\"Circular Histogram of Head Direction\")\n",
    "\n",
    "\n",
    "fig, ax = plot_spatial_angular_distributions(occupancy_map, subsample_factor=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "50*5*8\n",
    "\n",
    "# 135/5 -> 27\n",
    "# 472/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dada207",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbin_edges = global_pf2D.xbin\n",
    "ybin_edges = global_pf2D.ybin\n",
    "# Create evenly spaced bin edges from 0 to 360\n",
    "n_dir_bins: int = 8\n",
    "angle_dir_bin_edges = np.linspace(0, 360, n_dir_bins + 1)\n",
    "\n",
    "# Use pd.cut with the explicit bin edges\n",
    "global_pos_df['head_dir_angle_binned'] = pd.cut(global_pos_df['approx_head_dir_degrees'], bins=angle_dir_bin_edges, labels=False, include_lowest=True)\n",
    "global_pos_df = global_pos_df.position.adding_binned_position_columns(xbin_edges=xbin_edges, ybin_edges=ybin_edges)\n",
    "global_pos_df = global_pos_df.dropna(axis='index', subset=['binned_x', 'binned_y', 'head_dir_angle_binned'])\n",
    "global_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbin_edges = global_pf2D.xbin\n",
    "ybin_edges = global_pf2D.ybin\n",
    "# Create evenly spaced bin edges from 0 to 360\n",
    "n_dir_bins: int = 8\n",
    "angle_dir_bin_edges = np.linspace(0, 360, n_dir_bins + 1)\n",
    "\n",
    "# Use pd.cut with the explicit bin edges\n",
    "global_pos_df['head_dir_angle_binned'] = pd.cut(global_pos_df['approx_head_dir_degrees'], bins=angle_dir_bin_edges, labels=False, include_lowest=True)\n",
    "global_pos_df = global_pos_df.position.adding_binned_position_columns(xbin_edges=xbin_edges, ybin_edges=ybin_edges)\n",
    "global_pos_df = global_pos_df.dropna(axis='index', subset=['binned_x', 'binned_y', 'head_dir_angle_binned'])\n",
    "global_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e655c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbin_edges = global_pf2D.xbin\n",
    "ybin_edges = global_pf2D.ybin\n",
    "# Create evenly spaced bin edges from 0 to 360\n",
    "n_dir_bins: int = 8\n",
    "angle_dir_bin_edges = np.linspace(0, 360, n_dir_bins + 1)\n",
    "\n",
    "# Use pd.cut with the explicit bin edges\n",
    "global_pos_df['head_dir_angle_binned'] = pd.cut(global_pos_df['approx_head_dir_degrees'], bins=angle_dir_bin_edges, labels=False, include_lowest=True)\n",
    "global_pos_df = global_pos_df.position.adding_binned_position_columns(xbin_edges=xbin_edges, ybin_edges=ybin_edges)\n",
    "global_pos_df = global_pos_df.dropna(axis='index', subset=['binned_x', 'binned_y', 'head_dir_angle_binned'])\n",
    "global_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d91528",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now I have the columns `global_pos_df[['binned_x', 'binned_y', 'head_dir_angle_binned']]` and I'd like to visualize a heatmap showing:\n",
    "1. and "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
