"""
This type stub file was generated by pyright.
"""

import numpy as np
import pandas as pd
from typing import Any, Callable, Dict, List, Optional, Tuple
from nptyping import NDArray
from dataclasses import dataclass
from attrs import define
from pyphocorehelpers.function_helpers import function_attributes

def safe_get(list, index, fallback_value):
    """Similar to dict's .get(key, fallback) function but for lists. Returns a fallback/default value if the index is not valid for the list, otherwise returns the value at that index.
    Args:
        list (_type_): a list-like object
        index (_type_): an index into the list
        fallback_value (_type_): any value to be returned when the indexing fails

    Returns:
        _type_: the value in the list, or the fallback_value is the index is not valid for the list.
    """
    ...

def safe_len(v): # -> int | None:
    """ 2023-05-08 - tries to return the length of v if possible, otherwise returns None """
    ...

def safe_find_index_in_list(a_list, a_search_obj): # -> int | None:
    """ tries to find the index of `a_search_obj` in the list `a_list` 
    If found, returns the index
    If not found, returns None (instead of throwing a ValueError which is the default)
    
    Example:
        an_ax = plots.axs[2]
        safe_find_index_in_list(plots.axs, an_ax)
        # list(plots.axs).index(an_ax)
    """
    ...

def is_consecutive_no_gaps(arr, enable_debug_print=...): # -> bool:
    """ Checks whether a passed array/list is a series of ascending indicies without gaps
    
    arr: listlike: checks if the series is from [0, ... , len(arr)-1]
    
    Usage:
        neuron_IDXs = extracted_neuron_IDXs
        is_consecutive_no_gaps(cell_ids, neuron_IDXs)
    """
    ...

def bidirectional_setdiff1d(arr0, arr1): # -> tuple[NDArray[Any], NDArray[Any]]:
    """ returns a tuple containing the bidirectional setdiff1D in each direction (they can differ) """
    ...

def sorted_slice(a, l, r):
    ...

def chunks(iterable, size=...): # -> Generator[Generator[Any, Any, None], Any, None]:
    """ Chunking

    Args:
        iterable ([type]): [description]
        size (int, optional): [description]. Defaults to 10.

    Usage:
        laps_pages = [list(chunk) for chunk in _chunks(sess.laps.lap_id, curr_num_subplots)]
    """
    ...

def build_pairwise_indicies(target_indicies, debug_print=...): # -> list[tuple[Any, Any]]:
    """ Builds pairs of indicies from a simple list of indicies, for use in computing pairwise operations.
    
    Example:
        target_indicies = np.arange(5) # [0, 1, 2, 3, 4]
        out_pair_indicies = build_pairwise_indicies(target_indicies)
            > out_pair_indicies: [(0, 1), (1, 2), (2, 3), (3, 4)]
    
    Args:
        target_indicies ([type]): [description]
        debug_print (bool, optional): [description]. Defaults to False.

    Returns:
        [type]: [description]
  
    Usage:
        target_indicies = np.arange(5)
        out_pair_indicies = build_pairwise_indicies(target_indicies)
        # out_pair_indicies = list(out_pair_indicies)
        # print(f'out_pair_indicies: {list(out_pair_indicies)}')


        print(f'out_pair_indicies: {list(out_pair_indicies)}')

        for i, pair in enumerate(list(out_pair_indicies)):
            # first_item_lap_idx, next_item_lap_idx
            print(f'i: {i}, pair: {pair}')
    """
    ...

def interleave_elements(start_points, end_points, debug_print: bool = ...): # -> NDArray[Any]:
    """ Given two equal sized arrays, produces an output array of double that size that contains elements of start_points interleaved with elements of end_points
    Example:
        a_starts = ['A','B','C','D']
        a_ends = ['a','b','c','d']
        a_interleaved = interleave_elements(a_starts, a_ends)
        >> a_interleaved: ['A','a','B','b','C','c','D','d']
    """
    ...

def are_all_equal(arr) -> bool:
    """ returns True if arr is empty, or if all elements of arr are equal to each other """
    ...

def get_dict_subset(a_dict, included_keys=..., require_all_keys=...): # -> dict[Any, Any]:
    """Gets a subset of a dictionary from a list of keys (included_keys)

    Args:
        a_dict ([type]): [description]
        included_keys ([type], optional): [description]. Defaults to None.
        require_all_keys: Bool, if True, requires all keys in included_keys to be in the dictionary (a_dict)

    Returns:
        [type]: [description]
    """
    ...

def validate_reverse_index_map(value_to_original_index_reverse_map, neuron_IDXs, cell_ids, debug_print=...): # -> bool:
    """
    Used to be called `validate_cell_IDs_to_CellIDXs_map`

    value_to_original_index_reverse_map: is a dictioanry that has any thing for its keys, but each
        Example:
            # Allows reverse indexing into the linear imported array using the original cell ID indicies:
            id_arr = [ 2  3  4  5  7  8  9 10 11 12 14 17 18 21 22 23 24 25 26 27 28 29 33 34 38 39 42 44 45 46 47 48 53 55 57 58 61 62 63 64]
            linear_flitered_ids = np.arange(len(id_arr)) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
            value_to_original_index_reverse_map = dict(zip(id_arr, linear_flitered_ids))    
     
    
    Usage:
        cell_ids = extracted_cell_ids
        neuron_IDXs = extracted_neuron_IDXs
        reverse_cellID_index_map = ipcDataExplorer.active_session.neurons.reverse_cellID_index_map
        validate_reverse_index_map(reverse_cellID_index_map, cell_ids, neuron_IDXs)
    """
    ...

def nested_dict_set(dic, key_list, value, create_missing=...):
    """ Allows setting the value of a nested dictionary hierarchy by drilling in with the keys in key_list, creating intermediate dictionaries if needed.
    
    Attribution and Credit:
        https://stackoverflow.com/a/49290758/9732163
    
    Usage:
        d = {}
        nested_set(d, ['person', 'address', 'city'], 'New York')
        d
        >> {'person': {'address': {'city': 'New York'}}}
    """
    ...

def flatpaths_to_nested_dicts(flat_paths_form_dict, default_value_override=..., flat_path_delimiter=..., debug_print=...): # -> dict[Any, Any]:
    """ Reciprocal of nested_dicts_to_flatpaths(...)

    Args:
        flat_paths_list (_type_): _description_
        default_value_override (str, optional): _description_. Defaults to 'Test Value'.
        debug_print (bool, optional): _description_. Defaults to False.

    Returns:
        _type_: _description_
        
    Usage:
        from pyphocorehelpers.indexing_helpers import nested_dicts_to_flatpaths, flatpaths_to_nested_dicts
        flatpaths_to_nested_dicts(['SpikeAnalysisComputations._perform_spike_burst_detection_computation',
        'ExtendedStatsComputations._perform_placefield_overlap_computation',
        'ExtendedStatsComputations._perform_firing_rate_trends_computation',
        'ExtendedStatsComputations._perform_extended_statistics_computation',
        'DefaultComputationFunctions._perform_velocity_vs_pf_density_computation',
        'DefaultComputationFunctions._perform_two_step_position_decoding_computation',
        'DefaultComputationFunctions._perform_position_decoding_computation',
        'PlacefieldComputations._perform_time_dependent_placefield_computation',
        'PlacefieldComputations._perform_baseline_placefield_computation'])


        flatpaths_to_nested_dicts(_temp_compuitations_flat_functions_list)
    
    """
    ...

_GLOBAL_MAX_DEPTH = ...
def nested_dicts_to_flatpaths(curr_key, curr_value, max_depth=..., depth=..., flat_path_delimiter=..., debug_print=...): # -> dict[Any, Any] | dict[Any | str, Any] | None:
    """ Reciprocal of flatpaths_to_nested_dicts(...)

        curr_key: None to start
        curr_value: assumed to be nested_hierarchy_dict to start


    Usage:
        from pyphocorehelpers.indexing_helpers import nested_dicts_to_flatpaths, flatpaths_to_nested_dicts
        _out_flatpaths_dict = nested_dicts_to_flatpaths('', _temp_compuitations_functions_list)
        _out_flatpaths_dict

        {'SpikeAnalysisComputations._perform_spike_burst_detection_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeAnalysisComputations._perform_spike_burst_detection_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'ExtendedStatsComputations._perform_placefield_overlap_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ExtendedStats.ExtendedStatsComputations._perform_placefield_overlap_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'ExtendedStatsComputations._perform_firing_rate_trends_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ExtendedStats.ExtendedStatsComputations._perform_firing_rate_trends_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'ExtendedStatsComputations._perform_extended_statistics_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ExtendedStats.ExtendedStatsComputations._perform_extended_statistics_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'DefaultComputationFunctions._perform_velocity_vs_pf_density_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions.DefaultComputationFunctions._perform_velocity_vs_pf_density_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'DefaultComputationFunctions._perform_two_step_position_decoding_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions.DefaultComputationFunctions._perform_two_step_position_decoding_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'DefaultComputationFunctions._perform_position_decoding_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions.DefaultComputationFunctions._perform_position_decoding_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult)>,
 'PlacefieldComputations._perform_time_dependent_placefield_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.PlacefieldComputations.PlacefieldComputations._perform_time_dependent_placefield_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>,
 'PlacefieldComputations._perform_baseline_placefield_computation': <function pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.PlacefieldComputations.PlacefieldComputations._perform_baseline_placefield_computation(computation_result: pyphoplacecellanalysis.General.Model.ComputationResults.ComputationResult, debug_print=False)>}
 
 
        _input_test_functions_list = _temp_compuitations_functions_list
        _out_flatpaths_dict = nested_dicts_to_flatpaths('', _input_test_functions_list)
        _original_hierarchical_test_dict = flatpaths_to_nested_dicts(_out_flatpaths_dict)
        assert _original_hierarchical_test_dict == _input_test_functions_list, "ERROR, flatpaths_to_nested_dicts(nested_dicts_to_flatpaths(INPUT)) should be identity transforms (should == INPUT), but they do not!"

        
    """
    ...

def apply_to_dict_values(a_dict: dict, a_callable: Callable, include_condition: Callable = ...) -> dict:
    """ applies the Callable a_callable to the values of a_dict 
    e.g. 

    from pyphocorehelpers.indexing_helpers import apply_to_dict_values
    """
    ...

def list_of_dicts_to_dict_of_lists(list_of_dicts): # -> dict[Any, Any]:
    ...

@function_attributes(short_name=None, tags=['numpy', 'safe', 'indexing', 'list'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2023-05-08 20:06', related_items=[])
def safe_numpy_index(arr, idxs: np.ndarray): # -> list[Any]:
    """ tries to prevent errors when arr is a list and idxs is a numpy array. """
    ...

def safe_np_vstack(arr): # -> NDArray[Any]:
    """ a version of np.vstack that doesn't throw a ValueError on empty lists
        from pyphocorehelpers.indexing_helpers import safe_np_vstack

    """
    ...

def safe_np_hstack(arr): # -> NDArray[Any]:
    """ a version of np.hstack that doesn't throw a ValueError on empty lists
        from pyphocorehelpers.indexing_helpers import safe_np_hstack

    """
    ...

def dict_to_full_array(a_dict: Dict, full_indicies: NDArray, fill_value=...) -> NDArray:
    """ 
    a_dict: the dictionary of values you want to build into a NDArray
    full_indicies: NDArray - the completely list of all possible indicies that you want to build the array for. Any matching entries in `a_dict.keys()` will be filled with their corresponding value, otherwise `fill_value` will be used.
    Returns a NDArray of size: (len(full_indicies), )
    """
    ...

class NumpyHelpers:
    """ various extensions and generalizations for numpy arrays 
    
    from pyphocorehelpers.indexing_helpers import NumpyHelpers


    """
    @classmethod
    def all_array_generic(cls, pairwise_numpy_fn, list_of_arrays: List[NDArray], **kwargs) -> bool:
        """ A n-element generalization of a specified pairwise numpy function such as `np.array_equiv`
        Usage:
        
            list_of_arrays = list(xbins.values())
            NumpyHelpers.all_array_generic(list_of_arrays=list_of_arrays)

        """
        ...
    
    @classmethod
    def all_array_equal(cls, list_of_arrays: List[NDArray], equal_nan=...) -> bool:
        """ A n-element generalization of `np.array_equal`
        Usage:
        
            list_of_arrays = list(xbins.values())
            NumpyHelpers.all_array_equal(list_of_arrays=list_of_arrays)

        """
        ...
    
    @classmethod
    def all_array_equiv(cls, list_of_arrays: List[NDArray]) -> bool:
        """ A n-element generalization of `np.array_equiv`
        Usage:
        
            list_of_arrays = list(xbins.values())
            NumpyHelpers.all_array_equiv(list_of_arrays=list_of_arrays)

        """
        ...
    
    @classmethod
    def all_allclose(cls, list_of_arrays: List[NDArray], rtol: float = ..., atol: float = ..., equal_nan: bool = ...) -> bool:
        """ A n-element generalization of `np.allclose`
        Usage:
        
            list_of_arrays = list(xbins.values())
            NumpyHelpers.all_allclose(list_of_arrays=list_of_arrays)

        """
        ...
    


def safe_pandas_get_group(dataframe_group, key):
    """ returns an empty dataframe if the key isn't found in the group."""
    ...

def partition(df: pd.DataFrame, partitionColumn: str) -> Tuple[NDArray, NDArray]:
    """ splits a DataFrame df on the unique values of a specified column (partitionColumn) to return a unique DataFrame for each unique value in the column.

    Usage:

    from pyphocorehelpers.indexing_helpers import partition


    History: refactored from `pyphoplacecellanalysis.PhoPositionalData.analysis.helpers`
    """
    ...

def partition_df(df: pd.DataFrame, partitionColumn: str) -> Tuple[NDArray, List[pd.DataFrame]]:
    """ splits a DataFrame df on the unique values of a specified column (partitionColumn) to return a unique DataFrame for each unique value in the column.

    USEFUL NOTE: to get a dict, do `partitioned_dfs = dict(zip(*partition_df(spikes_df, partitionColumn='new_epoch_IDX')))`
    
    Usage:
        from pyphocorehelpers.indexing_helpers import partition_df
        
        partitioned_dfs = dict(zip(*partition_df(spikes_df, partitionColumn='new_epoch_IDX')))


    History: refactored from `pyphoplacecellanalysis.PhoPositionalData.analysis.helpers`
    """
    ...

def partition_df_dict(df: pd.DataFrame, partitionColumn: str) -> Dict[Any, pd.DataFrame]:
    """ splits a DataFrame df on the unique values of a specified column (partitionColumn) to return a unique DataFrame for each unique value in the column.

    Usage:
        from pyphocorehelpers.indexing_helpers import partition_df_dict
        
        partitioned_dfs = partition_df_dict(spikes_df, partitionColumn='new_epoch_IDX')

    History: refactored from `pyphoplacecellanalysis.PhoPositionalData.analysis.helpers`
    """
    ...

def find_neighbours(value, df, colname): # -> list[Any]:
    """Claims to be O(N)
    From https://stackoverflow.com/questions/30112202/how-do-i-find-the-closest-values-in-a-pandas-series-to-an-input-number
    
    Args:
        value ([type]): [description]
        df ([type]): [description]
        colname ([type]): [description]

    Returns:
        [type]: [description]
    """
    ...

def simple_merge(*dfs_list, debug_print=...) -> pd.DataFrame:
    """ naievely merges several dataframes with an equal number of rows (and in the same order) into a single dataframe with all of the unique columns of the individual dfs. Any duplicate columns will be removed.

    Usage:
        dfs_list = (deepcopy(neuron_identities_table), deepcopy(long_short_fr_indicies_analysis_table), deepcopy(neuron_replay_stats_table))
        dfs_list = (deepcopy(neuron_identities_table), deepcopy(long_short_fr_indicies_analysis_table), deepcopy(neuron_replay_stats_table))
        df_combined, dropped_duplicate_columns = simple_merge(*dfs_list, debug_print=False)
        df_combined

    """
    ...

def join_on_index(*dfs, join_index=..., suffixes_list=...) -> pd.DataFrame:
    """ Joins a series of dataframes on a common index (`join_index`)
    from pyphocorehelpers.indexing_helpers import join_on_index
    
    suffixes_list = (('_lsfria', '_jfra'), ('_jfra', '_lspd'))
    joined_df = join_on_index(long_short_fr_indicies_df, neuron_replay_stats_df, rate_remapping_df, join_index='aclu', suffixes_list=suffixes_list)

    """
    ...

def reorder_columns(df: pd.DataFrame, column_name_desired_index_dict: Dict[str, int]) -> pd.DataFrame:
    """Reorders specified columns in a DataFrame while preserving other columns.
    
    Pure: Does not modify the df

    Args:
        df (pd.DataFrame): The DataFrame to reorder.
        column_name_desired_index_dict (Dict[str, int]): A dictionary where keys are column names
            to reorder and values are their desired indices in the reordered DataFrame.

    Returns:
        pd.DataFrame: The DataFrame with specified columns reordered while preserving remaining columns.

    Raises:
        ValueError: If any column in the dictionary is not present in the DataFrame.
        
        
    Usage:
    
        from pyphocorehelpers.indexing_helpers import reorder_columns
        dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))
        reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))
        
        ## Move the "height" columns to the end
        result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))
        result_df
                
    """
    ...

def reorder_columns_relative(df: pd.DataFrame, column_names: list[str], relative_mode=...) -> pd.DataFrame:
    """Reorders specified columns in a DataFrame while preserving other columns.
    
    Pure: Does not modify the df

    Args:
        df (pd.DataFrame): The DataFrame to reorder.
        column_name_desired_index_dict (Dict[str, int]): A dictionary where keys are column names
            to reorder and values are their desired indices in the reordered DataFrame.

    Returns:
        pd.DataFrame: The DataFrame with specified columns reordered while preserving remaining columns.

    Raises:
        ValueError: If any column in the dictionary is not present in the DataFrame.
        
        
    Usage:
    
        from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative
        
        ## Move the "height" columns to the end
        result_df = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), existing_columns)), relative_mode='end')
        result_df
                
    """
    ...

def get_bin_centers(bin_edges):
    """ For a series of 1D bin edges given by bin_edges, returns the center of the bins. Output will have one less element than bin_edges. """
    ...

def get_bin_edges(bin_centers): # -> NDArray[Any]:
    """ For a series of 1D bin centers given by bin_centers, returns the edges of the bins. Output will have one more element than bin_centers
        Reciprocal of get_bin_centers(bin_edges)
    """
    ...

@dataclass
class BinningInfo:
    """Docstring for BinningInfo."""
    variable_extents: tuple
    step: float
    num_bins: int
    bin_indicies: np.ndarray
    ...


class BinningContainer:
    """A container that allows accessing either bin_edges (self.edges) or bin_centers (self.centers) """
    edges: np.ndarray
    centers: np.ndarray
    edge_info: BinningInfo
    center_info: BinningInfo
    def __init__(self, edges: Optional[np.ndarray] = ..., centers: Optional[np.ndarray] = ..., edge_info: Optional[BinningInfo] = ..., center_info: Optional[BinningInfo] = ...) -> None:
        ...
    
    @classmethod
    def build_edge_binning_info(cls, edges): # -> BinningInfo:
        ...
    
    @classmethod
    def build_center_binning_info(cls, centers, variable_extents): # -> BinningInfo:
        ...
    
    def setup_from_edges(self, edges: np.ndarray, edge_info: Optional[BinningInfo] = ...): # -> None:
        ...
    


def compute_spanning_bins(variable_values, num_bins: int = ..., bin_size: float = ..., variable_start_value: float = ..., variable_end_value: float = ...): # -> tuple[NDArray[floating[Any]], BinningInfo]:
    """[summary]

    Args:
        variable_values ([type]): The variables to be binned, used to determine the start and end edges of the returned bins.
        num_bins (int, optional): The total number of bins to create. Defaults to None.
        bin_size (float, optional): The size of each bin. Defaults to None.
        variable_start_value (float, optional): The minimum value of the binned variable. If specified, overrides the lower binned limit instead of computing it from variable_values. Defaults to None.
        variable_end_value (float, optional): The maximum value of the binned variable. If specified, overrides the upper binned limit instead of computing it from variable_values. Defaults to None.
        debug_print (bool, optional): Whether to print debug messages. Defaults to False.

    Raises:
        ValueError: [description]

    Returns:
        np.array<float>: The computed bins
        BinningInfo: information about how the binning was performed
        
    Usage:
        ## Binning with Fixed Number of Bins:    
        xbin_edges, xbin_edges_binning_info = compute_spanning_bins(pos_df.x.to_numpy(), bin_size=active_config.computation_config.grid_bin[0]) # bin_size mode
        print(xbin_edges_binning_info)
        ## Binning with Fixed Bin Sizes:
        xbin_edges_edges, xbin_edges_binning_info = compute_spanning_bins(pos_df.x.to_numpy(), num_bins=num_bins) # num_bins mode
        print(xbin_edges_binning_info)
        
    """
    ...

def compute_position_grid_size(*any_1d_series, num_bins: tuple): # -> tuple[NDArray[float64], list[Any], list[Any]]:
    """  Computes the required bin_sizes from the required num_bins (for each dimension independently)
    Usage:
    out_grid_bin_size, out_bins, out_bins_infos = compute_position_grid_size(curr_kdiba_pipeline.sess.position.x, curr_kdiba_pipeline.sess.position.y, num_bins=(64, 64))
    active_grid_bin = tuple(out_grid_bin_size)
    print(f'active_grid_bin: {active_grid_bin}') # (3.776841861770752, 1.043326930905373)
    """
    ...

def debug_print_1D_bin_infos(bins, label=...): # -> None:
    """ prints info about the 1D bins provided 
    Usage:
        debug_print_1D_bin_infos(time_window_centers, label='time_window_centers')
        >> time_window_centers: [1211.71 1211.96 1212.21 ... 2076.96 2077.21 2077.46], count: 3464, start: 1211.7133460667683, end: 2077.4633460667683, unique_steps: [0.25]
    """
    ...

RowColTuple = ...
PaginatedGridIndexSpecifierTuple = ...
RequiredSubplotsTuple = ...
@function_attributes(short_name='compute_paginated_grid_config', tags=['page', 'grid', 'config'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2023-03-30 16:47')
def compute_paginated_grid_config(num_required_subplots, max_num_columns, max_subplots_per_page=..., data_indicies=..., last_figure_subplots_same_layout=..., debug_print=...): # -> tuple[RequiredSubplotsTuple, list[list[Any]], list[RowColTuple]]:
    """ Fills row-wise first, and constrains the subplots values to just those that you need
    Args:
        num_required_subplots ([type]): [description]
        max_num_columns ([type]): [description]
        max_subplots_per_page ([type]): If None, pagination is effectively disabled and all subplots will be on a single page.
        data_indicies ([type], optional): your indicies into your original data that will also be accessible in the main loop. Defaults to None.
        last_figure_subplots_same_layout (bool): if True, the last page has the same number of items (same # columns and # rows) as the previous (full/complete) pages.
        
        
    Example:
        from pyphocorehelpers.indexing_helpers import compute_paginated_grid_config
        subplot_no_pagination_configuration, included_combined_indicies_pages, page_grid_sizes = compute_paginated_grid_config(nMapsToShow, max_num_columns=subplots.num_columns, max_subplots_per_page=max_subplots_per_page, data_indicies=included_unit_indicies, last_figure_subplots_same_layout=last_figure_subplots_same_layout)
        num_pages = len(included_combined_indicies_pages)
    
    """
    ...

def build_spanning_grid_matrix(x_values, y_values, debug_print=...): # -> tuple[NDArray[Any], list[tuple[Any, ...]], tuple[int, int]]:
    """ builds a 2D matrix with entries spanning x_values across axis 0 and spanning y_values across axis 1.
        
        For example, used to build a grid of position points from xbins and ybins.
    Usage:
        from pyphocorehelpers.indexing_helpers import build_spanning_grid_matrix
        all_positions_matrix, flat_all_positions_matrix, original_data_shape = build_spanning_grid_matrix(active_one_step_decoder.xbin_centers, active_one_step_decoder.ybin_centers)
        
    Outputs:
        all_positions_matrix: a 3D matrix # .shape # (num_cols, num_rows, 2)
        flat_all_positions_matrix: a list of 2-tuples of length num_rows * num_cols
        original_data_shape: a tuple containing the shape of the original data (num_cols, num_rows)
    """
    ...

@define(slots=False)
class Paginator:
    """ 2023-05-02 - helper that allows easily creating paginated data either for batch or realtime usage. 

    Independent of any plotting technology. Just meant to hold and paginate the data.
    
    
    TODO 2023-05-02 - See also:
    ## paginated outputs for shared cells
    included_unit_indicies_pages = [[curr_included_unit_index for (a_linear_index, curr_row, curr_col, curr_included_unit_index) in v] for page_idx, v in enumerate(included_combined_indicies_pages)] # a list of length `num_pages` containing up to 10 items

    # Can build a list of keyword arguments that will be provided to the function of interest ahead of time
    paginated_shared_cells_kwarg_list = [dict(included_unit_neuron_IDs=curr_included_unit_indicies,
        active_identifying_ctx=active_identifying_session_ctx.adding_context(collision_prefix='_batch_plot_test', display_fn_name='batch_plot_test', plot_result_set='shared', page=f'{page_idx+1}of{num_pages}', aclus=f"{curr_included_unit_indicies}"),
        fignum=f'shared_{page_idx}', fig_idx=page_idx, n_max_page_rows=n_max_page_rows) for page_idx, curr_included_unit_indicies in enumerate(included_unit_indicies_pages)]

    Example:
    
        from pyphocorehelpers.indexing_helpers import Paginator
        
        ## Provide a tuple or list containing equally sized sequences of items:
        sequencesToShow = (rr_aclus, rr_laps, rr_replays)
        a_paginator = Paginator.init_from_data(sequencesToShow, max_num_columns=1, max_subplots_per_page=20, data_indicies=None, last_figure_subplots_same_layout=False)
        # If a paginator was constructed with `sequencesToShow = (rr_aclus, rr_laps, rr_replays)`, then:
        included_page_data_indicies, included_page_data_items = a_paginator.get_page_data(page_idx=1)
        curr_page_rr_aclus, curr_page_rr_laps, curr_page_rr_replays = included_page_data_items

    Extended Example:
        from pyphoplacecellanalysis.GUI.Qt.Widgets.PaginationCtrl.PaginationControlWidget import PaginationControlWidget
        a_paginator_controller_widget = PaginationControlWidget(n_pages=a_paginator.num_pages)

    """
    sequencesToShow: tuple
    subplot_no_pagination_configuration: RequiredSubplotsTuple
    included_combined_indicies_pages: list[list[PaginatedGridIndexSpecifierTuple]]
    page_grid_sizes: list[RowColTuple]
    nItemsToShow: int = ...
    num_pages: int = ...
    @property
    def num_items_per_page(self): # -> NDArray[Any]:
        """The number of items displayed on each page (one number per page).
        e.g. array([20, 20, 20, 20, 20,  8])
        """
        ...
    
    @property
    def max_num_items_per_page(self):
        """The number of items on the page with the maximum number of items.
        e.g. 20
        """
        ...
    
    @classmethod
    def init_from_data(cls, sequencesToShow, max_num_columns=..., max_subplots_per_page=..., data_indicies=..., last_figure_subplots_same_layout=...): # -> Self:
        """ creates a Paginator object from a tuple of equal length sequences using `compute_paginated_grid_config`"""
        ...
    
    def get_page_data(self, page_idx: int): # -> tuple[NDArray[Any], tuple[Any | list[Any], ...]]:
        """ 
        Usage:
            # If a paginator was constructed with `sequencesToShow = (rr_aclus, rr_laps, rr_replays)`, then:
            included_page_data_indicies, included_page_data_items = a_paginator.get_page_data(page_idx=0)
            curr_page_rr_aclus, curr_page_rr_laps, curr_page_rr_replays = included_page_data_items

        """
        ...
    


def np_ffill_1D(arr: np.ndarray, debug_print=...): # -> Any:
    '''  'forward-fill' the nan values in array arr. 
    By that I mean replacing each nan value with the nearest valid value from the left.
    row-wise by default
    
    Example:

    Input:
        array([[  5.,  nan,  nan,   7.,   2.],
        [  3.,  nan,   1.,   8.,  nan],
        [  4.,   9.,   6.,  nan,  nan]])
       
       
    Desired Solution:
        array([[  5.,   5.,   5.,  7.,  2.],
        [  3.,   3.,   1.,  8.,  8.],
        [  4.,   9.,   6.,  6.,  6.]])
       
       
    Most efficient solution from StackOverflow as timed by author of question: https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array
    Solution provided by Divakar.
    
    '''
    ...

def np_bfill_1D(arr: np.ndarray): # -> Any:
    """ backfills the np.nan values instead of forward filling them 
    Simple solution for bfill provided by financial_physician in comment below
    """
    ...

