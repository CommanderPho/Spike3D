{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext viztracer\n",
    "from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "text_formatter: PlainTextFormatter = IPython.get_ipython().display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, CapturedException, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables, CapturedException\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer, RankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/media/MAX/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')]\n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "\tglobal global_data_root_parent_path\n",
    "\tnew_global_data_root_parent_path = new_path.resolve()\n",
    "\tglobal_data_root_parent_path = new_global_data_root_parent_path\n",
    "\tprint(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "\tassert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\t\t\t\n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # DONE. Very good. Many good Pfs, many good replays.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays ---- VERY weird effect of the replays, a sharp drop to strongly negative values more than 3/4 through the experiment.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays! Interesting see-saw!\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # 2024-01-10 new RANKORDER APOGEE | DONE, Added replay selections. A TON of putative replays in general, most bad, but some good. LOOKIN GOOD!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='twolong_LR_pf1Dsession_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few --- \"ZeroDivisionError: float division by zero\"\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3') ### KeyError: 'maze1_odd'\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5') ### \n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # DONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "list(curr_active_pipeline.global_computation_results.computed_data.keys())\n",
    "\n",
    "\n",
    "# 2024-01-22 ERROR: when the pipeline is manually saved, its global_computations seem to be saved to the pickle too. After modifying how global computations are loaded from pickle, the following global computations code block no longer appropriately overwrites the existing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded'], debug_print=True)\n",
    "# global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=[k for k in list(curr_active_pipeline.global_computation_results.computed_data.keys())], debug_print=True) # drop all global keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba46b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.574268400Z",
     "start_time": "2023-11-16T23:21:35.966373700Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "### GLOBAL COMPUTATIONS:\n",
    "extended_computations_include_includelist=['lap_direction_determination', #'pf_computation', 'firing_rate_trends',# 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "     'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', \n",
    "    'long_short_post_decoding', # #TODO 2024-01-19 05:49: - [ ] `'long_short_post_decoding' is broken for some reason `AttributeError: 'NoneType' object has no attribute 'active_filter_epochs'``\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    'directional_decoders_decode_continuous'\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_decode_continuous'] # \n",
    "\n",
    "\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # curr_active_pipeline.load_pickled_global_computation_results()\n",
    "        curr_active_pipeline.load_pickled_global_computation_results(allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist) # is new\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results: {e}')\n",
    "        raise\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     exception_info = sys.exc_info()\n",
    "#     e = CapturedException(e, exception_info)\n",
    "#     print(f'second half threw: {e}')\n",
    "\n",
    "# 4m 5.2s for inst fr computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    'directional_decoders_decode_continuous'\n",
    "] # do only specified\n",
    "\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps',\n",
    "#     # 'merged_directional_placefields',\n",
    "#     # 'directional_decoders_decode_continuous',\n",
    "# ]\n",
    "force_recompute_override_computations_includelist = None\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# force_recompute_override_computations_includelist = ['_decode_continuous_using_directional_decoders']\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(extended_computations_include_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.02}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_decode_continuous'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77babf98",
   "metadata": {},
   "source": [
    "## Continue Saving/Exporting stuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114aa311",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)\n",
    "# curr_active_pipeline.save_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {},
   "source": [
    "# Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## long_short_decoding_analyses:\n",
    "from attrs import astuple\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import LeaveOneOutDecodingAnalysis\n",
    "\n",
    "curr_long_short_decoding_analyses: LeaveOneOutDecodingAnalysis = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8f0cf",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "curr_long_short_decoding_analyses.long_results_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83acf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_v_observed_result.observed_from_expected_diff_ptp_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult, DirectionalLapsResult, DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_decoders_decode_result: DirectionalDecodersDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "spikes_df = directional_decoders_decode_result.spikes_df\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "\n",
    "most_recent_time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# most_recent_time_bin_size\n",
    "most_recent_continuously_decoded_dict = deepcopy(directional_decoders_decode_result.most_recent_continuously_decoded_dict)\n",
    "# most_recent_continuously_decoded_dict\n",
    "\n",
    "## Adds in the 'pseudo2D' decoder in:\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# time_bin_size: float = 0.01\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = continuously_decoded_result_cache_dict[time_bin_size]\n",
    "pseudo2D_decoder_continuously_decoded_result = continuously_decoded_dict.get('pseudo2D', None)\n",
    "if pseudo2D_decoder_continuously_decoded_result is None:\n",
    "\t# compute here...\n",
    "\t## Currently used for both cases to decode:\n",
    "\tt_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\tsingle_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]}) # Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "\tsingle_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "\tspikes_df = directional_decoders_decode_result.spikes_df\n",
    "\tpseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = pseudo2D_decoder.decode_specific_epochs(spikes_df=deepcopy(spikes_df), filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "\tcontinuously_decoded_dict['pseudo2D'] = pseudo2D_decoder_continuously_decoded_result\n",
    "\tcontinuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260739a4f36c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1c6006aba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d3bf5955d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "# appearing_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c292d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Saving/Loading `DirectionalLaps_2Hz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6c50e",
   "metadata": {
    "tags": [
     "save",
     "persistance"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import save_rank_order_results\n",
    "\n",
    "# DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "# DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "# print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "# NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "# NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "# print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "formatted_time = get_now_rounded_time_str()\n",
    "print(formatted_time)\n",
    "save_rank_order_results(curr_active_pipeline, day_date=f\"{formatted_time}\") # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm\n",
    "# '2024-01-09_0125PM-minimum_inclusion_fr-5-included_qclu_values-[1, 2]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path = Path('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/').resolve()\n",
    "sorted(search_path.glob(f\"{DAY_DATE_TO_USE}*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da8a7c",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import SaveStringGenerator\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData\n",
    "\n",
    "# Load the data from a file into the pipeline:\n",
    "# out_filename_str: str = '2023-12-11-minimum_inclusion_fr_Hz_2_included_qclu_values_1-2_' # specific\n",
    "\n",
    "minimum_inclusion_fr_Hz: float = 5.0\n",
    "included_qclu_values: List[int] = [1,2]\n",
    "out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date=f'{DAY_DATE_TO_USE}_11am') # '2023-12-21_349am'\n",
    "# out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date='2023-12-22_312pm') # '2023-12-21_349am'\n",
    "print(f'out_filename_str: \"{out_filename_str}\"')\n",
    "# day_date_str: str = '2023-12-11_with_tuple_newer_'\n",
    "# day_date_str: str = ''\n",
    "directional_laps_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}DirectionalLaps.pkl').resolve()\n",
    "assert directional_laps_output_path.exists()\n",
    "# loaded_directional_laps, loaded_rank_order = loadData(directional_laps_output_path)\n",
    "loaded_directional_laps = loadData(directional_laps_output_path)\n",
    "assert (loaded_directional_laps is not None)\n",
    "# assert (loaded_rank_order is not None)\n",
    "\n",
    "rank_order_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}RankOrder.pkl').resolve()\n",
    "loaded_rank_order = loadData(rank_order_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the loaded data to the pipeline:\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'], curr_active_pipeline.global_computation_results.computed_data['RankOrder'] = loaded_directional_laps, loaded_rank_order\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd521ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {},
   "source": [
    "# POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "\n",
    "## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_replay_epochs_df\n",
    "\n",
    "# Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "# directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "# directional_likelihoods_df\n",
    "\n",
    "# 2023-12-15 - Newest method:\n",
    "# laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "# ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "# ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "# # Concatenate the three DataFrames along the columns axis:\n",
    "# # Assert that all DataFrames have the same number of rows:\n",
    "# assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "# # Assert that all DataFrames have at least one row:\n",
    "# assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "# assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "# assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "# merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "# merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "# merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca6b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripple_merged_complete_epoch_stats_df\n",
    "laps_merged_complete_epoch_stats_df\n",
    "['long_best_direction_indices', 'short_best_direction_indices', 'combined_best_direction_indicies', 'long_relative_direction_likelihoods', 'short_relative_direction_likelihoods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time series of Long-likely events\n",
    "# type(long_RL_results) # DynamicParameters\n",
    "long_LR_pf1D_Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920bfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df\n",
    "laps_most_likely_direction_from_decoder\n",
    "long_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps, partial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def register_type_display(func_to_register, type_to_register):\n",
    "\t\"\"\" adds the display function (`func_to_register`) it decorates to the class (`type_to_register) as a method\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\t@wraps(func_to_register)\n",
    "\tdef wrapper(*args, **kwargs):\n",
    "\t\treturn func_to_register(*args, **kwargs)\n",
    "\n",
    "\tfunction_name: str = func_to_register.__name__ # get the name of the function to be added as the property\n",
    "\tsetattr(type_to_register, function_name, wrapper) # set the function as a method with the same name as the decorated function on objects of the class.\t\n",
    "\treturn wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "\t\"\"\" \n",
    "\tnum='RipplesRankOrderZscore'\n",
    "\t\"\"\"\n",
    "\tprint(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "\tfig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "\tax_dict = fig.subplot_mosaic(\n",
    "\t\t[\n",
    "\t\t\t[\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "\t\t\t[\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "\t\t],\n",
    "\t)\n",
    "\tplots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "\t)\n",
    "\treturn MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "merged_complete_epoch_stats_df\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_1247pm_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732743e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns\n",
    "\n",
    "dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))\n",
    "reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dceda30",
   "metadata": {},
   "source": [
    "## 2023-12-21 - Computing Spearman Percentiles as an alternative to the Z-score from shuffling, which does not seem to work for small numbers of active cells in an event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values, shuffled_results_output_dict, combined_variable_names, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles = rank_order_results.ripple_new_output_tuple\n",
    "# shuffled_results_output_dict['short_LR_pearson_Z']\n",
    "print(list(shuffled_results_output_dict.keys())) # ['short_LR_pearson_Z', 'short_LR_spearman_Z', 'short_RL_pearson_Z', 'short_RL_spearman_Z', 'long_LR_pearson_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_spearman_Z']\n",
    "\n",
    "['long_LR_pearson_Z', 'long_RL_pearson_Z', 'short_LR_pearson_Z', 'short_RL_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b40bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-12-22 - Add the LR-LR, RL-RL differences\n",
    "merged_complete_epoch_stats_df['LongShort_LR_quantile_diff'] = merged_complete_epoch_stats_df['LR_Long_rank_percentile'] - merged_complete_epoch_stats_df['LR_Short_rank_percentile']\n",
    "merged_complete_epoch_stats_df['LongShort_RL_quantile_diff'] = merged_complete_epoch_stats_df['RL_Long_rank_percentile'] - merged_complete_epoch_stats_df['RL_Short_rank_percentile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import _plot_significant_event_quantile_fig\n",
    "\n",
    "# active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "# if isinstance(global_events, pd.DataFrame):\n",
    "#     active_replay_epochs = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df())\n",
    "\n",
    "\n",
    "# _out = _plot_significant_event_quantile_fig(curr_active_pipeline, significant_ripple_combined_epoch_stats_df=significant_ripple_combined_epoch_stats_df)\n",
    "# _out\n",
    "\n",
    "marker_style = dict(linestyle='None', color='#ff7f0eff', markersize=6, markerfacecolor='#ff7f0eb4', markeredgecolor='#ff7f0eff')\n",
    "\n",
    "    # dict(facecolor='#ff7f0eb4', size=8.0)\n",
    "    # fignum='best_quantiles'\n",
    "\n",
    "# ripple_combined_epoch_stats_df['combined_best_direction_indicies']\n",
    "\n",
    "_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y='LongShort_BestDir_quantile_diff', title='Sig. (>0.95) Best Quantile Diff', **marker_style, marker='o')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd89199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "perform_update_title_subtitle(fig=fig_long_pf_1D, ax=ax_long_pf_1D, title_string=title_string, subtitle_string=subtitle_string, active_context=active_context, use_flexitext_titles=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neuropy.utils.matplotlib_helpers import draw_epoch_regions\n",
    "epochs_collection, epoch_labels = draw_epoch_regions(curr_active_pipeline.sess.epochs, ax, defer_render=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(significant_BestDir_quantile_stats_df.columns))\n",
    "['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']\n",
    "\n",
    "for a_name in ['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']:\n",
    "\t_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y=a_name, title=f'Sig. (>0.95) {a_name}', **marker_style, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "\n",
    "df = quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].copy()\n",
    "# Create the subplots and loop through columns\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i, col in enumerate(df.columns):\n",
    "    df[col].plot.hist(ax=axes[i], bins=21)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd61a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsLayoutWidget(show=True)\n",
    "win.resize(800,350)\n",
    "win.setWindowTitle('Z-Scorer: Histogram')\n",
    "plt1 = win.addPlot()\n",
    "vals = quantile_results_df.LR_Long_rank_percentile\n",
    "fisher_z_transformed_vals = np.arctanh(vals)\n",
    "\n",
    "## compute standard histogram\n",
    "y, x = np.histogram(vals) # , bins=np.linspace(-3, 8, 40)\n",
    "# fisher_z_transformed_y, x = np.histogram(fisher_z_transformed_vals, bins=x)\n",
    "\n",
    "## Using stepMode=\"center\" causes the plot to draw two lines for each sample.\n",
    "## notice that len(x) == len(y)+1\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "# plt1.plot(x, fisher_z_transformed_y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,255,100,50), name='fisher_z_values')\n",
    "\n",
    "# ## Now draw all points as a nicely-spaced scatter plot\n",
    "y = pg.pseudoScatter(vals, spacing=0.15)\n",
    "# #plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5)\n",
    "plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5, symbolPen=(255,255,255,200), symbolBrush=(0,0,255,150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.concat((ripple_combined_epoch_stats_df, ripple_p_values_epoch_stats_df), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43327521",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index).any())\n",
    "# ripple_combined_epoch_stats_df.label.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.label).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "unique_Probe_Epoch_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "\t# probe_epoch_df = active_selected_spikes_df[a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id']]\n",
    "\t# epoch_unique_aclus = probe_epoch_df.aclu.unique()\n",
    "\tmask = (a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id'])\n",
    "\t# epoch_unique_aclus = active_selected_spikes_df.loc[mask, 'aclu'].unique()\n",
    "\tfor a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "\t\t# Shuffle aclus here:\n",
    "\t\tactive_selected_spikes_df.loc[mask, 'aclu'] = active_selected_spikes_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\t\tactive_selected_spikes_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# ## Shuffle aclus here:\n",
    "\t\t# # probe_epoch_df.aclu.sample(1000)\n",
    "\t\t# # a_aclu_peak_map\n",
    "\t\t# # Assuming 'df' is your DataFrame and 'column_name' is the column you want to shuffle\n",
    "\t\t# probe_epoch_df['aclu'] = probe_epoch_df['aclu'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\t# probe_epoch_df[f'{a_decoder_name}_pf_peak_x'] = probe_epoch_df.aclu.map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.aclu.map(a_aclu_peak_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c306ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-01-09 - More Efficient\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df: pd.DataFrame):\n",
    "    \"\"\" 2024-01-09 - Candidate for moving into RankOrderComputations \n",
    "    captures: decoder_names\n",
    "    \n",
    "    Usage:\n",
    "    \n",
    "    shuffled_dfs = _perform_efficient_shuffle(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "    \n",
    "    \"\"\"\n",
    "    decoder_names = track_templates.get_decoder_names()\n",
    "    \n",
    "    ## Compute real values here:\n",
    "    epoch_id_grouped_selected_spikes_df = active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?\n",
    "\n",
    "    # spearman_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='spearman', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "    # pearson_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='pearson', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "\n",
    "    # real_stats_df = pd.concat((spearman_correlations, pearson_correlations), axis='columns')\n",
    "    # real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()] # drop duplicated 'Probe_Epoch_id' column\n",
    "    # # Change column type to uint64 for column: 'Probe_Epoch_id'\n",
    "    # real_stats_df = real_stats_df.astype({'Probe_Epoch_id': 'uint64'})\n",
    "    # # Rename column 'Probe_Epoch_id' to 'label'\n",
    "    # real_stats_df = real_stats_df.rename(columns={'Probe_Epoch_id': 'label'})\n",
    "    \n",
    "    # Parallelize correlation computations if required\n",
    "    correlations = []\n",
    "    for method in ['spearman', 'pearson']:\n",
    "        correlations.append(\n",
    "            epoch_id_grouped_selected_spikes_df.apply(\n",
    "                lambda group: RankOrderAnalyses._subfn_calculate_correlations(\n",
    "                    group, method=method, decoder_names=decoder_names)\n",
    "            )\n",
    "        )\n",
    "  \n",
    "    # Adjust and join all calculated correlations\n",
    "    real_stats_df = pd.concat(correlations, axis='columns').reset_index()\n",
    "    real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()]\n",
    "\n",
    "    real_stats_df.rename(columns={'Probe_Epoch_id': 'label'}, inplace=True)\n",
    "    real_stats_df['label'] = real_stats_df['label'].astype('uint64')  # in-place type casting\n",
    "    \n",
    "    return real_stats_df\n",
    "\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "def _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    \"\"\" 2024-01-09 - Performs the shuffles in a simple way\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "\n",
    "    # Create a list to hold the shuffled dataframes\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        # Working on a copy of the DataFrame\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "            \n",
    "            # Shuffle 'aclu' values\n",
    "            shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "            \n",
    "            # # Apply aclu peak map dictionary to 'aclu' column\n",
    "            # for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            #     shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "            \n",
    "\n",
    "        # end `for a_probe_epoch_ID`\n",
    "        # Once done, apply the aclu peak maps to shuffled_df's 'aclu' column:\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df[f'{a_decoder_name}_pf_peak_x'] = shuffled_df.aclu.map(a_aclu_peak_map)\n",
    "            \n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "        \n",
    "        # Adding the shuffled DataFrame to the list\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "        \n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "def _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles: int = 5):\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    def map_dict_to_group(group, a_dict, column):\n",
    "        group[column] = group[column].map(a_dict)\n",
    "        return group\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'] = shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df = shuffled_df.groupby('Probe_Epoch_id').apply(map_dict_to_group, a_dict=a_aclu_peak_map, column=f'{a_decoder_name}_pf_peak_x')\n",
    "\n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "\n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-suggested_perform_efficient_shuffle.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "shuffled_dfs, shuffled_stats_dfs = _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 50, 1m 21.2s, 10, 16.1s\n",
    "# shuffled_dfs, shuffled_stats_dfs = _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 10, 12.8s\n",
    "\n",
    "\n",
    "shuffled_dfs\n",
    "shuffled_stats_dfs\n",
    "# 5, 4.1 sec\n",
    "# 0.5s!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values = shuffled_stats_dfs\n",
    "# Build the output `stacked_arrays`: _________________________________________________________________________________ #\n",
    "\n",
    "stacked_arrays = np.stack([a_shuffle_real_stats_df[combined_variable_names].to_numpy() for a_shuffle_real_stats_df in output_active_epoch_computed_values], axis=0) # for compatibility: .shape (n_shuffles, n_epochs, n_columns)\n",
    "# stacked_df = pd.concat(output_active_epoch_computed_values, axis='index')\n",
    "\n",
    "## Drop any shuffle indicies where NaNs are returned for any of the stats values.\n",
    "is_valid_row = np.logical_not(np.isnan(stacked_arrays)).all(axis=(1,2)) # row [0, 66, :] is bad, ... so is [1, 66, :], ... [20, 66, :], ... they are repeated!!\n",
    "n_valid_shuffles = np.sum(is_valid_row)\n",
    "if debug_print:\n",
    "\tprint(f'n_valid_shuffles: {n_valid_shuffles}')\n",
    "valid_stacked_arrays = stacked_arrays[is_valid_row] ## Get only the rows where all elements along both axis (1, 2) are True\n",
    "\n",
    "# Need: valid_stacked_arrays, real_stacked_arrays, combined_variable_names\n",
    "combined_epoch_stats_df: pd.DataFrame = pd.DataFrame(real_stacked_arrays, columns=combined_variable_names)\n",
    "combined_variable_z_score_column_names = [f\"{a_name}_Z\" for a_name in combined_variable_names] # combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']\n",
    "\n",
    "## Extract the stats values for each shuffle from `valid_stacked_arrays`:\n",
    "n_epochs = np.shape(real_stacked_arrays)[0]\n",
    "n_variables = np.shape(real_stacked_arrays)[1]\n",
    "\n",
    "# valid_stacked_arrays.shape: (n_shuffles, n_epochs, n_variables)\n",
    "assert n_epochs == np.shape(valid_stacked_arrays)[-2]\n",
    "assert n_variables == np.shape(valid_stacked_arrays)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "num_shuffles = 5\n",
    "\n",
    "# Define the operation to be run in parallel for a shuffle iteration\n",
    "def shuffle_iteration(i):\n",
    "    # Working on a copy of the DataFrame\n",
    "    shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "    for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "        mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "\n",
    "        # Shuffle 'aclu' values\n",
    "        shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        # Apply aclu peak map dictionary to 'aclu' column\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    # Return the shuffled DataFrame\n",
    "    return shuffled_df\n",
    "\n",
    "# Create a list to hold the shuffled dataframes\n",
    "shuffled_dfs = Parallel(n_jobs=-1)(delayed(shuffle_iteration)(i) for i in range(num_shuffles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()]\n",
    "print(peak_column_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    # Apply aclu peak map dictionary to each decoder name\n",
    "    for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "        active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df['aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffles = {}\n",
    "    for i in range(num_shuffles):\n",
    "        shuffles[i] = active_selected_spikes_df.copy()\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffles[i]['Probe_Epoch_id'])\n",
    "            # Shuffle multiple columns here:\n",
    "            for a_decoder_name in decoder_aclu_peak_map_dict.keys():\n",
    "                shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'].sample(frac=1).values\n",
    "    return shuffles\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "shuffled_dfs = _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "# shuffled_dfs\n",
    "# 5, 1.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle 'aclu' values\n",
    "shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "\n",
    "# Shuffle aclu and their corresponding peaks: ['aclu', 'long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()] # ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']] = shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']].sample(frac=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d641689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(output_active_epoch_computed_values) # 0.946189 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2023-12-13 02:07: - [ ] Figure out how 'Probe_Epoch_id' maps to `ripple_result_tuple.active_epochs`\n",
    "ripple_result_tuple.active_epochs\n",
    "rank_order_results.LR_ripple.ranked_aclus_stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the pf_x information for each aclu:\n",
    "## 2023-10-11 - Get the long/short peak locations\n",
    "# decoder_peak_coms_list = [a_decoder.pf.ratemap.peak_tuning_curve_center_of_masses[is_good_aclus] for a_decoder in decoder_args]\n",
    "decoder_aclu_peak_location_dict_list = [dict(zip(neuron_IDs, peak_locations)) for neuron_IDs, peak_locations in zip(track_templates.decoder_neuron_IDs_list, track_templates.decoder_peak_location_list)]\n",
    "decoder_aclu_peak_location_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de234ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_tuning_curve_center_of_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_LR_pf_peak_ranks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replays:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "if isinstance(global_replays, pd.DataFrame):\n",
    "\tglobal_replays = Epoch(global_replays.epochs.get_valid_df())\n",
    "\n",
    "# get the aligned epochs and the z-scores aligned to them:\n",
    "active_replay_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_replays.to_dataframe().copy(), is_laps=False)\n",
    "active_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6384a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laps:\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps).trimmed_to_non_overlapping()\n",
    "active_laps_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_laps.to_dataframe(), is_laps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only the significant events (|z| > 1.96):\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "filtered_z_score_df, (n_events, n_significant_events, percent_significant_events) = RankOrderAnalyses.find_only_significant_events(rank_order_results, high_z_criteria=1.96)\n",
    "filtered_z_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_z_score_df.index.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86532662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-11-20 - Finding high-significance periods for Kamran:\n",
    "z_threshold = 1.96\n",
    "is_greater_than_z_threshold_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) > z_threshold)\n",
    "is_greater_than_z_threshold_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) > z_threshold)\n",
    "is_significant_either = np.logical_or(is_greater_than_z_threshold_long, is_greater_than_z_threshold_short)\n",
    "is_significant_either\n",
    "\n",
    "# is_greater_than_3std_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) >= 3.0)\n",
    "# is_greater_than_3std_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) >= 3.0)\n",
    "# is_significant_either = np.logical_or(is_greater_than_3std_long, is_greater_than_3std_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_ripple_epochs = deepcopy(Epoch(ripple_result_tuple.active_epochs)).boolean_indicies_slice(is_significant_either)\n",
    "# significant_ripple_epochs = deepcopy(global_replays).boolean_indicies_slice(is_significant_either)\n",
    "significant_ripple_epochs.to_dataframe()\n",
    "\n",
    "# significant_ripple_epochs.filename = Path(f'output/2023-11-27_SignificantReplayRipples').resolve()\n",
    "# significant_ripple_epochs.to_neuroscope()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8beece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_epochs = ripple_result_tuple.active_epochs\n",
    "active_epochs: Epoch = rank_order_results.RL_ripple.epochs_df # Epoch(rank_order_results.RL_ripple.epochs_df)\n",
    "# type(active_epochs)\n",
    "active_epochs.n_epochs\n",
    "# rank_order_results.RL_ripple.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ffe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.epochs_df\n",
    "rank_order_results.LR_ripple.spikes_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39525572",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_variable_names: ['LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson']\n",
    "combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f47973",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.build_display_context_for_filtered_session(filtered_session_name='maze_any', display_fn_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b319650",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ebf52",
   "metadata": {},
   "source": [
    "#### Iterates through the epochs (via the slider) and saves out the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f73ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(r'C:\\Users\\pho\\Desktop\\2023-12-19 Exports').resolve()\n",
    "all_save_paths = _out_rank_order_event_raster_debugger.export_figure_all_slider_values(export_path=export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_y_values_dict = {_active_plot_identifier:{int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs} for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()}\n",
    "aclu_max_y_values_dict = {_active_plot_identifier:np.max(list({int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs}.values())) for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()} # {'long_LR': 51.48039215686274, 'long_RL': 53.5, 'short_LR': 51.48039215686274, 'short_RL': 53.5}\n",
    "global_max_y_value = np.max(list(aclu_max_y_values_dict.values()))\n",
    "global_max_y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01518ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_neurons = np.max([len(v) for v in _out_rank_order_event_raster_debugger.plots_data.unsorted_original_neuron_IDs_lists])\n",
    "max_n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots.all_separate_plots['long_LR']['root_plot']\n",
    "\n",
    "\n",
    "root_plots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_alt_directional_merged_decoders_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d02ad",
   "metadata": {},
   "source": [
    "## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab95d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dabb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive-mode parameters:\n",
    "_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=True, scrollable_figure=True, defer_render=False)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_curr_interaction_mode_kwargs = _interactive_mode_kwargs # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive:\n",
    "_non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "_curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727307e",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Marginal Scatter Plots from `alt_directional_merged_decoders_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "use_single_time_bin_per_epoch = False\n",
    "active_display_context = curr_active_pipeline.build_display_context_for_session('plot_all_epoch_bins_marginal_predictions', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size) # \n",
    "if use_single_time_bin_per_epoch:\n",
    "\tactive_display_context = active_display_context.adding_context_if_missing(use_single_time_bin_per_epoch=use_single_time_bin_per_epoch)\n",
    "\n",
    "# 'directional_decoded_epochs_marginals'\n",
    "collector_decoded_epoch_marginals = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs_marginals', curr_active_pipeline.get_session_context(), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=active_display_context,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsave_figure=True, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85fcb",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Marginal Yellow-Blue Plots from `alt_directional_merged_decoders_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5876c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_context = owning_pipeline_reference.sess.get_context()\n",
    "# Build the active context directly:\n",
    "active_display_context: IdentifyingContext = curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size)\n",
    "if use_single_time_bin_per_epoch:\n",
    "\tactive_display_context = active_display_context.adding_context_if_missing(use_single_time_bin_per_epoch=use_single_time_bin_per_epoch)\n",
    "active_display_context\n",
    "\n",
    "## Plot the decoded epoch bins of the custom result:\n",
    "_out_decoded_epochs = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(), #active_display_context,\n",
    "\tmax_num_lap_epochs = 80, max_num_ripple_epochs = 100,\n",
    "\t# render_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "\trender_directional_marginal_laps=False, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=True,\n",
    "\t# constrained_layout=True, # layout='none',\n",
    "\t# build_fn='basic_view', constrained_layout=True, # 25.5s\n",
    "\tbuild_fn='insets_view', constrained_layout=True, #constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast, 4.2s\n",
    "\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True, \n",
    "\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t)\n",
    "collector_decoded_epochs = _out_decoded_epochs['collector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_only_keys = [item for item in active_display_context.keys() if 'lap' in item] # items exclusive to laps: ['laps_t_bin']\n",
    "ripple_only_keys = [item for item in active_display_context.keys() if 'ripple' in item]\n",
    "laps_context = active_display_context.get_subset(subset_excludelist=ripple_only_keys) # laps specific context filtering out the ripple keys\n",
    "ripple_context = active_display_context.get_subset(subset_excludelist=laps_only_keys) # ripple specific context filtering out the laps keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4169a",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Build General Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## `alt_directional_merged_decoders_result`\n",
    "from PendingNotebookCode import test_build_new_marginals_df\n",
    "\n",
    "# `alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result`\n",
    "\n",
    "# laps_time_bin_marginals_df = test_build_new_marginals_df(alt_directional_merged_decoders_result)\n",
    "laps_time_bin_marginals_df: pd.DataFrame = test_build_new_marginals_df(a_decoder_result=deepcopy(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t a_track_identity_marginals=alt_directional_merged_decoders_result.laps_directional_marginals_tuple[0]\n",
    "\t\t\t\t\t\t\t )\n",
    "laps_time_bin_marginals_df\n",
    "\n",
    "ripple_time_bin_marginals_df: pd.DataFrame = test_build_new_marginals_df(a_decoder_result=deepcopy(alt_directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t a_track_identity_marginals=alt_directional_merged_decoders_result.ripple_directional_marginals_tuple[0]\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "ripple_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "\n",
    "perform_write_to_file_callback = None\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = deepcopy(laps_time_bin_marginals_df)\n",
    "ripple_all_epoch_bins_marginals_df = deepcopy(ripple_time_bin_marginals_df)\n",
    "\n",
    "if active_context is not None:\n",
    "\tdisplay_context = active_context.adding_context('display_fn', display_fn_name='plot_all_epoch_bins_marginal_predictions')\n",
    "\t\n",
    "# These subset contexts are used to filter out lap/ripple only keys.\n",
    "# e.g. active_context=curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size)\n",
    "\t# only want laps_t_bin on the laps plot and ripple_t_bin on the ripples plot\n",
    "laps_only_keys = [item for item in display_context.keys() if 'lap' in item] # items exclusive to laps: ['laps_t_bin']\n",
    "ripple_only_keys = [item for item in display_context.keys() if 'ripple' in item]\n",
    "laps_display_context = display_context.get_subset(subset_excludelist=ripple_only_keys) # laps specific context filtering out the ripple keys\n",
    "ripple_display_context = display_context.get_subset(subset_excludelist=laps_only_keys) # ripple specific context filtering out the laps keys\n",
    "\n",
    "\n",
    "with mpl.rc_context({'figure.figsize': (12.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42,\n",
    "\t\t\t\t\t\t\"axes.spines.left\": False, \"axes.spines.right\": False, \"axes.spines.bottom\": False, \"axes.spines.top\": False,\n",
    "\t\t\t\t\t\t\"axes.edgecolor\": \"none\", \"xtick.bottom\": False, \"xtick.top\": False, \"ytick.left\": False, \"ytick.right\": False}):\n",
    "\t# Create a FigureCollector instance\n",
    "\twith FigureCollector(name='plot_all_epoch_bins_marginal_predictions', base_context=display_context) as collector:\n",
    "\n",
    "\t\t## Define common operations to do after making the figure:\n",
    "\t\tdef setup_common_after_creation(a_collector, fig, axes, sub_context, title=f'<size:22> Sig. (>0.95) <weight:bold>Best</> <weight:bold>Quantile Diff</></>'):\n",
    "\t\t\t\"\"\" Captures:\n",
    "\n",
    "\t\t\tt_split, t_start, t_end)\n",
    "\t\t\t\"\"\"\n",
    "\t\t\ta_collector.contexts.append(sub_context)\n",
    "\t\t\t\n",
    "\t\t\tfor ax in (axes if isinstance(axes, Iterable) else [axes]):\n",
    "\t\t\t\t# Update the xlimits with the new bounds\n",
    "\t\t\t\tax.set_ylim(0.0, 1.0)\n",
    "\t\t\t\t# Add epoch indicators\n",
    "\t\t\t\t_tmp_output_dict = PlottingHelpers.helper_matplotlib_add_long_short_epoch_indicator_regions(ax=ax, t_split=t_delta, t_start=t_start, t_end=t_end)\n",
    "\t\t\t\t# Update the xlimits with the new bounds\n",
    "\t\t\t\tax.set_xlim(t_start, t_end)\n",
    "\t\t\t\t# Draw a horizontal line at y=0.5\n",
    "\t\t\t\tax.axhline(y=0.5, color=(0,0,0,1)) # , linestyle='--'\n",
    "\t\t\t\t## This is figure level stuff and only needs to be done once:\n",
    "\t\t\t\t# `flexitext` version:\n",
    "\t\t\t\ttext_formatter = FormattedFigureText()\n",
    "\t\t\t\tax.set_title('')\n",
    "\t\t\t\tfig.suptitle('')\n",
    "\t\t\t\t# top=0.84, bottom=0.125, left=0.07, right=0.97,\n",
    "\t\t\t\t# text_formatter.setup_margins(fig, top_margin=1.0, left_margin=0.0, right_margin=1.0, bottom_margin=0.05)\n",
    "\t\t\t\ttext_formatter.setup_margins(fig, top_margin=0.84, left_margin=0.07, right_margin=0.97, bottom_margin=0.125)\n",
    "\t\t\t\t# fig.subplots_adjust(top=top_margin, left=left_margin, right=right_margin, bottom=bottom_margin)\n",
    "\t\t\t\t# title_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin, title, va=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\t\ttitle_text_obj = flexitext(text_formatter.left_margin, 0.98, title, va=\"top\", xycoords=\"figure fraction\") # 0.98, va=\"top\" means the top edge of the title will be aligned to the fig_y=0.98 mark of the figure.\n",
    "\t\t\t\t# footer_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "\t\t\t\t#                             text_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t#                             va=\"top\", xycoords=\"figure fraction\")\n",
    "\n",
    "\t\t\t\tfooter_text_obj = flexitext((text_formatter.left_margin * 0.1), (0.0025), ## (va=\"bottom\", (0.0025)) - this means that the bottom edge of the footer text is aligned with the fig_y=0.0025 in figure space\n",
    "\t\t\t\t\t\t\t\t\t\t\ttext_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t\t\t\t\t\t\t\tva=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\n",
    "\t\t\tif ((perform_write_to_file_callback is not None) and (sub_context is not None)):\n",
    "\t\t\t\tperform_write_to_file_callback(sub_context, fig)\n",
    "\t\t\t\n",
    "\t\t# Plot for BestDir\n",
    "\t\tfig, ax = collector.subplots(num='Laps_Marginal', clear=True)\n",
    "\t\t_out_Laps = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=laps_all_epoch_bins_marginals_df,\n",
    "\t\t\tx='t_bin_center',\n",
    "\t\t\ty='P_Long',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=laps_display_context.adding_context('subplot', subplot_name='Laps all_epoch_binned Marginals'), \n",
    "\t\t\t\t\t\t\t\t\ttitle=f'<size:22> Laps <weight:bold>all_epoch_binned</> Marginals</>')\n",
    "\t\t\n",
    "\t\tfig, ax = collector.subplots(num='Ripple_Marginal', clear=True)\n",
    "\t\t_out_Ripple = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=ripple_all_epoch_bins_marginals_df,\n",
    "\t\t\tx='t_bin_center',\n",
    "\t\t\ty='P_Long',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=ripple_display_context.adding_context('subplot', subplot_name='Ripple all_epoch_binned Marginals'), \n",
    "\t\t\t\t\t\ttitle=f'<size:22> Ripple <weight:bold>all_epoch_binned</> Marginals</>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d655560",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_time_bin_marginals_df['lap_idx'] = laps_time_bin_marginals_df.index.to_numpy()\n",
    "laps_time_bin_marginals_df['lap_start_t'] = laps_epochs_df['start'].to_numpy()\n",
    "laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-01-19 - Can decode position from the pseudo2D posterior directly, or by using the pseudo2D decoder to determine the best direction and track_id and use the corresponding 1D decoder's predicted position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f76dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-01-19 - Export All Epoch Time bin marginals to CSV also\n",
    "## Laps:\n",
    "laps_epochs_df: pd.DataFrame = deepcopy(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs).to_dataframe()\n",
    "laps_directional_marginals_tuple = DirectionalMergedDecodersResult.determine_directional_likelihoods(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals = DirectionalMergedDecodersResult.determine_long_short_likelihoods(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "track_identity_marginals, track_identity_all_epoch_bins_marginal, most_likely_track_identity_from_decoder, is_most_likely_track_identity_Long = laps_track_identity_marginals\n",
    "\n",
    "laps_marginals_df: pd.DataFrame = pd.DataFrame(np.hstack((laps_directional_all_epoch_bins_marginal, track_identity_all_epoch_bins_marginal)), columns=['P_LR', 'P_RL', 'P_Long', 'P_Short'])\n",
    "laps_marginals_df['lap_idx'] = laps_marginals_df.index.to_numpy()\n",
    "laps_marginals_df['lap_start_t'] = laps_epochs_df['start'].to_numpy()\n",
    "laps_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(laps_marginals_df)\n",
    "laps_marginals_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local computation: check laps\n",
    "laps = curr_active_pipeline.sess.laps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(start=0.030, step=0.01, stop=0.10) # [0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc018065",
   "metadata": {},
   "source": [
    "# Call perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6522e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "\n",
    "BATCH_DATE_TO_USE: str = '2024-02-02_Lab' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "# collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve() # Linux\n",
    "# collected_outputs_path: Path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs').resolve() # GreatLakes\n",
    "collected_outputs_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs').resolve() # Apogee\n",
    "\n",
    "\n",
    "def perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict, save_hdf=True, save_csvs=True) -> dict:\n",
    "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    print(f'perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...,across_session_results_extended_dict: {across_session_results_extended_dict})')\n",
    "    from copy import deepcopy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from neuropy.utils.debug_helpers import parameter_sweeps\n",
    "    from neuropy.core.laps import Laps\n",
    "    from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _check_result_laps_epochs_df_performance\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "    from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "    # Export CSVs:\n",
    "    def export_marginals_df_csv(marginals_df: pd.DataFrame, data_identifier_str: str, parent_output_path: Path, active_context):\n",
    "        \"\"\" captures nothing\n",
    "        \"\"\"\n",
    "        # output_date_str: str = get_now_rounded_time_str()\n",
    "        output_date_str: str = get_now_day_str()\n",
    "        # parent_output_path: Path = Path('output').resolve()\n",
    "        # active_context = curr_active_pipeline.get_session_context()\n",
    "        session_identifier_str: str = active_context.get_description()\n",
    "        assert output_date_str is not None\n",
    "        out_basename = '-'.join([output_date_str, session_identifier_str, data_identifier_str]) # '2024-01-04|kdiba_gor01_one_2006-6-09_1-22-43|(laps_marginals_df).csv'\n",
    "        out_filename = f\"{out_basename}.csv\"\n",
    "        out_path = parent_output_path.joinpath(out_filename).resolve()\n",
    "        marginals_df.to_csv(out_path)\n",
    "        return out_path \n",
    "\n",
    "\n",
    "    def _subfn_process_time_bin_swept_results(curr_active_pipeline, output_extracted_result_tuples):\n",
    "        \"\"\" After the sweeps are complete and multiple (one for each time_bin_size swept) indepdnent dfs are had with the four results types this function concatenates each of the four into a single dataframe for all time_bin_size values with a column 'time_bin_size'. \n",
    "        It also saves them out to CSVs in a manner similar to what `compute_and_export_marginals_dfs_completion_function` did to be compatible with `2024-01-23 - Across Session Point and YellowBlue Marginal CSV Exports.ipynb`\n",
    "        Captures: save_csvs\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        several_time_bin_sizes_laps_time_bin_marginals_df_list = []\n",
    "        several_time_bin_sizes_laps_per_epoch_marginals_df_list = []\n",
    "\n",
    "        several_time_bin_sizes_ripple_time_bin_marginals_df_list = []\n",
    "        several_time_bin_sizes_ripple_per_epoch_marginals_df_list = []\n",
    "\n",
    "\n",
    "        # for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
    "        for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df, a_ripple_time_bin_marginals_df, a_ripple_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
    "            a_sweep_dict = dict(a_sweep_tuple)\n",
    "            \n",
    "            # Shared\n",
    "            desired_laps_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
    "            desired_ripple_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
    "            \n",
    "            # a_laps_time_bin_marginals_df.\n",
    "            df = a_laps_time_bin_marginals_df\n",
    "            df['time_bin_size'] = desired_laps_decoding_time_bin_size # desired_laps_decoding_time_bin_size\n",
    "            # df['session_name'] = session_name\n",
    "            df = a_laps_all_epoch_bins_marginals_df\n",
    "            df['time_bin_size'] = desired_laps_decoding_time_bin_size\n",
    "\n",
    "            df = a_ripple_time_bin_marginals_df\n",
    "            df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "            df = a_ripple_all_epoch_bins_marginals_df\n",
    "            df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "\n",
    "            several_time_bin_sizes_laps_time_bin_marginals_df_list.append(a_laps_time_bin_marginals_df)\n",
    "            several_time_bin_sizes_laps_per_epoch_marginals_df_list.append(a_laps_all_epoch_bins_marginals_df)\n",
    "            \n",
    "            several_time_bin_sizes_ripple_time_bin_marginals_df_list.append(a_ripple_time_bin_marginals_df)\n",
    "            several_time_bin_sizes_ripple_per_epoch_marginals_df_list.append(a_ripple_all_epoch_bins_marginals_df)\n",
    "\n",
    "\n",
    "        ## Build across_sessions join dataframes:\n",
    "        several_time_bin_sizes_time_bin_laps_df: pd.DataFrame = pd.concat(several_time_bin_sizes_laps_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
    "        several_time_bin_sizes_laps_df: pd.DataFrame = pd.concat(several_time_bin_sizes_laps_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
    "\n",
    "        several_time_bin_sizes_time_bin_ripple_df: pd.DataFrame = pd.concat(several_time_bin_sizes_ripple_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
    "        several_time_bin_sizes_ripple_df: pd.DataFrame = pd.concat(several_time_bin_sizes_ripple_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
    "\n",
    "        # Export time_bin_swept results to CSVs:\n",
    "        if save_csvs:\n",
    "            assert collected_outputs_path.exists()\n",
    "            active_context = curr_active_pipeline.get_session_context()\n",
    "            laps_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_laps_df, data_identifier_str=f'(laps_time_bin_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            laps_out_path = export_marginals_df_csv(several_time_bin_sizes_laps_df, data_identifier_str=f'(laps_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            ripple_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_ripple_df, data_identifier_str=f'(ripple_time_bin_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            ripple_out_path = export_marginals_df_csv(several_time_bin_sizes_ripple_df, data_identifier_str=f'(ripple_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "        else:\n",
    "            laps_time_bin_marginals_out_path, laps_out_path, ripple_time_bin_marginals_out_path, ripple_out_path = None, None, None, None\n",
    "            \n",
    "        return (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
    "        # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
    "        \n",
    "\n",
    "\n",
    "    def add_session_df_columns(df: pd.DataFrame, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
    "        \"\"\" adds session-specific information to the marginal dataframes \"\"\"\n",
    "        df['session_name'] = session_name \n",
    "        if curr_session_t_delta is not None:\n",
    "            df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
    "        return df\n",
    "\n",
    "\n",
    "    ## Single decode:\n",
    "    def _try_single_decode(owning_pipeline_reference, directional_merged_decoders_result, use_single_time_bin_per_epoch: bool, desired_laps_decoding_time_bin_size: Optional[float]=None, desired_ripple_decoding_time_bin_size: Optional[float]=None, desired_shared_decoding_time_bin_size: Optional[float]=None, minimum_event_duration: Optional[float]=None):\n",
    "        \"\"\" decodes laps and ripples for a single bin size. \n",
    "        \n",
    "        minimum_event_duration: if provided, excludes all events shorter than minimum_event_duration\n",
    "        \"\"\"\n",
    "        if desired_shared_decoding_time_bin_size is not None:\n",
    "            assert desired_laps_decoding_time_bin_size is None\n",
    "            assert desired_ripple_decoding_time_bin_size is None\n",
    "            desired_laps_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
    "            desired_ripple_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
    "            \n",
    "\n",
    "        ## Decode Laps:\n",
    "        laps_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs)\n",
    "        if not isinstance(laps_epochs_df, pd.DataFrame):\n",
    "            laps_epochs_df = laps_epochs_df.to_dataframe()\n",
    "        # global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
    "        min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(laps_epochs_df['duration'].to_numpy())\n",
    "        min_bounded_laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
    "        if desired_laps_decoding_time_bin_size < min_bounded_laps_decoding_time_bin_size:\n",
    "            print(f'WARN: desired_laps_decoding_time_bin_size: {desired_laps_decoding_time_bin_size} < min_bounded_laps_decoding_time_bin_size: {min_bounded_laps_decoding_time_bin_size}... hopefully it works.')\n",
    "        laps_decoding_time_bin_size: float = desired_laps_decoding_time_bin_size # allow direct use\n",
    "        if use_single_time_bin_per_epoch:\n",
    "            laps_decoding_time_bin_size = None\n",
    "        directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=laps_epochs_df,\n",
    "                                                                                                                                                        decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "        \n",
    "\n",
    "        ## Decode Ripples:\n",
    "        if desired_ripple_decoding_time_bin_size is not None:\n",
    "            # global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(owning_pipeline_reference.filtered_sessions[global_epoch_name].replay))\n",
    "            replay_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs)\n",
    "            if not isinstance(replay_epochs_df, pd.DataFrame):\n",
    "                replay_epochs_df = replay_epochs_df.to_dataframe()\n",
    "            # min_possible_ripple_time_bin_size: float = find_minimum_time_bin_duration(replay_epochs_df['duration'].to_numpy())\n",
    "            # min_bounded_ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_ripple_time_bin_size) # 10ms # 0.002\n",
    "            # if desired_ripple_decoding_time_bin_size < min_bounded_ripple_decoding_time_bin_size:\n",
    "            #     print(f'WARN: desired_ripple_decoding_time_bin_size: {desired_ripple_decoding_time_bin_size} < min_bounded_ripple_decoding_time_bin_size: {min_bounded_ripple_decoding_time_bin_size}... hopefully it works.')\n",
    "            ripple_decoding_time_bin_size: float = desired_ripple_decoding_time_bin_size # allow direct use            \n",
    "            ## Drop those less than the time bin duration\n",
    "            print(f'DropShorterMode:')\n",
    "            pre_drop_n_epochs = len(replay_epochs_df)\n",
    "            if minimum_event_duration is not None:                \n",
    "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > minimum_event_duration]\n",
    "                post_drop_n_epochs = len(replay_epochs_df)\n",
    "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
    "                print(f'\\tminimum_event_duration present (minimum_event_duration={minimum_event_duration}).\\n\\tdropping {n_dropped_epochs} that are shorter than our minimum_event_duration of {minimum_event_duration}.', end='\\t')\n",
    "            else:\n",
    "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > desired_ripple_decoding_time_bin_size]\n",
    "                post_drop_n_epochs = len(replay_epochs_df)\n",
    "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
    "                print(f'\\tdropping {n_dropped_epochs} that are shorter than our ripple decoding time bin size of {desired_ripple_decoding_time_bin_size}', end='\\t') \n",
    "\n",
    "            print(f'{post_drop_n_epochs} remain.')\n",
    "            directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=replay_epochs_df,\n",
    "                                                                                                                                                                                            decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "\n",
    "        directional_merged_decoders_result.perform_compute_marginals()\n",
    "        return directional_merged_decoders_result\n",
    "        \n",
    "\n",
    "    def _update_result_laps(a_result: DecodedFilterEpochsResult, laps_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" captures nothing. Can reusing the same laps_df as it makes no modifications to it. \n",
    "        \n",
    "        e.g. a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple]\n",
    "        \"\"\"\n",
    "        result_laps_epochs_df: pd.DataFrame = a_result.laps_epochs_df\n",
    "        ## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions\n",
    "        result_laps_epochs_df['maze_id'] = laps_df['maze_id'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
    "        ## add the 'is_LR_dir' groud-truth column in:\n",
    "        result_laps_epochs_df['is_LR_dir'] = laps_df['is_LR_dir'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
    "        \n",
    "        laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir = a_result.laps_directional_marginals_tuple\n",
    "        laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = a_result.laps_track_identity_marginals_tuple\n",
    "        ## Add the decoded results to the laps df:\n",
    "        result_laps_epochs_df['is_most_likely_track_identity_Long'] = laps_is_most_likely_track_identity_Long\n",
    "        result_laps_epochs_df['is_most_likely_direction_LR'] = laps_is_most_likely_direction_LR_dir\n",
    "        return result_laps_epochs_df\n",
    "\n",
    "    # BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "    assert collected_outputs_path.exists()\n",
    "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "    active_context = curr_active_pipeline.get_session_context()\n",
    "    session_ctxt_key:str = active_context.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "    \n",
    "    ## INPUT PARAMETER: time_bin_size sweep paraemters\n",
    "    desired_shared_decoding_time_bin_size = np.linspace(start=0.030, stop=0.10, num=6)\n",
    "    \n",
    "    # Shared time bin sizes\n",
    "    # all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_sizes, use_single_time_bin_per_epoch=[False], desired_ripple_decoding_time_bin_size=[None])\n",
    "    all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_shared_decoding_time_bin_size=desired_shared_decoding_time_bin_size, use_single_time_bin_per_epoch=[False], minimum_event_duration=[desired_shared_decoding_time_bin_size[-1]]) # with Ripples\n",
    "    # len(all_param_sweep_options)\n",
    "    \n",
    "    ## Perfrom the computations:\n",
    "\n",
    "    # DirectionalMergedDecoders: Get the result after computation:\n",
    "    ## Copy the default result:\n",
    "    directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "    alt_directional_merged_decoders_result: DirectionalMergedDecodersResult = deepcopy(directional_merged_decoders_result)\n",
    "\n",
    "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size-{laps_decoding_time_bin_size}_{data_identifier_str}\"\n",
    "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size_sweep_results\"\n",
    "    out_path_basename_str: str = f\"{CURR_BATCH_OUTPUT_PREFIX}_time_bin_size_sweep_results\"\n",
    "    # out_path_filenname_str: str = f\"{out_path_basename_str}.csv\"\n",
    "\n",
    "    out_path_filenname_str: str = f\"{out_path_basename_str}.h5\"\n",
    "    out_path: Path = collected_outputs_path.resolve().joinpath(out_path_filenname_str).resolve()\n",
    "    print(f'\\out_path_str: \"{out_path_filenname_str}\"')\n",
    "    print(f'\\tout_path: \"{out_path}\"')\n",
    "    \n",
    "    # Ensure it has the 'lap_track' column\n",
    "    ## Compute the ground-truth information using the position information:\n",
    "    # adds columns: ['maze_id', 'is_LR_dir']\n",
    "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "    laps_obj: Laps = curr_active_pipeline.sess.laps\n",
    "    laps_obj.update_lap_dir_from_smoothed_velocity(pos_input=curr_active_pipeline.sess.position)\n",
    "    laps_obj.update_maze_id_if_needed(t_start=t_start, t_delta=t_delta, t_end=t_end)\n",
    "    laps_df = laps_obj.to_dataframe()\n",
    "    \n",
    "    # Uses: session_ctxt_key, all_param_sweep_options\n",
    "    output_alt_directional_merged_decoders_result = {} # empty dict\n",
    "    output_laps_decoding_accuracy_results_dict = {} # empty dict\n",
    "    output_extracted_result_tuples = {}\n",
    "\n",
    "    for a_sweep_dict in all_param_sweep_options:\n",
    "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "        print(f'a_sweep_dict: {a_sweep_dict}')\n",
    "        # Convert parameters to string because Parquet supports metadata as string\n",
    "        a_sweep_str_params = {key: str(value) for key, value in a_sweep_dict.items() if value is not None}\n",
    "        \n",
    "        output_alt_directional_merged_decoders_result[a_sweep_tuple] = _try_single_decode(curr_active_pipeline, alt_directional_merged_decoders_result, **a_sweep_dict)\n",
    "\n",
    "        laps_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_time_bin_marginals_df.copy()\n",
    "        laps_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_all_epoch_bins_marginals_df.copy()\n",
    "        \n",
    "        ## Ripples:\n",
    "        ripple_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_time_bin_marginals_df.copy()\n",
    "        ripple_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_all_epoch_bins_marginals_df.copy()\n",
    "\n",
    "        session_name = curr_session_name\n",
    "        curr_session_t_delta = t_delta\n",
    "        \n",
    "        for a_df, a_time_bin_column_name in zip((laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df), ('t_bin_center', 'lap_start_t', 't_bin_center', 'ripple_start_t')):\n",
    "            ## Add the session-specific columns:\n",
    "            a_df = add_session_df_columns(a_df, session_name, curr_session_t_delta, a_time_bin_column_name)\n",
    "\n",
    "        ## Build the output tuple:\n",
    "        output_extracted_result_tuples[a_sweep_tuple] = (laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df)\n",
    "        \n",
    "        # desired_laps_decoding_time_bin_size_str: str = a_sweep_str_params.get('desired_laps_decoding_time_bin_size', None)\n",
    "        laps_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_decoding_time_bin_size\n",
    "        # ripple_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_decoding_time_bin_size\n",
    "        actual_laps_decoding_time_bin_size_str: str = str(laps_decoding_time_bin_size)\n",
    "        if save_hdf and (actual_laps_decoding_time_bin_size_str is not None):\n",
    "            laps_time_bin_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_time_bin_marginals_df', format='table', data_columns=True)\n",
    "            laps_all_epoch_bins_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_all_epoch_bins_marginals_df', format='table', data_columns=True)\n",
    "\n",
    "        ## TODO: output ripple .h5 here if desired.\n",
    "            \n",
    "\n",
    "        # get the current lap object and determine the percentage correct:\n",
    "        result_laps_epochs_df: pd.DataFrame = _update_result_laps(a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple], laps_df=laps_df)\n",
    "        (is_decoded_track_correct, is_decoded_dir_correct, are_both_decoded_properties_correct), (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly) = _check_result_laps_epochs_df_performance(result_laps_epochs_df)\n",
    "        output_laps_decoding_accuracy_results_dict[laps_decoding_time_bin_size] = (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly)\n",
    "        \n",
    "\n",
    "    ## Output the performance:\n",
    "    output_laps_decoding_accuracy_results_df: pd.DataFrame = pd.DataFrame(output_laps_decoding_accuracy_results_dict.values(), index=output_laps_decoding_accuracy_results_dict.keys(), \n",
    "                    columns=['percent_laps_track_identity_estimated_correctly',\n",
    "                            'percent_laps_direction_estimated_correctly',\n",
    "                            'percent_laps_estimated_correctly'])\n",
    "    output_laps_decoding_accuracy_results_df.index.name = 'laps_decoding_time_bin_size'\n",
    "    ## Save out the laps peformance result\n",
    "    if save_hdf:\n",
    "        output_laps_decoding_accuracy_results_df.to_hdf(out_path, key=f'{session_ctxt_key}/laps_decoding_accuracy_results', format='table', data_columns=True)\n",
    "\n",
    "    ## Call the subfunction to process the time_bin_size swept result and produce combined output dataframes:\n",
    "    combined_multi_timebin_outputs_tuple = _subfn_process_time_bin_swept_results(curr_active_pipeline, output_extracted_result_tuples)\n",
    "    # Unpacking:    \n",
    "    # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "    # add to output dict\n",
    "    # across_session_results_extended_dict['compute_and_export_marginals_dfs_completion_function'] = _out\n",
    "    across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'] = (out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple)\n",
    "    # can unpack like:\n",
    "    (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "    print(f'>>\\t done with {curr_session_context}')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "    return across_session_results_extended_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e9f51",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f6b31",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(None, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=False)\n",
    "out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file_pat\n",
    "collected_outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_laps_decoding_accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_histograms( data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str ) -> None:\n",
    "#     # get the pre-delta epochs\n",
    "#     pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "#     post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "#     descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "#     # plot pre-delta histogram\n",
    "#     pre_delta_df.hist(column='P_Long')\n",
    "#     plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "#     plt.show()\n",
    "\n",
    "#     # plot post-delta histogram\n",
    "#     post_delta_df.hist(column='P_Long')\n",
    "#     plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "def plot_histograms(data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str) -> None:\n",
    "    \"\"\" plots a stacked histogram of the many time-bin sizes \"\"\"\n",
    "    # get the pre-delta epochs\n",
    "    pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "    post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "    descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "    # plot pre-delta histogram\n",
    "    time_bin_sizes = pre_delta_df['time_bin_size'].unique()\n",
    "    \n",
    "    figure_identifier: str = f\"{descriptor_str}_preDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = pre_delta_df[pre_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot post-delta histogram\n",
    "    time_bin_sizes = post_delta_df['time_bin_size'].unique()\n",
    "    figure_identifier: str = f\"{descriptor_str}_postDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = post_delta_df[post_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# # You can use it like this:\n",
    "# plot_histograms('Laps', 'All Sessions', all_sessions_laps_time_bin_df, \"75 ms\")\n",
    "# plot_histograms('Ripples', 'All Sessions', all_sessions_ripple_time_bin_df, \"75 ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from neuropy.utils.matplotlib_helpers import pho_jointplot\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# def pho_jointplot(*args, **kwargs):\n",
    "# \t\"\"\" wraps sns.jointplot to allow adding titles/axis labels/etc.\"\"\"\n",
    "# \ttitle = kwargs.pop('title', None)\n",
    "# \t_out = sns.jointplot(*args, **kwargs)\n",
    "# \tif title is not None:\n",
    "# \t\tplt.suptitle(title)\n",
    "# \treturn _out\n",
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "plot_histograms('Ripples', 'One Session', several_time_bin_sizes_time_bin_ripple_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(\n",
    "#     several_time_bin_sizes_laps_df, x=\"P_Long\", col=\"species\", row=\"time_bin_size\",\n",
    "#     binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    "# )\n",
    "\n",
    "sns.displot(\n",
    "    several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', row=\"time_bin_size\",\n",
    "    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be351c18",
   "metadata": {},
   "source": [
    "# 2024-01-31 - Reinvestigation regarding remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d7495",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d9b54",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## From Jonathan Long/Short Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3641aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df = deepcopy(jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "## try to add the 2D peak information to the cells in `neuron_replay_stats_df`:\n",
    "neuron_replay_stats_df['long_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['long_pf2D_peak_y'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_y'] = pd.NA\n",
    "\n",
    "# flat_peaks_df: pd.DataFrame = deepcopy(active_peak_prominence_2d_results['flat_peaks_df']).reset_index(drop=True)\n",
    "long_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[long_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "short_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[short_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), long_filtered_flat_peaks_df.neuron_id.to_numpy()), ['long_pf2D_peak_x', 'long_pf2D_peak_y']] = long_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), short_filtered_flat_peaks_df.neuron_id.to_numpy()), ['short_pf2D_peak_x', 'short_pf2D_peak_y']] = short_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "\n",
    "both_included_neuron_stats_df = deepcopy(neuron_replay_stats_df[neuron_replay_stats_df['LS_pf_peak_x_diff'].notnull()]).drop(columns=['track_membership', 'neuron_type'])\n",
    "both_included_neuron_stats_df\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# _out_scatter = sns.scatterplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', y='aclu') # , hue='aclu'\n",
    "# _out_scatter.show()\n",
    "# _out_hist = sns.histplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_long_pf].to_numpy()\n",
    "short_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_short_pf].to_numpy()\n",
    "\n",
    "long_pf_aclus, short_pf_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18cfba",
   "metadata": {},
   "source": [
    "## 2024-02-06 - `directional_compute_trial_by_trial_correlation_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383e8ea",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import compute_spatial_binned_activity_via_pfdt, compute_trial_by_trial_correlation_matrix\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import TrialByTrialActivity\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs\n",
    "\n",
    "# track_templates.shared_LR_aclus_only_neuron_IDs\n",
    "# track_templates.shared_RL_aclus_only_neuron_IDs\n",
    "\n",
    "\n",
    "## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "\t# if `KeyError: 'pf1D_dt'` recompute\n",
    "\tcurr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "active_pf_2D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_1D_dt)\n",
    "# active_pf_dt.res\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c34fd9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_decoders_aclu_peak_location_df(width=None))\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e102bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(LR_shift_x, LR_shift, LR_neuron_ids), (RL_shift_x, RL_shift, RL_neuron_ids) = track_templates.get_long_short_decoder_shifts()\n",
    "\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "included_aclus_only = deepcopy(both_included_neuron_stats_df.aclu.to_numpy()) # do we need these to be shared? I don't think so.\n",
    "\n",
    "LR_shift_df: pd.DataFrame = pd.DataFrame({'aclu': LR_neuron_ids, 'shift': LR_shift, 'shift_x': LR_shift_x})\n",
    "RL_shift_df: pd.DataFrame = pd.DataFrame({'aclu': RL_neuron_ids, 'shift': RL_shift, 'shift_x': RL_shift_x})\n",
    "\n",
    "# LR_shift_df = LR_shift_df[np.isin(LR_shift_df['aclu'], included_aclus_only)]\n",
    "# RL_shift_df = RL_shift_df[np.isin(RL_shift_df['aclu'], included_aclus_only)]\n",
    "\n",
    "## Get only the non-zero values:\n",
    "# Filter rows based on column: 'shift'\n",
    "LR_shift_df = LR_shift_df[LR_shift_df['shift'].abs() > 0.01]\n",
    "RL_shift_df = RL_shift_df[RL_shift_df['shift'].abs() > 0.01]\n",
    "\n",
    "LR_shift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504580c3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## Single Global Laps version:\n",
    "laps_df = deepcopy(global_any_laps_epochs_obj.to_dataframe())\n",
    "n_laps = len(laps_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01804af",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "decoder_aclu_num_peaks_df: pd.DataFrame = track_templates.get_decoders_aclu_num_peaks_df()\n",
    "decoder_aclu_num_peaks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b90dc",
   "metadata": {},
   "source": [
    "## 2024-02-08 - Filter to find only the clear remap examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import TrialByTrialActivity\n",
    "from pyphocorehelpers.indexing_helpers import dict_to_full_array\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0cf56",
   "metadata": {},
   "source": [
    "### Get num peaks exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_ids_dict = {k:v.neuron_ids for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# neuron_ids_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42a8d",
   "metadata": {},
   "source": [
    "### Get stability for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ec2d7",
   "metadata": {},
   "source": [
    "#### 2024-02-08 - 3pm - new stability dataframe to look at stability of each cell across decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57869e27",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# for k,v in directional_active_lap_pf_results_dicts.items():\n",
    "# stability_dict = {k:v.aclu_to_stability_score_dict for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict = {k:dict_to_full_array(v.aclu_to_stability_score_dict, full_indicies=any_decoder_neuron_IDs, fill_value=0.0) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "\n",
    "\n",
    "# list(stability_dict.values())\n",
    "\n",
    "stability_dict = {k:list(v.aclu_to_stability_score_dict.values()) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "## all the same size hopefully!\n",
    "# [len(v) for v in list(stability_dict.values())]\n",
    "\n",
    "stability_df: pd.DataFrame = pd.DataFrame({'aclu': any_decoder_neuron_IDs, **stability_dict})\n",
    "# stability_df.rename(dict(zip([], [])))\n",
    "stability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_df.plot.scatter(x='aclu', y=['long_LR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3308ff",
   "metadata": {},
   "source": [
    "### Ensure that we're only getting the location of the maximum peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_neuron_IDs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1698d3a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f206ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# target_df[['long_LR_num_peaks', 'short_LR_num_peaks']]\n",
    "\n",
    "unimodal_max_num_peaks = 1\n",
    "\n",
    "target_df: pd.DataFrame = deepcopy(decoder_aclu_num_peaks_df)\n",
    "\n",
    "# Filter rows based on columns: 'long_LR_num_peaks', 'short_LR_peak'\n",
    "# unimodal_LR_target_df = target_df[(target_df['long_LR_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_LR_num_peaks'] <= unimodal_max_num_peaks)]\n",
    "unimodal_LR_target_df = target_df[((target_df['long_LR_num_peaks'] <= unimodal_max_num_peaks) & (target_df['long_LR_num_peaks'] > 0)) & ((target_df['short_LR_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_LR_num_peaks'] > 0))]\n",
    "\n",
    "# unimodal_RL_target_df = target_df[(target_df['long_RL_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_RL_num_peaks'] <= unimodal_max_num_peaks)]\n",
    "unimodal_RL_target_df = target_df[((target_df['long_RL_num_peaks'] <= unimodal_max_num_peaks) & (target_df['long_RL_num_peaks'] > 0)) & ((target_df['short_RL_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_RL_num_peaks'] > 0))]\n",
    "\n",
    "# np.union1d(unimodal_LR_target_df.aclu.to_numpy(), unimodal_RL_target_df.aclu.to_numpy())\n",
    "# unimodal_all_target_df = target_df[target_df.aclu.isin(np.union1d(unimodal_LR_target_df.aclu.to_numpy(), unimodal_RL_target_df.aclu.to_numpy()))]\n",
    "unimodal_all_target_df = target_df[target_df.aclu.isin(np.intersect1d(unimodal_LR_target_df.aclu.to_numpy(), unimodal_RL_target_df.aclu.to_numpy()))]\n",
    "\n",
    "# unimodal_LR_target_df\n",
    "# unimodal_RL_target_df\n",
    "unimodal_all_target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_location_df_merged[['LR_peak_diff', 'RL_peak_diff']].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-02-08 - Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5de5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_peak_heatmap_test\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent\n",
    "\n",
    "# INPUTS: directional_active_lap_pf_results_dicts, test_aclu: int = 26, xbin_centers, decoder_aclu_peak_location_df_merged\n",
    "\n",
    "def plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu: int = 26, **kwargs):\n",
    "    \"\"\" 2024-02-06 - Plot all four decoders for a single aclu, with overlayed red lines for the detected peaks. \n",
    "    \n",
    "    plot_single_heatmap_set_with_points\n",
    "\n",
    "    plot_cell_position_binned_activity_over_time\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## TEst: Look at a single aclu value\n",
    "    # test_aclu: int = 26\n",
    "    # test_aclu: int = 28\n",
    "    \n",
    "    active_context: IdentifyingContext = kwargs.get('active_context', IdentifyingContext())\n",
    "    active_context = active_context.overwriting_context(aclu=aclu)\n",
    "\n",
    "    decoders_tuning_curves_dict = kwargs.get('decoders_tuning_curves_dict', None)\n",
    "    \n",
    "    matching_aclu_df = decoder_aclu_peak_location_df_merged[decoder_aclu_peak_location_df_merged.aclu == aclu].copy()\n",
    "    assert len(matching_aclu_df) > 0, f\"matching_aclu_df: {matching_aclu_df} for aclu == {aclu}\"\n",
    "    new_peaks_dict: Dict = list(matching_aclu_df.itertuples(index=False))[0]._asdict() # {'aclu': 28, 'long_LR_peak': 185.29063638457257, 'long_RL_peak': nan, 'short_LR_peak': 176.75276643746625, 'short_RL_peak': nan, 'LR_peak_diff': 8.537869947106316, 'RL_peak_diff': nan}\n",
    "        \n",
    "    # long_LR_name, long_RL_name, short_LR_name, short_RL_name\n",
    "    curr_aclu_z_scored_tuning_map_matrix_dict = {}\n",
    "    curr_aclu_mean_epoch_peak_location_dict = {}\n",
    "    curr_aclu_median_peak_location_dict = {}\n",
    "    curr_aclu_extracted_decoder_peak_locations_dict = {}\n",
    "\n",
    "    ## Find the peak location for each epoch:\n",
    "    for a_name, a_decoder_directional_active_lap_pf_result in directional_active_lap_pf_results_dicts.items():\n",
    "        # print(f'a_name: {a_name}')\n",
    "        matrix_idx = a_decoder_directional_active_lap_pf_result.aclu_to_matrix_IDX_map[aclu]\n",
    "        curr_aclu_z_scored_tuning_map_matrix = a_decoder_directional_active_lap_pf_result.z_scored_tuning_map_matrix[:,matrix_idx,:] # .shape (22, 80, 56)\n",
    "        curr_aclu_z_scored_tuning_map_matrix_dict[a_name] = curr_aclu_z_scored_tuning_map_matrix\n",
    "\n",
    "        # curr_aclu_mean_epoch_peak_location_dict[a_name] = np.nanmax(curr_aclu_z_scored_tuning_map_matrix, axis=-1)\n",
    "        assert np.shape(curr_aclu_z_scored_tuning_map_matrix)[-1] == len(xbin_centers), f\"np.shape(curr_aclu_z_scored_tuning_map_matrix)[-1]: {np.shape(curr_aclu_z_scored_tuning_map_matrix)} != len(xbin_centers): {len(xbin_centers)}\"\n",
    "        curr_peak_value = new_peaks_dict[f'{a_name}_peak']\n",
    "        # print(f'curr_peak_value: {curr_peak_value}')\n",
    "        curr_aclu_extracted_decoder_peak_locations_dict[a_name] = curr_peak_value\n",
    "\n",
    "        curr_aclu_mean_epoch_peak_location_dict[a_name] = np.nanargmax(curr_aclu_z_scored_tuning_map_matrix, axis=-1)\n",
    "        curr_aclu_mean_epoch_peak_location_dict[a_name] = xbin_centers[curr_aclu_mean_epoch_peak_location_dict[a_name]] # convert to actual positions instead of indicies\n",
    "        curr_aclu_median_peak_location_dict[a_name] = np.nanmedian(curr_aclu_mean_epoch_peak_location_dict[a_name])\n",
    "\n",
    "    # curr_aclu_mean_epoch_peak_location_dict # {'maze1_odd': array([ 0, 55, 54, 55, 55, 53, 50, 55, 52, 52, 55, 53, 53, 52, 51, 52, 55, 55, 53, 55, 55, 54], dtype=int64), 'maze2_odd': array([46, 45, 43, 46, 45, 46, 46, 46, 45, 45, 44, 46, 44, 45, 46, 45, 44, 44, 45, 45], dtype=int64)}\n",
    "\n",
    "\n",
    "    if decoders_tuning_curves_dict is not None:\n",
    "        curr_aclu_tuning_curves_dict = {name:v.get(aclu, None) for name, v in decoders_tuning_curves_dict.items()}\n",
    "    else:\n",
    "        curr_aclu_tuning_curves_dict = None\n",
    "                \n",
    "    # point_value = curr_aclu_median_peak_location_dict\n",
    "    point_value = curr_aclu_extracted_decoder_peak_locations_dict\n",
    "    fig, ax_dict = plot_peak_heatmap_test(curr_aclu_z_scored_tuning_map_matrix_dict, xbin=xbin, point_dict=point_value, tuning_curves_dict=curr_aclu_tuning_curves_dict, include_tuning_curves=True)\n",
    "    # Set window title and plot title\n",
    "    perform_update_title_subtitle(fig=fig, ax=None, title_string=f\"Position-Binned Activity per Lap - aclu {aclu}\", subtitle_string=None, active_context=active_context, use_flexitext_titles=True)\n",
    "\n",
    "    # fig, ax_dict = plot_peak_heatmap_test(curr_aclu_z_scored_tuning_map_matrix_dict, xbin=xbin, point_dict=curr_aclu_extracted_decoder_peak_locations_dict) # , defer_show=True\n",
    "    \n",
    "    # fig.show()\n",
    "    return fig, ax_dict\n",
    "\n",
    "\n",
    "\n",
    "decoders_tuning_curves_dict = track_templates.decoder_normalized_tuning_curves_dict_dict.copy()\n",
    "\n",
    "extra_decoder_values_dict = {'tuning_curves': decoders_tuning_curves_dict, 'points': decoder_aclu_peak_location_df_merged}\n",
    "\n",
    "# decoders_tuning_curves_dict\n",
    "xbin_centers = deepcopy(active_pf_dt.xbin_centers)\n",
    "xbin = deepcopy(active_pf_dt.xbin)\n",
    "fig, ax_dict = plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, extra_decoder_values_dict=extra_decoder_values_dict, aclu=4, \n",
    "                                                   decoders_tuning_curves_dict=decoders_tuning_curves_dict, decoder_aclu_peak_location_df_merged=decoder_aclu_peak_location_df_merged,\n",
    "                                                    active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a399108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot a couple\n",
    "# long_pf_aclus, short_pf_aclus\n",
    "\n",
    "# plot_aclus = [4,  5,  8,  9]\n",
    "plot_aclus = [11, 18, 68]\n",
    "# plot_aclus = [68]\n",
    "\n",
    "for test_aclu in plot_aclus:\n",
    "\tfig, ax_dict = plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, extra_decoder_values_dict=extra_decoder_values_dict, aclu=test_aclu, \n",
    "                                                   decoders_tuning_curves_dict=decoders_tuning_curves_dict, decoder_aclu_peak_location_df_merged=decoder_aclu_peak_location_df_merged,\n",
    "                                                    active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "\t\n",
    "\tfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_location_df_merged.aclu.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78872f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# def plot_heatmap_grid(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu_list, **kwargs):\n",
    "#     \"\"\" 2024-02-06 - Plot all four decoders for a set of aclus, with overlayed red lines for the detected peaks. \"\"\"\n",
    "    \n",
    "#     ## calculate square root and round up to get number of rows and cols for subplots\n",
    "#     grid_size = math.ceil(math.sqrt(len(aclu_list)))\n",
    "\n",
    "#     ## Create a subplot grid\n",
    "#     fig, axs = plt.subplots(grid_size, grid_size)\n",
    "\n",
    "#     for i, aclu in enumerate(aclu_list):\n",
    "#         curr_ax = axs[i//grid_size, i%grid_size]\n",
    "\n",
    "#         ## Call your original function and pass in curr_ax as the 'ax' parameter\n",
    "#         plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged, aclu=aclu, ax=curr_ax, **kwargs)\n",
    "    \n",
    "#     return fig, axs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import TimedAnimation\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import ipywidgets as widgets  \n",
    "\n",
    "def plot_heatmap_tabs(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu_list, **kwargs):\n",
    "    # Create widgets\n",
    "    out = widgets.Output()\n",
    "    tab = widgets.Tab(children = [out for aclu in aclu_list])\n",
    "   \n",
    "    for index, aclu in enumerate(aclu_list):\n",
    "        tab.set_title(index, f'ACLU {aclu}')\n",
    "        with tab.children[index]:\n",
    "            plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, \n",
    "                                                 decoder_aclu_peak_location_df_merged, aclu=aclu, **kwargs)\n",
    "            plt.show()\n",
    "   \n",
    "    display(tab)\n",
    "\n",
    "# Provide list of aclu's:\n",
    "# aclu_list = [11, 18, 68]\n",
    "aclu_list = track_templates.any_decoder_neuron_IDs[:5]\n",
    "# fig, axs = plot_heatmap_grid(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged, aclu_list=aclu_list, decoders_tuning_curves_dict=decoders_tuning_curves_dict, active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "plot_heatmap_tabs(directional_active_lap_pf_results_dicts, xbin_centers, xbin,\n",
    "                  decoder_aclu_peak_location_df_merged, aclu_list=aclu_list, \n",
    "                  decoders_tuning_curves_dict=decoders_tuning_curves_dict, \n",
    "                  active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f217aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = fig.get_layout_engine()\n",
    "layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f23ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these layout changes don't seem to take effect until the window containing the figure is resized.\n",
    "# fig.set_layout_engine('compressed') # TAKEWAY: Use 'compressed' instead of 'constrained'\n",
    "fig.set_layout_engine('none') # disabling layout engine. Strangely still allows window to resize and the plots scale, so I'm not sure what the layout engine is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(directional_active_lap_pf_results_dicts.keys()) # ['maze1_odd', 'maze1_even', 'maze2_odd', 'maze2_even']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef13541",
   "metadata": {},
   "source": [
    "# 2024-02-02 - napari_plot_directional_trial_by_trial_activity_viz Trial-by-trial Correlation Matrix C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd735e",
   "metadata": {},
   "source": [
    "###  Show Trial-by-trial Correlation Matrix C in `napari`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72df59",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "# import afinder\n",
    "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_from_layers_dict\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_trial_by_trial_activity_viz\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_plot_directional_trial_by_trial_activity_viz\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_export_image_sequence\n",
    "\n",
    "## Directional\n",
    "directional_viewer, directional_image_layer_dict, custom_direction_split_layers_dict = napari_plot_directional_trial_by_trial_activity_viz(directional_active_lap_pf_results_dicts, include_trial_by_trial_correlation_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9934d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Global:\n",
    "viewer, image_layer_dict = napari_trial_by_trial_activity_viz(z_scored_tuning_map_matrix, C_trial_by_trial_correlation_matrix, title='Trial-by-trial Correlation Matrix C', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f61fa",
   "metadata": {},
   "source": [
    "# Napari Plotting Long/Short Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80146cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import test_LinearTrackDimensions_2D_pyqtgraph, LinearTrackDimensions, LinearTrackInstance\n",
    "\n",
    "long_track_dims = LinearTrackDimensions(track_length=170.0)\n",
    "short_track_dims = LinearTrackDimensions(track_length=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get grid_bin_bounds:\n",
    "long_grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "short_grid_bin_bounds = deepcopy(short_pf2D.config.grid_bin_bounds)\n",
    "\n",
    "long_grid_bin_bounds\n",
    "short_grid_bin_bounds\n",
    "linear_track_instance = LinearTrackInstance.init_from_grid_bin_bounds(grid_bin_bounds=long_grid_bin_bounds)\n",
    "linear_track_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app, w, cw, (long_track_dims, long_rect_items, long_rects), (short_track_dims, short_rect_items, short_rects) = test_LinearTrackDimensions_2D_pyqtgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aeb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Napari Shapes Layer Test:\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_napari_track_shapes_layer\n",
    "\n",
    "# add the image\n",
    "# viewer = napari.view_image(data.camera(), name='photographer')\n",
    "\n",
    "test_shapes_viewer = napari.Viewer() # name='Test Shapes Viewer'\n",
    "# add the tracks\n",
    "long_rectangles_poly_shapes_layer, short_rectangles_poly_shapes_layer = add_napari_track_shapes_layer(test_shapes_viewer, long_rect_items, short_rect_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_layer_info(long_rectangles_poly_shapes_layer)\n",
    "extract_layer_info(short_rectangles_poly_shapes_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_rectangles_poly_shapes_layer.bounding_box\n",
    "# long_rectangles_poly_shapes_layer.interaction_box\n",
    "long_rectangles_poly_shapes_layer.corner_pixels # np.array([[ 44, 124], [376, 161]])\n",
    "# long_rectangles_poly_shapes_layer.frame\n",
    "\n",
    "\n",
    "# long_rectangles_poly_shapes_layer.rotate = 90\n",
    "# data_to_world, world_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_rectangles_poly_shapes_layer.rotate = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af347488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2024-02-02 22:31: - [ ] These need to be update for global support\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_add_aclu_slider\n",
    "\n",
    "def build_filename_from_viewer(viewer, desired_save_parent_path: Path, slider_axis_IDX: int = 0) -> Path:\n",
    "    \"\"\"\n",
    "    Captures: curr_active_pipeline, neuron_ids, global_any_name\n",
    "    \n",
    "     Usage:\n",
    "        file_out_path = build_filename_from_viewer(viewer)\n",
    "        viewer.screenshot(path=file_out_path, canvas_only=True, flash=False)\n",
    "\n",
    "    \"\"\"\n",
    "    # desired_save_parent_path = Path('/home/halechr/Desktop/test_napari_out').resolve()\n",
    "\n",
    "    matrix_aclu_IDX: int = int(viewer.dims.current_step[slider_axis_IDX])\n",
    "    # find the aclu value for this index:\n",
    "    aclu: int = int(neuron_ids[matrix_aclu_IDX])\n",
    "    curr_context = curr_active_pipeline.build_display_context_for_filtered_session(global_any_name, 'napari_trial_by_trial_activity_viz', aclu=str(aclu))\n",
    "    curr_context_string: str = curr_context.get_description() #.get_description(suffix_items=[f'aclu-{aclu}'])\n",
    "    filename_string: str = f\"{curr_context_string}.png\"\n",
    "\n",
    "    file_out_path = desired_save_parent_path.joinpath(filename_string).resolve()\n",
    "    return file_out_path\n",
    "\n",
    "\n",
    "# desired_save_parent_path = Path('/home/halechr/Desktop/test_napari_out').resolve()\n",
    "desired_save_parent_path = Path(r'C:\\Users\\pho\\Desktop\\test_napari_out').resolve()\n",
    "_connected_on_update_slider_event = napari_add_aclu_slider(viewer=directional_viewer, neuron_ids=neuron_ids)\n",
    "imageseries_output_directory = napari_export_image_sequence(viewer=viewer, imageseries_output_directory=desired_save_parent_path, slider_axis_IDX=0, build_filename_from_viewer_callback_fn=build_filename_from_viewer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_viewer.Config\n",
    "directional_viewer.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {},
   "source": [
    "# 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_spike_rasters_pyqtplot_2D', 'maze_any') # 'maze_any'\n",
    "assert isinstance(_out_graphics_dict, dict)\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = _out_graphics_dict['spike_raster_plt_2d'], _out_graphics_dict['spike_raster_plt_3d'], _out_graphics_dict['spike_raster_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8eceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "# graphics_output_dict = curr_active_pipeline.display('_display_long_short_pf1D_comparison', active_identifying_session_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_long_short_pf1D_comparison', active_identifying_session_ctx,\n",
    "                                                     include_includelist=[long_LR_name, short_LR_name, global_LR_name], active_context=active_identifying_session_ctx.overwriting_context(dir='LR'), included_any_context_neuron_ids=LR_shift_df.aclu.unique())\n",
    "\n",
    "\n",
    "# fig, axs, plot_data = graphics_output_dict['fig'], graphics_output_dict['axs'], graphics_output_dict['plot_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51201e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "\n",
    "_out = TemplateDebugger.init_templates_debugger(track_templates) # , included_any_context_neuron_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot.display_function_items\n",
    "\n",
    "# '_display_directional_template_debugger'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd064d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_directional_template_debugger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba36573",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_template_debugger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_display_directional_merged_pfs'\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=False, plot_long_directional=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting on 2024-02-06 to display the LR/RL directions instead of the All/Long/Short pfs:\n",
    "def _display_directional_merged_pfs(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "\t\t\t\t\t\t\t\t\tplot_all_directions=True, plot_long_directional=False, plot_short_directional=False, **kwargs):\n",
    "\t\"\"\" Plots the merged pseduo-2D pfs/ratemaps. Plots: All-Directions, Long-Directional, Short-Directional in seperate windows. \n",
    "\t\n",
    "\tHistory: this is the Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "\t\"\"\"\n",
    "\tfrom pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "\tfrom pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow \n",
    "\t\n",
    "\n",
    "\tdefer_render = kwargs.pop('defer_render', False)\n",
    "\tdirectional_merged_decoders_result: DirectionalMergedDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\tactive_merged_pf_plots_data_dict = {} #empty dict\n",
    "\t\n",
    "\tif plot_all_directions:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.all_directional_pf1D_Decoder.pf # all-directions\n",
    "\tif plot_long_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.long_directional_pf1D_Decoder.pf # Long-only\n",
    "\tif plot_short_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Short-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.short_directional_pf1D_Decoder.pf # Short-only\n",
    "\n",
    "\tout_plots_dict = {}\n",
    "\t\n",
    "\tfor active_context, active_pf_2D in active_merged_pf_plots_data_dict.items():\n",
    "\t\t# figure_format_config = {} # empty dict for config\n",
    "\t\tfigure_format_config = {'scrollability_mode': LayoutScrollability.NON_SCROLLABLE} # kwargs # kwargs as default figure_format_config\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig: BasicBinnedImageRenderingWindow  = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "\t\n",
    "\t\t# Set the window title from the context\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "\t\tout_plots_dict[active_context] = out_all_pf_2D_pyqtgraph_binned_image_fig\n",
    "\n",
    "\t\t# Tries to update the display of the item:\n",
    "\t\tnames_list = [v for v in list(out_all_pf_2D_pyqtgraph_binned_image_fig.plots.keys()) if v not in ('name', 'context')]\n",
    "\t\tfor a_name in names_list:\n",
    "\t\t\t# Adjust the size of the text for the item by passing formatted text\n",
    "\t\t\ta_plot: pg.PlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[a_name].mainPlotItem # PlotItem \n",
    "\t\t\t# no clue why 2 is a good value for this...\n",
    "\t\t\ta_plot.titleLabel.setMaximumHeight(2)\n",
    "\t\t\ta_plot.layout.setRowFixedHeight(0, 2)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif not defer_render:\n",
    "\t\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.show()\n",
    "\n",
    "\treturn out_plots_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc19a",
   "metadata": {},
   "source": [
    "# 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc55f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import ContinuousPeakLocationRepresentingMixin\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from scipy.signal import find_peaks\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', sortby=None)\n",
    "\n",
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap: Ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, peaks_results_df = active_ratemap.compute_tuning_curve_modes(height=0.2, width=None)\n",
    "\n",
    "\n",
    "included_columns = ['pos', 'peak_heights'] # the columns of interest that you want in the final dataframe.\n",
    "included_columns_renamed = dict(zip(included_columns, ['peak', 'peak_height']))\n",
    "decoder_peaks_results_dfs = [a_decoder.pf.ratemap.get_tuning_curve_peak_df(height=0.2, width=None) for a_decoder in (track_templates.long_LR_decoder, track_templates.long_RL_decoder, track_templates.short_LR_decoder, track_templates.short_RL_decoder)]\n",
    "prefix_names = [f'{a_decoder_name}_' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "all_included_columns = ['aclu', 'series_idx', 'subpeak_idx'] + included_columns # Used to filter out the unwanted columns from the output\n",
    "\n",
    "# [['aclu', 'series_idx', 'subpeak_idx', 'pos']]\n",
    "\n",
    "# rename_list_fn = lambda a_prefix: {'pos': f\"{a_prefix}pos\"}\n",
    "rename_list_fn = lambda a_prefix: {a_col_name:f\"{a_prefix}{included_columns_renamed[a_col_name]}\" for a_col_name in included_columns}\n",
    "\n",
    "# column_names = [f'{a_decoder_name}_peak' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "\n",
    "# dataFrames = decoder_peaks_results_dfs\n",
    "# names = self.get_decoder_names()\n",
    "\n",
    "# rename 'pos' column in each dataframe and then reduce to perform cumulative outer merge\n",
    "result_df = decoder_peaks_results_dfs[0][all_included_columns].rename(columns=rename_list_fn(prefix_names[0]))\n",
    "for df, a_prefix in zip(decoder_peaks_results_dfs[1:], prefix_names[1:]):\n",
    "    result_df = pd.merge(result_df, df[all_included_columns].rename(columns=rename_list_fn(a_prefix)), on=['aclu', 'series_idx', 'subpeak_idx'], how='outer')\n",
    "\n",
    "# result = reorder_columns(result, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "# list(filter(lambda column: column.endswith('_peak_heights'), result.columns))\n",
    "# result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))\n",
    "# result_df\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "result_df = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), relative_mode='end')\n",
    "result_df\n",
    "# print(list(result.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ccd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\" 2023-06-01 - \n",
    "        \n",
    "        ## TODO 2023-06-02 NOW, NEXT: this might not work in 'AGG' mode because it tries to render it with QT, but we can see.\n",
    "        \n",
    "        Usage:\n",
    "            (pagination_controller_L, pagination_controller_S), (fig_L, fig_S), (ax_L, ax_S), (final_context_L, final_context_S), (active_out_figure_paths_L, active_out_figure_paths_S) = _subfn_prepare_plot_long_and_short_stacked_epoch_slices(curr_active_pipeline, defer_render=False)\n",
    "        \"\"\"\n",
    "        from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices_paginated\n",
    "\n",
    "        ## long_short_decoding_analyses:\n",
    "        curr_long_short_decoding_analyses = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "        ## Extract variables from results object:\n",
    "        long_results_obj, short_results_obj = curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj\n",
    "        long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "        pagination_controller_L, active_out_figure_paths_L, final_context_L = plot_decoded_epoch_slices_paginated(curr_active_pipeline, long_results_obj, curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='replays', decoder='long_results_obj'), included_epoch_indicies=included_epoch_indicies, save_figure=save_figure, **kwargs)\n",
    "        fig_L = pagination_controller_L.plots.fig\n",
    "        ax_L = fig_L.get_axes()\n",
    "        if defer_render:\n",
    "            widget_L = pagination_controller_L.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "            widget_L.close()\n",
    "            pagination_controller_L = None\n",
    "\n",
    "        \n",
    "        pagination_controller_S, active_out_figure_paths_S, final_context_S = plot_decoded_epoch_slices_paginated(curr_active_pipeline, short_results_obj, curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='replays', decoder='short_results_obj'), included_epoch_indicies=included_epoch_indicies, save_figure=save_figure, **kwargs)\n",
    "        fig_S = pagination_controller_S.plots.fig\n",
    "        ax_S = fig_S.get_axes()\n",
    "        if defer_render:\n",
    "            widget_S = pagination_controller_S.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "            widget_S.close()\n",
    "            pagination_controller_S = None\n",
    "\n",
    "        return (pagination_controller_L, pagination_controller_S), (fig_L, fig_S), (ax_L, ax_S), (final_context_L, final_context_S), (active_out_figure_paths_L, active_out_figure_paths_S)\n",
    "\n",
    "peaks_results_df = track_templates.get_decoders_aclu_peak_location_df().sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True)\n",
    "peaks_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d377cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict: Dict = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index().set_index('aclu').to_dict()['subpeak_idx_count'] # number of peaks (\"models\" for each aclu)\n",
    "# aclu_n_peaks_dict\n",
    "\n",
    "# peaks_results_df = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index()\n",
    "\n",
    "# peaks_results_df[peaks_results_df.aclu == 5]\n",
    "# peaks_results_df.aclu.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeccfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ratemap.n_neurons\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=active_ratemap.neuron_ids, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aclu_n_peaks_dict\n",
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aebbf1",
   "metadata": {},
   "source": [
    "# 2024-02-08 Directional Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_ripple_filter_epochs_decoder_result_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da36235",
   "metadata": {},
   "source": [
    "### 2024-02-09 - Recover Radon Transform info to confirm the Pseudo2D decoder-based detection of long/short replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2420d6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "## From scratch:\n",
    "from neuropy.core.session.dataSession import Laps\n",
    "from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions import compute_radon_transforms, perform_compute_radon_transforms\n",
    "\n",
    "def _compute_epoch_decoding_radon_transform_for_decoder(a_directional_pf1D_Decoder, a_directional_laps_filter_epochs_decoder_result, a_directional_ripple_filter_epochs_decoder_result, nlines=4192, margin=16, n_jobs=4):\n",
    "    \"\"\" Decodes the laps and the ripples and their RadonTransforms using the provided decoder.\n",
    "    ~12.2s per decoder.\n",
    "\n",
    "    \"\"\"\n",
    "    a_directional_pf1D_Decoder = deepcopy(a_directional_pf1D_Decoder)\n",
    "    # pos_bin_size: the size of the x_bin in [cm]\n",
    "    if a_directional_pf1D_Decoder.pf.bin_info is not None:\n",
    "        pos_bin_size = float(a_directional_pf1D_Decoder.pf.bin_info['xstep'])\n",
    "    else:\n",
    "        ## if the bin_info is for some reason not accessible, just average the distance between the bin centers.\n",
    "        pos_bin_size = np.diff(a_directional_pf1D_Decoder.pf.xbin_centers).mean()\n",
    "    \n",
    "    laps_radon_transform_extras = []\n",
    "    laps_radon_transform_df, *laps_radon_transform_extras = a_directional_laps_filter_epochs_decoder_result.compute_radon_transforms(pos_bin_size=pos_bin_size, nlines=nlines, margin=margin, n_jobs=n_jobs)\n",
    "\n",
    "    ## Decode Ripples:\n",
    "    if a_directional_ripple_filter_epochs_decoder_result is not None:\n",
    "        ripple_radon_transform_extras = []\n",
    "        # ripple_radon_transform_df = compute_radon_transforms(a_directional_pf1D_Decoder, a_directional_ripple_filter_epochs_decoder_result)\n",
    "        ripple_radon_transform_df, *ripple_radon_transform_extras = a_directional_ripple_filter_epochs_decoder_result.compute_radon_transforms(pos_bin_size=pos_bin_size, nlines=nlines, margin=margin, n_jobs=n_jobs)\n",
    "    else:\n",
    "        ripple_radon_transform_extras = None\n",
    "        ripple_radon_transform_df = None\n",
    "\n",
    "    return laps_radon_transform_df, laps_radon_transform_extras, ripple_radon_transform_df, ripple_radon_transform_extras\n",
    "\n",
    "\n",
    "# Inputs: all_directional_pf1D_Decoder, alt_directional_merged_decoders_result\n",
    "def _compute_epoch_decoding_for_decoder(a_directional_pf1D_Decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size: float = 0.5, desired_ripple_decoding_time_bin_size: float = 0.1, use_single_time_bin_per_epoch: bool=False):\n",
    "    \"\"\" Decodes the laps and the ripples and their RadonTransforms using the provided decoder.\n",
    "    ~12.2s per decoder.\n",
    "\n",
    "    \"\"\"\n",
    "    # Modifies alt_directional_merged_decoders_result, a copy of the original result, with new timebins\n",
    "    long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "    a_directional_pf1D_Decoder = deepcopy(a_directional_pf1D_Decoder)\n",
    "\n",
    "    if use_single_time_bin_per_epoch:\n",
    "        print(f'WARNING: use_single_time_bin_per_epoch=True so time bin sizes will be ignored.')\n",
    "        \n",
    "    ## Decode Laps:\n",
    "    global_any_laps_epochs_obj = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
    "    min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(global_any_laps_epochs_obj.to_dataframe()['duration'].to_numpy())\n",
    "    laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
    "    if use_single_time_bin_per_epoch:\n",
    "        laps_decoding_time_bin_size = None\n",
    "\n",
    "    a_directional_laps_filter_epochs_decoder_result = a_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df), filter_epochs=global_any_laps_epochs_obj, decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "    # laps_radon_transform_df = compute_radon_transforms(a_directional_pf1D_Decoder, a_directional_laps_filter_epochs_decoder_result)\n",
    "\n",
    "    ## Decode Ripples:\n",
    "    if desired_ripple_decoding_time_bin_size is not None:\n",
    "        global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "        min_possible_time_bin_size: float = find_minimum_time_bin_duration(global_replays['duration'].to_numpy())\n",
    "        ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_time_bin_size) # 10ms # 0.002\n",
    "        if use_single_time_bin_per_epoch:\n",
    "            ripple_decoding_time_bin_size = None\n",
    "        a_directional_ripple_filter_epochs_decoder_result = a_directional_pf1D_Decoder.decode_specific_epochs(deepcopy(curr_active_pipeline.sess.spikes_df), filter_epochs=global_replays, decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "        # ripple_radon_transform_df = compute_radon_transforms(a_directional_pf1D_Decoder, a_directional_ripple_filter_epochs_decoder_result)\n",
    "\n",
    "    else:\n",
    "        a_directional_ripple_filter_epochs_decoder_result = None\n",
    "        # ripple_radon_transform_df = None\n",
    "\n",
    "    ## Post Compute Validations:\n",
    "    # alt_directional_merged_decoders_result.perform_compute_marginals()\n",
    "    return a_directional_laps_filter_epochs_decoder_result, a_directional_ripple_filter_epochs_decoder_result #, (laps_radon_transform_df, ripple_radon_transform_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852742b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_directional_laps_filter_epochs_decoder_result = deepcopy(decoder_laps_filter_epochs_decoder_result_dict[a_name])\n",
    "n_time_bins = a_directional_laps_filter_epochs_decoder_result.nbins\n",
    "# n_time_bins\n",
    "active_posterior = a_directional_laps_filter_epochs_decoder_result.p_x_given_n_list\n",
    "\n",
    "[np.shape(an_active_posterior) for a_number_time_bins, an_active_posterior in zip(a_directional_laps_filter_epochs_decoder_result.nbins, a_directional_laps_filter_epochs_decoder_result.p_x_given_n_list)]\n",
    "\n",
    "# Each epoch is (n_pos_bins, n_epoch_time_bins)\n",
    "# [(56, 66),\n",
    "#  (56, 102),\n",
    "#  (56, 226),\n",
    "#  ...\n",
    "# ]\n",
    "\n",
    "# [(np.shape(an_active_posterior)[-1] == a_number_time_bins) for a_number_time_bins, an_active_posterior in zip(a_directional_laps_filter_epochs_decoder_result.nbins, a_directional_laps_filter_epochs_decoder_result.p_x_given_n_list)]\n",
    "# active_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97b0fb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "## Main Custom Decoder Computation:\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "## Decode epochs for all four decoders:\n",
    "decoder_laps_filter_epochs_decoder_result_dict = {}\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = {}\n",
    "\n",
    "# decoder_laps_radon_transform_df_dict = {}\n",
    "# decoder_ripple_radon_transform_df_dict = {}\n",
    "\n",
    "for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    # decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name], (decoder_laps_radon_transform_df_dict[a_name], decoder_ripple_radon_transform_df_dict[a_name]) =\n",
    "    decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_epoch_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "\n",
    "# decoder_laps_radon_transform_df_dict ## ~4m\n",
    "## OUTPUTS: decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, decoder_laps_filter_epochs_decoder_result_dict, decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d06ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_radon_transform_df_dict = {}\n",
    "decoder_ripple_radon_transform_df_dict = {}\n",
    "\n",
    "decoder_laps_radon_transform_extras_dict = {}\n",
    "decoder_ripple_radon_transform_extras_dict = {}\n",
    "\n",
    "\n",
    "for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    # decoder_laps_radon_transform_df_dict[a_name], decoder_ripple_radon_transform_df_dict[a_name] = _compute_epoch_decoding_radon_transform_for_decoder(a_decoder, decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name], n_jobs=4)\n",
    "    decoder_laps_radon_transform_df_dict[a_name], decoder_laps_radon_transform_extras_dict[a_name], decoder_ripple_radon_transform_df_dict[a_name], decoder_ripple_radon_transform_extras_dict[a_name] = _compute_epoch_decoding_radon_transform_for_decoder(a_decoder, decoder_laps_filter_epochs_decoder_result_dict[a_name], decoder_ripple_filter_epochs_decoder_result_dict[a_name], n_jobs=4)\n",
    "\n",
    "\n",
    "# laps_radon_transform_df, ripple_radon_transform_df = \n",
    "    \n",
    "# 6m 19.7s - nlines=8192, margin=16, n_jobs=1\n",
    "# 17m 57.6s - nlines=24000, margin=16, n_jobs=1\n",
    "# 4m 31.9s -  nlines=8192, margin=16, n_jobs=4\n",
    "# Still running 14m later - neighbours: 8 = int(margin: 32 / pos_bin_size: 3.8054171165052444)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca34ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using convolution to sum neighbours\n",
    "arr = np.apply_along_axis(\n",
    "    np.convolve, axis=0, arr=arr, v=np.ones(2 * neighbours + 1), mode=\"same\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decoder_ripple_filter_epochs_decoder_result_dict[a_name]) # pyphoplacecellanalysis.Analysis.Decoder.reconstruction.DecodedFilterEpochsResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399efd5",
   "metadata": {},
   "source": [
    "##  2024-02-13 - Saving manual decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc95ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "## Variables to save to be passed to the function: Ideally get their passed name:\n",
    "\n",
    "# ripple_decoding_time_bin_size, laps_decoding_time_bin_size, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict\n",
    "# {'ripple_decoding_time_bin_size':ripple_decoding_time_bin_size, 'laps_decoding_time_bin_size':laps_decoding_time_bin_size, 'decoder_laps_filter_epochs_decoder_result_dict':decoder_laps_filter_epochs_decoder_result_dict, 'decoder_ripple_filter_epochs_decoder_result_dict':decoder_ripple_filter_epochs_decoder_result_dict, 'decoder_laps_radon_transform_df_dict':decoder_laps_radon_transform_df_dict, 'decoder_ripple_radon_transform_df_dict':decoder_ripple_radon_transform_df_dict}\n",
    "\n",
    "override_output_parent_path = None\n",
    "out_filename_str: str = '-'.join([DAY_DATE_TO_USE]) #SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date=DAY_DATE_TO_USE)\n",
    "# print(f'save_rank_order_results(...): out_filename_str: \"{out_filename_str}\"')\n",
    "output_parent_path: Path = (override_output_parent_path or curr_active_pipeline.get_output_path()).resolve()\n",
    "\n",
    "try:\n",
    "\toutput_path = output_parent_path.joinpath(f'{out_filename_str}_CustomDecodingResults.pkl').resolve()\n",
    "\tprint(f'saving to \"{output_path}\"...')\n",
    "\tsaveData(output_path, ({'ripple_decoding_time_bin_size':ripple_decoding_time_bin_size, 'laps_decoding_time_bin_size':laps_decoding_time_bin_size, 'decoder_laps_filter_epochs_decoder_result_dict':decoder_laps_filter_epochs_decoder_result_dict,\n",
    "\t\t\t\t\t\t  'decoder_ripple_filter_epochs_decoder_result_dict':decoder_ripple_filter_epochs_decoder_result_dict, 'decoder_laps_radon_transform_df_dict':decoder_laps_radon_transform_df_dict, 'decoder_ripple_radon_transform_df_dict':decoder_ripple_radon_transform_df_dict,\n",
    "\t\t\t\t\t\t  'decoder_laps_radon_transform_extras_dict': decoder_laps_radon_transform_extras_dict, 'decoder_ripple_radon_transform_extras_dict': decoder_ripple_radon_transform_extras_dict,\n",
    "                          \n",
    "                          }))\n",
    "except BaseException as e:\n",
    "\tprint(f'issue saving \"{output_path}\": error: {e}')\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData\n",
    "\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-13_CustomDecodingResults.pkl\").resolve()\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-13_9pm_CustomDecodingResults.pkl\").resolve()\n",
    "load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-14_CustomDecodingResults.pkl\").resolve()\n",
    "assert load_path.exists()\n",
    "loaded_dict = loadData(load_path, debug_print=False)\n",
    "## UNPACK HERE:\n",
    "ripple_decoding_time_bin_size = loaded_dict['ripple_decoding_time_bin_size']\n",
    "laps_decoding_time_bin_size = loaded_dict['laps_decoding_time_bin_size']\n",
    "decoder_laps_filter_epochs_decoder_result_dict = loaded_dict['decoder_laps_filter_epochs_decoder_result_dict']\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = loaded_dict['decoder_ripple_filter_epochs_decoder_result_dict']\n",
    "decoder_laps_radon_transform_df_dict = loaded_dict['decoder_laps_radon_transform_df_dict']\n",
    "decoder_ripple_radon_transform_df_dict = loaded_dict['decoder_ripple_radon_transform_df_dict']\n",
    "## New 2024-02-14 - Noon:\n",
    "decoder_laps_radon_transform_extras_dict = loaded_dict['decoder_laps_radon_transform_extras_dict']\n",
    "decoder_ripple_radon_transform_extras_dict = loaded_dict['decoder_ripple_radon_transform_extras_dict']\n",
    "\n",
    "\n",
    "# {'ripple_decoding_time_bin_size':ripple_decoding_time_bin_size, 'laps_decoding_time_bin_size':laps_decoding_time_bin_size, 'decoder_laps_filter_epochs_decoder_result_dict':decoder_laps_filter_epochs_decoder_result_dict, 'decoder_ripple_filter_epochs_decoder_result_dict':decoder_ripple_filter_epochs_decoder_result_dict, 'decoder_laps_radon_transform_df_dict':decoder_laps_radon_transform_df_dict, 'decoder_ripple_radon_transform_df_dict':decoder_ripple_radon_transform_df_dict}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309c3e1",
   "metadata": {},
   "source": [
    "Write an example transform named \"Variable list into python dictionary\" that takes a comma-separated list of python variables like: \n",
    "`ripple_decoding_time_bin_size, laps_decoding_time_bin_size, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict`\n",
    "and transforms them into a valid python dictionary like:\n",
    "`{'ripple_decoding_time_bin_size':ripple_decoding_time_bin_size, 'laps_decoding_time_bin_size':laps_decoding_time_bin_size, 'decoder_laps_filter_epochs_decoder_result_dict':decoder_laps_filter_epochs_decoder_result_dict, 'decoder_ripple_filter_epochs_decoder_result_dict':decoder_ripple_filter_epochs_decoder_result_dict, 'decoder_laps_radon_transform_df_dict':decoder_laps_radon_transform_df_dict, 'decoder_ripple_radon_transform_df_dict':decoder_ripple_radon_transform_df_dict}`\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1523af2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "## NOTE: To plot the radon transforms the values must be added to the result object's active_filter_epochs dataframe.\n",
    "from neuropy.utils.mixins.binning_helpers import BinningContainer\n",
    "\n",
    "\n",
    "def _compute_matching_best_indicies(a_marginals_df: pd.DataFrame, index_column_name: str = 'most_likely_decoder_index', second_index_column_name: str = 'best_decoder_index', enable_print=True):\n",
    "    \"\"\" count up the number that the RadonTransform and the most-likely direction agree \"\"\"\n",
    "    num_total_epochs: int = len(a_marginals_df)\n",
    "    agreeing_rows_count: int = (a_marginals_df[index_column_name] == a_marginals_df[second_index_column_name]).sum()\n",
    "    agreeing_rows_ratio = float(agreeing_rows_count)/float(num_total_epochs)\n",
    "    if enable_print:\n",
    "        print(f'agreeing_rows_count/num_total_epochs: {agreeing_rows_count}/{num_total_epochs}\\n\\tagreeing_rows_ratio: {agreeing_rows_ratio}')\n",
    "    return agreeing_rows_ratio, (agreeing_rows_count, num_total_epochs)\n",
    "\n",
    "def _update_decoder_result_active_filter_epoch_columns(a_result_obj, a_radon_transform_df, columns=['score', 'velocity', 'intercept', 'speed']):\n",
    "    \"\"\" Joins the radon-transform result into the `a_result_obj.filter_epochs` dataframe.\n",
    "    \n",
    "    decoder_laps_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_laps_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=decoder_laps_radon_transform_df_dict[a_name])\n",
    "    decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_ripple_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=decoder_ripple_radon_transform_df_dict[a_name])\n",
    "    \n",
    "    \"\"\"\n",
    "    # assert a_result_obj.active_filter_epochs.n_epochs == np.shape(a_radon_transform_df)[0]\n",
    "    assert a_result_obj.num_filter_epochs == np.shape(a_radon_transform_df)[0]\n",
    "    if isinstance(a_result_obj.filter_epochs, pd.DataFrame):\n",
    "        a_result_obj.filter_epochs.drop(columns=columns, inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "        a_result_obj.filter_epochs = a_result_obj.filter_epochs.join(a_radon_transform_df) # add the newly computed columns to the Epochs object\n",
    "    else:\n",
    "        # Otherwise it's an Epoch object\n",
    "        a_result_obj.filter_epochs._df.drop(columns=columns, inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "        a_result_obj.filter_epochs._df = a_result_obj.filter_epochs.to_dataframe().join(a_radon_transform_df) # add the newly computed columns to the Epochs object\n",
    "    return a_result_obj\n",
    "\n",
    "## INPUTS: decoder_laps_radon_transform_df_dict\n",
    "def _build_merged_radon_transform_df(decoder_laps_radon_transform_df_dict, columns=['score', 'velocity', 'intercept', 'speed']) ->  pd.DataFrame:\n",
    "    \"\"\"Build a single merged dataframe from the radon transform results for all four decoders.\n",
    "    \n",
    "    Creates columns like: score_long_LR, score_short_LR, ...\n",
    "    \"\"\"\n",
    "    radon_transform_merged_df: pd.DataFrame = None\n",
    "    # filter_columns_fn = lambda df: df[['score']]\n",
    "    filter_columns_fn = lambda df: df[columns]\n",
    "    for a_name, a_df in decoder_laps_radon_transform_df_dict.items():\n",
    "        # a_name: str = a_name.capitalize()\n",
    "        if radon_transform_merged_df is None:\n",
    "            radon_transform_merged_df = filter_columns_fn(deepcopy(a_df))\n",
    "            radon_transform_merged_df = radon_transform_merged_df.add_suffix(f\"_{a_name}\") # suffix the columns so they're unique\n",
    "        else:\n",
    "            ## append to the initial_df\n",
    "            # initial_df = initial_df.join(deepcopy(a_df), lsuffix=None, rsuffix=f'_{a_name}')\n",
    "            radon_transform_merged_df = radon_transform_merged_df.join(filter_columns_fn(deepcopy(a_df)).add_suffix(f\"_{a_name}\"), lsuffix=None, rsuffix=None)\n",
    "\n",
    "    # Get the column name with the maximum value for each row\n",
    "    # initial_df['best_decoder_index'] = initial_df.idxmax(axis=1)\n",
    "    radon_transform_merged_df['best_decoder_index'] = radon_transform_merged_df.apply(lambda row: np.argmax(np.abs(row.values)), axis=1)\n",
    "\n",
    "    ## OUTPUTS: radon_transform_merged_df, decoder_laps_radon_transform_df_dict\n",
    "    return radon_transform_merged_df\n",
    "\n",
    "## INPUTS: laps_all_epoch_bins_marginals_df, radon_transform_merged_df\n",
    "def _build_merged_marginals_df(an_all_epoch_bins_marginals_df: pd.DataFrame, radon_transform_merged_df: pd.DataFrame) ->  pd.DataFrame:\n",
    "    \"\"\" ## Compare the radon-transform ['score'] column for each decoder\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Get the probability of each decoder:\n",
    "    a_marginals_df = deepcopy(an_all_epoch_bins_marginals_df)\n",
    "    a_marginals_df['P_Long_LR'] = a_marginals_df['P_LR'] * a_marginals_df['P_Long']\n",
    "    a_marginals_df['P_Long_RL'] = a_marginals_df['P_RL'] * a_marginals_df['P_Long']\n",
    "    a_marginals_df['P_Short_LR'] = a_marginals_df['P_LR'] * a_marginals_df['P_Short']\n",
    "    a_marginals_df['P_Short_RL'] = a_marginals_df['P_RL'] * a_marginals_df['P_Short']\n",
    "    assert np.allclose(a_marginals_df[['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']].sum(axis=1), 1.0)\n",
    "    # Get the column name with the maximum value for each row\n",
    "    # a_marginals_df['most_likely_decoder_index'] = a_marginals_df[['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']].idxmax(axis=1)\n",
    "    a_marginals_df['most_likely_decoder_index'] = a_marginals_df[['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']].apply(lambda row: np.argmax(row.values), axis=1)\n",
    "\n",
    "    ## Merge in the RadonTransform df:\n",
    "    a_marginals_df: pd.DataFrame = a_marginals_df.join(radon_transform_merged_df)\n",
    "\n",
    "    # ## count up the number that the RadonTransform and the most-likely direction agree\n",
    "    # agreeing_rows_ratio, (agreeing_rows_count, num_total_epochs) = _compute_matching_best_indicies(a_marginals_df, index_column_name='most_likely_decoder_index', second_index_column_name='best_decoder_index', enable_print=True)\n",
    "    return a_marginals_df\n",
    "\n",
    "\n",
    "def _compute_weighted_correlations(decoder_decoded_epochs_result_dict, debug_print=False):\n",
    "    \"\"\" \n",
    "    ## Weighted Correlation can only be applied to decoded posteriors, not spikes themselves.\n",
    "    ### It works by assessing the degree to which a change in position corresponds to a change in time. For a simple diagonally increasing trajectory across the track at early timebins position will start at the bottom of the track, and as time increases the position also increases. The \"weighted\" part just corresponds to making use of the confidence probabilities of the decoded posterior: instead of relying on only the most-likely position we can include all information returned. Naturally will emphasize sharp decoded positions and de-emphasize diffuse ones.\n",
    "\n",
    "    Usage:\n",
    "        decoder_laps_weighted_corr_df_dict = _compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(decoder_laps_filter_epochs_decoder_result_dict))\n",
    "        decoder_ripple_weighted_corr_df_dict = _compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(decoder_ripple_filter_epochs_decoder_result_dict))\n",
    "\n",
    "    \"\"\"\n",
    "    from neuropy.analyses.decoders import wcorr\n",
    "    # INPUTS: decoder_decoded_epochs_result_dict\n",
    "\n",
    "    weighted_corr_data_dict = {}\n",
    "\n",
    "    # for a_name in track_templates.get_decoder_names():\n",
    "    for a_name, curr_results_obj in decoder_decoded_epochs_result_dict.items():            \n",
    "        weighted_corr_data = np.array([wcorr(a_P_x_given_n) for a_P_x_given_n in curr_results_obj.p_x_given_n_list]) # each `wcorr(a_posterior)` call returns a float\n",
    "        if debug_print:\n",
    "            print(f'a_name: \"{a_name}\"\\n\\tweighted_corr_data.shape: {np.shape(weighted_corr_data)}') # (84, ) - (n_epochs, )\n",
    "        weighted_corr_data_dict[a_name] = pd.DataFrame({'wcorr': weighted_corr_data})\n",
    "\n",
    "    ## end for\n",
    "    return weighted_corr_data_dict\n",
    "\n",
    "\n",
    "def _compute_complete_df_metrics(track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, decoder_laps_df_dict: Dict[str, pd.DataFrame], decoder_ripple_df_dict: Dict[str, pd.DataFrame], active_df_columns = ['wcorr']):\n",
    "    \"\"\" generalized to work with any result dfs not just Radon Transforms\n",
    "    \n",
    "    Usage:\n",
    "\n",
    "        (laps_radon_transform_merged_df, ripple_radon_transform_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _compute_complete_df_metrics(track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                                                decoder_laps_df_dict=deepcopy(decoder_laps_radon_transform_df_dict), decoder_ripple_df_dict=deepcopy(decoder_ripple_radon_transform_df_dict), active_df_columns = ['score', 'velocity', 'intercept', 'speed'])\n",
    "\n",
    "        (laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _compute_complete_df_metrics(track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                                                decoder_laps_df_dict=deepcopy(decoder_laps_weighted_corr_df_dict), decoder_ripple_df_dict=deepcopy(decoder_ripple_weighted_corr_df_dict), active_df_columns = ['wcorr'])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ## INPUTS: track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, decoder_laps_filter_epochs_decoder_result_dict, decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict\n",
    "    for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "        decoder_laps_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_laps_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=decoder_laps_df_dict[a_name], columns=active_df_columns)\n",
    "        decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_ripple_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=decoder_ripple_df_dict[a_name], columns=active_df_columns)\n",
    "\n",
    "    laps_metric_merged_df = _build_merged_radon_transform_df(decoder_laps_df_dict, columns=active_df_columns)\n",
    "    ripple_metric_merged_df = _build_merged_radon_transform_df(decoder_ripple_df_dict, columns=active_df_columns)\n",
    "    ## OUTPUTS: laps_radon_transform_merged_df, ripple_radon_transform_merged_df\n",
    "\n",
    "    ## Compare the radon-transform ['score'] column for each decoder\n",
    "    laps_all_epoch_bins_marginals_df = deepcopy(directional_merged_decoders_result.laps_all_epoch_bins_marginals_df)\n",
    "    ripple_all_epoch_bins_marginals_df = deepcopy(directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df)\n",
    "    laps_metric_merged_df = _build_merged_marginals_df(laps_all_epoch_bins_marginals_df, laps_metric_merged_df)\n",
    "    ripple_metric_merged_df = _build_merged_marginals_df(ripple_all_epoch_bins_marginals_df, ripple_metric_merged_df)\n",
    "\n",
    "    ## Extract the individual decoder probability into the .active_epochs\n",
    "    decoder_name_to_decoder_probability_column_map = dict(zip(track_templates.get_decoder_names(), ['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']))\n",
    "    for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "        # decoder_laps_filter_epochs_decoder_result_dict[a_name]\n",
    "        # decoder_ripple_filter_epochs_decoder_result_dict[a_name].filter_epochs\n",
    "        ## Get a dataframe containing only the appropriate column for this decoder\n",
    "        a_prob_column_name:str = decoder_name_to_decoder_probability_column_map[a_name]\n",
    "        per_decoder_df_columns = ['P_decoder']\n",
    "        a_laps_decoder_prob_df: pd.DataFrame = pd.DataFrame({'P_decoder': laps_metric_merged_df[a_prob_column_name].to_numpy()})\n",
    "        a_ripple_decoder_prob_df: pd.DataFrame = pd.DataFrame({'P_decoder': ripple_metric_merged_df[a_prob_column_name].to_numpy()})\n",
    "        \n",
    "        decoder_laps_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_laps_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=a_laps_decoder_prob_df, columns=per_decoder_df_columns)\n",
    "        decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_ripple_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=a_ripple_decoder_prob_df, columns=per_decoder_df_columns)\n",
    "\n",
    "    return (laps_metric_merged_df, ripple_metric_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "\n",
    "## Radon Transform:\n",
    "(laps_radon_transform_merged_df, ripple_radon_transform_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _compute_complete_df_metrics(track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                                            decoder_laps_df_dict=deepcopy(decoder_laps_radon_transform_df_dict), decoder_ripple_df_dict=deepcopy(decoder_ripple_radon_transform_df_dict), active_df_columns = ['score', 'velocity', 'intercept', 'speed'])\n",
    "## count up the number that the RadonTransform and the most-likely direction agree\n",
    "laps_radon_stats = _compute_matching_best_indicies(laps_radon_transform_merged_df, index_column_name='most_likely_decoder_index', second_index_column_name='best_decoder_index', enable_print=True)\n",
    "# agreeing_rows_ratio, (agreeing_rows_count, num_total_epochs) = laps_radon_stats\n",
    "ripple_radon_stats = _compute_matching_best_indicies(ripple_radon_transform_merged_df, index_column_name='most_likely_decoder_index', second_index_column_name='best_decoder_index', enable_print=True)\n",
    "\n",
    "\n",
    "## Weighted Correlation\n",
    "decoder_laps_weighted_corr_df_dict = _compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(decoder_laps_filter_epochs_decoder_result_dict))\n",
    "decoder_ripple_weighted_corr_df_dict = _compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(decoder_ripple_filter_epochs_decoder_result_dict))\n",
    "(laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _compute_complete_df_metrics(track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                                            decoder_laps_df_dict=deepcopy(decoder_laps_weighted_corr_df_dict), decoder_ripple_df_dict=deepcopy(decoder_ripple_weighted_corr_df_dict), active_df_columns = ['wcorr'])\n",
    "## count up the number that the RadonTransform and the most-likely direction agree\n",
    "laps_wcorr_stats = _compute_matching_best_indicies(laps_weighted_corr_merged_df, index_column_name='most_likely_decoder_index', second_index_column_name='best_decoder_index', enable_print=True)\n",
    "# agreeing_rows_ratio, (agreeing_rows_count, num_total_epochs) = laps_radon_stats\n",
    "ripple_wcorr_stats = _compute_matching_best_indicies(ripple_weighted_corr_merged_df, index_column_name='most_likely_decoder_index', second_index_column_name='best_decoder_index', enable_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(ripple_radon_transform_merged_df.columns)) # ['P_LR', 'P_RL', 'P_Long', 'P_Short', 'ripple_idx', 'ripple_start_t', 'P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL', 'most_likely_decoder_index', 'score_long_LR', 'velocity_long_LR', 'intercept_long_LR', 'speed_long_LR', 'score_long_RL', 'velocity_long_RL', 'intercept_long_RL', 'speed_long_RL', 'score_short_LR', 'velocity_short_LR', 'intercept_short_LR', 'speed_short_LR', 'score_short_RL', 'velocity_short_RL', 'intercept_short_RL', 'speed_short_RL', 'best_decoder_index']\n",
    "\n",
    "# ['P_LR', 'P_RL', 'P_Long', 'P_Short', 'ripple_idx', 'ripple_start_t', 'P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL', 'most_likely_decoder_index',\n",
    "#   'score_long_LR', 'velocity_long_LR', 'intercept_long_LR', 'speed_long_LR',\n",
    "#   'score_long_RL', 'velocity_long_RL', 'intercept_long_RL', 'speed_long_RL',\n",
    "#   'score_short_LR', 'velocity_short_LR', 'intercept_short_LR', 'speed_short_LR',\n",
    "#   'score_short_RL', 'velocity_short_RL', 'intercept_short_RL', 'speed_short_RL',\n",
    "#   'best_decoder_index']\n",
    "\n",
    "\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the decoder likelihood and the radon transform score for the best decoder index:\n",
    "# print(list(ripple_radon_transform_merged_df.columns)) # ['P_LR', 'P_RL', 'P_Long', 'P_Short', 'ripple_idx', 'ripple_start_t', 'P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL', 'most_likely_decoder_index', 'score_long_LR', 'score_long_RL', 'score_short_LR', 'score_short_RL', 'best_decoder_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: ripple_radon_transform_merged_df, decoder_specific_Radon_transform_score_columns\n",
    "\n",
    "\n",
    "decoder_specific_probability_columns = ['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']\n",
    "decoder_specific_Radon_transform_score_columns = ['score_long_LR', 'score_long_RL', 'score_short_LR', 'score_short_RL']\n",
    "\n",
    "best_decoder_index = ripple_radon_transform_merged_df['best_decoder_index'].to_numpy()\n",
    "# best_decoder_index.shape # (611,)\n",
    "\n",
    "\n",
    "# ripple_radon_transform_merged_df['best_decoder_index']\n",
    "\n",
    "decoder_specific_probability_mat = ripple_radon_transform_merged_df[decoder_specific_probability_columns].to_numpy() # .shape (611, 4)\n",
    "decoder_specific_Radon_transform_score_mat = ripple_radon_transform_merged_df[decoder_specific_Radon_transform_score_columns].to_numpy() # .shape (611, 4)\n",
    "n_epochs = np.shape(decoder_specific_Radon_transform_score_mat)[0] # 611\n",
    "# n_epochs \n",
    "best_decoder_probability = np.array([decoder_specific_probability_mat[i, best_decoder_index[i]] for i in np.arange(n_epochs)])\n",
    "best_decoder_Radon_transform_score = np.array([decoder_specific_Radon_transform_score_mat[i, best_decoder_index[i]] for i in np.arange(n_epochs)])\n",
    "# best_decoder_Radon_transform_score.shape # (611,)\n",
    "# best_decoder_Radon_transform_score\n",
    "\n",
    "# best_decoder_probability.shape # (611,)\n",
    "\n",
    "\n",
    "# [best_decoder_index].shape\n",
    "\n",
    "\n",
    "# Assuming best_decoder_probability and best_decoder_Radon_transform_score have been computed successfully and have the same shape\n",
    "correlation_matrix = np.corrcoef(best_decoder_probability, best_decoder_Radon_transform_score)\n",
    "\n",
    "# Extract the correlation coefficient from the matrix\n",
    "point_wise_correlation = correlation_matrix[0, 1]  # This gets the correlation between the two arrays\n",
    "\n",
    "print(point_wise_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e080916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your previously calculated vectors\n",
    "# best_decoder_probability = np.array([...])\n",
    "# best_decoder_Radon_transform_score = np.array([...])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(best_decoder_probability, best_decoder_Radon_transform_score)\n",
    "\n",
    "# Optional: Specify the labels for axes\n",
    "plt.xlabel('Best Decoder Probability')\n",
    "plt.ylabel('Best Decoder Radon Transform Score')\n",
    "\n",
    "# Optional: Specify the title of the graph\n",
    "plt.title('Scatter Plot of Best Decoder Probability vs. Radon Transform Score')\n",
    "\n",
    "# Show the scatter plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For these, a perfectly uniform distribution takes values of 0.25 for all four decoders. Here, we want to look at difference above uniform for the best decoder.\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "\n",
    "_out = TemplateDebugger.init_templates_debugger(track_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8161a",
   "metadata": {},
   "source": [
    "### 2024-02-08 - Plot Radon Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices_paginated\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController\n",
    "\n",
    "def align_decoder_pagination_controller_windows(pagination_controller_dict):\n",
    "    \"\"\" resizes and aligns all windows \n",
    "    Usage:\n",
    "        align_decoder_pagination_controller_windows(pagination_controller_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    from pyphocorehelpers.gui.Qt.widget_positioning_helpers import WidgetPositioningHelpers, DesiredWidgetLocation, WidgetGeometryInfo\n",
    "    ## Connects the first plotter's pagination controls to the other three controllers so that they are directly driven, by the first.\n",
    "    a_controlling_pagination_controller = pagination_controller_dict['long_LR'] # DecodedEpochSlicesPaginatedFigureController\n",
    "    a_controlling_widget = a_controlling_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "    # controlled widgets\n",
    "    controlled_pagination_controllers_list = (pagination_controller_dict['long_RL'], pagination_controller_dict['short_LR'], pagination_controller_dict['short_RL'])\n",
    "\n",
    "    fixed_height_pagination_control_bar: float = 21.0\n",
    "    target_height: float = a_controlling_widget.window().height()\n",
    "    ratio_content_height = (target_height - fixed_height_pagination_control_bar) / target_height\n",
    "    print(f'fixed_height_pagination_control_bar: {fixed_height_pagination_control_bar}, target_height: {target_height}, ratio_content_height: {ratio_content_height}')\n",
    "\n",
    "    target_window = a_controlling_widget.window()\n",
    "    for a_controlled_pagination_controller in controlled_pagination_controllers_list:\n",
    "        # hide the pagination widget:\n",
    "        a_controlled_widget = a_controlled_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "        WidgetPositioningHelpers.align_window_edges(target_window, a_controlled_widget.window(), relative_position = 'right_of', resize_to_main=(1.0, ratio_content_height)) # use ratio_content_height to compensate for the lack of a pagination scroll bar\n",
    "        target_window = a_controlled_widget.window() # update to reference the newly moved window\n",
    "        ratio_content_height = 1.0 # after the first window, 1.0 should be used since they're all the same height\n",
    "\n",
    "\n",
    "def _subfn_prepare_plot_multi_decoders_stacked_epoch_slices(curr_active_pipeline, track_templates, decoder_decoded_epochs_result_dict, epochs_name:str ='laps', included_epoch_indicies=None, defer_render=True, save_figure=True, **kwargs):\n",
    "    \"\"\" 2024-02-14 - Adapted from the function that plots the Long/Short decoded epochs side-by-side for comparsion and updated to work with the multi-decoder track templates.\n",
    "    \n",
    "    ## TODO 2023-06-02 NOW, NEXT: this might not work in 'AGG' mode because it tries to render it with QT, but we can see.\n",
    "    \n",
    "    Usage:\n",
    "        (pagination_controller_L, pagination_controller_S), (fig_L, fig_S), (ax_L, ax_S), (final_context_L, final_context_S), (active_out_figure_paths_L, active_out_figure_paths_S) = _subfn_prepare_plot_long_and_short_stacked_epoch_slices(curr_active_pipeline, defer_render=False)\n",
    "    \"\"\"\n",
    "    # from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices_paginated\n",
    "\n",
    "    # epochs_name:str ='replays'\n",
    "\n",
    "    ## long_short_decoding_analyses:\n",
    "    # curr_long_short_decoding_analyses = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "    ## Extract variables from results object:\n",
    "    # long_results_obj, short_results_obj = curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj\n",
    "    # long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "    # , animated=True\n",
    "    # params.enable_flat_line_drawing = enable_flat_line_drawing\n",
    "    # params.skip_plotting_measured_positions = kwargs.pop('skip_plotting_measured_positions', False)\n",
    "    # params.skip_plotting_most_likely_positions = kwargs.pop('skip_plotting_most_likely_positions', False)\n",
    "\n",
    "    params_kwargs = dict(skip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True)\n",
    "    pagination_controller_dict = {}\n",
    "    for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "        pagination_controller_dict[a_name] = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(decoder_decoded_epochs_result_dict[a_name].filter_epochs,\n",
    "                                                                                            decoder_decoded_epochs_result_dict[a_name],\n",
    "                                                                                            xbin=a_decoder.xbin, global_pos_df=curr_active_pipeline.sess.position.df,\n",
    "                                                                                            a_name='DecodedEpochSlices', active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs=epochs_name, decoder=a_name),\n",
    "                                                                                            max_subplots_per_page=8, debug_print=False, included_epoch_indicies=included_epoch_indicies, **params_kwargs) # , save_figure=save_figure\n",
    "\n",
    "        # pagination_controller_dict[a_name], active_out_figure_paths_L, final_context_L = plot_decoded_epoch_slices_paginated(curr_active_pipeline, decoder_laps_filter_epochs_decoder_result_dict[a_name], curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='replays', decoder='long_results_obj'), included_epoch_indicies=included_epoch_indicies, save_figure=save_figure, **kwargs)\n",
    "        # fig_L = pagination_controller_L.plots.fig\n",
    "        # ax_L = fig_L.get_axes()\n",
    "        # if defer_render:\n",
    "        #     widget_L = pagination_controller_L.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "        #     widget_L.close()\n",
    "        #     pagination_controller_L = None\n",
    "\n",
    "    # root_dockAreaWindow, app = DockAreaWrapper.wrap_with_dockAreaWindow(epochs_editor.plots.win, None, title='Pho Directional Decoder DecodedEpochSlices')\n",
    "        \n",
    "    # Constrains each of the plotters at least to the minimum height:\n",
    "    for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "        a_pagination_controller.params.all_plots_height\n",
    "        # resize to minimum height\n",
    "        a_widget = a_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "        # a_widget.size()\n",
    "        a_widget.setMinimumHeight(a_pagination_controller.params.all_plots_height)\n",
    "\n",
    "    # return (pagination_controller_L, pagination_controller_S), (fig_L, fig_S), (ax_L, ax_S), (final_context_L, final_context_S), (active_out_figure_paths_L, active_out_figure_paths_S)\n",
    "    return pagination_controller_dict\n",
    "\n",
    "def convert_decoder_pagination_controller_dict_to_controlled(pagination_controller_dict):\n",
    "    ## Connects the first plotter's pagination controls to the other three controllers so that they are directly driven, by the first.\n",
    "    a_controlling_pagination_controller = pagination_controller_dict['long_LR'] # DecodedEpochSlicesPaginatedFigureController\n",
    "    a_controlling_widget = a_controlling_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "\n",
    "    # controlled widgets\n",
    "    controlled_pagination_controllers_list = (pagination_controller_dict['long_RL'], pagination_controller_dict['short_LR'], pagination_controller_dict['short_RL'])\n",
    "\n",
    "    new_connections_dict = []\n",
    "\n",
    "    for a_controlled_pagination_controller in controlled_pagination_controllers_list:\n",
    "        # hide the pagination widget:\n",
    "        a_controlled_widget = a_controlled_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "        # a_controlled_widget.on_paginator_control_widget_jump_to_page(page_idx=0)\n",
    "        a_connection = a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget.jump_to_page.connect(a_controlled_pagination_controller.on_paginator_control_widget_jump_to_page) # bind connection\n",
    "        new_connections_dict.append(a_connection)\n",
    "        # a_controlled_widget.ui.connections['paginator_controller_widget_jump_to_page'] = _a_connection\n",
    "        a_controlled_widget.ui.paginator_controller_widget.hide()\n",
    "\n",
    "    return new_connections_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pagination_controller_dict =  _subfn_prepare_plot_multi_decoders_stacked_epoch_slices(curr_active_pipeline, track_templates, decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', included_epoch_indicies=None, defer_render=False, save_figure=False)\n",
    "# pagination_controller_dict =  _subfn_prepare_plot_multi_decoders_stacked_epoch_slices(curr_active_pipeline, track_templates, decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='replays', included_epoch_indicies=None, defer_render=False, save_figure=False)\n",
    "align_decoder_pagination_controller_windows(pagination_controller_dict)\n",
    "new_connections_dict = convert_decoder_pagination_controller_dict_to_controlled(pagination_controller_dict)\n",
    "# new_connections_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bca164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider\n",
    "\n",
    "# Build Radon Transforms and add them:\n",
    "radon_transform_laps_data_dict = RadonTransformPlotDataProvider.decoder_build_radon_transform_data_dict(track_templates, decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict)\n",
    "radon_transform_ripple_data_dict = RadonTransformPlotDataProvider.decoder_build_radon_transform_data_dict(track_templates, decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "## Add the radon_transform_lines to each of the four figures:\n",
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    if a_pagination_controller.params.active_identifying_figure_ctx.epochs == 'laps':\n",
    "      RadonTransformPlotDataProvider.add_data_to_pagination_controller(a_pagination_controller, radon_transform_laps_data_dict[a_name], update_controller_on_apply=False)\n",
    "    elif a_pagination_controller.params.active_identifying_figure_ctx.epochs == 'ripple':\n",
    "       RadonTransformPlotDataProvider.add_data_to_pagination_controller(a_pagination_controller, radon_transform_ripple_data_dict[a_name], update_controller_on_apply=False)\n",
    "    else:\n",
    "       raise NotImplementedError(a_pagination_controller.params.active_identifying_figure_ctx)\n",
    "\n",
    "        #.epochs_name #.epochs_name\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ae126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import WeightedCorrelationPlotter\n",
    "\n",
    "# Build Radon Transforms and add them:\n",
    "# wcorr_laps_data_dict = WeightedCorrelationPlotter.decoder_build_weighted_correlation_data_dict(track_templates, decoder_decoded_epochs_result_dict=decoder_laps_weighted_corr_df_dict)\n",
    "# wcorr_ripple_data_dict = WeightedCorrelationPlotter.decoder_build_weighted_correlation_data_dict(track_templates, decoder_decoded_epochs_result_dict=decoder_ripple_weighted_corr_df_dict)\n",
    "wcorr_laps_data_dict = WeightedCorrelationPlotter.decoder_build_weighted_correlation_data_dict(track_templates, decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict)\n",
    "wcorr_ripple_data_dict = WeightedCorrelationPlotter.decoder_build_weighted_correlation_data_dict(track_templates, decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "## Add the radon_transform_lines to each of the four figures:\n",
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    if a_pagination_controller.params.active_identifying_figure_ctx.epochs == 'laps':\n",
    "        WeightedCorrelationPlotter.add_data_to_pagination_controller(a_pagination_controller, wcorr_laps_data_dict[a_name], update_controller_on_apply=False)\n",
    "    elif a_pagination_controller.params.active_identifying_figure_ctx.epochs == 'ripple':\n",
    "        WeightedCorrelationPlotter.add_data_to_pagination_controller(a_pagination_controller, wcorr_ripple_data_dict[a_name], update_controller_on_apply=False)\n",
    "    else:\n",
    "       raise NotImplementedError(a_pagination_controller.params.active_identifying_figure_ctx)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.general_parameter_containers import VisualizationParameters, RenderPlotsData, RenderPlots\n",
    "from pyphocorehelpers.gui.PhoUIContainer import PhoUIContainer\n",
    "\n",
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    # a_pagination_controller.params.debug_print = True\n",
    "    a_plots: RenderPlots = a_pagination_controller.plots\n",
    "    a_params = a_pagination_controller.params\n",
    "    a_widget = a_pagination_controller.ui.mw\n",
    "    figs = a_plots.fig\n",
    "    axs = a_plots.axs\n",
    "    # for ax in axs:\n",
    "    #     # ax.legend()\n",
    "    #     ax.clear()\n",
    "    # a_widget.draw()\n",
    "\n",
    "    a_params.is_selected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.general_parameter_containers import VisualizationParameters, RenderPlotsData, RenderPlots\n",
    "from pyphocorehelpers.gui.PhoUIContainer import PhoUIContainer\n",
    "\n",
    "a_pagination_controller: DecodedEpochSlicesPaginatedFigureController = pagination_controller_dict['long_LR']\n",
    "# a_pagination_controller.ui.\n",
    "a_plots: RenderPlots = a_pagination_controller.plots\n",
    "a_params = a_pagination_controller.params\n",
    "figs = a_plots.fig\n",
    "axs = a_plots.axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a_plots.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_params.skip_plotting_measured_positions\n",
    "a_params.skip_plotting_most_likely_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b179be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Qt.widget_positioning_helpers import WidgetPositioningHelpers, DesiredWidgetLocation, WidgetGeometryInfo\n",
    "\n",
    "# desired_window_geometry: WidgetGeometryInfo = WidgetGeometryInfo.init_from_widget(a_pagination_controller.ui.mw.window()) # WidgetGeometryInfo(minimumSize=PyQt5.QtCore.QSize(180, 800), maximumSize=PyQt5.QtCore.QSize(16777215, 16777215), baseSize=PyQt5.QtCore.QSize(), sizePolicy=<PyQt5.QtWidgets.QSizePolicy object at 0x000001AAD24F5740>, geometry=PyQt5.QtCore.QRect(1213, 69, 655, 1191))\n",
    "# desired_window_geometry\n",
    "\n",
    "align_decoder_pagination_controller_windows(pagination_controller_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plots['weighted_corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, extant_plots in a_plots['weighted_corr'].items():\n",
    "    extant_wcorr_text = extant_plots.get('wcorr_text', None)\n",
    "    # extant_wcorr_text = extant_plots.pop('wcorr_text', None)\n",
    "    print(f'extant_wcorr_text: {extant_wcorr_text}')\n",
    "    # plot the radon transform line on the epoch:\n",
    "    if (extant_wcorr_text is not None):\n",
    "        # already exists, clear the existing ones. \n",
    "        # Let's assume we want to remove the 'Quadratic' line (line2)\n",
    "        print(f'removing extant text object at index: {i}.')\n",
    "        # extant_wcorr_text.remove()\n",
    "        extant_wcorr_text.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['legend.title_fontsize']\n",
    "\n",
    "\n",
    "plt.rcParamsDefault['legend.title_fontsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e539ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_single_window(pagination_controller_dict):\n",
    "    \"\"\" 2024-02-14 - Copied from `RankOrderRastersDebugger`'s approach. Merges the four separate decoded epoch windows into single figure with a separate dock for each decoder.\n",
    "    [/c:/Users/pho/repos/Spike3DWorkEnv/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/GUI/PyQtPlot/Widgets/ContainerBased/RankOrderRastersDebugger.py:261](vscode://file/c:/Users/pho/repos/Spike3DWorkEnv/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/GUI/PyQtPlot/Widgets/ContainerBased/RankOrderRastersDebugger.py:261)\n",
    "\n",
    "    \"\"\"\n",
    "    from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "    from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig\n",
    "    from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n",
    "\n",
    "    # pagination_controller_dict = _obj.plots.rasters_display_outputs\n",
    "    all_widgets = {a_decoder_name:a_pagination_controller.ui.mw for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "    all_windows = {a_decoder_name:a_pagination_controller.ui.mw.window() for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "    all_separate_plots = {a_decoder_name:a_pagination_controller.plots for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "    all_separate_plots_data = {a_decoder_name:a_pagination_controller.plots_data for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "    all_separate_params = {a_decoder_name:a_pagination_controller.params for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "\n",
    "    main_plot_identifiers_list = list(all_windows.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "    \n",
    "    # all_separate_data_all_spots = {a_decoder_name:a_raster_setup_tuple.plots_data.all_spots for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "    # all_separate_data_all_scatterplot_tooltips_kwargs = {a_decoder_name:a_raster_setup_tuple.plots_data.all_scatterplot_tooltips_kwargs for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "    # all_separate_data_new_sorted_rasters = {a_decoder_name:a_raster_setup_tuple.plots_data.new_sorted_raster for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "    # all_separate_data_spikes_dfs = {a_decoder_name:a_raster_setup_tuple.plots_data.spikes_df for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "\n",
    "    # # Extract the plot/renderable items\n",
    "    # all_separate_root_plots = {a_decoder_name:a_pagination_controller.plots.root_plot for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "    # all_separate_grids = {a_decoder_name:a_raster_setup_tuple.plots.grid for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "    # all_separate_scatter_plots = {a_decoder_name:a_raster_setup_tuple.plots.scatter_plot for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "    # all_separate_debug_header_labels = {a_decoder_name:a_raster_setup_tuple.plots.debug_header_label for a_decoder_name, a_raster_setup_tuple in pagination_controller_dict.items()}\n",
    "\n",
    "    # Embedding in docks:\n",
    "    root_dockAreaWindow, app = DockAreaWrapper.build_default_dockAreaWindow(title='Pho Combined Directioanl Decoder Decoded Epochs')\n",
    "    # icon = try_get_icon(icon_path=\":/Icons/Icons/visualizations/template_1D_debugger.ico\")\n",
    "    # if icon is not None:\n",
    "    #     root_dockAreaWindow.setWindowIcon(icon)\n",
    "\n",
    "    ## Build Dock Widgets:\n",
    "    def get_utility_dock_colors(orientation, is_dim):\n",
    "        \"\"\" used for CustomDockDisplayConfig for non-specialized utility docks \"\"\"\n",
    "        # Common to all:\n",
    "        if is_dim:\n",
    "            fg_color = '#aaa' # Grey\n",
    "        else:\n",
    "            fg_color = '#fff' # White\n",
    "\n",
    "        # a purplish-royal-blue\n",
    "        if is_dim:\n",
    "            bg_color = '#d8d8d8'\n",
    "            border_color = '#717171'\n",
    "        else:\n",
    "            bg_color = '#9d9d9d'\n",
    "            border_color = '#3a3a3a'\n",
    "\n",
    "        return fg_color, bg_color, border_color\n",
    "\n",
    "\n",
    "    # decoder_names_list = ('long_LR', 'long_RL', 'short_LR', 'short_RL')\n",
    "    _out_dock_widgets = {}\n",
    "    dock_configs = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_LR_dock_colors, showCloseButton=False), CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_RL_dock_colors, showCloseButton=False),\n",
    "                    CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_LR_dock_colors, showCloseButton=False), CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_RL_dock_colors, showCloseButton=False))))\n",
    "    # dock_add_locations = (['left'], ['left'], ['right'], ['right'])\n",
    "    # dock_add_locations = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (['right'], ['right'], ['right'], ['right'])))\n",
    "    dock_add_locations = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (['right'], ['right'], ['right'], ['right'])))\n",
    "\n",
    "    for i, (a_decoder_name, a_win) in enumerate(all_windows.items()):\n",
    "        # if (a_decoder_name == 'short_RL'):\n",
    "        #     short_LR_dock = root_dockAreaWindow.find_display_dock('short_LR')\n",
    "        #     assert short_LR_dock is not None\n",
    "        #     dock_add_locations['short_RL'] = ['bottom', short_LR_dock]\n",
    "        #     print(f'using overriden dock location.')\n",
    "        _out_dock_widgets[a_decoder_name] = root_dockAreaWindow.add_display_dock(identifier=a_decoder_name, widget=a_win, dockSize=(430,700), dockAddLocationOpts=dock_add_locations[a_decoder_name], display_config=dock_configs[a_decoder_name], autoOrientation=False)\n",
    "\n",
    "    # #TODO 2024-02-14 18:44: - [ ] Comgbine the separate items into one of the single `DecodedEpochSlicesPaginatedFigureController` objects (or a new one)?\n",
    "    # root_dockAreaWindow.resize(600, 900)\n",
    "\n",
    "    # ## Build final .plots and .plots_data:\n",
    "    # _obj.plots = RenderPlots(name=name, root_dockAreaWindow=root_dockAreaWindow, apps=all_apps, all_windows=all_windows, all_separate_plots=all_separate_plots,\n",
    "    #                             root_plots=all_separate_root_plots, grids=all_separate_grids, scatter_plots=all_separate_scatter_plots, debug_header_labels=all_separate_debug_header_labels,\n",
    "    #                             dock_widgets=_out_dock_widgets, text_items_dict=None) # , ctrl_widgets={'slider': slider}\n",
    "    # _obj.plots_data = RenderPlotsData(name=name, main_plot_identifiers_list=main_plot_identifiers_list,\n",
    "    #                                     seperate_all_spots_dict=all_separate_data_all_spots, seperate_all_scatterplot_tooltips_kwargs_dict=all_separate_data_all_scatterplot_tooltips_kwargs, seperate_new_sorted_rasters_dict=all_separate_data_new_sorted_rasters, seperate_spikes_dfs_dict=all_separate_data_spikes_dfs,\n",
    "    #                                     on_update_active_epoch=on_update_active_epoch, on_update_active_scatterplot_kwargs=on_update_active_scatterplot_kwargs, **{k:v for k, v in _obj.plots_data.to_dict().items() if k not in ['name']})\n",
    "    # _obj.ui = PhoUIContainer(name=name, app=app, root_dockAreaWindow=root_dockAreaWindow, ctrl_layout=ctrl_layout, **ctrl_widgets_dict, **info_labels_widgets_dict, on_valueChanged=valueChanged, logTextEdit=logTextEdit, dock_configs=dock_configs, controlled_references=None)\n",
    "    # _obj.params = VisualizationParameters(name=name, is_laps=False, enable_show_spearman=True, enable_show_pearson=False, enable_show_Z_values=True, use_plaintext_title=False, **param_kwargs)\n",
    "\n",
    "\n",
    "    # ## Cleanup when done:\n",
    "    # for a_decoder_name, a_root_plot in _obj.plots.root_plots.items():\n",
    "    #     a_root_plot.setTitle(title=a_decoder_name)\n",
    "    #     # a_root_plot.setTitle(title=\"\")\n",
    "    #     a_left_axis = a_root_plot.getAxis('left')# axisItem\n",
    "    #     a_left_axis.setLabel(a_decoder_name)\n",
    "    #     a_left_axis.setStyle(showValues=False)\n",
    "    #     a_left_axis.setTicks([])\n",
    "    #     # a_root_plot.hideAxis('bottom')\n",
    "    #     # a_root_plot.hideAxis('bottom')\n",
    "    #     a_root_plot.hideAxis('left')\n",
    "    #     a_root_plot.setYRange(-0.5, float(_obj.max_n_neurons))\n",
    "\n",
    "    return app, root_dockAreaWindow, _out_dock_widgets, dock_configs\n",
    "\n",
    "\n",
    "app, root_dockAreaWindow, _out_dock_widgets, dock_configs = merge_single_window(pagination_controller_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "    # Get context for current page of items:\n",
    "    current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "    a_paginator = a_pagination_controller.paginator\n",
    "    total_num_pages = int(a_paginator.num_pages)\n",
    "    page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "    print(page_context)\n",
    "\n",
    "    ## Get the figure/axes:\n",
    "    a_plots: RenderPlots = a_pagination_controller.plots\n",
    "    a_plot_data = a_pagination_controller.plots_data\n",
    "\n",
    "    a_params = a_pagination_controller.params\n",
    "    a_params.skip_plotting_measured_positions\n",
    "\n",
    "    figs = a_plots.fig\n",
    "    axs = a_plots.axs\n",
    "\n",
    "    # # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    # with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    #     curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs\n",
    "# a_plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12562693",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figs.canvas.draw_idle()\n",
    "figs.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "def export_decoder_pagination_controller_figure_page(pagination_controller_dict, curr_active_pipeline):\n",
    "    \"\"\" resizes and aligns all windows \n",
    "\n",
    "    Captures: curr_active_pipeline\n",
    "    \n",
    "    \"\"\"\n",
    "    for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "        display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "        # Get context for current page of items:\n",
    "        current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "        a_paginator = a_pagination_controller.paginator\n",
    "        total_num_pages = int(a_paginator.num_pages)\n",
    "        page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "        print(page_context)\n",
    "\n",
    "        ## Get the figure/axes:\n",
    "        a_plots: RenderPlots = a_pagination_controller.plots\n",
    "        a_params = a_pagination_controller.params\n",
    "        \n",
    "        # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "        with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "            figs = a_plots.fig\n",
    "            axs = a_plots.axs\n",
    "            curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)\n",
    "\n",
    "\n",
    "# export_decoder_pagination_controller_figure_page(pagination_controller_dict, curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_paginator = a_pagination_controller.paginator\n",
    "total_num_pages = int(a_paginator.num_pages)\n",
    "page_idx_sweep = np.arange(total_num_pages)\n",
    "page_num_sweep = page_idx_sweep + 1 # switch to 1-indexed\n",
    "# page_num_sweep\n",
    "\n",
    "for a_page_idx, a_page_num in zip(page_idx_sweep, page_num_sweep):\n",
    "    print(f'switching to page: a_page_idx: {a_page_idx}, a_page_num: {a_page_num} of total_num_pages: {total_num_pages}')\n",
    "    a_pagination_controller.on_paginator_control_widget_jump_to_page(page_idx=a_page_idx)\n",
    "    a_pagination_controller.ui.mw.draw()\n",
    "    export_decoder_pagination_controller_figure_page(pagination_controller_dict, curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190122f0",
   "metadata": {},
   "source": [
    "## Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eed0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = _out_pagination_controller.plots['radon_transform'][7]\n",
    "extant_line = _out['line'] # matplotlib.lines.Line2D\n",
    "extant_line.linestyle = 'none'\n",
    "# extant_line.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82687641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(curr_active_pipeline.filtered_contexts.keys())) # ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "\n",
    "# long_any_name\n",
    "\n",
    "# long_LR_name\n",
    "\n",
    "# Converting between decoder names and filtered epoch names:\n",
    "{'long':'maze1', 'short':'maze2'}\n",
    "{'LR':'odd', 'RL':'even'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_name_to_session_context_name = dict(zip(track_templates.get_decoder_names(), (long_LR_name, long_RL_name, short_LR_name, short_RL_name)))\n",
    "decoder_name_to_session_context_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single `DecodedEpochSlicesPaginatedFigureController` Example:\n",
    "a_name: str = 'long_LR'\n",
    "a_context_name: str = 'maze1_odd'\n",
    "\n",
    "included_epoch_indicies = None\n",
    "defer_render = False\n",
    "save_figure = False\n",
    "kwargs = {}\n",
    "\n",
    "\n",
    "a_decoder = track_templates.get_decoders_dict()[a_name]\n",
    "_out_pagination_controller = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(decoder_laps_filter_epochs_decoder_result_dict[a_name].filter_epochs,\n",
    "                                                                                            decoder_laps_filter_epochs_decoder_result_dict[a_name],\n",
    "                                                                                            xbin=a_decoder.xbin, global_pos_df=curr_active_pipeline.sess.position.df,\n",
    "                                                                                            a_name='TestDecodedEpochSlicesPaginationController', active_context=curr_active_pipeline.build_display_context_for_filtered_session(filtered_session_name=a_context_name, display_fn_name='DecodedEpochSlicesPaginatedFigureController'),\n",
    "                                                                                            max_subplots_per_page=2, debug_print=False)\n",
    "# _out_pagination_controller\n",
    "\n",
    "\n",
    "\n",
    "# pagination_controller_L, active_out_figure_paths_L, final_context_L = plot_decoded_epoch_slices_paginated(curr_active_pipeline, decoder_laps_filter_epochs_decoder_result_dict[a_name], curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='laps', decoder=a_name), included_epoch_indicies=included_epoch_indicies, save_figure=save_figure, **kwargs)\n",
    "\n",
    "\n",
    "# for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "#     # a_radon_transform_df = decoder_laps_radon_transform_df_dict[a_name]\n",
    "#     # a_result_obj = decoder_laps_filter_epochs_decoder_result_dict[a_name]\n",
    "#     # assert a_result_obj.active_filter_epochs.n_epochs == np.shape(a_radon_transform_df)[0]\n",
    "#     # a_result_obj.active_filter_epochs._df.drop(columns=['score', 'velocity', 'intercept', 'speed'], inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "#     # a_result_obj.active_filter_epochs._df = a_result_obj.active_filter_epochs.to_dataframe().join(a_radon_transform_df) # add the newly computed columns to the Epochs object\n",
    "#     decoder_laps_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_laps_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=decoder_laps_radon_transform_df_dict[a_name])\n",
    "#     decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _update_decoder_result_active_filter_epoch_columns(a_result_obj=decoder_ripple_filter_epochs_decoder_result_dict[a_name], a_radon_transform_df=decoder_ripple_radon_transform_df_dict[a_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abcb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_num_slices: int = _out_pagination_controller.params.active_num_slices\n",
    "single_plot_fixed_height: float = _out_pagination_controller.params.single_plot_fixed_height\n",
    "all_plots_height: float = _out_pagination_controller.params.all_plots_height\n",
    "print(f'all_plots_height: {all_plots_height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93ea19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bce3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc3fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f879b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcef848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import _add_maze_id_to_epochs\n",
    "\n",
    "\n",
    "## Add new weighted correlation results as new columns in existing filter_epochs df:\n",
    "active_filter_epochs = long_results_obj.active_filter_epochs\n",
    "# Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "active_filter_epochs = _add_maze_id_to_epochs(active_filter_epochs, short_session.t_start)\n",
    "active_filter_epochs._df['weighted_corr_LONG'] = epoch_long_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_SHORT'] = epoch_short_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_spearman_LONG'] = epoch_long_weighted_corr_results[:,1]\n",
    "active_filter_epochs._df['weighted_corr_spearman_SHORT'] = epoch_short_weighted_corr_results[:,1]\n",
    "\n",
    "\n",
    "active_filter_epochs\n",
    "active_filter_epochs.to_dataframe()\n",
    "## plot the `weighted_corr_LONG` over time\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=1, nrows=active_num_rows, sharex=True, sharey=sharey, figsize=figsize)\n",
    "\n",
    "## Weighted Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_LONG', title='weighted_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "## Weighted Spearman Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_LONG', title='weighted_spearman_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Spearman Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='score_LONG', title='Radon Transform Score during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='score_SHORT', xlabel='Replay Epoch Time', ylabel='Replay Radon Transform Score', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d56479",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "example_stacked_epoch_graphics = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', defer_render=False, save_figure=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b17c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dockAreaWindow, app = DockAreaWrapper.wrap_with_dockAreaWindow(epochs_editor.plots.win, None, title='Pho Directional Laps Templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa753a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc07b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e05a739e",
   "metadata": {},
   "source": [
    "# 2024-02-13 - Plot the correlation between the Radon score and the decoder certainty for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e028231",
   "metadata": {},
   "source": [
    "### Decoder confidence is how far away the value is from 0.5. A value of 0.5 indicates no bias towards long or short, and should recieve a value of zero. Bias in either direction (towards 1.0 or 0.0) should recieve a increasing certainty value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549853f",
   "metadata": {},
   "source": [
    "This is most easily accomplished by shifting the values towards zero and applying an absolute value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c080b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_is_most_likely_direction_LR_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eff559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laps_all_epoch_bins_marginals_df\n",
    "rescaled_P_Long = np.abs(((ripple_all_epoch_bins_marginals_df['P_Long'] - 0.5) * 2))\n",
    "rescaled_P_Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get radon scores:\n",
    "ripple_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decoder_decoded_epochs_result_dict) # >> 'dict'\n",
    "\n",
    "def full_type(v):\n",
    "\t\"\"\" tries to get the full type annotation for a passed variable, meaning if it's a container type it tries to resolve the types of its inner elements like would be produced for a typehint.\n",
    "\t\n",
    "\tInstead of returning just 'dict' as the type, it should return 'Dict[str, DecodedFilterEpochsResult]` for example.\n",
    "\t\"\"\"\n",
    "    # If it's a container type, get its basic type:\n",
    "\touter_type : str = type(v)\n",
    "\tnested_types = []\n",
    "\tfor k, inner_v in v.items():\n",
    "\t\t# get the type of the keys\n",
    "\t\tnested_types.append((type(k), type(inner_v)))\n",
    "\n",
    "\treturn outer_type, nested_types\n",
    "\n",
    "\n",
    "\n",
    "outer_type, nested_types = full_type(decoder_decoded_epochs_result_dict)\n",
    "\n",
    "outer_type\n",
    "nested_types\n",
    "\n",
    "# nested_types: [(str, pyphoplacecellanalysis.Analysis.Decoder.reconstruction.DecodedFilterEpochsResult),\n",
    "#  (str, pyphoplacecellanalysis.Analysis.Decoder.reconstruction.DecodedFilterEpochsResult),\n",
    "#  (str, pyphoplacecellanalysis.Analysis.Decoder.reconstruction.DecodedFilterEpochsResult),\n",
    "#  (str, pyphoplacecellanalysis.Analysis.Decoder.reconstruction.DecodedFilterEpochsResult)]\n",
    "\n",
    "typename_replace_dict = {'dict':'Dict', 'list':'List', 'tuple':'Tuple'}\n",
    "\n",
    "\n",
    "outer_type.__module__ # 'builtins'\n",
    "outer_type.__name__ # 'dict'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db78bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Background Pipeline Computation\n",
    "# Ideally initiates a computation in the background, and then updates the pipeline only when all of them are done. Should be non-blocking during actual computation until it finishes.\n",
    "\n",
    "\n",
    "\n",
    "from multiprocessing import Process, freeze_support, Manager\n",
    "# Define your function as before\n",
    "# def _compute_epoch_decoding_for_decoder(...):\n",
    "#     ...\n",
    "\n",
    "def multiprocessing_function(params):\n",
    "    # Unpack parameters here for clarity\n",
    "    decoder, pipeline, laps_decoding_time_bin_size, ripple_decoding_time_bin_size, results_dict = params\n",
    "    results = _compute_epoch_decoding_for_decoder(decoder, pipeline, laps_decoding_time_bin_size, ripple_decoding_time_bin_size)\n",
    "    results_dict[decoder.__hash__()] = results # Use a hash or another unique identifier for the key\n",
    "\n",
    "def main():\n",
    "    with Manager() as manager:\n",
    "        decoder_results = manager.dict()\n",
    "\n",
    "        processes = []\n",
    "        for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "            params = (a_decoder, curr_active_pipeline, laps_decoding_time_bin_size, ripple_decoding_time_bin_size, decoder_results)\n",
    "            process = Process(target=multiprocessing_function, args=(params,))\n",
    "            process.start()\n",
    "            processes.append(process)\n",
    "\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "        \n",
    "        # Convert the manager dictionary to a regular dictionary\n",
    "        decoder_results = dict(decoder_results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()  # Required for Windows if the spawned process will also use multiprocessing\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0174187368f176085df53c2cec5d63d0b8400e63ae887d077931481ed3fab96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
