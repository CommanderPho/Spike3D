{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext autoreload\n",
    "# %autoreload 3\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file, save_copydict_to_text_file, read_copydict_from_text_file, invert_filedict\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsHelpers\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget\n",
    "\n",
    "# BATCH_DATE_TO_USE = '2024-01-09_GL' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2023-11-15_Lab' # used for filenames throught the notebook\n",
    "BATCH_DATE_TO_USE = '2024-01-10_Lab' # used for filenames throught the notebook\n",
    "\n",
    "# scripts_output_path = Path('/home/halechr/cloud/turbo/Data/Output/gen_scripts/').resolve() # Greatlakes\n",
    "# scripts_output_path = Path('output/gen_scripts/').resolve() # Apogee\n",
    "scripts_output_path = Path('/home/halechr/FastData/gen_scripts/').resolve() # Lab\n",
    "assert scripts_output_path.exists()\n",
    "\n",
    "collected_outputs_path = scripts_output_path.joinpath('../collected_outputs').resolve()\n",
    "collected_outputs_path.mkdir(exist_ok=True)\n",
    "assert collected_outputs_path.exists()\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8f43b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# fullwidth_path_widget(collected_outputs_path, file_name_label='Scripts Collected Outputs Path:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d958aaf",
   "metadata": {},
   "source": [
    "## Build Processing Scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb673d3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import MAIN_get_template_string\n",
    "\n",
    "# custom_user_completion_function_template_code, custom_user_completion_functions_dict = MAIN_get_template_string()\n",
    "\n",
    "# curr_context_template: str = \"\"\" \n",
    "#     BATCH_DATE_TO_USE = '2024-01-04_Lab' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "#     collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve()\n",
    "#     assert collected_outputs_path.exists()\n",
    "#     curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "#     CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "#     print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "# \"\"\"\n",
    "\n",
    "# def _helper_get_curr_batch_context(curr_active_pipeline):\n",
    "#     BATCH_DATE_TO_USE = '2024-01-04_Lab' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "#     collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve()\n",
    "#     assert collected_outputs_path.exists()\n",
    "#     curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "#     CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "#     print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "#     return CURR_BATCH_OUTPUT_PREFIX, curr_session_name, collected_outputs_path, BATCH_DATE_TO_USE\n",
    "\n",
    "\n",
    "\n",
    "custom_user_completion_functions_dict = {\n",
    "                                    # \"export_rank_order_results_completion_function\": export_rank_order_results_completion_function,\n",
    "                                    # \"figures_rank_order_results_completion_function\": figures_rank_order_results_completion_function,\n",
    "                                    # \"compute_and_export_marginals_dfs_completion_function\": compute_and_export_marginals_dfs_completion_function,\n",
    "                                    }\n",
    "\n",
    "# _template_defn_string: str = '\\n\\n'.join([inspect.getsource(a_fn) for a_name, a_fn in custom_user_completion_functions_dict.items()])\n",
    "    \n",
    "## Build the template string:\n",
    "template_str: str = f\"\"\"\n",
    "custom_user_completion_functions = []\n",
    "\"\"\"\n",
    "\n",
    "for a_name, a_fn in custom_user_completion_functions_dict.items():\n",
    "    fcn_defn_str: str = inspect.getsource(a_fn)\n",
    "    template_str = f\"{template_str}\\n{fcn_defn_str}\\ncustom_user_completion_functions.append({a_name})\\n\\n\"\n",
    "    \n",
    "template_str\n",
    "\n",
    "# template_str: str = f\"\"\"\n",
    "# {inspect.getsource(export_rank_order_results_completion_function)}\n",
    "# custom_user_completion_functions = [export_rank_order_results_completion_function]\n",
    "# \"\"\"\n",
    "\n",
    "custom_user_completion_function_template_code = template_str\n",
    "print(custom_user_completion_function_template_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa767589",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts, display_generated_scripts_ipywidget\n",
    "\n",
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "## Setup Functions:\n",
    "extended_computations_include_includelist=['pf_computation', #'pfdt_computation', 'firing_rate_trends',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    # 'extended_stats',\n",
    "    # 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', # 'long_short_rate_remapping',\n",
    "    # 'ratemap_peaks_prominence2d',\n",
    "    # 'long_short_inst_spike_rate_groups',\n",
    "    # 'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "]\n",
    "# force_recompute_override_computations_includelist = ['rank_order_shuffle_analysis']\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "force_recompute_override_computations_includelist = ['merged_directional_placefields', 'rank_order_shuffle_analysis']\n",
    "\n",
    "\n",
    "\n",
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'/media/MAX/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/FastData'), Path(r'/Volumes/MoverNew/data')] # Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/cloud/turbo/Data'), , Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/turbo/Data'), \n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "\n",
    "## Build Slurm Scripts:\n",
    "session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts,\n",
    "                                                                                                               output_directory=scripts_output_path, use_separate_run_directories=True,\n",
    "                                                                                                               should_freeze_pipeline_updates=True, \n",
    "                                                                                                               extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, # ['split_to_directional_laps', 'rank_order_shuffle_analysis'],\n",
    "                                                                                                            #    should_perform_figure_generation_to_file=False, should_force_reload_all=True,\n",
    "                                                                                                                batch_session_completion_handler_kwargs=dict(enable_hdf5_output=False),\n",
    "                                                                                                                # custom_user_completion_functions=custom_user_completion_functions,\n",
    "                                                                                                                custom_user_completion_function_template_code=custom_user_completion_function_template_code,\n",
    "                                                                                                                )\n",
    "\n",
    "computation_script_paths = [x[0] for x in output_python_scripts]\n",
    "generate_figures_script_paths = [x[1] for x in output_python_scripts]\n",
    "print(f'generate_figures_script_paths: {generate_figures_script_paths}')\n",
    "_out_widget = display_generated_scripts_ipywidget(included_session_contexts, output_python_scripts)\n",
    "display(_out_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b61a8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 1\n",
    "# List of your script paths\n",
    "# script_paths = computation_script_paths\n",
    "script_paths = generate_figures_script_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f98344",
   "metadata": {},
   "source": [
    "## Execute the generated scripts in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2fb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "## recieves: max_parallel_executions, script_paths\n",
    "\"\"\"\n",
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 5\n",
    "# List of your script paths\n",
    "script_paths = output_python_scripts\n",
    "\"\"\"\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    try:\n",
    "        result = subprocess.run(['python', script_path], capture_output=True, text=True)\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except Exception as e:\n",
    "        return script_path, None, str(e)\n",
    "\n",
    "# Create a progress bar\n",
    "progress_bar = widgets.IntProgress(value=0, min=0, max=len(script_paths), description='Running:', bar_style='info')\n",
    "display(progress_bar)\n",
    "\n",
    "# Run scripts in parallel with a limit on the number of parallel instances\n",
    "with ProcessPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "    futures = {executor.submit(run_script, path): path for path in script_paths}\n",
    "    for future in as_completed(futures):\n",
    "        script, stdout, stderr = future.result()\n",
    "        progress_bar.value += 1  # Update the progress bar\n",
    "        if stderr:\n",
    "            print(f\"Error in {script}: {stderr}\")\n",
    "        else:\n",
    "            print(f\"Completed {script}\")\n",
    "\n",
    "# Progress bar will automatically update as scripts complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc1270",
   "metadata": {},
   "source": [
    "# âŒšðŸ§µ Supposed Execution with Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216480c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import os\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.isfile(script_path):\n",
    "            return script_path, None, f\"Error: File {script_path} does not exist\"\n",
    "\n",
    "        result = subprocess.run(['python', script_path], capture_output=True, text=True, timeout=1800) # 30 minutes timeout\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        return script_path, None, f\"FileNotFoundError: {str(fnf_error)}\"\n",
    "    except subprocess.TimeoutExpired as timeout_error:\n",
    "        return script_path, None, f\"TimeoutError: {str(timeout_error)}\"\n",
    "    except Exception as e:\n",
    "        return script_path, None, f\"Error: {str(e)}\\nTraceback: {traceback.format_exc()}\"\n",
    "\n",
    "# Retry mechanism\n",
    "num_retries = 3\n",
    "retry_count = {path : 0 for path in script_paths}\n",
    "\n",
    "# Run scripts in parallel with a limit on the number of parallel instances\n",
    "with ProcessPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "    futures = {executor.submit(run_script, path): path for path in script_paths}\n",
    "    for future in as_completed(futures):\n",
    "        script, stdout, stderr = future.result()\n",
    "        # Retry if error and retry count is not exceeded.\n",
    "        if stderr and retry_count[script] < num_retries:\n",
    "            retry_count[script] += 1\n",
    "            futures[executor.submit(run_script, script)] = script\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdcbbf",
   "metadata": {},
   "source": [
    "## using `dask` for parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "## recieves: max_parallel_executions, script_paths\n",
    "\"\"\"\n",
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 5\n",
    "# List of your script paths\n",
    "script_paths = output_python_scripts\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a Dask Client\n",
    "client = Client()\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    try:\n",
    "        result = subprocess.run(['python', script_path], capture_output=True, text=True)\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except Exception as e:\n",
    "        return script_path, None, str(e)\n",
    "\n",
    "# Create a list of delayed tasks\n",
    "tasks = [dask.delayed(run_script)(path) for path in computation_script_paths]\n",
    "\n",
    "# Run the tasks in parallel\n",
    "results = dask.compute(*tasks, scheduler='processes', num_workers=max_parallel_executions)\n",
    "\n",
    "# Process and display the results\n",
    "for script, stdout, stderr in results:\n",
    "    if stderr:\n",
    "        print(f\"Error in {script}: {stderr}\")\n",
    "    else:\n",
    "        print(f\"Completed {script}\")\n",
    "\n",
    "# Shutdown Dask client\n",
    "client.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f583772",
   "metadata": {},
   "source": [
    "## Resume normal stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data')] # , Path(r'/home/halechr/FastData'), Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/turbo/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = UserAnnotationsManager.get_hardcoded_good_sessions()\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "\n",
    "\n",
    "# Output Paths:\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]\n",
    "copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix=BATCH_DATE_TO_USE, only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "check_output_h5_files(included_h5_paths)\n",
    "# from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts\n",
    "\n",
    "# ## Build Slurm Scripts:\n",
    "# session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "# included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts, output_directory=Path('output/generated_slurm_scripts/').resolve(), use_separate_run_directories=True, should_perform_figure_generation_to_file=False)\n",
    "# display(output_python_scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Output File Processing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_dict_h5_files = copy_movedict(copy_dict)\n",
    "moved_files_dict_h5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e3954-15a4-449c-b927-56d5d79153c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_copydict_output_filename=f'backed_up_files_copydict_{BATCH_DATE_TO_USE}.csv'\n",
    "moved_files_copydict_file_path = Path(global_data_root_parent_path).joinpath(moved_files_copydict_output_filename).resolve() # Use Default\n",
    "print(f'moved_files_copydict_file_path: {moved_files_copydict_file_path}')\n",
    "\n",
    "_out_string, filedict_out_path = save_copydict_to_text_file(moved_files_dict_h5_files, moved_files_copydict_file_path, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab869730",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_moved_files_dict_files = read_copydict_from_text_file(moved_files_copydict_file_path, debug_print=False)\n",
    "read_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_moved_files_dict_files\n",
    "restore_moved_files_dict_files = invert_filedict(read_moved_files_dict_files)\n",
    "restore_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data, process, and produce figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "graphics_output_dict = AcrossSessionsResults.post_compute_all_sessions_processing(global_data_root_parent_path=global_data_root_parent_path, BATCH_DATE_TO_USE=BATCH_DATE_TO_USE, plotting_enabled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d829b90",
   "metadata": {},
   "source": [
    "## 2023-11-15 - For manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load the saved across-session results:\n",
    "# Outputs: across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list, neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table\n",
    "\n",
    "inst_fr_output_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "## Load all across-session tables from the pickles:\n",
    "output_path_suffix: str = f'{BATCH_DATE_TO_USE}'\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=output_path_suffix) # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "# neuron_replay_stats_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
