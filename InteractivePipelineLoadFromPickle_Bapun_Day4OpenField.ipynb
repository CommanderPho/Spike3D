{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45f3f6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ InteractivePipelineLoadFromPickle (Independent Load-only Visualization Notebook) - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:34:16.396240Z",
     "start_time": "2025-01-07T11:34:09.057603Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "pho-run-2024",
     "run-load",
     "run-main",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# PyQtInspect:                                                                                                                                                                                                                                                                         #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# 1. Launch `pqi-server` in a new terminal BEFORE running this notebook cell. Be sure to click 'Serve' button in the GUI that appears so this notebook can connect.\n",
    "\n",
    "# # IMPORTANT: Call settrace BEFORE importing PyQt5\n",
    "# import PyQtInspect.pqi as pqi\n",
    "\n",
    "# # Connect to the server (default: localhost:19394)\n",
    "# # Make sure the server is already running!\n",
    "# pqi.settrace(\n",
    "#     host='127.0.0.1',\n",
    "#     port=19394,  # Default port, or use the port shown in the server GUI\n",
    "#     qt_support='pyqt5',  # or 'auto' for auto-detection\n",
    "#     patch_multiprocessing=False\n",
    "# )\n",
    "\n",
    "# # !pip install viztracer\n",
    "%load_ext viztracer\n",
    "from viztracer import VizTracer\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "from pyphocorehelpers.gui.Jupyter.AsyncExecutionHelper import run_async\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "from pyphocorehelpers.ipython_helpers import CustomFormatterMagics\n",
    "\n",
    "# Register the magic\n",
    "ip.register_magics(CustomFormatterMagics) # %%ndarray_preview height=500, width=None, include_plaintext_repr=False, include_shape=False, horizontal_layout=True\n",
    "\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "from pyphocorehelpers.pho_jupyter_preview_widget.ipython_helpers import PreviewWidgetMagics\n",
    "\n",
    "ip.register_magics(PreviewWidgetMagics)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "from pyphocorehelpers.indexing_helpers import get_dict_subset\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "from pyphocorehelpers.print_helpers import render_scrollable_colored_table_from_dataframe, render_scrollable_colored_table\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "\n",
    "from neuropy.utils.indexing_helpers import PandasHelpers, NumpyHelpers\n",
    "from neuropy.utils.indexing_helpers import flatten\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch, EpochsAccessor, ensure_dataframe, ensure_Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "# from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import metadata_attributes\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "## Pho Programming Helpers:\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "from pyphocorehelpers.notebook_helpers import NotebookCellExecutionLogger, NotebookProcessor\n",
    "\n",
    "_notebook_path:Path = Path(IPythonHelpers.try_find_notebook_filepath(IPython.extract_module_locals())).resolve() # Finds the path of THIS notebook\n",
    "_notebook_execution_logger: NotebookCellExecutionLogger = NotebookCellExecutionLogger(notebook_path=_notebook_path, enable_logging_to_file=False) # Builds a logger that records info about this notebook\n",
    "_notebook_processor: NotebookProcessor = NotebookProcessor(path=_notebook_path)\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme # used in perform_pipeline_save\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "import pyphoplacecellanalysis.General.type_aliases as types # import neuropy.utils.type_aliases as types\n",
    "\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "from neuropy.utils.mixins.binning_helpers import transition_matrix\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.transition_matrix import TransitionMatrixComputations\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates, get_proper_global_spikes_df\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import HardcodedProcessingParameters\n",
    "\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import HardcodedProcessingParameters\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "def get_global_variable(var_name):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    return globals()[var_name]\n",
    "    \n",
    "def update_global_variable(var_name, value):\n",
    "    \"\"\" used by `PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(...)` to update the notebook's variables \"\"\"\n",
    "    globals()[var_name] = value\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/home/halechr/FastData'), Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/media/halechr/MAX/Data'), Path(r'W:\\Data'), Path(r'H:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/Users/pho/cloud/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "    global global_data_root_parent_path\n",
    "    new_global_data_root_parent_path = new_path.resolve()\n",
    "    global_data_root_parent_path = new_global_data_root_parent_path\n",
    "    print(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "    assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "            \n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load",
     "run-main",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# BAPUN data format                                                                                                    #\n",
    "# ==================================================================================================================== #\n",
    "active_data_mode_name = 'bapun'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('Bapun')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='bapun',animal='RatU', session_name='RatUDay5OpenfieldSD') # NEW 2025-12-15 -- working but the epochs are a little weird, I had to manually rename them and they still don't seem right\n",
    "curr_context = IdentifyingContext(format_name='bapun',animal='RatN', session_name='Day4OpenField') # ,exper_name='one' -- This is the one with the 2 behaviors ('roam', 'sprinkle') in the same open field box\n",
    "# curr_context = IdentifyingContext(format_name='bapun',animal='RatK', session_name='Day4Openfield') # DOESN\"T WORK, NO NEU{RONS OR SOMETHING.\n",
    "\n",
    "\n",
    "# Create a dictionary with the parameters to override\n",
    "override_parameters = {\n",
    "    'preprocessing.laps.use_direction_dependent_laps': False,\n",
    "    # 'preprocessing.laps.use_direction_dependent_laps': False\n",
    "}\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name) #.resolve()\n",
    "\n",
    "# basedir: Path = Path('/media/halechr/MAX/Data/Rachel/cho/cho_241117_2_merged') # DO NOT `.resolve()``\n",
    "# basedir: Path = Path(r'H:\\Data\\Bapun\\RatS\\Day5TwoNovel')\n",
    "print(f'basedir: {str(basedir)}')\n",
    "Assert.path_exists(basedir)\n",
    "\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist = ['pf_computation', 'pfdt_computation', 'position_decoding']\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# # saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "selector, on_value_change = PipelineJupyterHelpers._build_pipeline_custom_processing_mode_selector_widget(update_global_variable_fn=update_global_variable, debug_print=False, enable_full_view=True)\n",
    "# selector.value = 'clean_run'\n",
    "selector.value = 'continued_run'\n",
    "# selector.value = 'final_run'\n",
    "on_value_change(dict(new=selector.value)) ## do update manually so the workspace variables reflect the set values\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "print(f\"saving_mode: {saving_mode}, force_reload: {force_reload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    NotebookProcessor.get_running_notebook_path()\n",
    "except Exception as e:\n",
    "    print(f'failed to get notebook path with error {e}. Skipping.')\n",
    "    # raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33267230",
   "metadata": {},
   "source": [
    "# Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21a4df",
   "metadata": {
    "tags": [
     "run-group-0",
     "run-load",
     "run-main",
     "active-2025-01-16",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import PipelineJupyterHelpers, CustomProcessingPhases, PipelinePickleFileSelectorWidget\n",
    "\n",
    "# ## INPUTS: basedir\n",
    "# active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir)\n",
    "\n",
    "extended_computations_include_includelist_phase_dict: Dict[str, CustomProcessingPhases] = CustomProcessingPhases.get_extended_computations_include_includelist_phase_dict()\n",
    "\n",
    "current_phase: CustomProcessingPhases = CustomProcessingPhases[selector.value]  # Assuming selector.value is an instance of CustomProcessingPhases\n",
    "extended_computations_include_includelist: List[str] = [key for key, value in extended_computations_include_includelist_phase_dict.items() if value <= current_phase]\n",
    "display(extended_computations_include_includelist)\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'] # \n",
    "\n",
    "# ## INPUTS: basedir\n",
    "active_session_pickle_file_widget = PipelinePickleFileSelectorWidget(directory=basedir, on_update_global_variable_callback=update_global_variable, on_get_global_variable_callback=get_global_variable)\n",
    "\n",
    "_subfn_load, _subfn_save, _subfn_compute, _subfn_compute_new = active_session_pickle_file_widget._build_load_save_callbacks(global_data_root_parent_path=global_data_root_parent_path, active_data_mode_name=active_data_mode_name, basedir=basedir, saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                                             extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist)\n",
    "\n",
    "\n",
    "# Display the widget\n",
    "display(active_session_pickle_file_widget.servable())\n",
    "# active_session_pickle_file_widget.local_file_browser_widget.servable()\n",
    "# active_session_pickle_file_widget.global_file_browser_widget.servable()\n",
    "# display(active_session_pickle_file_widget.local_file_browser_widget.servable())\n",
    "# display(active_session_pickle_file_widget.global_file_browser_widget.servable())\n",
    "\n",
    "# OUTPUTS: active_session_pickle_file_widget, widget.active_local_pkl, widget.active_global_pkl\n",
    "\n",
    "if selector.value == 'clean_run':\n",
    "    ## handle a clean run specially, this will create the pkls and not load them\n",
    "    print(f'clean run!')\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    # active_session_pickle_file_widget.is_compute_button_disabled = False # enable the compute button always during a clean run\n",
    "    # active_session_pickle_file_widget.is_load_button_disabled = True\n",
    "    \n",
    "    new_default_local_pkl_file: Path = active_session_pickle_file_widget.directory.joinpath(default_selected_local_file_name).resolve()\n",
    "    print(f'new_default_local_pkl_file: {new_default_local_pkl_file}')\n",
    "\n",
    "    active_session_pickle_file_widget.selected_local_pkl_files = [new_default_local_pkl_file]\n",
    "    active_session_pickle_file_widget.selected_global_pkl_files = []\n",
    "    active_session_pickle_file_widget._update_load_save_button_disabled_state()\n",
    "    print(f'active_session_pickle_file_widget.is_load_button_disabled: {active_session_pickle_file_widget.is_load_button_disabled}')\n",
    "    print(f'active_session_pickle_file_widget.is_compute_button_disabled: {active_session_pickle_file_widget.is_compute_button_disabled}')\n",
    "    print(f'active_local_pkl: \"{active_session_pickle_file_widget.active_local_pkl}\"')\n",
    "    print(f'active_global_pkl: \"{active_session_pickle_file_widget.active_global_pkl}\"')\n",
    "    active_session_pickle_file_widget.load_button.disabled = False\n",
    "    active_session_pickle_file_widget.compute_button.disabled = False\n",
    "else:\n",
    "    # not `clean_run` mode, continuing processing which might include loading from pickles\n",
    "    ## try selecting the first\n",
    "    did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "\n",
    "    ## Set default local comp pkl:\n",
    "    default_selected_local_file_name: str = 'loadedSessPickle.pkl'\n",
    "    if not active_session_pickle_file_widget.is_local_file_names_list_empty:\n",
    "        default_local_section_indicies = [active_session_pickle_file_widget.local_file_browser_widget._data['File Name'].tolist().index(default_selected_local_file_name)]\n",
    "        active_session_pickle_file_widget.local_file_browser_widget.selection = default_local_section_indicies\n",
    "\n",
    "    ## Set default global computation pkl:\n",
    "    default_selected_global_file_name: str = 'global_computation_results.pkl'\n",
    "    if not active_session_pickle_file_widget.is_global_file_names_list_empty:\n",
    "        default_global_section_indicies = [active_session_pickle_file_widget.global_file_browser_widget._data['File Name'].tolist().index(default_selected_global_file_name)]\n",
    "        active_session_pickle_file_widget.global_file_browser_widget.selection = default_global_section_indicies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b1147",
   "metadata": {
    "tags": [
     "run-main",
     "run-group-0"
    ]
   },
   "outputs": [],
   "source": [
    "did_find_valid_selection: bool = active_session_pickle_file_widget.try_select_first_valid_files()\n",
    "did_find_valid_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if did_find_valid_selection:\n",
    "#     _subfn_load()\n",
    "    \n",
    "did_find_valid_selection = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669bb69c",
   "metadata": {
    "tags": [
     "run-group-0"
    ]
   },
   "outputs": [],
   "source": [
    "if did_find_valid_selection:\n",
    "    curr_active_pipeline, custom_suffix, proposed_load_pkl_path = active_session_pickle_file_widget.on_load_local(global_data_root_parent_path=global_data_root_parent_path, active_data_mode_name=active_data_mode_name, basedir=basedir, saving_mode=saving_mode, force_reload=force_reload)\n",
    "    print(f'on_load_local(...) complete. workspace variables updated: curr_active_pipeline, custom_suffix, proposed_load_pkl_path')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if did_find_valid_selection:\n",
    "    try:\n",
    "        skip_global_load = False\n",
    "        curr_active_pipeline = active_session_pickle_file_widget.on_load_global(curr_active_pipeline=curr_active_pipeline, basedir=basedir, extended_computations_include_includelist=extended_computations_include_includelist, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "                                    skip_global_load=skip_global_load, force_reload=False, override_global_computation_results_pickle_path=active_session_pickle_file_widget.active_global_pkl)\n",
    "        # Update the global variable after loading global\n",
    "        print(f'on_load_global(...) complete. workspace variables updated: curr_active_pipeline, custom_suffix, proposed_load_pkl_path')\n",
    "    except Exception as e:\n",
    "        print(f'encountered exception loading global e: {e}.')\n",
    "        pass\n",
    "        # raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d280d3",
   "metadata": {},
   "source": [
    "## From `test_non_interactive_crash.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd6416",
   "metadata": {
    "tags": [
     "run-main",
     "active-2025-09-21",
     "run-fresh-load"
    ]
   },
   "outputs": [],
   "source": [
    "### Bapun Open-Field Experiment (2022-08-09 Analysis)\n",
    "from neuropy.core.session.SessionSelectionAndFiltering import build_custom_epochs_filters # used particularly to build Bapun-style filters\n",
    "\n",
    "active_data_mode_name = 'bapun'\n",
    "# active_data_mode_name = 'rachel'\n",
    "print(f'active_data_session_types_registered_classes_dict: {active_data_session_types_registered_classes_dict}')\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "\n",
    "# basedir = Path('/media/halechr/MAX/Data/Rachel/Cho_241117_Session2').resolve()\n",
    "## INPUTS: basedir \n",
    "override_parameters = {'rank_order_shuffle_analysis.minimum_inclusion_fr_Hz': 1.0}\n",
    "force_reload = force_reload #True\n",
    "print(f'force_reload: {force_reload}')\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties, override_basepath=Path(basedir), force_reload=force_reload) # , override_parameters_flat_keypaths_dict=override_parameters\n",
    "\n",
    "# _test_session = RachelDataSessionFormat.build_session(Path(r'R:\\data\\Rachel\\merged_M1_20211123_raw_phy'))\n",
    "# _test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "# _test_session\n",
    "\n",
    "## ~20m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42da889",
   "metadata": {},
   "source": [
    "##### Old not needed anymore manual comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621607ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch_names: List[str] = curr_active_pipeline.sess.epochs.to_dataframe()['label'].to_list()\n",
    "print(f'curr_epoch_names: {curr_epoch_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686d53e",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.session.SessionSelectionAndFiltering import build_custom_epochs_filters\n",
    "\n",
    "# epoch_name_includelist = ['pre', 'maze1', 'post1', 'maze2', 'post2']\n",
    "# epoch_name_includelist = ['pre', 'roam', 'sprinkle', 'post']\n",
    "# epoch_name_includelist = ['roam', 'sprinkle']\n",
    "\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['pre', 'maze1', 'post1', 'maze2', 'post2']) ## ALL possible epochs\n",
    "\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze1', 'maze2', 'maze_GLOBAL']) ## ALL possible epochs\n",
    "active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze1', 'maze2', 'maze_GLOBAL']) ## ALL possible epochs\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess)\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['pre', 'roam', 'maze', 'sprinkle', 'post']) ## ALL possible epochs\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['pre', 'roam', 'sprinkle', 'post']) ## ALL possible epochs\n",
    "\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['maze', 'sprinkle'])\n",
    "# active_session_filter_configurations = build_custom_epochs_filters(curr_active_pipeline.sess, epoch_name_includelist=['roam', 'sprinkle']) # , 'maze'\n",
    "\n",
    "# active_session_filter_configurations = active_data_mode_registered_class.build_filters_pyramidal_epochs(curr_active_pipeline.sess, epoch_name_includelist=['maze','sprinkle'])\n",
    "# active_session_filter_configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04689045",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.filter_sessions(active_session_filter_configurations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d068e",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "active_session_computation_configs = active_data_mode_registered_class.build_active_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=0.5)\n",
    "active_session_computation_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.computation_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcaaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_bin_bounds=(((-83.33747881216672, 110.15967332926644), (-94.89955475226206, 97.07387994733473)))\n",
    "\n",
    "\n",
    "bapun_open_field_grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0)))\n",
    "curr_active_pipeline.get_all_parameters()\n",
    "# curr_active_pipeline.update_parameters(grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0))))\n",
    "curr_active_pipeline.sess.config.grid_bin_bounds = (((-120.0, 120.0), (-120.0, 120.0)))\n",
    "\n",
    "\n",
    "# override_parameters_flat_keypaths_dict = {'grid_bin_bounds': (((-120.0, 120.0), (-120.0, 120.0))), # 'rank_order_shuffle_analysis.minimum_inclusion_fr_Hz': minimum_inclusion_fr_Hz,\n",
    "# \t\t\t\t\t\t\t\t\t\t#   'sess.config.preprocessing_parameters.laps.use_direction_dependent_laps': False, # lap_estimation_parameters\n",
    "#                                         }\n",
    "\n",
    "# curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_inclusion_fr_Hz = 1.0\n",
    "override_parameters_flat_keypaths_dict = {\n",
    "                            # 'grid_bin_bounds': (((-120.0, 120.0), (-120.0, 120.0))), # \n",
    "                            'rank_order_shuffle_analysis.minimum_inclusion_fr_Hz': minimum_inclusion_fr_Hz,\n",
    "                        #   'sess.config.preprocessing_parameters.laps.use_direction_dependent_laps': False, # lap_estimation_parameters\n",
    "                        }\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76553c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, EpochsAccessor, ensure_dataframe, ensure_Epoch\n",
    "\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# Update computation_epochs to be only the maze ones                                                                                                                                                                                                                                   #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "\n",
    "## activity_only_epochs_df:\n",
    "epochs_df = ensure_dataframe(deepcopy(curr_active_pipeline.sess.epochs))\n",
    "# activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(['maze1', 'maze2', 'maze_GLOBAL'])]\n",
    "\n",
    "activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(['maze1', 'maze2'])].epochs.get_non_overlapping_df()\n",
    "activity_only_epochs: Epoch = ensure_Epoch(activity_only_epochs_df, metadata=curr_active_pipeline.sess.epochs.metadata)\n",
    "\n",
    "## GLobal only ('maze_GLOBAL')\n",
    "epochs_df = ensure_dataframe(deepcopy(curr_active_pipeline.sess.epochs))\n",
    "global_activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(['maze_GLOBAL'])].epochs.get_non_overlapping_df()\n",
    "global_activity_only_epoch: Epoch = ensure_Epoch(global_activity_only_epochs_df, metadata=curr_active_pipeline.sess.epochs.metadata)\n",
    "\n",
    "## OUTPUTS: activity_only_epochs, global_activity_only_epoch\n",
    "\n",
    "## OUTPUTS: activity_only_epoch\n",
    "\n",
    "\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.filtered_sessions['maze'].epochs)\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.sess.epochs)\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(curr_active_pipeline.sess.epochs) ## prev\n",
    "active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(activity_only_epochs)\n",
    "\n",
    "global_only_sess_comp_config = deepcopy(active_session_computation_configs[0])\n",
    "global_only_sess_comp_config.pf_params.computation_epochs = deepcopy(global_activity_only_epoch)\n",
    "if len(active_session_computation_configs) < 2:\n",
    "    active_session_computation_configs.append(global_only_sess_comp_config)\n",
    "else:\n",
    "    active_session_computation_configs[1] = global_only_sess_comp_config\n",
    "\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = deepcopy(bapun_epochs)\n",
    "# active_session_computation_configs[1].pf_params.computation_epochs = deepcopy(curr_active_pipeline.filtered_sessions['maze'].epochs.to_dataframe())\n",
    "active_session_computation_configs\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs\n",
    "\n",
    "#    start   stop     label  duration\n",
    "# 0      0   7407       pre      7407\n",
    "# 1   7423  11483      maze      4060\n",
    "# 3  10186  11483  sprinkle      1297\n",
    "# 2  11497  25987      post     14490\n",
    "\n",
    "# [4 rows x 4 columns]\n",
    "\n",
    "## UPDATES: active_session_computation_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_only_epochs_df: pd.DataFrame = epochs_df[epochs_df['label'].isin(hardcoded_params.non_global_activity_session_names)]\n",
    "activity_only_epochs_df\n",
    "\n",
    "activity_only_epochs_df.loc[1, 'stop'] = activity_only_epochs_df.loc[2, 'start'] - 0.001\n",
    "activity_only_epochs_df.loc[1, 'label'] = 'roam' \n",
    "activity_only_epochs_df['duration'] = activity_only_epochs_df['stop'] -  activity_only_epochs_df['start']\n",
    "activity_only_epochs_df\n",
    "\n",
    "hardcoded_params.non_global_activity_session_names = ['roam', 'sprinkle']\n",
    "\n",
    "\n",
    "1   7125.0  11745.0         maze    4620.0\n",
    "2   9591.0  11745.0     sprinkle    2154.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d8497",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "active_session_computation_configs[0].pf_params.linearization_method = \"umap\"\n",
    "\n",
    "for an_epoch_name, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    ## forcibly compute the linearized position so it doesn't fallback to \"isomap\" method which eats all the memory\n",
    "    a_pos_df: pd.DataFrame = a_sess.position.compute_linearized_position(method='umap')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb73824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_only_epoch_names: List[str] = ['maze1', 'maze2', 'maze_GLOBAL']\n",
    "# active_computation_functions_name_includelist\n",
    "activity_only_epoch_names: List[str] = active_session_computation_configs[0].pf_params.computation_epochs.labels.tolist() ## should be same as config\n",
    "activity_only_epoch_names\n",
    "\n",
    "# # Create non-overlapping version\n",
    "# non_overlapping_epochs = ensure_Epoch(active_session_computation_configs[0].pf_params.computation_epochs.epochs.get_non_overlapping_df())\n",
    "# active_session_computation_configs[0].pf_params.computation_epochs = non_overlapping_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2519c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.computation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859379ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_session_computation_configs = curr_active_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "    \n",
    "active_computation_functions_name_includelist = ['pf_computation',\n",
    "                                                'pfdt_computation',\n",
    "                                                'position_decoding',\n",
    "                                                #  'position_decoding_two_step',\n",
    "                                                #  'extended_pf_peak_information',\n",
    "                                                ] # 'ratemap_peaks_prominence2d'\n",
    "\n",
    "\n",
    "## Loops through all configs\n",
    "for i, a_config in enumerate(active_session_computation_configs):\n",
    "    active_epoch_names: List[str] = a_config.pf_params.computation_epochs.labels.tolist() ## should be same as config\n",
    "    print(f'i: {i}, active_epoch_names: {active_epoch_names}') # (activity_only_epoch_names)\n",
    "\n",
    "    # curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation', '_perform_velocity_vs_pf_density_computation', '_perform_velocity_vs_pf_simplified_count_density_computation']) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "    # curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_includelist=active_computation_functions_name_includelist, enabled_filter_names=activity_only_epoch_names, overwrite_extant_results=True, fail_on_exception=False, debug_print=True) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "    curr_active_pipeline.perform_computations(a_config, computation_functions_name_includelist=active_computation_functions_name_includelist, enabled_filter_names=active_epoch_names, overwrite_extant_results=False, fail_on_exception=False, debug_print=True) # SpikeAnalysisComputations._perform_spike_burst_detection_computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess.epochs.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.active_completed_computation_result_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_includelist=active_computation_functions_name_includelist, enabled_filter_names=['maze1', 'maze2'], overwrite_extant_results=False, fail_on_exception=False, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491af66",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# curr_active_pipeline.computation_results['maze'].accumulated_errors\n",
    "curr_active_pipeline.clear_all_failed_computations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831df11",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display(root_output_dir=r'Output', should_smooth_maze=True) # TODO: pass a display config\n",
    "# curr_active_pipeline.prepare_for_display(root_output_dir=r'W:\\Data\\Output', should_smooth_maze=True) # TODO: pass a display config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e45325",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.pickle_path\n",
    "curr_active_pipeline.global_computation_results_pickle_path\n",
    "curr_active_pipeline.get_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09507ee4",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# _out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)\n",
    "_out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE)\n",
    "# _out = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-26.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4608e0",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.save_global_computation_results()#save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_2025-02-27.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797a9de",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "# include_includelist = ['pre', 'maze1', 'post1', 'maze2', 'post2', 'maze',]\n",
    "include_includelist = ['roam', 'sprinkle']\n",
    "# include_includelist = curr_active_pipeline.filtered_session_names\n",
    "include_includelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_session_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_includelist = curr_active_pipeline.filtered_session_names\n",
    "print(f'include_includelist: {include_includelist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d93f",
   "metadata": {
    "tags": [
     "run-main"
    ]
   },
   "outputs": [],
   "source": [
    "## Setup Computation Functions to be executed:\n",
    "# includelist Mode:\n",
    "computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                '_perform_position_decoding_computation', \n",
    "                                '_perform_firing_rate_trends_computation',\n",
    "                                '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                '_perform_two_step_position_decoding_computation',\n",
    "                                # '_perform_recursive_latent_placefield_decoding'\n",
    "                            ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "computation_functions_name_excludelist=None\n",
    "\n",
    "batch_extended_computations(curr_active_pipeline, included_computation_filter_names=computation_functions_name_includelist, include_includelist=include_includelist,\n",
    "                            include_global_functions=True, fail_on_exception=False, progress_print=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad14427",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Firing rate filter seems too high (5.0Hz, maybe should be lower at like 1.0Hz)?\n",
    "curr_active_pipeline.sess.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a8509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9845bf16",
   "metadata": {},
   "source": [
    "## NEW BATCH COMPUTE ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c088e73",
   "metadata": {
    "tags": [
     "run-main",
     "run-load",
     "run-fresh-load",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.BaseDataSessionFormats import HardcodedProcessingParameters\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import final_process_bapun_all_comps\n",
    "\n",
    "# try:\n",
    "curr_active_pipeline = final_process_bapun_all_comps(curr_active_pipeline=curr_active_pipeline, posthoc_save=False)\n",
    "# curr_active_pipeline = final_process_bapun_all_comps(curr_active_pipeline=curr_active_pipeline, posthoc_save=True)\n",
    "# except Exception as e:\n",
    "#     print(f'exception: {e}')\n",
    "#     # raise e\n",
    "#     pass    \n",
    "\n",
    "## 9m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28421f3",
   "metadata": {},
   "source": [
    "# 💾 Save Export Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d035840",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_complete_session_context()\n",
    "custom_save_filepaths, custom_save_filenames, custom_suffix = curr_active_pipeline.get_custom_pipeline_filenames_from_parameters()\n",
    "custom_save_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filenames['pipeline_pkl']\n",
    "custom_save_filenames['global_computation_pkl']\n",
    "\n",
    "pickle_path = 'loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl'\n",
    "global_computation_pkl = 'global_computation_results_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20555817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename=curr_active_pipeline.pickle_path.name) #active_pickle_filename=\n",
    "# curr_active_pipeline.save_global_computation_results(override_global_pickle_path=curr_active_pipeline.global_computation_results_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74080dd",
   "metadata": {
    "tags": [
     "run-perform-save-pkl"
    ]
   },
   "outputs": [],
   "source": [
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "if curr_active_pipeline.pickle_path is None:\n",
    "    active_pickle_filename = 'loadedSessPickle.pkl'\n",
    "else:\n",
    "    active_pickle_filename = curr_active_pipeline.pickle_path.name\n",
    "    \n",
    "print(f'active_pickle_filename: {active_pickle_filename}')\n",
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename=active_pickle_filename) #active_pickle_filename=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520284fb",
   "metadata": {
    "tags": [
     "run-perform-save-pkl"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results(override_global_pickle_path=curr_active_pipeline.global_computation_results_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678b695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1d1c4",
   "metadata": {},
   "source": [
    "#### Manual Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22341765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, override_pickle_path=curr_active_pipeline.pickle_path, active_pickle_filename='loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl') #active_pickle_filename=\n",
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename='loadedSessPickle_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl') #active_pickle_filename="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849549fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results(override_global_pickle_filename='global_computation_results_withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0_2025-01-20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import PipelineWithInputStage, PipelineWithLoadableStage, loadData, saveData\n",
    "\n",
    "## Custom Save\n",
    "curr_sess_pkl_path = basedir.joinpath('loadedSessPickle_2025-09-08.pkl')\n",
    "print(f'saving out to modern pickle: \"{curr_sess_pkl_path}\"')\n",
    "saveData(curr_sess_pkl_path, db=curr_active_pipeline, safe_save=True) # (v_dict, str(curr_item_type.__module__), str(curr_item_type.__name__)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52cc45",
   "metadata": {},
   "source": [
    "### 2024-06-25 - Load from saved custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337ddc6",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "# Loads custom pipeline pickles that were saved out via `custom_save_filepaths['pipeline_pkl'] = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename=custom_save_filenames['pipeline_pkl'])`\n",
    "\n",
    "## INPUTS: global_data_root_parent_path, active_data_mode_name, basedir, saving_mode, force_reload, custom_save_filenames\n",
    "# custom_suffix: str = '_withNewKamranExportedReplays'\n",
    "\n",
    "# custom_suffix: str = '_withNewComputedReplays'\n",
    "# custom_suffix: str = '_withNewComputedReplays-qclu_[1, 2]-frateThresh_5.0'\n",
    "\n",
    "# custom_save_filenames = {\n",
    "#     'pipeline_pkl':f'loadedSessPickle{custom_suffix}.pkl',\n",
    "#     'global_computation_pkl':f\"global_computation_results{custom_suffix}.pkl\",\n",
    "#     'pipeline_h5':f'pipeline{custom_suffix}.h5',\n",
    "# }\n",
    "# print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "# custom_save_filepaths = {k:v for k, v in custom_save_filenames.items()}\n",
    "\n",
    "# # ==================================================================================================================== #\n",
    "# # PIPELINE LOADING                                                                                                     #\n",
    "# # ==================================================================================================================== #\n",
    "# # load the custom saved outputs\n",
    "# active_pickle_filename = custom_save_filenames['pipeline_pkl'] # 'loadedSessPickle_withParameters.pkl'\n",
    "# print(f'active_pickle_filename: \"{active_pickle_filename}\"')\n",
    "# # assert active_pickle_filename.exists()\n",
    "# active_session_h5_filename = custom_save_filenames['pipeline_h5'] # 'pipeline_withParameters.h5'\n",
    "# print(f'active_session_h5_filename: \"{active_session_h5_filename}\"')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "## DO NOT allow recompute if the file doesn't exist!!\n",
    "# Computing loaded session pickle file results : \"W:/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_withNewComputedReplays.pkl\"... done.\n",
    "# Failure loading W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle_withNewComputedReplays.pkl.\n",
    "# proposed_load_pkl_path = basedir.joinpath(active_pickle_filename).resolve()\n",
    "\n",
    "## INPUTS: widget.active_global_pkl, widget.active_global_pkl\n",
    "\n",
    "if active_session_pickle_file_widget.active_global_pkl is None:\n",
    "    skip_global_load: bool = True\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skip_global_load: {skip_global_load}')\n",
    "else:\n",
    "    skip_global_load: bool = False\n",
    "    override_global_computation_results_pickle_path = active_session_pickle_file_widget.active_global_pkl.resolve()\n",
    "    Assert.path_exists(override_global_computation_results_pickle_path)\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "proposed_load_pkl_path = active_session_pickle_file_widget.active_local_pkl.resolve()\n",
    "Assert.path_exists(proposed_load_pkl_path)\n",
    "proposed_load_pkl_path\n",
    "\n",
    "custom_suffix: str = active_session_pickle_file_widget.try_extract_custom_suffix()\n",
    "print(f'custom_suffix: \"{custom_suffix}\"')\n",
    "\n",
    "## OUTPUTS: custom_suffix, proposed_load_pkl_path, (override_global_computation_results_pickle_path, skip_global_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_name_includelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eef26a",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: proposed_load_pkl_path\n",
    "# assert proposed_load_pkl_path.exists(), f\"for a saved custom the file must exist, but proposed_load_pkl_path: '{proposed_load_pkl_path}' does not!\"\n",
    "\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "\n",
    "with set_posix_windows():\n",
    "    curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                            computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                            saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                            skip_extended_batch_computations=True, debug_print=False, fail_on_exception=False, active_pickle_filename=proposed_load_pkl_path, \n",
    "                                            override_parameters_flat_keypaths_dict=override_parameters) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "print(f'Pipeline loaded from custom pickle!!')\n",
    "## OUTPUT: curr_active_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_session_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61969df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data_mode_name = 'bapun'\n",
    "\n",
    "# curr_active_pipeline.get_session_additional_parameters_context()\n",
    "# curr_active_pipeline.session_data_type\n",
    "\n",
    "DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict()[active_data_mode_name]\n",
    "DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()[active_data_mode_name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488363cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.BaseDataSessionFormats import HardcodedProcessingParameters\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "\n",
    "hardcoded_params: HardcodedProcessingParameters = BapunDataSessionFormatRegisteredClass._get_session_specific_parameters(session_context=curr_active_pipeline.get_session_context())\n",
    "hardcoded_params\n",
    "\n",
    "hardcoded_params.decoder_building_session_names\n",
    "hardcoded_params.non_global_activity_session_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@define(slots=False, eq=False, repr=False)\n",
    "class position_decoding_Parameters(HDF_SerializationMixin, AttrsBasedClassHelperMixin, BaseGlobalComputationParameters):\n",
    "    \"\"\" Docstring for position_decoding_Parameters. \n",
    "    \"\"\"\n",
    "    override_decoding_time_bin_size: Optional[float] = serialized_attribute_field(default=None)\n",
    "    ## PARAMS - these are class properties\n",
    "    override_decoding_time_bin_size_PARAM = param.Number(default=None, doc='override_decoding_time_bin_size param', label='override_decoding_time_bin_size')\n",
    "    # HDFMixin Conformances ______________________________________________________________________________________________ #\n",
    "    def to_hdf(self, file_path, key: str, **kwargs):\n",
    "        \"\"\" Saves the object to key in the hdf5 file specified by file_path\"\"\"\n",
    "        super().to_hdf(file_path, key=key, **kwargs)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "session_specific_parameters: Dict[IdentifyingContext, HardcodedProcessingParameters] = {\n",
    "\n",
    "    IdentifyingContext(format_name= 'bapun', animal= 'RatN', session_name= 'Day4OpenField'): HardcodedProcessingParameters(decoder_building_session_names=['roam', 'sprinkle', 'maze_GLOBAL'],\n",
    "                                                                                                                           ),\n",
    "                                                                                                                        \n",
    "    IdentifyingContext(format_name= 'bapun', animal= 'RatN', session_name= 'Day5TwoNovel'): HardcodedProcessingParameters(decoder_building_session_names=['maze1', 'maze2', 'maze_GLOBAL'],\n",
    "                                                                                                                           ),\n",
    "                                                                                                                    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.PipelineParameterClassTemplating import GlobalComputationParametersAttrsClassTemplating\n",
    "\n",
    "registered_merged_computation_function_default_kwargs_dict, code_str, nested_classes_dict, (imports_dict, imports_list, imports_string) = GlobalComputationParametersAttrsClassTemplating.main_generate_params_classes(curr_active_pipeline=curr_active_pipeline)\n",
    "\n",
    "code_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e10fea",
   "metadata": {},
   "source": [
    "## from `batch_load_session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From `pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing.batch_load_session` 2025-02-26 09:09 \n",
    "kwargs = {}\n",
    "epoch_name_includelist=None\n",
    "# active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "active_computation_functions_name_includelist=[]\n",
    "override_parameters_flat_keypaths_dict = override_parameters\n",
    "active_pickle_filename = proposed_load_pkl_path\n",
    "active_session_computation_configs = None\n",
    "fail_on_exception: bool = False\n",
    "\n",
    "saving_mode = PipelineSavingScheme.init(saving_mode)\n",
    "epoch_name_includelist = kwargs.get('epoch_name_includelist', ['maze1','maze2','maze'])\n",
    "# epoch_name_includelist = ['maze', 'sprinkle']\n",
    "# epoch_name_includelist = ['pre', 'maze', 'sprinkle', 'post']\n",
    "\n",
    "\n",
    "debug_print = kwargs.get('debug_print', False)\n",
    "assert 'skip_save' not in kwargs, f\"use saving_mode=PipelineSavingScheme.SKIP_SAVING instead\"\n",
    "# skip_save = kwargs.get('skip_save', False)\n",
    "# active_pickle_filename = kwargs.get('active_pickle_filename', 'loadedSessPickle.pkl')\n",
    "\n",
    "# active_session_computation_configs = kwargs.get('active_session_computation_configs', None)\n",
    "# computation_functions_name_includelist = kwargs.get('computation_functions_name_includelist', None)\n",
    "\n",
    "known_data_session_type_properties_dict = DataSessionFormatRegistryHolder.get_registry_known_data_session_type_dict(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "active_data_session_types_registered_classes_dict = DataSessionFormatRegistryHolder.get_registry_data_session_type_class_name_dict()\n",
    "\n",
    "active_data_mode_registered_class = active_data_session_types_registered_classes_dict[active_data_mode_name]\n",
    "active_data_mode_type_properties = known_data_session_type_properties_dict[active_data_mode_name]\n",
    "\n",
    "## Begin main run of the pipeline (load or execute):\n",
    "curr_active_pipeline = NeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties,\n",
    "    override_basepath=Path(basedir), force_reload=force_reload, active_pickle_filename=active_pickle_filename, skip_save_on_initial_load=True, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict)\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # should already be updated, but try it again anyway.\n",
    "\n",
    "was_loaded_from_file: bool =  curr_active_pipeline.has_associated_pickle # True if pipeline was loaded from an existing file, False if it was created fresh\n",
    "\n",
    "# Get the previous configs:\n",
    "# curr_active_pipeline.filtered_sessions\n",
    "# ['filtered_session_names', 'filtered_contexts', 'filtered_epochs', 'filtered_sessions']\n",
    "# loaded_session_filter_configurations = {k:v.filter_config['filter_function'] for k,v in curr_active_pipeline.active_configs.items()}\n",
    "# loaded_pipeline_computation_configs = {k:v.computation_config for k,v in curr_active_pipeline.active_configs.items()}\n",
    "\n",
    "\n",
    "## Build updated ones from the current configs:\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n",
    "if debug_print:\n",
    "    print(f'active_session_filter_configurations: {active_session_filter_configurations}')\n",
    "\n",
    "## Skip the filtering, it used to be performed bere but NOT NOW\n",
    "\n",
    "## TODO 2023-05-16 - set `curr_active_pipeline.active_configs[a_name].computation_config.pf_params.computation_epochs = curr_laps_obj` equivalent\n",
    "## TODO 2023-05-16 - determine appropriate binning from `compute_short_long_constrained_decoders` so it's automatically from the long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafaf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_session_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6081a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_name_includelist = kwargs.get('epoch_name_includelist', ['maze1','maze2','maze'])\n",
    "# epoch_name_includelist = ['roam', 'sprinkle']\n",
    "epoch_name_includelist = ['pre', 'roam', 'sprinkle', 'post']\n",
    "active_session_filter_configurations = active_data_mode_registered_class.build_default_filter_functions(sess=curr_active_pipeline.sess, epoch_name_includelist=epoch_name_includelist) # build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef85735",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_on_exception: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43096f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16a638",
   "metadata": {
    "tags": [
     "collect-2025-09-19"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "if active_session_computation_configs is None:\n",
    "    \"\"\"\n",
    "    If there are is provided computation config, get the default:\n",
    "    \"\"\"\n",
    "    # ## Compute shared grid_bin_bounds for all epochs from the global positions:\n",
    "    # global_unfiltered_session = curr_active_pipeline.sess\n",
    "    # # ((22.736279243974774, 261.696733348342), (49.989466271998936, 151.2870218547401))\n",
    "    # first_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[0]]\n",
    "    # # ((22.736279243974774, 261.696733348342), (125.5644705153173, 151.21507349463707))\n",
    "    # second_filtered_session = curr_active_pipeline.filtered_sessions[curr_active_pipeline.filtered_session_names[1]]\n",
    "    # # ((71.67666779621361, 224.37820920766043), (110.51617463644946, 151.2870218547401))\n",
    "\n",
    "    # grid_bin_bounding_session = first_filtered_session\n",
    "    # grid_bin_bounds = PlacefieldComputationParameters.compute_grid_bin_bounds(grid_bin_bounding_session.position.x, grid_bin_bounding_session.position.y)\n",
    "\n",
    "    ## OR use no grid_bin_bounds meaning they will be determined dynamically for each epoch:\n",
    "    # grid_bin_bounds = None\n",
    "    # time_bin_size = 0.03333 #1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = 0.1 # 10 fps\n",
    "    time_bin_size = kwargs.get('time_bin_size', 0.03333) # 0.03333 = 1.0/30.0 # decode at 30fps to match the position sampling frequency\n",
    "    # time_bin_size = kwargs.get('time_bin_size', 0.1) # 10 fps\n",
    "\n",
    "    # lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "    # assert lap_estimation_parameters is not None\n",
    "    active_session_computation_configs: List[DynamicContainer] = active_data_mode_registered_class.build_active_computation_configs(sess=curr_active_pipeline.sess, time_bin_size=time_bin_size, override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) # , grid_bin_bounds=grid_bin_bounds\n",
    "\n",
    "else:\n",
    "    # Use the provided `active_session_computation_configs`:\n",
    "    assert 'time_bin_size' not in kwargs, f\"time_bin_size kwarg provided but will not be used because a custom active_session_computation_configs was provided as well.\"\n",
    "\n",
    "active_session_computation_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation_functions_name_includelist = []\n",
    "computation_functions_name_includelist = ['pf_computation','firing_rate_trends', 'position_decoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Setup Computation Functions to be executed:\n",
    "if computation_functions_name_includelist is None:\n",
    "    # includelist Mode:\n",
    "    computation_functions_name_includelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        # '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]  # '_perform_pf_find_ratemap_peaks_peak_prominence2d_computation'\n",
    "    computation_functions_name_excludelist=None\n",
    "else:\n",
    "    print(f'using provided computation_functions_name_includelist: {computation_functions_name_includelist}')\n",
    "    computation_functions_name_excludelist=None\n",
    "\n",
    "## For every computation config we build a fake (duplicate) filter config).\n",
    "# OVERRIDE WITH TRUE:\n",
    "# curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps['use_direction_dependent_laps'] = True # override with True\n",
    "lap_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "assert lap_estimation_parameters is not None\n",
    "use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', False) # whether to split the laps into left and right directions\n",
    "# use_direction_dependent_laps: bool = lap_estimation_parameters.get('use_direction_dependent_laps', True) # whether to split the laps into left and right directions\n",
    "\n",
    "if (use_direction_dependent_laps or (len(active_session_computation_configs) > 3)):\n",
    "    lap_direction_suffix_list = ['_odd', '_even', '_any'] # ['maze1_odd', 'maze1_even', 'maze1_any', 'maze2_odd', 'maze2_even', 'maze2_any', 'maze_odd', 'maze_even', 'maze_any']\n",
    "    # lap_direction_suffix_list = ['_odd', '_even', ''] # no '_any' prefix, instead reuses the existing names\n",
    "    # assert len(lap_direction_suffix_list) == len(active_session_computation_configs), f\"len(lap_direction_suffix_list): {len(lap_direction_suffix_list)}, len(active_session_computation_configs): {len(active_session_computation_configs)}, \"\n",
    "else:\n",
    "    print(f'not using direction-dependent laps.')\n",
    "    lap_direction_suffix_list = ['']\n",
    "\n",
    "# active_session_computation_configs: this should contain three configs, one for each Epoch    \n",
    "active_session_computation_configs = [deepcopy(a_config) for a_config in active_session_computation_configs]\n",
    "\n",
    "#TODO 2024-10-30 13:22: - [ ] This is where we should override the params using `override_parameters_flat_keypaths_dict`\n",
    "# if override_parameters_flat_keypaths_dict is not None:\n",
    "# \tfor a_config in active_session_computation_configs:\n",
    "# \t\tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\ta_config.set_by_keypath(k, deepcopy(v))\n",
    "# \t\t\texcept Exception as e:\n",
    "# \t\t\t\t# raise e\n",
    "# \t\t\t\tprint(f'cannot set_by_keypath: {k} -- error: {e}. Skipping for now.')\n",
    "\n",
    "assert len(lap_direction_suffix_list) == len(active_session_computation_configs)\n",
    "updated_active_session_pseudo_filter_configs = {} # empty list, woot!\n",
    "\n",
    "\n",
    "for a_computation_suffix_name, a_computation_config in zip(lap_direction_suffix_list, active_session_computation_configs): # these should NOT be the same length: lap_direction_suffix_list: ['_odd', '_even', '_any']\n",
    "    # We need to filter and then compute with the appropriate config iteratively.\n",
    "    for a_filter_config_name, a_filter_config_fn in active_session_filter_configurations.items():\n",
    "        # TODO: Build a context:\n",
    "        a_combined_name: str = f'{a_filter_config_name}{a_computation_suffix_name}'\n",
    "        # if a_computation_suffix_name != '':\n",
    "        updated_active_session_pseudo_filter_configs[a_combined_name] = deepcopy(a_filter_config_fn) # this copy is just so that the values are recomputed with the appropriate config. This is a HACK\n",
    "    # end for filter_configs\n",
    "\n",
    "    ## Actually do the filtering now. We have \n",
    "    curr_active_pipeline.filter_sessions(updated_active_session_pseudo_filter_configs, changed_filters_ignore_list=['maze1','maze2','maze'], debug_print=False)\n",
    "\n",
    "    ## TODO 2023-01-15 - perform_computations for all configs!!\n",
    "    #TODO 2024-10-30 13:22: - [ ] This is where we should override the params\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tfor k, v in override_parameters_flat_keypaths_dict.items():\n",
    "    # \t\ta_filter_config_fn.set_by_keypath(k, deepcopy(v))\n",
    "\n",
    "    # if override_parameters_flat_keypaths_dict is not None:\n",
    "    # \tcurr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n",
    "    #TODO 2023-10-31 14:58: - [ ] This is where the computations are being done multiple times!\n",
    "    #TODO 2023-11-13 14:23: - [ ] With this approach, we can't actually properly filter the computation_configs for the relevant sessions ahead of time because they are calculated for a single computation config but across all sessions at once.\n",
    "    curr_active_pipeline.perform_computations(a_computation_config, computation_functions_name_includelist=computation_functions_name_includelist, computation_functions_name_excludelist=computation_functions_name_excludelist, fail_on_exception=fail_on_exception, debug_print=debug_print) #, overwrite_extant_results=False  ], fail_on_exception=True, debug_print=False)\n",
    "\n",
    "    if override_parameters_flat_keypaths_dict is not None:\n",
    "        curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict=override_parameters_flat_keypaths_dict) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_extended_batch_computations = False\n",
    "fail_on_exception = True\n",
    "if not skip_extended_batch_computations:\n",
    "    batch_extended_computations(curr_active_pipeline, include_global_functions=False, fail_on_exception=fail_on_exception, progress_print=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_computations(active_session_computation_configs[0], computation_functions_name_excludelist=['_perform_spike_burst_detection_computation'], debug_print=False, fail_on_exception=False) # includelist: ['_perform_baseline_placefield_computation']\n",
    "\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.prepare_for_display(root_output_dir=global_data_root_parent_path.joinpath('Output'), should_smooth_maze=True) # TODO: pass a display config\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to do `curr_active_pipeline.prepare_for_display(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    curr_active_pipeline.save_pipeline(saving_mode=saving_mode, active_pickle_filename=active_pickle_filename, override_pickle_path=kwargs.get('override_pickle_path', None))\n",
    "except Exception as e:\n",
    "    exception_info = sys.exc_info()\n",
    "    an_error = CapturedException(e, exception_info, curr_active_pipeline)\n",
    "    print(f'WARNING: Failed to save pipeline via `curr_active_pipeline.save_pipeline(...)` with error: {an_error}')\n",
    "    if fail_on_exception:\n",
    "        raise\n",
    "\n",
    "if not saving_mode.shouldSave:\n",
    "    print(f'saving_mode.shouldSave == False, so not saving at the end of batch_load_session')\n",
    "\n",
    "## Load pickled global computations:\n",
    "# If previously pickled global results were saved, they will typically no longer be relevent if the pipeline was recomputed. We need a system of invalidating/versioning the global results when the other computations they depend on change.\n",
    "# Maybe move into `batch_extended_computations(...)` or integrate with that somehow\n",
    "# curr_active_pipeline.load_pickled_global_computation_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e374c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "skip_global_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cb3e1",
   "metadata": {
    "tags": [
     "run-load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Global computations loading:                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "# Loads saved global computations that were saved out via: `custom_save_filepaths['global_computation_pkl'] = curr_active_pipeline.save_global_computation_results(override_global_pickle_filename=custom_save_filenames['global_computation_pkl'])`\n",
    "## INPUTS: custom_save_filenames\n",
    "## INPUTS: curr_active_pipeline, (override_global_computation_results_pickle_path, skip_global_load), extended_computations_include_includelist\n",
    "\n",
    "if skip_global_load:\n",
    "    override_global_computation_results_pickle_path = None\n",
    "    print(f'skipping global load because skip_global_load==True')\n",
    "else:\n",
    "    # override_global_computation_results_pickle_path = custom_save_filenames['global_computation_pkl']\n",
    "    print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "# Pre-load ___________________________________________________________________________________________________________ #\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list\n",
    "\n",
    "# Try Unpickling Global Computations to update pipeline ______________________________________________________________ #\n",
    "if (not force_reload) and (not skip_global_load): # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # INPUTS: override_global_computation_results_pickle_path\n",
    "        with set_posix_windows():\n",
    "            sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(override_global_computation_results_pickle_path=override_global_computation_results_pickle_path,\n",
    "                                                                                            allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist, ) # is new\n",
    "            print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "            did_any_paths_change: bool = curr_active_pipeline.post_load_fixup_sess_basedirs(updated_session_basepath=deepcopy(basedir)) ## use INPUT: basedir\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'force_reload: {force_reload}, saving_mode: {saving_mode}')\n",
    "force_reload\n",
    "saving_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e54ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: curr_active_pipeline.global_computation_results_pickle_path, skip_global_load\n",
    "## indicate that it was loaded with a custom suffix\n",
    "curr_active_pipeline.pickle_path ## correct\n",
    "curr_active_pipeline.global_computation_results_pickle_path ## correct\n",
    "\n",
    "print(f'override_pickle_path = \"{curr_active_pipeline.pickle_path}\",\\nactive_pickle_filename = \"{curr_active_pipeline.pickle_path.name}\"')\n",
    "print(f'override_global_pickle_path = \"{curr_active_pipeline.global_computation_results_pickle_path}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9674fd9",
   "metadata": {},
   "source": [
    "## OUTPUTS: `curr_active_pipeline`  0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣ 0️⃣0️⃣ RESUME Normal Pipeline Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f755b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 0️⃣ Shared Post-Pipeline load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "run-load",
     "run-main",
     "end-run"
    ]
   },
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Lab'\n",
    " \n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        BATCH_DATE_TO_USE = f'{BATCH_DATE_TO_USE}{custom_suffix}'\n",
    "        print(f'Adding custom suffix: \"{custom_suffix}\" - BATCH_DATE_TO_USE: \"{BATCH_DATE_TO_USE}\"')\n",
    "except NameError as err:\n",
    "    custom_suffix = None\n",
    "    print(f'NO CUSTOM SUFFIX.')\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                           r\"K:\\scratch\\collected_outputs\",\n",
    "                                                           '/Users/pho/data/collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "all",
     "run-load",
     "run-main",
     "end-run",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607a444",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# / 🛑 End Run Section 🛑\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🎨 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "all",
     "run-group-display",
     "run-spike_raster_window_test",
     "end-run",
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5423e",
   "metadata": {},
   "source": [
    "## `LauncherWidget`: GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df7ce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Display import DisplayFunctionItem\n",
    "from pyphocorehelpers.gui.Qt.tree_helpers import find_tree_item_by_text\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e48b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_str: str = curr_active_pipeline.get_complete_session_identifier_string()\n",
    "widget.setWindowTitle(f'Spike3D Launcher: {session_id_str}')\n",
    "treeWidget.root\n",
    "# curr_active_pipeline.get_session_additional_parameters_context()\n",
    "# curr_active_pipeline.get_complete_session_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84196484",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = dict()\n",
    "_out['_display_3d_interactive_tuning_curves_plotter'] = curr_active_pipeline.display(display_function='_display_3d_interactive_tuning_curves_plotter', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatN',session_name='Day4OpenField',filter_name='roam')) # _display_3d_interactive_tuning_curves_plotter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out['_display_3d_interactive_tuning_curves_plotter_sprinkle'] = curr_active_pipeline.display(display_function='_display_3d_interactive_tuning_curves_plotter', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatN',session_name='Day4OpenField',filter_name='sprinkle')) # _display_3d_interactive_tuning_curves_plotter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3119ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_output = _out['_display_3d_interactive_tuning_curves_plotter']\n",
    "# a_pf_pyvista_plotter = display_output['ipcDataExplorer']\n",
    "display_output['plotter']\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if auto_update is available and enable it\n",
    "if hasattr(display_output['plotter'], 'auto_update'):\n",
    "    display_output['plotter'].auto_update = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c294a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.peak_prominences import render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter\n",
    "\n",
    "display_output = {}\n",
    "active_config_name = 'roam'\n",
    "print(f'active_config_name: {active_config_name}')\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "pActiveTuningCurvesPlotter = None\n",
    "display_output = display_output | curr_active_pipeline.display('_display_3d_interactive_tuning_curves_plotter', active_config_name, extant_plotter=display_output.get('pActiveTuningCurvesPlotter', None), panel_controls_mode='Qt', should_nan_non_visited_elements=False, zScalingFactor=2000.0) # Works now!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf76728",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: curr_active_pipeline, active_config_name\n",
    "active_config_name = 'roam'\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "if active_peak_prominence_2d_results is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=[active_config_name], fail_on_exception=True, debug_print=True)\n",
    "    # curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=[short_LR_name, short_RL_name, long_any_name, short_any_name], fail_on_exception=False, debug_print=False) # or at least\n",
    "    active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "    assert active_peak_prominence_2d_results is not None, f\"bad even after computation\"\n",
    "\n",
    "# active_peak_prominence_2d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b751a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_peak_prominence_2d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33de028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.peak_prominences import render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter\n",
    "\n",
    "ipcDataExplorer = display_output['ipcDataExplorer']\n",
    "if 'pActiveTuningCurvesPlotter' not in display_output:\n",
    "    display_output['pActiveTuningCurvesPlotter'] = display_output.pop('plotter') # rename the key from the generic \"plotter\" to \"pActiveSpikesBehaviorPlotter\" to avoid collisions with others\n",
    "pActiveTuningCurvesPlotter = display_output['pActiveTuningCurvesPlotter']\n",
    "root_dockAreaWindow, placefieldControlsContainerWidget, pf_widgets = display_output['pane'] # for Qt mode\n",
    "\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter(ipcDataExplorer, active_peak_prominence_2d_results, debug_print=True, \n",
    "    include_contour_bounding_box=True,\n",
    "    include_text_labels=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can toggle them\n",
    "neuron_id = 4\n",
    "peaks = ipcDataExplorer.plots['tuningCurvePlotActors'][neuron_id].peaks\n",
    "peaks.contours.SetVisibility(1)  # Show contours\n",
    "peaks.boxes.SetVisibility(1)\n",
    "peaks.text.SetVisibility(1)\n",
    "peaks.peak_points.SetVisibility(1)\n",
    "ipcDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f95397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access peaks for a specific neuron\n",
    "neuron_id = 9\n",
    "peaks = ipcDataExplorer.plots['tuningCurvePlotActors'][neuron_id].peaks\n",
    "\n",
    "# # Toggle individual categories\n",
    "# peaks.contours.SetVisibility(1)  # Show contours\n",
    "# peaks.boxes.SetVisibility(1)     # Hide bounding boxes\n",
    "# peaks.text.SetVisibility(1)       # Hide text labels\n",
    "# peaks.peak_points.SetVisibility(1) # Show peak points\n",
    "\n",
    "# Or toggle all at once\n",
    "peaks.SetVisibility(1)  # Show everything\n",
    "# peaks.SetVisibility(0)   # Hide everything\n",
    "\n",
    "# Update the display\n",
    "ipcDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ipcDataExplorer.plots['tuningCurvePlotActors'])\n",
    "dict(ipcDataExplorer.plots['tuningCurvePlotActors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron_id, a_neuron_impl in dict(ipcDataExplorer.plots['tuningCurvePlotActors']).items():\n",
    "    if a_neuron_impl is not None:\n",
    "        print(neuron_id)\n",
    "        print(a_neuron_impl)\n",
    "\n",
    "        peaks = a_neuron_impl.peaks\n",
    "        # peaks.SetVisibility(0)   # Hide everything\n",
    "        peaks.contours.SetVisibility(1)  # Show contours\n",
    "        # peaks.boxes.SetVisibility(1)     # Hide bounding boxes\n",
    "        # peaks.text.SetVisibility(1)       # Hide text labels\n",
    "        # peaks.peak_points.SetVisibility(1) # Show peak points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_peak_prominence_2d_results.filtered_flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_peak_prominence_2d_results.flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d0e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.peak_prominences import render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter\n",
    "\n",
    "display_output = _out['_display_3d_interactive_tuning_curves_plotter']\n",
    "ipcDataExplorer = display_output['ipcDataExplorer']\n",
    "if 'pActiveTuningCurvesPlotter' not in display_output:\n",
    "    display_output['pActiveTuningCurvesPlotter'] = display_output.pop('plotter') # rename the key from the generic \"plotter\" to \"pActiveSpikesBehaviorPlotter\" to avoid collisions with others\n",
    "pActiveTuningCurvesPlotter = display_output['pActiveTuningCurvesPlotter']\n",
    "root_dockAreaWindow, placefieldControlsContainerWidget, pf_widgets = display_output['pane'] # for Qt mode\n",
    "\n",
    "# slab_results_dict: Dict[Tuple, SlabResult] = {k:SlabResult(**a_slab_result_dict) for k, a_slab_result_dict in simplified_obj.results.items()}\n",
    "\n",
    "# active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "\n",
    "active_peak_prominence_2d_results = simplified_obj # slab_results_dict: Dict[Tuple, SlabResult] = {k:SlabResult(**a_slab_result_dict) for k, a_slab_result_dict in simplified_obj.results.items()}\n",
    "render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter(ipcDataExplorer, active_peak_prominence_2d_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5dc3b3",
   "metadata": {},
   "source": [
    "## ✅ 2025-09-19 - Clean programmmatic figure outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c3103",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.figure_management import PhoActiveFigureManager2D, capture_new_figures_decorator\n",
    "fig_man = PhoActiveFigureManager2D(name=f'fig_man') # Initialize a new figure manager\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import programmatic_render_to_file, programmatic_display_to_PDF, extract_figures_from_display_function_output\n",
    "\n",
    "# fig_man.close_all()\n",
    "\n",
    "# subset_includelist = ['maze1', 'maze2', 'maze_GLOBAL'] # Day5TwoNovel\n",
    "# subset_includelist = ['roam', 'sprinkle'] # Day4\n",
    "\n",
    "subset_includelist = hardcoded_params.decoder_building_session_names\n",
    "print(f'subset_includelist: {subset_includelist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e400f3",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "# display_fn_kwargs = dict(subplots=(None, 9))\n",
    "display_fn_kwargs = dict(subplots=(None, 7))\n",
    "\n",
    "# _out = dict()\n",
    "# _out['_display_2d_placefield_result_plot_ratemaps_2D'] = curr_active_pipeline.display(display_function='_display_2d_placefield_result_plot_ratemaps_2D', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatS',session_name='Day5TwoNovel',filter_name='maze1'), **display_fn_kwargs) # _display_2d_placefield_result_plot_ratemaps_2D\n",
    "# _out['_display_2d_placefield_result_plot_ratemaps_2D'] = curr_active_pipeline.display(display_function='_display_2d_placefield_result_plot_ratemaps_2D', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatS',session_name='Day5TwoNovel',filter_name='maze2'), **display_fn_kwargs) # _display_2d_placefield_result_plot_ratemaps_2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a03d6",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "_out_list = programmatic_render_to_file(curr_active_pipeline=curr_active_pipeline, curr_display_function_name='_display_2d_placefield_result_plot_ratemaps_2D', subset_includelist=subset_includelist, \n",
    "                                        write_vector_format=True, write_png=True, debug_print=True, **display_fn_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a3369",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "_out_list = programmatic_render_to_file(curr_active_pipeline=curr_active_pipeline, curr_display_function_name='_display_2d_placefield_occupancy', subset_includelist=subset_includelist, \n",
    "                                        write_vector_format=True, write_png=True, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a240b9",
   "metadata": {},
   "source": [
    "## `Spike3DRasterWindowWidget` Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8b0e1",
   "metadata": {
    "tags": [
     "active-2025-09-21",
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "from neuropy.core.flattened_spiketrains import SpikesAccessor\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.helpers import ScrollableRasterViewOwnerMixin\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _setup_spike_raster_window_for_debugging\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import ScatterItemData # used in `NewSimpleRaster`\n",
    "\n",
    "active_spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "\n",
    "# INLINEING `build_spikes_data_values_from_df`: ______________________________________________________________________ #\n",
    "# curr_spike_x, curr_spike_y, curr_spike_pens, all_scatterplot_tooltips_kwargs, all_spots, curr_n = cls.build_spikes_data_values_from_df(spikes_df, config_fragile_linear_neuron_IDX_map, is_spike_included=is_spike_included, should_return_data_tooltips_kwargs=should_return_data_tooltips_kwargs, **kwargs)\n",
    "# All units at once approach:\n",
    "active_time_variable_name = active_spikes_df.spikes.time_variable_name\n",
    "print(f'active_time_variable_name: {active_time_variable_name}')\n",
    "if active_time_variable_name != 't': \n",
    "    active_spikes_df = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(active_spikes_df, required_columns_synonym_dict={\"t\":{active_time_variable_name,'t_rel_seconds', 't_seconds'}})\n",
    "    active_spikes_df = active_spikes_df.drop(columns=[active_time_variable_name], inplace=False) ## drop the old column    \n",
    "    active_time_variable_name = 't' ## get the new one\n",
    "    active_spikes_df.spikes.set_time_variable_name('t')\n",
    "    # default_datapoint_column_names = [active_spikes_df.spikes.time_variable_name, 'aclu', 'fragile_linear_neuron_IDX']\n",
    "    # active_datapoint_column_names = default_datapoint_column_names\n",
    "    active_spikes_df\n",
    "    \n",
    "# active_spikes_df.spikes.time_variable_name\n",
    "# active_spikes_df\n",
    "\n",
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
    "# spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True, active_session_configuration_context='maze_GLOBAL')\n",
    "spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True, active_session_configuration_context='maze_GLOBAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa609047",
   "metadata": {
    "tags": [
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_bapun_proper_epoch_intervals, build_bapun_all_epochs_df\n",
    "\n",
    "a_rect_item, an_interval_ds = build_bapun_proper_epoch_intervals(curr_active_pipeline=curr_active_pipeline, active_2d_plot=active_2d_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5aa341",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_interval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a147fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.update_epochs_from_configs_widget()\n",
    "# update_rendered_intervals_visualization_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a34551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch Display Configs Update Dictionary\n",
    "epoch_display_configs_update_dict = {\n",
    "    'SessionEpochs': dict(y_location=-1.0, height=0.9, pen_color='#490000', pen_opacity=1.0, brush_color='#f51616', brush_opacity=1.0),\n",
    "    'custom_paradigm': [\n",
    "        dict(y_location=0.0, height=1.0, pen_color='#ffffff', pen_opacity=1.0, brush_color='#1f77b4', brush_opacity=1.0),\n",
    "        dict(y_location=0.0, height=1.0, pen_color='#ffffff', pen_opacity=1.0, brush_color='#d62728', brush_opacity=1.0),\n",
    "        dict(y_location=0.0, height=1.0, pen_color='#ffffff', pen_opacity=1.0, brush_color='#f7b6d2', brush_opacity=1.0),\n",
    "        dict(y_location=0.0, height=1.0, pen_color='#ffffff', pen_opacity=1.0, brush_color='#9edae5', brush_opacity=1.0),\n",
    "    ],\n",
    "    'Laps': dict(y_location=-2.0, height=0.9, pen_color='#ff0000', pen_opacity=1.0, brush_color='#ff0000', brush_opacity=1.0),\n",
    "    'PBEs': dict(y_location=-3.0, height=0.9, pen_color='#ffffff', pen_opacity=1.0, brush_color='#808080', brush_opacity=1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch Display Configs Update Dictionary\n",
    "epoch_display_configs_update_dict = {\n",
    "    'epoch_1': dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "    'epoch_2': dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "    'epoch_3': [\n",
    "        dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "        dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "        dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "        dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "    ],\n",
    "    'epoch_4': dict(y_location=-12.0, height=7.5, pen_color='#00ffff', pen_opacity=0.8, brush_color='#00ffff', brush_opacity=0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170607c8",
   "metadata": {
    "tags": [
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "# preview_overview_scatter_plot: pg.ScatterPlotItem  = active_2d_plot.plots.preview_overview_scatter_plot # ScatterPlotItem \n",
    "# preview_overview_scatter_plot.setDownsampling(auto=True, method='subsample', dsRate=10)\n",
    "# main_graphics_layout_widget: pg.GraphicsLayoutWidget = active_2d_plot.ui.main_graphics_layout_widget\n",
    "wrapper_layout: pg.QtWidgets.QVBoxLayout = active_2d_plot.ui.wrapper_layout\n",
    "# main_content_splitter = active_2d_plot.ui.main_content_splitter # QSplitter\n",
    "layout = active_2d_plot.ui.layout\n",
    "background_static_scroll_window_plot = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "# active_window_container_layout = active_2d_plot.ui.active_window_container_layout # GraphicsLayout, first item of `main_graphics_layout_widget` -- just the active raster window I think, there is a strange black space above it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b1ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_menu = active_2d_plot._menuContextAddRenderable\n",
    "a_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735039ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c979d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('active_2d_plot', active_2d_plot, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.get_leaf_only_flat_dock_identifiers_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "identifer_str: str = 'intervals'\n",
    "# identifer_str: str = 'new_curves_separate_plot'\n",
    "# identifer_str: str = 'newDockedWidget'\n",
    "a_dock, widget = active_2d_plot.find_dock_item_tuple(identifer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12af674",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_graphics_layout_widget = widget.getRootGraphicsLayoutWidget()\n",
    "root_graphics_layout_widget\n",
    "plot_item = widget.getRootPlotItem()\n",
    "plot_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_parent_menu = widget.menu\n",
    "active_parent_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92737fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot._menuContextAddRenderable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtWidgets import QMenu, QAction\n",
    "from pyphoplacecellanalysis.GUI.Qt.Menus.LocalMenus_AddRenderable.LocalMenus_AddRenderable import LocalMenus_AddRenderable\n",
    "\n",
    "\n",
    "# Create your custom menu\n",
    "custom_menu = QMenu(\"My Custom Menu\")\n",
    "action1 = QAction(\"Action 1\", custom_menu)\n",
    "action1.triggered.connect(lambda: print(\"Action 1 clicked\"))\n",
    "custom_menu.addAction(action1)\n",
    "\n",
    "action2 = QAction(\"Action 2\", custom_menu)\n",
    "action2.triggered.connect(lambda: print(\"Action 2 clicked\"))\n",
    "custom_menu.addAction(action2)\n",
    "\n",
    "action3 = QAction(\"Action 3\", custom_menu)\n",
    "action3.triggered.connect(lambda: print(\"Action 3 clicked\"))\n",
    "custom_menu.addAction(action3)\n",
    "\n",
    "action4 = QAction(\"Action 4\", custom_menu)\n",
    "action4.triggered.connect(lambda: print(\"Action 4 clicked\"))\n",
    "custom_menu.addAction(action4)\n",
    "\n",
    "\n",
    "# LocalMenus_AddRenderable._helper_append_custom_menu_to_widget_context_menu_universal(parent_widget=new_curves_separate_plot, additional_menu=self._menuContextAddRenderable)\n",
    "parent_widget = widget # works for MatplotlibTimeSynchronizedWidget \n",
    "\n",
    "LocalMenus_AddRenderable._helper_append_custom_menu_to_widget_context_menu_universal(parent_widget=widget, additional_menu=custom_menu, debug_print=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305492ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For a PlotItem\n",
    "# plot_item = pg.PlotItem()\n",
    "## INPUTS: plot_item\n",
    "# Option A: Ensure menu is enabled (if it was disabled)\n",
    "if hasattr(plot_item, 'vb'):\n",
    "    plot_item.vb.setMenuEnabled(True)  # This will create the menu if it doesn't exist\n",
    "    \n",
    "    # Now check if menu exists\n",
    "    if plot_item.vb.menu is not None:\n",
    "        plot_item.vb.menu.addSeparator()\n",
    "        plot_item.vb.menu.addMenu(custom_menu)\n",
    "    else:\n",
    "        print(\"Warning: Menu is still None after enabling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For a ViewBox directly\n",
    "# viewbox = pg.ViewBox()\n",
    "viewbox = plot_item.vb\n",
    "if hasattr(viewbox, 'menu'):\n",
    "    viewbox.menu.addSeparator()\n",
    "    viewbox.menu.addAction(action1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920aa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For a ViewBox directly\n",
    "viewbox = pg.ViewBox()\n",
    "if hasattr(viewbox, 'menu'):\n",
    "    viewbox.menu.addSeparator()\n",
    "    viewbox.menu.addAction(action1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyQtInspect as pyqtinsp\n",
    "\n",
    "pyqtinsp.pqi.set_widget_highlight(widget=active_2d_plot, highlight=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(active_2d_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b71f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('active_2d_plot.ui', active_2d_plot.ui, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spike_raster_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_window.ui.wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.add_docked_marginal_track(curr_active_pipeline.sess.epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.list_all_rendered_intervals()\n",
    "active_2d_plot.add_laps_intervals(curr_active_pipeline.sess)\n",
    "\n",
    "active_2d_plot.add_rendered_intervals()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.sess.epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56888f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import SessionEpochs2DRenderTimeEpochs\n",
    "from neuropy.core.epoch import ensure_dataframe, ensure_Epoch, Epoch, EpochsAccessor\n",
    "# SessionEpochs2DRenderTimeEpochs.add_render_time_epochs(curr_sess=curr_active_pipeline.sess.epochs, destination_plot=active_2d_plot)\n",
    "\n",
    "active_ds = ensure_dataframe(curr_active_pipeline.sess.epochs)\n",
    "active_ds\n",
    "\n",
    "_out = active_2d_plot.add_rendered_intervals(active_ds, 'SessionEpochs')\n",
    "\n",
    "# num_epochs = len(curr_active_pipeline.sess.epochs)\n",
    "\n",
    "# pen_colors = {'pre': pg.mkColor('purple'), 'roam': pg.mkColor('red'), 'sprinkle': pg.mkColor('red'), 'post': pg.mkColor('purple')}\n",
    "# brush_colors = {'pre': pg.mkColor('purple'), 'roam': pg.mkColor('red'), 'sprinkle': pg.mkColor('red'), 'post': pg.mkColor('purple')}\n",
    "\n",
    "# pen_color = list(pen_colors.values())\n",
    "# brush_color = list(brush_colors.values())\n",
    "# for a_pen_color in pen_color:\n",
    "#     a_pen_color.setAlphaF(0.8)\n",
    "\n",
    "# for a_brush_color in brush_color:\n",
    "#     a_brush_color.setAlphaF(0.5)\n",
    "\n",
    "\n",
    "# active_df = cls._update_df_visualization_columns(active_df, y_location, height, pen_color, brush_color, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.rendered_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571754a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ds = ensure_dataframe(curr_active_pipeline.sess.epochs)\n",
    "global_epoch_only = ensure_Epoch(active_ds[active_ds['label'] == 'maze_GLOBAL'])\n",
    "global_epoch_only\n",
    "\n",
    "curr_active_pipeline.sess\n",
    "sess.position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359264c",
   "metadata": {},
   "source": [
    "## 2025-09-19 - Add Session Paradigm Epochs with a different color for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183719d",
   "metadata": {
    "tags": [
     "run-spike_raster_window_test"
    ]
   },
   "outputs": [],
   "source": [
    "## 2025-09-19 - Add Session Paradigm Epochs with a different color for each session\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_bapun_proper_epoch_intervals, build_bapun_all_epochs_df\n",
    "\n",
    "a_rect_item, an_interval_ds = build_bapun_proper_epoch_intervals(curr_active_pipeline=curr_active_pipeline, active_2d_plot=active_2d_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_rect_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot: Spike2DRaster = active_2d_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.perform_remove_epoch_intervals('SessionEpochs') ## not correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.remove_rendered_intervals('SessionEpochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397bc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x_min, curr_x_max, curr_y_min, curr_y_max = active_2d_plot.get_render_intervals_plot_range()\n",
    "# (curr_x_min, curr_x_max, curr_y_min, curr_y_max) # (0.14663333333333334, 25986.969433333332, -5.0, 41.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_series_positioning_dfs, all_series_compressed_positioning_dfs, all_series_compressed_positioning_update_dicts = active_2d_plot.recover_interval_datasources_update_dict_properties()\n",
    "# all_series_positioning_dfs\n",
    "all_series_compressed_positioning_dfs # ERROR: series_compressed_positioning_update_dict is None for custom_paradigm. it will not be represented in the output dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_configs_df = active_2d_plot.extract_interval_display_config_df()\n",
    "out_configs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.Datasources.IntervalDatasource import IntervalsDatasource\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.epochs_plotting_mixins import EpochDisplayConfig\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.helpers import RectangleRenderTupleHelpers\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Render2DEventRectanglesHelper import Render2DEventRectanglesHelper\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColorDataframeColumnHelpers, ColorFormatConverter, QColorColumnsAccessor\n",
    "\n",
    "custom_paradigm_ds: IntervalsDatasource = active_2d_plot.interval_datasources.custom_paradigm\n",
    "custom_paradigm_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_configs_df = active_2d_plot.extract_interval_display_config_df()\n",
    "out_configs_df\n",
    "added_col_names_map = out_configs_df.qcolor.convert_QColor_columns_to_hexcolor_columns()\n",
    "# added_col_names_map\n",
    "out_configs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_configs_dict = active_2d_plot.extract_interval_display_config_lists()\n",
    "out_configs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2826a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_paradigm_ds_df = deepcopy(custom_paradigm_ds._df)  # [''\n",
    "custom_paradigm_ds_df\n",
    "\n",
    "# ['lap_color', 'lap_accent_color'] ## str (hex-color), str (hex-color)\n",
    "# ['pen_color', 'brush_color'] ## QColor, QColor\n",
    "# ['pen', 'brush'] # QPen, QBrush\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b080ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_paradigm_ds_df.qcolor.find_valid_hex_columns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "added_col_names_map = custom_paradigm_ds_df.qcolor.convert_QColor_columns_to_hexcolor_columns()\n",
    "added_col_names_map\n",
    "# initial_labels: List[str] = deepcopy(list(custom_paradigm_ds_df.columns))\n",
    "\n",
    "# def is_valid_hex_label(a_label: str) -> bool:\n",
    "# \tif a_label.startswith('#')\n",
    "    \n",
    "# ColorFormatConverter.is_valid_hexstring(\n",
    "\n",
    "# extant_hex_color_labels = [k for k in initial_labels if k.endswith('_hex')]\n",
    "\n",
    "# extant_hex_color_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_paradigm_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c784b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_paradigm_ds_df['pen'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_viz_df = deepcopy(custom_paradigm_ds_df)\n",
    "series_viz_df['pen_color_hex'] = series_viz_df['pen'].map(lambda x: \"#\" + ColorDataframeColumnHelpers.QPen_to_dict(x)['color']).str.upper()\n",
    "series_viz_df['pen_width'] = series_viz_df['pen'].map(lambda x: ColorDataframeColumnHelpers.QPen_to_dict(x)['width'])\n",
    "series_viz_df['brush_color_hex'] = series_viz_df['brush'].map(lambda x: \"#\" + ColorDataframeColumnHelpers.QBrush_to_dict(x)['color']).str.upper()\n",
    "# series_viz_df['brush_color_hex'] = series_viz_df['brush_color_hex'].str.upper()\n",
    "series_viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67fa8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Render2DEventRectanglesHelper import Render2DEventRectanglesHelper\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.EpochRenderConfigWidget.EpochRenderConfigWidget import EpochRenderConfigsListWidget, EpochRenderConfigWidget\n",
    "\n",
    "an_epochs_display_list_widget: EpochRenderConfigsListWidget = active_2d_plot.ui.get('epochs_render_configs_widget', None)\n",
    "if an_epochs_display_list_widget is None:\n",
    "    # create a new one:    \n",
    "    raise NotImplementedError\n",
    "\n",
    "update_dict = an_epochs_display_list_widget.config_dicts_from_states()\n",
    "update_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brush_colors = [v['brush_color'] for v in update_dict['custom_paradigm']]\n",
    "# pen_colors = [v['pen_color'] for v in update_dict['custom_paradigm']]\n",
    "\n",
    "brush_colors_hex = [ColorFormatConverter.qColor_to_hexstring(v['brush_color'], include_alpha=True, use_HexArgb_instead_of_HexRGBA=False) for v in update_dict['custom_paradigm']]\n",
    "pen_colors_hex = [ColorFormatConverter.qColor_to_hexstring(v['pen_color'], include_alpha=True, use_HexArgb_instead_of_HexRGBA=False) for v in update_dict['custom_paradigm']]\n",
    "\n",
    "\n",
    "\n",
    "pen_colors_hex\n",
    "brush_colors_hex\n",
    "\n",
    "# pen_colors\n",
    "# brush_colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d58458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.misc import split_list_of_dicts\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsObjects.IntervalRectsItem import IntervalRectsItem, IntervalRectsItemData\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import General2DRenderTimeEpochs\n",
    "\n",
    "\n",
    "def _fixed_for_multi_update_df_visualization_columns(active_df: pd.DataFrame, y_location=None, height=None, pen_color=None, brush_color=None, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\" updates the columns of the provided active_df given the values specified. If values aren't provided, they aren't changed. \n",
    "        \n",
    "        active_df['series_vertical_offset', 'series_height', 'pen', 'brush']\n",
    "        \n",
    "        \"\"\"        \n",
    "        # Update only the provided columns while leaving the others intact\n",
    "        if y_location is not None:\n",
    "            ## y_location:\n",
    "            if isinstance(y_location, (list, tuple)):\n",
    "                active_df['series_vertical_offset'] = kwargs.setdefault('series_vertical_offset', [a_y_location for a_y_location in y_location])\n",
    "            else:\n",
    "                # Scalar value assignment:\n",
    "                active_df['series_vertical_offset'] = kwargs.setdefault('series_vertical_offset', y_location)\n",
    "                \n",
    "        if height is not None:\n",
    "            ## series_height:\n",
    "            if isinstance(height, (list, tuple)):\n",
    "                active_df['series_height'] = kwargs.setdefault('series_height', [a_height for a_height in height])\n",
    "            else:\n",
    "                # Scalar value assignment:\n",
    "                active_df['series_height'] = kwargs.setdefault('series_height', height)\n",
    "\n",
    "        if pen_color is not None:\n",
    "            ## pen_color:\n",
    "            if isinstance(pen_color, (list, tuple)):\n",
    "                active_df['pen'] = kwargs.setdefault('pen', [pg.mkPen(a_pen_color) for a_pen_color in pen_color])\n",
    "            else:\n",
    "                # Scalar value assignment:\n",
    "                active_df['pen'] = kwargs.setdefault('pen', pg.mkPen(pen_color)) \n",
    "            \n",
    "        if brush_color is not None:\n",
    "            ## brush_color:\n",
    "            if isinstance(brush_color, (list, tuple)):\n",
    "                active_df['brush'] = kwargs.setdefault('brush', [pg.mkBrush(a_color) for a_color in brush_color])  \n",
    "            else:\n",
    "                # Scalar value assignment:\n",
    "                active_df['brush'] = kwargs.setdefault('brush', pg.mkBrush(brush_color))\n",
    "        \n",
    "        return active_df #, kwargs\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "interval_key: str = 'custom_paradigm'\n",
    "# custom_paradigm_update_dict = \n",
    "interval_update_kwargs = update_dict[interval_key]\n",
    "# Extract visibility settings before updating datasource (handle both single dict and list of dicts)\n",
    "visibility_settings = None\n",
    "if isinstance(interval_update_kwargs, (list, tuple)):\n",
    "    ## list of update dicts - each item can have its own isVisible property\n",
    "    a_list_interval_update_kwargs = []\n",
    "    visibility_settings = []\n",
    "    for a_sub_interval_update_kwargs in interval_update_kwargs:\n",
    "        if not isinstance(a_sub_interval_update_kwargs, dict):\n",
    "            a_sub_interval_update_kwargs = a_sub_interval_update_kwargs.to_dict() # deal with EpochDisplayConfig \n",
    "        a_list_interval_update_kwargs.append(a_sub_interval_update_kwargs)\n",
    "        # Extract visibility from each item (can be None if not specified)\n",
    "        visibility_settings.append(a_sub_interval_update_kwargs.get('isVisible', None))\n",
    "        # self.interval_datasources[interval_key].update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, **(a_sub_interval_update_kwargs | kwargs))) ## Fully inline\n",
    "    ## END for a_sub_interval_update_kwargs in interval_update_kwargs...\n",
    "    \n",
    "    ## Update with list\n",
    "    # a_list_interval_update_kwargs = [a_sub_interval_update_kwargs for a_sub_interval_update_kwargs in interval_update_kwargs]\n",
    "    print(f'a_sub_interval_update_kwargs: {a_sub_interval_update_kwargs}')\n",
    "    print(f'a_list_interval_update_kwargs: {a_list_interval_update_kwargs}')\n",
    "    # active_2d_plot.interval_datasources[interval_key].update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, **(a_sub_interval_update_kwargs | kwargs))) ## Fully inline\n",
    "    active_2d_plot.interval_datasources[interval_key].update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, **(split_list_of_dicts(a_list_interval_update_kwargs) | kwargs))) ##  Fixed for multiple lists\n",
    "    ## #TODO 2026-02-02 13:13: - [ ] `visibility_settings` is never used.\n",
    "\n",
    "else:\n",
    "    ## single update item dict\n",
    "    if not isinstance(interval_update_kwargs, dict):\n",
    "        interval_update_kwargs = interval_update_kwargs.to_dict() # deal with EpochDisplayConfig \n",
    "    visibility_settings = interval_update_kwargs.get('isVisible', None)\n",
    "    print(f'interval_update_kwargs: {interval_update_kwargs}')\n",
    "    active_2d_plot.interval_datasources[interval_key].update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, **(interval_update_kwargs | kwargs))) ## Fully inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "visibility_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list_interval_update_kwargs ## convert from a list of dict to a dict-of-lists\n",
    "split_list_of_dicts(a_list_interval_update_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_update_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply visibility setting to rendered items if provided\n",
    "# For list configs: only apply if all items have the same visibility (or all None)\n",
    "# For single configs: apply directly\n",
    "if visibility_settings is not None and interval_key in active_2d_plot.rendered_epochs:\n",
    "    if isinstance(visibility_settings, list):\n",
    "        # List case: check if all non-None values are the same\n",
    "        non_none_visibilities = [v for v in visibility_settings if v is not None]\n",
    "        if len(non_none_visibilities) > 0:\n",
    "            # If all non-None values are the same, apply that visibility\n",
    "            if len(set(non_none_visibilities)) == 1:\n",
    "                is_visible = non_none_visibilities[0]\n",
    "                container = active_2d_plot.rendered_epochs[interval_key]\n",
    "                for a_plot, rect_item in container.items():\n",
    "                    if not isinstance(a_plot, str) and isinstance(rect_item, IntervalRectsItem):\n",
    "                        rect_item.setVisible(is_visible)\n",
    "            # If they differ, we can't set per-rectangle visibility, so skip\n",
    "            # (IntervalRectsItem is a single graphics item that renders all rectangles)\n",
    "    else:\n",
    "        # Single config case: apply directly\n",
    "        container = active_2d_plot.rendered_epochs[interval_key]\n",
    "        for a_plot, rect_item in container.items():\n",
    "            if not isinstance(a_plot, str) and isinstance(rect_item, IntervalRectsItem):\n",
    "                rect_item.setVisible(visibility_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac679093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [RectangleRenderTupleHelpers.QColor_to_simple_columns_dict(v)['hexColor'] for v in brush_colors]\n",
    "# [RectangleRenderTupleHelpers.QColor_to_simple_columns_dict(v)['alpha'] for v in brush_colors]\n",
    "\n",
    "[ColorFormatConverter.qColor_to_hexstring(v, include_alpha=True) for v in brush_colors]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine interval_keys that are missing from update_dict but exist in self.interval_datasources\n",
    "removed_interval_keys = [k for k in active_2d_plot.rendered_epoch_series_names if k not in update_dict] # need to use this and not `self.interval_datasources.keys()` directly because it has non-attribute members like 'name'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a698865",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_viz_df['pen_color'] == custom_paradigm_ds_df['pen_color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd3b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_serializable_df = custom_paradigm_ds.get_serialized_data(drop_duplicates=False)\n",
    "a_serializable_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f874c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_series_positioning_dfs, all_series_compressed_positioning_dfs, all_series_compressed_positioning_update_dicts = active_2d_plot.recover_interval_datasources_update_dict_properties()\n",
    "# all_series_positioning_dfs\n",
    "all_series_compressed_positioning_dfs # ERROR: series_compressed_positioning_update_dict is None for custom_paradigm. it will not be represented in the output dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_configs_df = active_2d_plot.extract_interval_display_config_df()\n",
    "out_configs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_y_min, top_y_max = active_2d_plot.get_interval_y_extrema_locations()\n",
    "curr_x_min, curr_x_max, curr_y_min, curr_y_max = active_2d_plot.get_render_intervals_plot_range()\n",
    "new_y_min = min(curr_y_min, bottom_y_min)\n",
    "new_y_max = max(curr_y_max, top_y_max)\n",
    "for a_plot in active_2d_plot.interval_rendering_plots:\n",
    "    a_plot.setYRange(new_y_min, new_y_max, padding=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.get_render_intervals_plot_range(debug_print=True)\n",
    "curr_x_min, curr_x_max, curr_y_min, curr_y_max = active_2d_plot.get_render_intervals_plot_range(debug_print=True)\n",
    "\n",
    "# active_2d_plot.update_rendered_interval_heights(41.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55565cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_plot_item in active_2d_plot.interval_rendering_plots:\n",
    "    # a_plot_item: pg.PlotItem = active_2d_plot.interval_rendering_plots[0]\n",
    "    a_plot_item.setYRange(curr_y_min, curr_y_max)\n",
    "    # a_plot_item.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21862358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.build_epoch_intervals_visual_configs_widget()\n",
    "spike_raster_window.build_epoch_intervals_visual_configs_widget()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.build_or_update_epoch_render_configs_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d356bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label formatting function that accesses the dataframe\n",
    "def create_label_format_fn(datasource):\n",
    "    \"\"\"Creates a closure that captures the datasource's label column\"\"\"\n",
    "    label_column = datasource.df['label'].tolist()  # or whatever column has the names\n",
    "    \n",
    "    def _format_label_for_rect_data(rect_index: int, rect_data_tuple: Tuple) -> str:\n",
    "        \"\"\"Returns the label text for this interval. Captures: label_column\n",
    "        \"\"\"\n",
    "        start_t, series_vertical_offset, duration_t, series_height, pen, brush = rect_data_tuple\n",
    "        end_t = start_t + duration_t\n",
    "        item_label: str = f\"{datasource.custom_datasource_name}[{rect_index}]\"\n",
    "        if rect_index < len(label_column):\n",
    "            item_label = str(label_column[rect_index])\n",
    "\n",
    "        label_text = f\"{item_label}\\nStart: {start_t:.3f}\\nEnd: {end_t:.3f}\\nDuration: {duration_t:.3f}\" # The tooltip is set generically here to 'PBEs', 'Replays' or whatever the dataseries name is\n",
    "        return label_text        \n",
    "\n",
    "    return _format_label_for_rect_data\n",
    "\n",
    "\n",
    "def create_tooltip_format_fn(datasource):\n",
    "    \"\"\"Creates a closure that captures the datasource's label column\"\"\"\n",
    "    label_column = datasource.df['label'].tolist()  # or whatever column has the names\n",
    "    \n",
    "    def _custom_format_tooltip_for_rect_data(rect_index: int, rect_data_tuple: Tuple) -> str:\n",
    "        \"\"\" Captures: label_column\"\"\"\n",
    "        start_t, series_vertical_offset, duration_t, series_height, pen, brush = rect_data_tuple\n",
    "        end_t = start_t + duration_t\n",
    "        item_label: str = f\"{datasource.custom_datasource_name}[{rect_index}]\"\n",
    "        if rect_index < len(label_column):\n",
    "            item_label = str(label_column[rect_index])\n",
    "            print(f'\\tgot specific label: \"{item_label}\"')\n",
    "            \n",
    "        tooltip_text = f\"{item_label}\\nStart: {start_t:.3f}\\nEnd: {end_t:.3f}\\nDuration: {duration_t:.3f}\" # The tooltip is set generically here to 'PBEs', 'Replays' or whatever the dataseries name is\n",
    "        return tooltip_text\n",
    "\n",
    "\n",
    "    return _custom_format_tooltip_for_rect_data\n",
    "\n",
    "\n",
    "\n",
    "## INPUTS: an_interval_ds, an_interval_rects_item\n",
    "\n",
    "# Create the label formatter\n",
    "label_format_fn = create_label_format_fn(an_interval_ds)\n",
    "tooltip_format_fn = create_tooltip_format_fn(an_interval_ds)\n",
    "\n",
    "# Update the rendered intervals with labels\n",
    "# an_interval_rects_item.item_label_format_fn = deepcopy(label_format_fn)\n",
    "# an_interval_rects_item._current_hovered_item_tooltip_format_fn = deepcopy(tooltip_format_fn)\n",
    "\n",
    "for plot_name, rect_item_dict in active_2d_plot.get_all_rendered_intervals_dict().items():\n",
    "    if plot_name == 'custom_paradigm':\n",
    "        print(f'updating \"custom_paradigm\" intervals for plot_name: \"{plot_name}\"...')\n",
    "        rect_item: IntervalRectsItem = rect_item_dict['RootPlot']\n",
    "        rect_item.format_item_tooltip_fn = deepcopy(tooltip_format_fn)\n",
    "        rect_item.item_label_format_fn = deepcopy(label_format_fn)\n",
    "        # rect_item._current_hovered_item_tooltip_format_fn = deepcopy(tooltip_format_fn)\n",
    "        # Need to regenerate the labels - this requires recreating the item OR you can manually add labels\n",
    "        print(f'\\tdone.')\n",
    "        \n",
    "    else:\n",
    "        print('WARN: \"custom_paradigm\" intervals not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.get_all_rendered_intervals_dict()\n",
    "# active_2d_plot.update() ## crashes the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_final_interval_df\n",
    "active_2d_plot.build_or_update_epoch_render_configs_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15484bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsObjects.IntervalRectsItem import IntervalRectsItem\n",
    "\n",
    "def _custom_format_tooltip_for_rect_data(rect_index: int, rect_data_tuple: Tuple) -> str:\n",
    "    start_t, series_vertical_offset, duration_t, series_height, pen, brush = rect_data_tuple\n",
    "    end_t = start_t + duration_t\n",
    "    tooltip_text = f\"{name}[{rect_index}]\\nStart: {start_t:.3f}\\nEnd: {end_t:.3f}\\nDuration: {duration_t:.3f}\" # The tooltip is set generically here to 'PBEs', 'Replays' or whatever the dataseries name is\n",
    "    return tooltip_text\n",
    "\n",
    "\n",
    "# active_2d_plot.interval_datasources\n",
    "\n",
    "_out_rendered_intervals = active_2d_plot.get_all_rendered_intervals_dict()\n",
    "an_interval_rects_item: IntervalRectsItem = _out_rendered_intervals['custom_paradigm']['RootPlot']\n",
    "an_interval_rects_item._current_hovered_item_tooltip_format_fn = deepcopy(_custom_format_tooltip_for_rect_data)\n",
    "an_interval_rects_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_interval_ds.custom_datasource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1920a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7492550",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.add_rendered_intervals(an_interval_ds, name=f'custom_paradigm', debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_interval_rects_item.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ddf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.remove_rendered_intervals("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_test_split_laps_epochs_formatting_dict = {\n",
    "    'LapsAll':dict(y_location=-10.0, height=7.5, pen_color=inline_mkColor('white', 0.8), brush_color=inline_mkColor('white', 0.5)),\n",
    "    'LapsTrain':dict(y_location=-2.0, height=1.5, pen_color=inline_mkColor('purple', 0.8), brush_color=inline_mkColor('purple', 0.5)),\n",
    "    'LapsTest':dict(y_location=-12.0, height=1.5, pen_color=inline_mkColor('green', 0.8), brush_color=inline_mkColor('green', 0.5)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c707d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_paradigm_df: pd.DataFrame = ensure_dataframe(sess.paradigm)\n",
    "# curr_paradigm_df: pd.DataFrame = ensure_dataframe(sess.epochs_bak)\n",
    "\n",
    "\n",
    "curr_paradigm_df: pd.DataFrame = np.load(f\"W:/Data/Bapun/RatU/RatUDay5OpenfieldSD/RatU_Day5OpenfieldSD_2021-08-04_08-44-31.paradigm.npy\", allow_pickle=True).tolist()['epochs']\n",
    "curr_paradigm_df['duration'] = curr_paradigm_df['stop'] - curr_paradigm_df['start']\n",
    "curr_paradigm_df\n",
    "\n",
    "# curr_paradigm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e07415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.EpochsEditorItem import EpochsEditor # perform_plot_laps_diagnoser\n",
    "import matplotlib.pyplot as plt\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers, ColorFormatConverter\n",
    "from neuropy.core.epoch import Epoch, EpochsAccessor, ensure_dataframe, ensure_Epoch, EpochHelpers\n",
    "\n",
    "def generate_colors(n_epoch):\n",
    "    cmap = plt.get_cmap('tab20', n_epoch)\n",
    "    return [plt.matplotlib.colors.rgb2hex(cmap(i)) for i in range(n_epoch)]\n",
    "\n",
    "\n",
    "sess = curr_active_pipeline.sess # global_session\n",
    "\n",
    "# pos_df = sess.compute_position_laps() # ensures the laps are computed if they need to be:\n",
    "position_obj = deepcopy(sess.position)\n",
    "position_obj.compute_higher_order_derivatives()\n",
    "pos_df = position_obj.compute_smoothed_position_info(N=20) ## Smooth the velocity curve to apply meaningful logic to it\n",
    "pos_df = position_obj.to_dataframe()\n",
    "# Drop rows with missing data in columns: 't', 'velocity_x_smooth' and 2 other columns. This occurs from smoothing\n",
    "pos_df = pos_df.dropna(subset=['t', 'x_smooth', 'velocity_x_smooth', 'acceleration_x_smooth']).reset_index(drop=True)\n",
    "# curr_laps_df = sess.laps.to_dataframe()\n",
    "\n",
    "# curr_paradigm_df = ensure_dataframe(sess.paradigm)\n",
    "\n",
    "curr_paradigm_df = ensure_dataframe(curr_paradigm_df)\n",
    "curr_paradigm_df = curr_paradigm_df[np.logical_not(np.isin(curr_paradigm_df['label'], ['maze_GLOBAL', 'maze']))] ## exclude the global epoch\n",
    "curr_paradigm_df = EpochHelpers.assign_overlap_y_offset(curr_paradigm_df, start_col='start', stop_col='stop', out_col='overlap_y_offset') \n",
    "n_epochs: int = len(curr_paradigm_df)\n",
    "# epoch_color_strs: List[str] = generate_colors(n_epochs)\n",
    "epoch_color_strs: List[str] = [ColorFormatConverter.qColor_to_hexstring(v, include_alpha=False) for v in ColormapHelpers.mpl_to_pg_colormap(mpl_cmap_name='tab20', resolution=n_epochs).getColors(mode='qcolor')]\n",
    "curr_paradigm_df['lap_color'] = \"#10FF44\"\n",
    "curr_paradigm_df['lap_color'] = epoch_color_strs\n",
    "curr_paradigm_df['lap_accent_color'] = '#FFFFFF'\n",
    "curr_paradigm_df\n",
    "## Create a new window:\n",
    "custom_epoch_label_kwargs = dict(epoch_label_position=0.05, epoch_label_rotateAxis=(0,0), epoch_label_anchor=(0.0, 1.0))\n",
    "epochs_editor = EpochsEditor.init_laps_diagnoser(pos_df, curr_paradigm_df, include_velocity=False, include_accel=False, span=(0.05, 0.95), movable=False, **custom_epoch_label_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_editor.plots\n",
    "# epochs_editor.rebuild_epoch_regions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.plots.main_plot_widget\n",
    "\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "main_plot_widget.setMinimumHeight(20.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout\n",
    "# main_graphics_layout_widget.ci # GraphicsLayout\n",
    "main_graphics_layout_widget.ci.childItems()\n",
    "# main_graphics_layout_widget.setHidden(True) ## hides too much\n",
    "main_graphics_layout_widget.setHidden(False)\n",
    "\n",
    "# main_graphics_layout_widget\n",
    "\n",
    "active_window_container_layout.setBorder(pg.mkPen('yellow', width=4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_window_container_layout.allChildItems()\n",
    "active_window_container_layout.setPreferredHeight(200.0)\n",
    "active_window_container_layout.setMaximumHeight(800.0)\n",
    "active_window_container_layout.setSpacing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stretch factors to control priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(0, 400)  # Plot1: lowest priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(1, 2)  # Plot2: mid priority\n",
    "main_graphics_layout_widget.ci.layout.setRowStretchFactor(2, 2)  # Plot3: highest priority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ParameterTreeWidget import create_parameter_tree_widget\n",
    "# win, param_tree = create_pipeline_filter_parameter_tree()\n",
    "win, param_tree = create_parameter_tree_widget(curr_active_pipeline.get_all_parameters())\n",
    "win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f61dd",
   "metadata": {
    "tags": [
     "_perform_plot_multi_decoder_meas_pred_position_track",
     "active-2025-01-16"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DecoderIdentityColors\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_plot_multi_decoder_meas_pred_position_track\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "## Build the new dock track:\n",
    "dock_identifier: str = 'Continuous Decoding Performance'\n",
    "ts_widget, fig, ax_list, dDisplayItem = active_2d_plot.add_new_matplotlib_render_plot_widget(name=dock_identifier)\n",
    "## Get the needed data:\n",
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n",
    "\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "all_directional_continuously_decoded_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {k:v for k, v in (continuously_decoded_dict or {}).items() if k in TrackTemplates.get_decoder_names()} ## what is plotted in the `f'{a_decoder_name}_ContinuousDecode'` rows by `AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand`\n",
    "## OUT: all_directional_continuously_decoded_dict\n",
    "## Draw the position meas/decoded on the plot widget\n",
    "## INPUT: fig, ax_list, all_directional_continuously_decoded_dict, track_templates\n",
    "\n",
    "_out_artists =  _perform_plot_multi_decoder_meas_pred_position_track(curr_active_pipeline, fig, ax_list, desired_time_bin_size=0.058, enable_flat_line_drawing=True)\n",
    "\n",
    "## sync up the widgets\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget(dock_identifier, sync_mode=SynchronizedPlotMode.TO_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38325e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['truth_decoder_name'] = pos_df['truth_decoder_name'].fillna('')\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_color_dict: Dict[types.DecoderName, str] = DecoderIdentityColors.build_decoder_color_dict()\n",
    "\n",
    "decoded_pos_line_kwargs = dict(lw=1.0, color='gray', alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "inactive_decoded_pos_line_kwargs = dict(lw=0.3, alpha=0.2, marker='.', markersize=2, animated=False)\n",
    "active_decoded_pos_line_kwargs = dict(lw=1.0, alpha=0.8, marker='+', markersize=6, animated=False)\n",
    "\n",
    "\n",
    "_out_data = {}\n",
    "_out_data_plot_kwargs = {}\n",
    "# curr_active_pipeline.global_computation_results.t\n",
    "for a_decoder_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    a_continuously_decoded_result = all_directional_continuously_decoded_dict[a_decoder_name]\n",
    "    a_decoder_color = decoder_color_dict[a_decoder_name]\n",
    "    \n",
    "    assert len(a_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = a_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = a_continuously_decoded_result.time_bin_containers[0]\n",
    "    time_window_centers = time_bin_containers.centers\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "    a_marginal_x = a_continuously_decoded_result.marginal_x_list[0]\n",
    "    # active_time_window_variable = a_decoder.active_time_window_centers\n",
    "    active_time_window_variable = time_window_centers\n",
    "    active_most_likely_positions_x = a_marginal_x['most_likely_positions_1D'] # a_decoder.most_likely_positions[:,0].T\n",
    "    _out_data[a_decoder_name] = pd.DataFrame({'t': time_window_centers, 'x': active_most_likely_positions_x, 'binned_time': np.arange(len(time_window_centers))})\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "    _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "    _out_data[a_decoder_name]['is_active_decoder_time'] = (_out_data[a_decoder_name]['truth_decoder_name'].fillna('', inplace=False) == a_decoder_name)\n",
    "\n",
    "    # is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "    active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "    active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "    active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "    ## could fill y with np.nan instead of getting shorter?\n",
    "    _out_data_plot_kwargs[a_decoder_name] = (dict(x=active_decoder_time_points, y=active_decoder_most_likely_positions_x, color=a_decoder_color, **active_decoded_pos_line_kwargs), dict(x=active_decoder_inactive_time_points, y=active_decoder_inactive_most_likely_positions_x, color=a_decoder_color, **inactive_decoded_pos_line_kwargs))\n",
    "\n",
    "_out_data_plot_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8972db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']\n",
    "\n",
    "# is_active_decoder_time = (_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name)\n",
    "active_decoder_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] == a_decoder_name]['x'].to_numpy()\n",
    "active_decoder_inactive_time_points = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['t'].to_numpy()\n",
    "active_decoder_inactive_most_likely_positions_x = _out_data[a_decoder_name][_out_data[a_decoder_name]['truth_decoder_name'] != a_decoder_name]['x'].to_numpy()\n",
    "\n",
    "_out_data[a_decoder_name] = ((active_decoder_time_points, active_decoder_most_likely_positions_x), (active_decoder_inactive_time_points, active_decoder_inactive_most_likely_positions_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_dfs = partition_df_dict(pos_df, partitionColumn='truth_decoder_name')\n",
    "\n",
    "a_decoder_name: str = 'short_LR'\n",
    "a_binned_time_grouped_df = partitioned_dfs[a_decoder_name].groupby('binned_time', axis='index', dropna=True)\n",
    "a_binned_time_grouped_df = a_binned_time_grouped_df.median().dropna(axis='index', subset=['x']) ## without the `.dropna(axis='index', subset=['x'])` part it gets an exhaustive df for all possible values of 'binned_time', even those not listed\n",
    "\n",
    "a_matching_binned_times = a_binned_time_grouped_df.reset_index(drop=False)['binned_time']\n",
    "a_matching_binned_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into two dfs for each decoder -- the supported and the unsupported\n",
    "partition\n",
    "\n",
    "PandasHelpers.safe_pandas_get_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.dropna(axis='index', subset=['lap', 'truth_decoder_name'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df: pd.DataFrame = global_laps_obj.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import EpochHelpers\n",
    "\n",
    "## INPUTS: global_laps\n",
    "_out_split_pseudo2D_posteriors_dict = {}\n",
    "_out_split_pseudo2D_out_dict = {}\n",
    "pre_filtered_col_names = ['pre_filtered_most_likely_position_indicies', 'pre_filtered_most_likely_position'] # 'pre_filtered_time_bin_containers', 'pre_filtered_p_x_given_n', \n",
    "post_filtered_col_names = [a_col_name.removeprefix('pre_filtered_') for a_col_name in pre_filtered_col_names] # ['time_bin_containers', 'most_likely_position_indicies', 'most_likely_position']\n",
    "print(post_filtered_col_names)\n",
    "for a_time_bin_size, pseudo2D_decoder_continuously_decoded_result in continuously_decoded_pseudo2D_decoder_dict.items():\n",
    "    print(f'a_time_bin_size: {a_time_bin_size}')\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size] = {'pre_filtered_p_x_given_n': None, 'pre_filtered_time_bin_containers': None, 'pre_filtered_most_likely_position_indicies': None, 'pre_filtered_most_likely_position': None, \n",
    "                                                     'is_timebin_included': None, 'p_x_given_n': None} # , 'time_window_centers': None\n",
    "    # pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "    assert len(pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list) == 1\n",
    "    p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]\n",
    "    # p_x_given_n = pseudo2D_decoder_continuously_decoded_result.p_x_given_n_list[0]['p_x_given_n']\n",
    "    time_bin_containers = pseudo2D_decoder_continuously_decoded_result.time_bin_containers[0]\n",
    "    # time_window_centers = time_bin_containers.centers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_position_indicies_list[0])\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'] = deepcopy(pseudo2D_decoder_continuously_decoded_result.most_likely_positions_list[0])\n",
    "    ## INPUTS: time_bin_containers, global_laps\n",
    "    left_edges = deepcopy(time_bin_containers.left_edges)\n",
    "    right_edges = deepcopy(time_bin_containers.right_edges)\n",
    "    continuous_time_binned_computation_epochs_df: pd.DataFrame = pd.DataFrame({'start': left_edges, 'stop': right_edges, 'label': np.arange(len(left_edges))})\n",
    "    is_timebin_included: NDArray = EpochHelpers.find_epochs_overlapping_other_epochs(epochs_df=continuous_time_binned_computation_epochs_df, epochs_df_required_to_overlap=deepcopy(global_laps))\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_p_x_given_n'] = p_x_given_n\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_time_bin_containers'] = time_bin_containers\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['is_timebin_included'] = is_timebin_included\n",
    "    # continuous_time_binned_computation_epochs_df['is_in_laps'] = is_timebin_included\n",
    "    ## filter by whether it's included or not:\n",
    "    p_x_given_n = p_x_given_n[:, :, is_timebin_included]\n",
    "    # time_window_centers = \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['p_x_given_n'] = p_x_given_n\n",
    "    # _out_split_pseudo2D_out_dict[a_time_bin_size]['time_window_centers'] = time_window_centers[is_timebin_included]\n",
    "    # p_x_given_n.shape # (62, 4, 209389)\n",
    "\n",
    "    ## Split across the 2nd axis to make 1D posteriors that can be displayed in separate dock rows:\n",
    "    assert p_x_given_n.shape[1] == 4, f\"expected the 4 pseudo-y bins for the decoder in p_x_given_n.shape[1]. but found p_x_given_n.shape: {p_x_given_n.shape}\"\n",
    "    # split_pseudo2D_posteriors_dict = {k:np.squeeze(p_x_given_n[:, i, :]) for i, k in enumerate(('long_LR', 'long_RL', 'short_LR', 'short_RL'))}\n",
    "    _out_split_pseudo2D_posteriors_dict[a_time_bin_size] = deepcopy(p_x_given_n)\n",
    "    \n",
    "    # for a_col_name in pre_filtered_col_names:\n",
    "    #     filtered_col_name = a_col_name.removeprefix('pre_filtered_')\n",
    "    #     print(f'a_col_name: {a_col_name}, filtered_col_name: {filtered_col_name}, shape: {np.shape(_out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name])}')\n",
    "    #     _out_split_pseudo2D_out_dict[a_time_bin_size][filtered_col_name] = _out_split_pseudo2D_out_dict[a_time_bin_size][a_col_name][is_timebin_included, :]\n",
    "        \n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position_indicies'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position_indicies'][:, is_timebin_included]\n",
    "    _out_split_pseudo2D_out_dict[a_time_bin_size]['most_likely_position'] = _out_split_pseudo2D_out_dict[a_time_bin_size]['pre_filtered_most_likely_position'][is_timebin_included, :]\n",
    "    \n",
    "\n",
    "p_x_given_n.shape # (n_position_bins, n_decoding_models, n_time_bins) - (57, 4, 29951)\n",
    "\n",
    "## OUTPUTS: _out_split_pseudo2D_posteriors_dict, _out_split_pseudo2D_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_most_likely_position_comparsions\n",
    "\n",
    "# fig, axs = plot_most_likely_position_comparsions(pho_custom_decoder, axs=ax, sess.position.to_dataframe())\n",
    "fig, axs = plot_most_likely_position_comparsions(computation_result.computed_data['pf2D_Decoder'], computation_result.sess.position.to_dataframe(), **overriding_dict_with(lhs_dict={'show_posterior':True, 'show_one_step_most_likely_positions_plots':True}, **kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4129c9",
   "metadata": {},
   "source": [
    "# 🖼️⚓❎ Time Synchronized Plotting with position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f834b",
   "metadata": {
    "tags": [
     "active-2025-09-21",
     "📈good_vis_2026-02-05"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.Mixins.AnimalTrajectoryPlottingMixin import AnimalTrajectoryPlottingMixin\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPositionDecoderPlotter import TimeSynchronizedPositionDecoderPlotter\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_combined_time_synchronized_Bapun_decoders_window\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster, SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.SpecificDockWidgetManipulatingMixin import SpecificDockWidgetManipulatingMixin\n",
    "\n",
    "hardcoded_params: HardcodedProcessingParameters = BapunDataSessionFormatRegisteredClass._get_session_specific_parameters(session_context=curr_active_pipeline.get_session_context())\n",
    "# hardcoded_params.decoder_building_session_names\n",
    "# hardcoded_params.non_global_activity_session_names\n",
    "\n",
    "\n",
    "# pg.setConfigOptions(useOpenGL=True)  # do this BEFORE creating plots/widgets\n",
    "\n",
    "## Uses the `global_computation_results.computed_data['DirectionalDecodersDecoded']`\n",
    "# directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "\n",
    "# _out_container: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=hardcoded_params.non_global_activity_session_names, fixed_window_duration = 3.0)\n",
    "_out_container_new: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=hardcoded_params.non_global_activity_session_names, fixed_window_duration = 1.0,\n",
    "    directional_decoders_decode_result=directional_decoders_decode_result,\n",
    "    controlling_widget=active_2d_plot, create_new_controlling_widget=False,\n",
    ")\n",
    "\n",
    "active_2d_plot: Spike2DRaster = _out_container_new.ui.controlling_widget\n",
    "sync_plotters: Dict[str, TimeSynchronizedPositionDecoderPlotter] = _out_container_new.ui.sync_plotters\n",
    "win: PhoDockAreaContainingWindow = _out_container_new.ui.root_dockAreaWindow\n",
    "# a_sync_plotter: TimeSynchronizedPositionDecoderPlotter = sync_plotters['roam']\n",
    "# a_sync_plotter.curr_position\n",
    "\n",
    "# ## Disable debug print to speed up animation\n",
    "# for a_plotter_name, a_plotter in sync_plotters.items():\n",
    "#     a_plotter.params.debug_print = False\n",
    "\n",
    "\n",
    "## INPUTS: _out_container, active_2d_plot, _out_container, sync_plotters, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "win.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for an_epoch_name, a_plotter in sync_plotters.items():\n",
    "    # display(a_plotter.params.debug_print)\n",
    "    # a_plotter.params.debug_print = True\n",
    "    # display(a_plotter.params.debug_print)\n",
    "    # a_plotter\n",
    "    a_plotter.ui.root_plot.setTitle(f'PositionDecoder -  t = {a_plotter.last_window_time}')    \n",
    "\n",
    "\n",
    "# a_plotter.ui.root_plot.setTitle(f'PositionDecoder -  t = {a_plotter.last_window_time}')\n",
    "\n",
    "# a_plotter.params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524922e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output videos:\n",
    "## INPUTS: sync_plotters\n",
    "export_video_paths = {}\n",
    "\n",
    "export_video_parent_folder = curr_active_pipeline.get_output_path().joinpath('videos').resolve()\n",
    "export_video_parent_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# k = 'maze1'\n",
    "# a_plotter = sync_plotters[k]\n",
    "\n",
    "for an_epoch_name, a_plotter in sync_plotters.items():\n",
    "    an_export_video_path = export_video_parent_folder.joinpath(f'decoder_{an_epoch_name}.avi')\n",
    "    print(f'exporting to \"{an_export_video_path}\"')\n",
    "    # With custom settings\n",
    "    video_path = a_plotter.export_video(\n",
    "        output_path=an_export_video_path,\n",
    "        start_t=7423.0,\n",
    "        # start_t=11183.07048563492, # 20750.0,\n",
    "        # end_t=20750.0 + 244,\n",
    "        end_t=11483.0,\n",
    "        fps=24.0,\n",
    "        width=720,\n",
    "        height=720,\n",
    "        progress_print=True,\n",
    "        debug_print=False\n",
    "    )\n",
    "    print(f'\\texport to video_path: \"{video_path.resolve().as_posix()}\" complete.')\n",
    "    export_video_paths[an_epoch_name] = video_path\n",
    "\n",
    "print(f'done exporting all videos.')\n",
    "export_video_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f95d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess.epochs\n",
    "\n",
    "curr_active_pipeline.sess.active_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87216be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v.window()\n",
    "v.ui.root_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f32d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@define(slots=False)\n",
    "class VideoExportHelper:\n",
    "    \n",
    "    @function_attributes(short_name=None, tags=['video', 'export', 'mp4', 'avi', 'output'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2025-11-24 23:09', related_items=[])\n",
    "    @classmethod\n",
    "    def perform_export_video(cls, win, time_window_centers, output_path: str, start_t: Optional[float] = None, end_t: Optional[float] = None, fps: float = 30.0, width: Optional[int] = None, height: Optional[int] = None, progress_print: bool = True, debug_print: bool = False):\n",
    "        \"\"\"Efficiently export a video from the TimeSynchronizedPositionDecoderPlotter instance (faster than real-time playback)\n",
    "        \n",
    "        This method iterates through time points, updates the plotter, captures frames using\n",
    "        pyqtgraph's ImageExporter, and saves them as a video using OpenCV.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the output video file (e.g., 'output/videos/decoder_video.avi')\n",
    "            start_t: Start time for video export. If None, uses the first available time window center.\n",
    "            end_t: End time for video export. If None, uses the last available time window center.\n",
    "            fps: Frames per second for the output video (default: 30.0)\n",
    "            width: Width of exported frames in pixels. If None, uses current widget width.\n",
    "            height: Height of exported frames in pixels. If None, uses current widget height.\n",
    "            progress_print: Whether to print progress messages (default: True)\n",
    "            debug_print: Whether to print debug information (default: False)\n",
    "            \n",
    "        Returns:\n",
    "            Path: Path to the saved video file\n",
    "            \n",
    "        Usage:\n",
    "            plotter = TimeSynchronizedPositionDecoderPlotter(...)\n",
    "            video_path = plotter.export_video('output/videos/decoder.avi', start_t=100.0, end_t=200.0, fps=30.0)\n",
    "        \"\"\"\n",
    "        from pyphoplacecellanalysis.External.pyqtgraph.exporters.ImageExporter import ImageExporter\n",
    "        from pyphoplacecellanalysis.External.pyqtgraph import functions as fn\n",
    "        from pathlib import Path\n",
    "        import cv2\n",
    "        import sys\n",
    "        \n",
    "        # Get time window centers\n",
    "        if len(time_window_centers) == 0:\n",
    "            raise ValueError(\"No time window centers available for video export\")\n",
    "        \n",
    "        # Determine time range\n",
    "        if start_t is None:\n",
    "            start_t = float(time_window_centers[0])\n",
    "        if end_t is None:\n",
    "            end_t = float(time_window_centers[-1])\n",
    "        \n",
    "        # Find valid time indices\n",
    "        start_idx = np.searchsorted(time_window_centers, start_t, side='left')\n",
    "        end_idx = np.searchsorted(time_window_centers, end_t, side='right')\n",
    "        \n",
    "        if start_idx >= end_idx:\n",
    "            raise ValueError(f\"Invalid time range: start_t={start_t}, end_t={end_t}. No valid frames found.\")\n",
    "        \n",
    "        # Get frame indices\n",
    "        frame_indices: NDArray = np.arange(start_idx, end_idx)\n",
    "        n_frames: int = len(frame_indices)\n",
    "        \n",
    "        if progress_print:\n",
    "            print(f'Exporting video: {n_frames} frames from t={time_window_centers[start_idx]:.2f} to t={time_window_centers[end_idx-1]:.2f}')\n",
    "        \n",
    "        # Get widget dimensions\n",
    "        if width is None or height is None:\n",
    "            widget_size = self.ui.root_graphics_layout_widget.size()\n",
    "            if width is None:\n",
    "                width = widget_size.width()\n",
    "            if height is None:\n",
    "                height = widget_size.height()\n",
    "        \n",
    "        # Disable debug printing during export for performance\n",
    "        original_debug_print = self.params.debug_print\n",
    "        self.params.debug_print = debug_print\n",
    "        \n",
    "        # Create ImageExporter for the root plot\n",
    "        exporter = ImageExporter(self.ui.root_plot)\n",
    "        exporter.parameters()['width'] = width\n",
    "        exporter.parameters()['height'] = height\n",
    "        exporter.parameters()['antialias'] = True\n",
    "        \n",
    "        # Process events to ensure widget is rendered\n",
    "        QtWidgets.QApplication.processEvents()\n",
    "        \n",
    "        # Capture frames\n",
    "        frames = []\n",
    "        for i, frame_idx in enumerate(frame_indices):\n",
    "            if progress_print and (i % max(1, n_frames // 20) == 0 or i == n_frames - 1):\n",
    "                print(f'Capturing frame {i+1}/{n_frames} (t={time_window_centers[frame_idx]:.2f})')\n",
    "            \n",
    "            # Update plotter to current time\n",
    "            t = time_window_centers[frame_idx]\n",
    "            self.update(t, defer_render=False)\n",
    "            \n",
    "            # Process events to ensure rendering\n",
    "            QtWidgets.QApplication.processEvents()\n",
    "            \n",
    "            # Capture frame\n",
    "            qimage = exporter.export(toBytes=True)\n",
    "            \n",
    "            # Convert QImage to numpy array\n",
    "            # ImageExporter returns ARGB32 format, which has endianness-dependent byte order\n",
    "            # On little-endian: bytes are [B, G, R, A] in memory\n",
    "            # On big-endian: bytes are [A, R, G, B] in memory\n",
    "            img_array = fn.ndarray_from_qimage(qimage)\n",
    "            \n",
    "            # Handle ARGB32 format conversion based on byte order\n",
    "            if img_array.shape[2] == 4:\n",
    "                # ARGB32 format - extract RGB channels based on byte order\n",
    "                if sys.byteorder == 'little':\n",
    "                    # Little-endian: channels are [B, G, R, A] in memory\n",
    "                    # Extract [B, G, R] which is already BGR for OpenCV\n",
    "                    bgr_array = img_array[:, :, [0, 1, 2]]  # B, G, R\n",
    "                else:\n",
    "                    # Big-endian: channels are [A, R, G, B] in memory\n",
    "                    # Extract [R, G, B] and convert to BGR\n",
    "                    rgb_array = img_array[:, :, [1, 2, 3]]  # R, G, B\n",
    "                    bgr_array = rgb_array[:, :, [2, 1, 0]]  # Convert to BGR\n",
    "            elif img_array.shape[2] == 3:\n",
    "                # Already RGB format, convert to BGR for OpenCV\n",
    "                bgr_array = img_array[:, :, [2, 1, 0]]  # Convert RGB to BGR\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image format with {img_array.shape[2]} channels\")\n",
    "            \n",
    "            frames.append(bgr_array)\n",
    "        \n",
    "        # Restore original debug print setting\n",
    "        self.params.debug_print = original_debug_print\n",
    "        \n",
    "        if progress_print:\n",
    "            print(f'Converting {len(frames)} frames to video...')\n",
    "        \n",
    "        # Convert frames list to numpy array: (n_frames, height, width, channels)\n",
    "        frames_array = np.stack(frames, axis=0)\n",
    "        \n",
    "        # Save video using OpenCV directly (since save_array_as_video expects grayscale)\n",
    "        import cv2\n",
    "        video_filepath = Path(output_path).resolve()\n",
    "        video_parent_path = video_filepath.parent\n",
    "        if not video_parent_path.exists():\n",
    "            if progress_print:\n",
    "                print(f'Creating output directory: {video_parent_path}')\n",
    "            video_parent_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get dimensions\n",
    "        n_frames, height, width, n_channels = frames_array.shape\n",
    "        \n",
    "        # Initialize video writer (OpenCV uses BGR, so we need to convert from RGB)\n",
    "        fourcc = cv2.VideoWriter_fourcc('M','J','P','G')  # MJPEG codec (fast, good quality)\n",
    "        out = cv2.VideoWriter(str(video_filepath), fourcc, fps, (width, height), isColor=True)\n",
    "        \n",
    "        if not out.isOpened():\n",
    "            raise RuntimeError(f\"Failed to open video writer for {video_filepath}\")\n",
    "        \n",
    "        # Write frames\n",
    "        progress_print_every_n_frames = max(1, n_frames // 20)\n",
    "        for i in range(n_frames):\n",
    "            if progress_print and (i % progress_print_every_n_frames == 0 or i == n_frames - 1):\n",
    "                print(f'Writing frame {i+1}/{n_frames} to video')\n",
    "            \n",
    "            # Frames are already in BGR format, write directly\n",
    "            out.write(frames_array[i])\n",
    "        \n",
    "        # Close video writer\n",
    "        out.release()\n",
    "        \n",
    "        if progress_print:\n",
    "            print(f'Video exported successfully to: {video_filepath}')\n",
    "        \n",
    "        return video_filepath\n",
    "    \n",
    "\n",
    "time_window_centers = deepcopy(v.time_window_centers)\n",
    "video_filepath = VideoExportHelper.perform_export_video(win=win, time_window_centers=time_window_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad9ee",
   "metadata": {
    "tags": [
     "active-2025-09-21"
    ]
   },
   "outputs": [],
   "source": [
    "_out_pbe_tracks, _out_pbe_overview_tracks = _out_container_new.add_pbes_full_result_marginals(pbes_full_result=pbes_full_result)\n",
    "\n",
    "# grouped_dock_items_dict = _out_container_new.build_overview_and_windowed_dockgroups() ## Not quite ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241f945",
   "metadata": {},
   "source": [
    "## Reorder the dock items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e167155",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(active_2d_plot.dock_manager_widget.dynamic_display_dict.keys())\n",
    "# original_identifier_order = ['interval_overview', 'intervals', 'rasters[raster_overview]', 'rasters[raster_window]', 'global context', 'global context (overview)', 'pbe[0.06]', 'pbe[0.06] (Overview)']\n",
    "desired_identifier_order = ['interval_overview', 'rasters[raster_overview]', 'global context (overview)', 'pbe[0.06] (Overview)', 'rasters[raster_window]', 'intervals', 'global context', 'pbe[0.06]'] ## #TODO 2025-09-21 15:41: - [ ] Enforce this order programmatically?!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.dock_manager_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ac9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: active_2d_plot\n",
    "grouped_dock_items_dict = active_2d_plot.ui.dynamic_docked_widget_container.get_dockGroup_dock_dict()\n",
    "nested_dock_items = {}\n",
    "nested_dynamic_docked_widget_container_widgets = {}\n",
    "for dock_group_name, flat_group_dockitems_list in grouped_dock_items_dict.items():\n",
    "    dDisplayItem, nested_dynamic_docked_widget_container = active_2d_plot.ui.dynamic_docked_widget_container.build_wrapping_nested_dock_area(flat_group_dockitems_list, dock_group_name=dock_group_name)\n",
    "    nested_dock_items[dock_group_name] = dDisplayItem\n",
    "    nested_dynamic_docked_widget_container_widgets[dock_group_name] = nested_dynamic_docked_widget_container\n",
    "\n",
    "## OUTPUTS: nested_dock_items, nested_dynamic_docked_widget_container_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dock_items_dict = active_2d_plot.ui.dynamic_docked_widget_container.get_dockGroup_dock_dict()\n",
    "grouped_dock_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00426b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "grouped_dock_items_dict = build_overview_and_windowed_dockgroups(active_2d_plot)\n",
    "grouped_dock_items_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e67b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = active_2d_plot.dock_manager_widget.layout_dockGroups()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635e921",
   "metadata": {},
   "source": [
    "### 📈🔃❎ Exporting as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyqtgraph.exporters import ImageExporter\n",
    "from PIL import Image\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.CustomGraphicsLayoutWidget import CustomGraphicsLayoutWidget\n",
    "\n",
    "## \"playback\" refers to output video:\n",
    "desired_playback_duration: float = 8 * 60.0 # 8m\n",
    "\n",
    "# \"session\" refers to the actual recording session:\n",
    "desired_session_time_range_duration: float = (16.0 * 60.0) # 1m\n",
    "# session_start_t: float = 23170.37047485091 #20740.63578640229 # 11631.186907154472 # 11451.186907154472 # 11391.186907154472 # 7665.232126354053 # active_2d_plot.total_data_start_time + 4.0 * 60.0 # 4 minutes into start of recording ## Day 5\n",
    "# session_start_t: float = 11023.018433333335 # 10843.018433333334 # active_2d_plot.total_data_start_time + 4.0 * 60.0 # 4 minutes into start of recording\n",
    "# session_start_t: float = 23170.37047485091 #20740.63578640229 # 11631.186907154472 # 11451.186907154472 # 11391.186907154472 # active_2d_plot.total_data_start_time + 4.0 * 60.0 # 4 minutes into start of recording ## day 4\n",
    "\n",
    "session_start_t: float = active_2d_plot.total_data_start_time + 20.0 * 60.0 # 4 minutes into start of recording ## day 4\n",
    "\n",
    "desired_session_time_range: Tuple[float, float] = (session_start_t, (session_start_t + desired_session_time_range_duration))\n",
    "\n",
    "playback_speed_factor: float = (desired_playback_duration / desired_session_time_range_duration)\n",
    "\n",
    "print(f'playback_speed_factor: {playback_speed_factor}')\n",
    "time_window_duration: float = active_2d_plot.active_window_duration\n",
    "print(f'time_window_duration: {time_window_duration}')\n",
    "\n",
    "## INPUTS: _out_container, active_2d_plot, _out_container, sync_plotters, \n",
    "desired_framerate: float = 2.0\n",
    "desired_frame_duration_sec: float = 1.0/desired_framerate\n",
    "print(f'desired_frame_duration_sec: {desired_frame_duration_sec}')\n",
    "\n",
    "# ## All Frames from entire recording (too long)\n",
    "# total_duration: float = active_2d_plot.total_data_duration\n",
    "# desired_num_total_frames: int = int(np.ceil((total_duration * desired_framerate)))\n",
    "# frame_start_indicies = np.linspace(active_2d_plot.total_data_start_time,  active_2d_plot.total_data_end_time, num=desired_num_total_frames)\n",
    "\n",
    "## Plot only for the range of interest:\n",
    "desired_num_total_frames: int = int(np.ceil((desired_session_time_range_duration * desired_framerate)))\n",
    "frame_start_indicies = np.linspace(desired_session_time_range[0], desired_session_time_range[1], num=desired_num_total_frames)\n",
    "frame_end_indices = frame_start_indicies + desired_frame_duration_sec\n",
    "\n",
    "print(f'desired_num_total_frames: {desired_num_total_frames}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Disable debug print to speed up animation\n",
    "for a_plotter_name, a_plotter in sync_plotters.items():\n",
    "    a_plotter.params.debug_print = False\n",
    "    a_plotter.enable_debug_print = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a285a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.active_window_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f338de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_end_timestamp = next_start_timestamp + self.animation_active_time_window.window_duration\n",
    "\n",
    "def _frame_update(frame_start_t, frame_end_t):\n",
    "    active_2d_plot.update_scroll_window_region(frame_start_t, frame_end_t, block_signals=True)\n",
    "    active_2d_plot.window_scrolled.emit(frame_start_t, frame_end_t)\n",
    "    QtWidgets.QApplication.processEvents()\n",
    "    win.repaint()\n",
    "\n",
    "\n",
    "_frame_update(desired_session_time_range[0], (desired_session_time_range[0]+time_window_duration))\n",
    "\n",
    "\n",
    "timestep_delta_sec: float = 0.5 # half second step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step one frame:\n",
    "frame_start_t: float = active_2d_plot.active_window_start_time + timestep_delta_sec ## current time plus delta\n",
    "_frame_update(frame_start_t, (frame_start_t + time_window_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (frame_start_t, frame_end_t) in enumerate(zip(frame_start_indicies, frame_end_indices)):\n",
    "    print(f'frame[{i}]: ({frame_start_t}, {frame_end_t}):')\n",
    "    # active_2d_plot.on_window_changed(frame_start_t, frame_end_t)\n",
    "    # active_2d_plot.update_scroll_window_region(frame_start_t, frame_end_t, block_signals=True)\n",
    "    # active_2d_plot.window_scrolled.emit(frame_start_t, frame_end_t)\n",
    "    # pg.SignalProxy(driver.window_scrolled, delay=0.2, rateLimit=60, slot=drivable.on_window_changed_rate_limited)\n",
    "    # QtWidgets.QApplication.processEvents()\n",
    "    # win.repaint()\n",
    "    # _frame_update(frame_start_t, frame_end_t)\n",
    "    _frame_update(frame_start_t, (frame_start_t + time_window_duration))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b5ab2",
   "metadata": {},
   "source": [
    "### Build Marginals over track context and plot them on the timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _add_context_marginal_to_timeline, _add_context_decoded_epoch_marginals_to_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decoded_epochs_track_name: str = f'{epochs_name}[{decoding_time_bin_size}]'\n",
    "\n",
    "\n",
    "_out = _add_context_marginal_to_timeline(active_2d_plot, a_filter_epochs_decoded_result=all_context_filter_epochs_decoder_result, name='global context')\n",
    "_out_pbe_tracks = _add_context_decoded_epoch_marginals_to_timeline(active_2d_plot=active_2d_plot, decoded_epochs_result=pbe_decoder_result, name=f\"pbe[{decoding_time_bin_size}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "_out_new = _add_context_marginal_to_timeline(active_2d_plot, a_filter_epochs_decoded_result=all_context_filter_epochs_decoder_result, name='global context (overview)')\n",
    "_out_new\n",
    "\n",
    "identifier_name, widget, matplotlib_fig, matplotlib_fig_axes, dock_item = _out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf478a",
   "metadata": {},
   "source": [
    "## 2025-10-21 - Plot Laps in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19772480",
   "metadata": {
    "tags": [
     "laps",
     "2026-02-09_sprinklevsroam",
     "🟢 active-2026-02-11"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericMatplotlibContainer\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_2d, plot_lap_trajectories_3d\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import _plot_helper_add_arrow\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# pseudo3D_decoder\n",
    "# INPUTS: _lap_burst_detection_results\n",
    "max_num_subplots: int = 25\n",
    "\n",
    "# active_container = _container_container.container\n",
    "active_container = _container_container.masked_container\n",
    "\n",
    "\n",
    "_fig_out_dict = {}\n",
    "\n",
    "epoch_names = ['sprinkle', 'roam']\n",
    "widget_out_dict = {}\n",
    "for an_epoch_name in epoch_names:\n",
    "    # k = 'roam'\n",
    "    # k = 'sprinkle'\n",
    "    # k = 'maze_GLOBAL'\n",
    "\n",
    "    a_sess = curr_active_pipeline.filtered_sessions[an_epoch_name]\n",
    "    pos_df = a_sess.position.to_dataframe()\n",
    "    pos_df['speed_xy'] = np.sqrt(np.power(pos_df['velocity_x_smooth'], 2) +  np.power(pos_df['velocity_y_smooth'], 2))\n",
    "\n",
    "    laps_df: pd.DataFrame = ensure_dataframe(a_sess.laps)\n",
    "    # for k, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    #     pos_df = a_sess.position.to_dataframe()\n",
    "    spikes_df = a_sess.spikes_df.spikes.adding_lap_identity_column(laps_epoch_df=a_sess.laps.to_dataframe(), epoch_id_key_name='lap')\n",
    "    # spikes_df\n",
    "\n",
    "    lap_only_pos_df: pd.DataFrame = pos_df.dropna(subset=['lap'], inplace=False)\n",
    "    lap_only_pos_df['lap'] = lap_only_pos_df['lap'].astype(int)\n",
    "    lap_pos_df_dict = lap_only_pos_df.pho.partition_df_dict('lap')\n",
    "    # lap_pos_df_dict\n",
    "    lap_only_pos_df\n",
    "\n",
    "\n",
    "    a_decoder = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "    # a_result2D: DecodedFilterEpochsResult = decoded_local_epochs_result.frame_divided_epochs_results[an_epoch_name]\n",
    "    a_new_global_decoder2D = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "    # a_result2D = results2D.a_result2D\n",
    "    # a_new_global_decoder2D = results2D.a_new_global_decoder2D\n",
    "    ## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "    xbin = deepcopy(a_new_global_decoder2D.xbin)\n",
    "    xbin_centers = deepcopy(a_new_global_decoder2D.xbin_centers)\n",
    "    ybin_centers = deepcopy(a_new_global_decoder2D.ybin_centers)\n",
    "    ybin = deepcopy(a_new_global_decoder2D.ybin)\n",
    "\n",
    "    plotter_kwargs = dict(xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers)\n",
    "    \n",
    "\n",
    "    # PLOT _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    # arrow_concentration_kwargs = dict(\n",
    "    #     arrow_skip = 50, time_cmap='viridis',\n",
    "    #     mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "    # \tarrow_length_multiplier = 0.2, arrow_length_constant = 0.05,\n",
    "    # \tarrow_lw = 0.5,\n",
    "    # )\n",
    "\n",
    "    arrow_concentration_kwargs = dict(\n",
    "        arrow_skip = 50, time_cmap='viridis',\n",
    "        mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "        arrow_length_multiplier = 0.05, arrow_length_constant = 0.01,\n",
    "        arrow_lw = 0.5,\n",
    "    )\n",
    "\n",
    "    plot_lap_trajectories_2d_kwargs = dict(\n",
    "        curr_num_subplots=(6*5), active_page_index=0, fixed_columns = 6,\n",
    "    )\n",
    "\n",
    "    out3: GenericMatplotlibContainer = plot_lap_trajectories_2d(a_sess, **plot_lap_trajectories_2d_kwargs, use_time_gradient_line=True, arrow_concentration_kwargs=arrow_concentration_kwargs,\n",
    "                                                                fig_size_inches=None)\n",
    "    \n",
    "    # out3: GenericMatplotlibContainer = plot_lap_trajectories_3d(a_sess, **plot_lap_trajectories_2d_kwargs, \n",
    "    #                                                             single_combined_plot = True,\n",
    "    #                                                             # use_time_gradient_line=True, arrow_concentration_kwargs=arrow_concentration_kwargs,\n",
    "    #                                                             # fig_size_inches=None,\n",
    "    #                                                             )\n",
    "    \n",
    "    _fig_out_dict[an_epoch_name] = out3\n",
    "    p3, axs, laps_pages3 = out3.fig, out3.axes, out3.plots_data.laps_pages\n",
    "    perform_update_title_subtitle(fig=out3.fig, ax=None, title_string=f\"{an_epoch_name} - 2d runs\", subtitle_string=f\"{an_epoch_name}\")\n",
    "    \n",
    "\n",
    "    widget_out_dict[an_epoch_name] = p3.canvas.parent()\n",
    "    # _fig_out_dict[k] = plot_lap_trajectories_3d_napari(a_sess, lap_id_dependent_z_offset=4.0)\n",
    "    # viewer, layer, lap_ids = _fig_out_dict[k]\n",
    "    # viewer will be shown if show=True\n",
    "\n",
    "    # p3\n",
    "    \n",
    "laps_merged_out: GenericPyQtGraphContainer = DockAreaWrapper.wrap_horizontally_with_dockAreaWindow(title='Laps 2D', **widget_out_dict)\n",
    "laps_merged_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traja\n",
    "from traja.contrib import rdp\n",
    "from traja import TrajaCollection\n",
    "\n",
    "# trajCol1: TrajaCollection = None\n",
    "\n",
    "print(np.shape(lap_only_pos_df))\n",
    "\n",
    "a_lap_only_pos_df: pd.DataFrame = lap_only_pos_df.rename(columns={'t': 'time'}, inplace=False)\n",
    "mask = rdp(a_lap_only_pos_df[['x', 'y']].to_numpy(), algo=\"iter\", return_mask=True)\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_lap_only_pos_df = a_lap_only_pos_df[mask]\n",
    "downsampled_lap_only_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downsampled_lap_only_pos_df = traja.resample_time(a_lap_only_pos_df, step_time='250L') ## 500ms\n",
    "print(np.shape(downsampled_lap_only_pos_df))\n",
    "\n",
    "\n",
    "rdp(\n",
    "# (57470, 29)\n",
    "# (5264, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c1306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0f0ee",
   "metadata": {
    "tags": [
     "🟢 active-2026-02-11"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.MovementBurstDetection import compute_movement_trajectories_from_bursts\n",
    "    \n",
    "\n",
    "burst_detector_kwargs = dict(\n",
    "            min_burst_duration=1.5,      # Minimum burst duration in seconds\n",
    "            min_rest_duration=0.1,       # Minimum rest period between bursts\n",
    "            velocity_smoothing=0.15,     # Smoothing for velocity calculation\n",
    "            bocd_hazard=120,             # Sensitivity of changepoint detection\n",
    "            # clustering_method='hdbscan',  # Clustering algorithm\n",
    "            # clustering_method='dbscan',  # Clustering algorithm\n",
    "            clustering_method='dip',  # Use DipExt from clustpy\n",
    "            use_gpu=False,             # Set to True if you have CUDA\n",
    ")\n",
    "_lap_burst_detection_results, _out_laps = compute_movement_trajectories_from_bursts(curr_active_pipeline, **burst_detector_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1223cb41",
   "metadata": {
    "tags": [
     "active-2026-02-11",
     "🟢 active-2026-02-11"
    ]
   },
   "source": [
    "## 2026-02-10 - Plot Both Laps and interleaved non-laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f340a",
   "metadata": {
    "tags": [
     "🟢 active-2026-02-11"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericMatplotlibContainer\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_2d, plot_lap_trajectories_3d, plot_lap_trajectories_3d_napari\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import _plot_helper_add_arrow\n",
    "\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_3d_napari\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# INPUTS: _lap_burst_detection_results\n",
    "max_num_subplots: int = 25\n",
    "\n",
    "# active_container = _container_container.container\n",
    "active_container = _container_container.masked_container\n",
    "\n",
    "_fig_out_dict = {}\n",
    "\n",
    "epoch_names = ['sprinkle', 'roam']\n",
    "widget_out_dict = {}\n",
    "for an_epoch_name in epoch_names:\n",
    "    # k = 'roam'\n",
    "    # k = 'sprinkle'\n",
    "    # k = 'maze_GLOBAL'\n",
    "\n",
    "    detector, results, analyzer, summary = _lap_burst_detection_results[an_epoch_name]\n",
    "    a_laps = results['laps_obj'] # Laps\n",
    "    segments_epoch_df: pd.DataFrame = results['segments_epoch_df']\n",
    "    segments_epoch_df = segments_epoch_df.epochs.get_valid_df()\n",
    "\n",
    "    a_sess = curr_active_pipeline.filtered_sessions[an_epoch_name]\n",
    "\n",
    "    ## INPUTS: a_laps\n",
    "    # Extract position data for each burst\n",
    "    df_clean = results['processed_data']\n",
    "    curr_position_df = df_clean\n",
    "    # curr_position_df = self.position.to_dataframe() # get the position dataframe from the session\n",
    "    curr_laps_df = a_laps.to_dataframe()\n",
    "    curr_position_df = curr_position_df.position.adding_lap_info(laps_df=curr_laps_df, inplace=False)\n",
    "    \n",
    "    curr_position_df = curr_position_df.time_point_event.adding_epochs_identity_column(epochs_df=segments_epoch_df, epoch_id_key_name='segment_id', epoch_label_column_name='label', override_time_variable_name='t',\n",
    "                                                            no_interval_fill_value=-1, should_replace_existing_column=True, drop_non_epoch_events=False)\n",
    "\n",
    "    \n",
    "    # curr_position_df_split = curr_position_df.pho.partition_df_dict(partitionColumn='segment_id')\n",
    "    segment_ids, curr_position_df_split = curr_position_df.pho.partition_df(partitionColumn='segment_id') # : List[pd.DataFrame]\n",
    "    segment_ids: NDArray = np.array(segment_ids[:max_num_subplots]).astype(int)\n",
    "    curr_position_df_split: List[pd.DataFrame] = curr_position_df_split[:max_num_subplots]\n",
    "\n",
    "    # curr_position_df_split = curr_position_df.pho.partition_df_dict(partitionColumn='segment_id')\n",
    "    # curr_position_df_split ## there are 1000+ of these\n",
    "    \n",
    "    ## OUTPUTS: epoch_ids, curr_position_df_split\n",
    "    \n",
    "    # PLOT _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    # arrow_concentration_kwargs = dict(\n",
    "    #     arrow_skip = 50, time_cmap='viridis',\n",
    "    #     mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "    # \tarrow_length_multiplier = 0.2, arrow_length_constant = 0.05,\n",
    "    # \tarrow_lw = 0.5,\n",
    "    # )\n",
    "\n",
    "    arrow_concentration_kwargs = dict(\n",
    "        arrow_skip = 50, time_cmap='viridis',\n",
    "        mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "        arrow_length_multiplier = 0.05, arrow_length_constant = 0.01,\n",
    "        arrow_lw = 0.5,\n",
    "    )\n",
    "\n",
    "    plot_lap_trajectories_2d_kwargs = dict(\n",
    "        # curr_num_subplots=(6*5), \n",
    "        active_page_index=0, fixed_columns = 6,\n",
    "    )\n",
    "\n",
    "    a_decoder = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "    # a_result2D: DecodedFilterEpochsResult = decoded_local_epochs_result.frame_divided_epochs_results[an_epoch_name]\n",
    "    a_new_global_decoder2D = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "    ## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "    xbin = deepcopy(a_new_global_decoder2D.xbin)\n",
    "    xbin_centers = deepcopy(a_new_global_decoder2D.xbin_centers)\n",
    "    ybin_centers = deepcopy(a_new_global_decoder2D.ybin_centers)\n",
    "    ybin = deepcopy(a_new_global_decoder2D.ybin)\n",
    "\n",
    "    plotter_kwargs = dict(xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers)\n",
    "    ## 2D:\n",
    "    # Choose the ripple epochs to plot:\\\n",
    "    a_result: DecodedFilterEpochsResult = None # a_decoded_filter_epochs_decoder_result_dict['long'] # 2D\n",
    "    num_filter_epochs: int = len(curr_position_df_split) # a_result.num_filter_epochs\n",
    "    print(f'k: {an_epoch_name} has num_filter_epochs: {num_filter_epochs}, len(segment_ids): {len(segment_ids)}')\n",
    "    a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=a_result, **plotter_kwargs)\n",
    "    fig, axs, laps_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(curr_position_df=curr_position_df, epoch_specific_position_dfs=curr_position_df_split, epoch_ids=segment_ids,\n",
    "                                                                            curr_num_subplots=num_filter_epochs,\n",
    "                                                                            **plot_lap_trajectories_2d_kwargs, # active_page_index=0, fixed_columns=10,\n",
    "                                                                            plot_actual_lap_lines=True, use_theoretical_tracks_instead=False)\n",
    "    _fig_out_dict[an_epoch_name] = GenericMatplotlibContainer.init_from_matplotlib_objects(name=f'splitTrajectories[{an_epoch_name}]', figures=[fig], axes=axs, plots_data={'laps_pages': laps_pages})\n",
    "\n",
    "    p3, axs, laps_pages3 = _fig_out_dict[an_epoch_name].fig, _fig_out_dict[an_epoch_name].axes, _fig_out_dict[an_epoch_name].plots_data.laps_pages\n",
    "    perform_update_title_subtitle(fig=_fig_out_dict[an_epoch_name].fig, ax=None, title_string=f\"{an_epoch_name} - 2d runs\", subtitle_string=f\"{an_epoch_name}\")\n",
    "    \n",
    "\n",
    "    widget_out_dict[an_epoch_name] = p3.canvas.parent()\n",
    "\n",
    "## END for an_epoch_name in epoch_names...\n",
    "\n",
    "\n",
    "    \n",
    "laps_merged_out: GenericPyQtGraphContainer = DockAreaWrapper.wrap_horizontally_with_dockAreaWindow(title='Trajectory Segments via `OptimizedMovementBurstDetector` 2D', **widget_out_dict)\n",
    "laps_merged_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed969bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2687ec8",
   "metadata": {},
   "source": [
    "### seemingly laps-only version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericMatplotlibContainer\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_2d, plot_lap_trajectories_3d, plot_lap_trajectories_3d_napari\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import _plot_helper_add_arrow\n",
    "\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_3d_napari\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "\n",
    "\n",
    "def _subfn_plot_single_decoder_laps(a_sess, k: str, num_pages: int, **kwargs) -> GenericMatplotlibContainer:\n",
    "    \"\"\" plots the grid of trajectories/laps for a single decoding epoch \n",
    "    \"\"\"\n",
    "    from pyphoplacecellanalysis.GUI.Qt.Widgets.PaginationCtrl.PaginationControlWidget import PaginationControlWidget\n",
    "\n",
    "    # def _build_page_controls(self, a_past_future_name: str, num_pages: int):\n",
    "    #     \"\"\"Build page navigation controls for a trajectory widget using PaginationControlWidget.\n",
    "        \n",
    "    #     Note: The controls widget is created but NOT added to the dock here.\n",
    "    #     It should be added to a container widget that also contains the plot.\n",
    "    #     This method just creates and stores the control widget.\n",
    "    #     \"\"\"\n",
    "    #     # Check if controls already exist\n",
    "    #     if a_past_future_name in self.page_controls and 'widget' in self.page_controls[a_past_future_name]:\n",
    "    #         # Controls already exist, just update them\n",
    "    #         self._update_page_controls_visibility(a_past_future_name, num_pages)\n",
    "    #         return\n",
    "        \n",
    "    #     # Create PaginationControlWidget\n",
    "    #     pagination_widget = PaginationControlWidget(n_pages=num_pages)\n",
    "        \n",
    "    #     # Connect signals\n",
    "    #     pagination_widget.jump_to_page.connect(lambda page_idx: self._on_page_jump(a_past_future_name, page_idx))\n",
    "    #     pagination_widget.jump_previous_page.connect(lambda: self._on_page_change(a_past_future_name, -1))\n",
    "    #     pagination_widget.jump_next_page.connect(lambda: self._on_page_change(a_past_future_name, 1))\n",
    "        \n",
    "    #     # Store references\n",
    "    #     if a_past_future_name not in self.page_controls:\n",
    "    #         self.page_controls[a_past_future_name] = {}\n",
    "    #     self.page_controls[a_past_future_name]['widget'] = pagination_widget\n",
    "        \n",
    "    #     # Set initial page index if needed\n",
    "    #     initial_page_idx = self.trajectory_active_page_idx.get(a_past_future_name, 0)\n",
    "    #     if initial_page_idx != 0:\n",
    "    #         pagination_widget.programmatically_update_page_idx(initial_page_idx, block_signals=True)\n",
    "        \n",
    "    #     # Set initial visibility\n",
    "    #     self._update_page_controls_visibility(a_past_future_name, num_pages)\n",
    "\n",
    "\n",
    "    # def _update_page_controls_visibility(self, a_past_future_name: str, num_pages: int):\n",
    "    #     \"\"\"Update visibility and state of page controls based on number of pages.\"\"\"\n",
    "    #     if a_past_future_name not in self.page_controls:\n",
    "    #         return\n",
    "        \n",
    "    #     page_controls = self.page_controls[a_past_future_name]\n",
    "    #     should_show = num_pages > 1\n",
    "    #     active_page_idx = self.trajectory_active_page_idx.get(a_past_future_name, 0)\n",
    "        \n",
    "    #     if 'widget' in page_controls and page_controls['widget'] is not None:\n",
    "    #         pagination_widget = page_controls['widget']\n",
    "    #         pagination_widget.setVisible(should_show)\n",
    "            \n",
    "    #         if should_show:\n",
    "    #             # Update the number of pages\n",
    "    #             if pagination_widget.state.n_pages != num_pages:\n",
    "    #                 pagination_widget.state.n_pages = num_pages\n",
    "    #                 pagination_widget._on_update_pagination()\n",
    "                \n",
    "    #             # Update the current page index if it changed externally\n",
    "    #             if pagination_widget.state.current_page_idx != active_page_idx:\n",
    "    #                 pagination_widget.programmatically_update_page_idx(active_page_idx, block_signals=True)\n",
    "\n",
    "\n",
    "    # def _on_page_jump(self, a_past_future_name: str, page_idx: int):\n",
    "    #     \"\"\"Handle direct page jump from PaginationControlWidget.\"\"\"\n",
    "    #     # Update the page index\n",
    "    #     self.trajectory_active_page_idx[a_past_future_name] = page_idx\n",
    "        \n",
    "    #     # Re-render the widget with the new page\n",
    "    #     self._refresh_trajectory_widget(a_past_future_name)\n",
    "\n",
    "\n",
    "    # def _on_page_change(self, a_past_future_name: str, direction: int):\n",
    "    #     \"\"\"Handle page navigation button clicks (direction: -1 for prev, 1 for next).\"\"\"\n",
    "    #     epochs_pages = self.trajectory_epochs_pages.get(a_past_future_name, [])\n",
    "    #     num_pages = len(epochs_pages)\n",
    "    #     if num_pages == 0:\n",
    "    #         return\n",
    "        \n",
    "    #     current_page = self.trajectory_active_page_idx.get(a_past_future_name, 0)\n",
    "    #     new_page = current_page + direction\n",
    "    #     new_page = max(0, min(new_page, num_pages - 1))\n",
    "        \n",
    "    #     if new_page != current_page:\n",
    "    #         self.trajectory_active_page_idx[a_past_future_name] = new_page\n",
    "            \n",
    "    #         # Update pagination widget if it exists\n",
    "    #         if a_past_future_name in self.page_controls and 'widget' in self.page_controls[a_past_future_name]:\n",
    "    #             pagination_widget = self.page_controls[a_past_future_name]['widget']\n",
    "    #             pagination_widget.programmatically_update_page_idx(new_page, block_signals=True)\n",
    "            \n",
    "    #         # Re-render the widget\n",
    "    #         self._refresh_trajectory_widget(a_past_future_name)\n",
    "\n",
    "    # ==================================================================================================================================================================================================================================================================================== #\n",
    "    # BEGIN FUNCTION BODY                                                                                                                                                                                                                                                                  #\n",
    "    # ==================================================================================================================================================================================================================================================================================== #\n",
    "    \n",
    "    # Create pagination controls BEFORE creating container\n",
    "    # Always create them (even if hidden initially) to ensure single initialization\n",
    "    # Use num_pages from current data, or 1 as placeholder if no pages yet\n",
    "    initial_num_pages = max(1, num_pages) if num_pages > 0 else 1\n",
    "\n",
    "\n",
    "    arrow_concentration_kwargs = dict(\n",
    "        arrow_skip = 50, time_cmap='viridis',\n",
    "        mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "        arrow_length_multiplier = 0.05, arrow_length_constant = 0.01,\n",
    "        arrow_lw = 0.5,\n",
    "    )\n",
    "\n",
    "    plot_lap_trajectories_2d_kwargs = dict(\n",
    "        curr_num_subplots=(6*5), active_page_index=0, fixed_columns = 6,\n",
    "    )\n",
    "\n",
    "    out3: GenericMatplotlibContainer = plot_lap_trajectories_2d(a_sess, **plot_lap_trajectories_2d_kwargs, use_time_gradient_line=True, arrow_concentration_kwargs=arrow_concentration_kwargs,\n",
    "                                                                fig_size_inches=None)\n",
    "    _fig_out_dict[k] = out3\n",
    "    p3, axs, laps_pages3 = out3.fig, out3.axes, out3.plots_data.laps_pages\n",
    "    perform_update_title_subtitle(fig=out3.fig, ax=None, title_string=f\"{k} - 2d runs\", subtitle_string=f\"{k}\")\n",
    "    \n",
    "    widget = p3.canvas.parent()\n",
    "    out3.ui.owning_widget = widget\n",
    "\n",
    "    # _build_page_controls(widget, k, initial_num_pages)\n",
    "    \n",
    "    return out3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo3D_decoder\n",
    "\n",
    "_fig_out_dict = {}\n",
    "\n",
    "epoch_names = ['sprinkle', 'roam']\n",
    "widget_out_dict = {}\n",
    "for an_epoch_name in epoch_names:\n",
    "    # k = 'roam'\n",
    "    # k = 'sprinkle'\n",
    "    # k = 'maze_GLOBAL'\n",
    "\n",
    "    a_sess = curr_active_pipeline.filtered_sessions[an_epoch_name]\n",
    "    pos_df = a_sess.position.to_dataframe()\n",
    "    pos_df['speed_xy'] = np.sqrt(np.power(pos_df['velocity_x_smooth'], 2) +  np.power(pos_df['velocity_y_smooth'], 2))\n",
    "\n",
    "    laps_df: pd.DataFrame = ensure_dataframe(a_sess.laps)\n",
    "    # for k, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    #     pos_df = a_sess.position.to_dataframe()\n",
    "    spikes_df = a_sess.spikes_df.spikes.adding_lap_identity_column(laps_epoch_df=a_sess.laps.to_dataframe(), epoch_id_key_name='lap')\n",
    "    # spikes_df\n",
    "\n",
    "    lap_only_pos_df: pd.DataFrame = pos_df.dropna(subset=['lap'], inplace=False)\n",
    "    lap_only_pos_df['lap'] = lap_only_pos_df['lap'].astype(int)\n",
    "    lap_pos_df_dict = lap_only_pos_df.pho.partition_df_dict('lap')\n",
    "    # lap_pos_df_dict\n",
    "    lap_only_pos_df\n",
    "    \n",
    "    # PLOT _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    # arrow_concentration_kwargs = dict(\n",
    "    #     arrow_skip = 50, time_cmap='viridis',\n",
    "    #     mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "    # \tarrow_length_multiplier = 0.2, arrow_length_constant = 0.05,\n",
    "    # \tarrow_lw = 0.5,\n",
    "    # )\n",
    "\n",
    "    # arrow_concentration_kwargs = dict(\n",
    "    #     arrow_skip = 50, time_cmap='viridis',\n",
    "    #     mutation_scale_multiplier = 20, mutation_scale_constant = 1,\n",
    "    #     arrow_length_multiplier = 0.05, arrow_length_constant = 0.01,\n",
    "    #     arrow_lw = 0.5,\n",
    "    # )\n",
    "\n",
    "    # plot_lap_trajectories_2d_kwargs = dict(\n",
    "    #     curr_num_subplots=(6*5), active_page_index=0, fixed_columns = 6,\n",
    "    # )\n",
    "\n",
    "    # out3: GenericMatplotlibContainer = plot_lap_trajectories_2d(a_sess, **plot_lap_trajectories_2d_kwargs, use_time_gradient_line=True, arrow_concentration_kwargs=arrow_concentration_kwargs,\n",
    "    #                                                             fig_size_inches=None)\n",
    "    # _fig_out_dict[k] = out3\n",
    "    # p3, axs, laps_pages3 = out3.fig, out3.axes, out3.plots_data.laps_pages\n",
    "    # perform_update_title_subtitle(fig=out3.fig, ax=None, title_string=f\"{k} - 2d runs\", subtitle_string=f\"{k}\")\n",
    "    \n",
    "\n",
    "    # widget_out_dict[k] = p3.canvas.parent()\n",
    "    # _fig_out_dict[k] = plot_lap_trajectories_3d_napari(a_sess, lap_id_dependent_z_offset=4.0)\n",
    "    # viewer, layer, lap_ids = _fig_out_dict[k]\n",
    "    # viewer will be shown if show=True\n",
    "    \n",
    "    kwargs = dict()\n",
    "\n",
    "    num_pages: int = 1\n",
    "    out3: GenericMatplotlibContainer = _subfn_plot_single_decoder_laps(a_sess=a_sess, k=an_epoch_name, num_pages=num_pages, **kwargs)\n",
    "    widget_out_dict[an_epoch_name] = out3.ui.owning_widget\n",
    "    \n",
    "    # p3\n",
    "    \n",
    "laps_merged_out: GenericPyQtGraphContainer = DockAreaWrapper.wrap_horizontally_with_dockAreaWindow(title='Laps 2D', **widget_out_dict)\n",
    "laps_merged_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, burst in enumerate(results['bursts']):\n",
    "    mask = (df_clean['t'] >= burst['start']) & (df_clean['t'] <= burst['end'])\n",
    "    burst_data = df_clean[mask].copy()\n",
    "    \n",
    "    # Add burst ID\n",
    "    burst_data['burst_id'] = i\n",
    "    \n",
    "    # You can now analyze each burst individually\n",
    "    print(f\"Burst {i}: {len(burst_data)} points, \"\n",
    "          f\"distance={burst['total_distance']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize\n",
    "fig, axes = analyzer.visualize_results(results, save_path='burst_detection_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc68a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on your data\n",
    "results = detector.detect_bursts(pos_df)\n",
    "\n",
    "# Analyze results\n",
    "analyzer = BurstAnalyzer()\n",
    "summary = analyzer.summarize_bursts(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7905eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42923d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize\n",
    "analyzer.visualize_results(results, save_path='burst_detection_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect bursts\n",
    "print(\"\\nDetecting bursts with optimized pipeline...\")\n",
    "results = detector.detect_bursts(pos_df)\n",
    "\n",
    "# Analyze results\n",
    "analyzer = BurstAnalyzer()\n",
    "summary = analyzer.summarize_bursts(results)\n",
    "\n",
    "print(f\"\\nDetection Summary:\")\n",
    "print(f\"  Total bursts: {summary['total_bursts']}\")\n",
    "if summary['total_bursts'] > 0:\n",
    "    print(f\"  Total burst duration: {summary['total_burst_duration']:.1f}s\")\n",
    "    print(f\"  Mean burst duration: {summary['mean_burst_duration']:.2f} ± {summary['std_burst_duration']:.2f}s\")\n",
    "    print(f\"  Mean burst speed: {summary['mean_burst_speed']:.3f}\")\n",
    "    print(f\"  Total distance during bursts: {summary['total_distance']:.2f}\")\n",
    "    if 'burst_frequency' in summary:\n",
    "        print(f\"  Burst frequency: {summary['burst_frequency']:.3f} Hz\")\n",
    "\n",
    "# Display individual bursts\n",
    "print(f\"\\nDetected Bursts:\")\n",
    "for i, burst in enumerate(results['bursts']):\n",
    "    print(f\"  Burst {i+1}: {burst['start']:.1f}-{burst['end']:.1f}s \"\n",
    "            f\"(dur: {burst['duration']:.1f}s, speed: {burst['mean_speed']:.3f}, \"\n",
    "            f\"dist: {burst.get('total_distance', 0):.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"\\nGenerating visualization...\")\n",
    "analyzer.visualize_segmentation(results, save_path='optimized_burst_detection.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6538552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector (using ensemble method for best results)\n",
    "detector = MovementBurstDetector(\n",
    "    min_burst_duration=0.8,\n",
    "    min_rest_duration=1.2,\n",
    "    velocity_smoothing=0.15,\n",
    "    method='combo'  # Best performing ensemble method\n",
    ")\n",
    "\n",
    "# Detect bursts\n",
    "results = detector.detect_bursts(pos_df)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nDetected {len(results['bursts'])} movement bursts:\")\n",
    "## OUTPUTS: results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-\" * 60)\n",
    "for i, burst in enumerate(results['bursts']):\n",
    "    print(f\"Burst {i+1}:\")\n",
    "    print(f\"  Time: {burst['start']:.1f} - {burst['end']:.1f} s \"\n",
    "            f\"(duration: {burst['duration']:.1f} s)\")\n",
    "    print(f\"  Mean speed: {burst['mean_speed']:.2f}\")\n",
    "    print(f\"  Distance: {burst.get('total_distance', 0):.2f}\")\n",
    "    print(f\"  Tortuosity: {burst.get('tortuosity', 0):.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "fig = visualize_bursts(pos_df, results, save_path='burst_detection.png')\n",
    "\n",
    "# Evaluate different methods\n",
    "all_results = evaluate_burst_detection(pos_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e18406",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_dir_2D_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df.pho.partition_df_dict('maze_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15736a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_df[['velocity_x_smooth', 'velocity_y_smooth']]\n",
    "\n",
    "# pos_df['speed_xy'] = np.sqrt(np.power(pos_df['velocity_x_smooth'], 2) +  np.power(pos_df['velocity_y_smooth'], 2))\n",
    "# pos_df\n",
    "\n",
    "pos_df.plot(x='t', y='speed_xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb155f2",
   "metadata": {
    "tags": [
     "vis-nice"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sess.laps.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performed 3 aggregations grouped on column: 'lap'\n",
    "each_lap_agg_stats_df = lap_only_pos_df.groupby(['lap']).agg(t_count=('t', 'count'), speed_min=('speed', 'min'), speed_max=('speed', 'max')).reset_index()\n",
    "each_lap_agg_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_sess.compute_laps_position_df()\n",
    "# a_sess.compute_spikes_PBEs()\n",
    "\n",
    "# if 'lap' not in a_sess.spikes_df.columns:\n",
    "    # spikes_df = a_sess.spikes_df.spikes.adding_lap_identity_column(laps_epoch_df=a_sess.laps.to_dataframe(), epoch_id_key_name='lap')\n",
    "spikes_df = a_sess.spikes_df.spikes.adding_lap_identity_column(laps_epoch_df=a_sess.laps.to_dataframe(), epoch_id_key_name='lap')\n",
    "spikes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.Mixins.LapsVisualizationMixin import LapsVisualizationMixin\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_3d\n",
    "\n",
    "## single_combined_plot == True mode (mode 1.):\n",
    "plotter, laps_pages = plot_lap_trajectories_3d(a_sess, single_combined_plot=True, color_by_speed=False, lap_id_dependent_z_offset=3.5)\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "## single_combined_plot == True mode (mode 1.):\n",
    "plotter, laps_pages = plot_lap_trajectories_3d(a_sess, single_combined_plot=False, maximum_fixed_columns=10, color_by_speed=False, lap_id_dependent_z_offset=3.5)\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de73dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvistaqt import BackgroundPlotter, MultiPlotter\n",
    "# p[0,0]\n",
    "\n",
    "scenes_output_path = Path('data/3d_scenes').resolve()\n",
    "assert scenes_output_path.exists()\n",
    "\n",
    "\n",
    "bg_p: BackgroundPlotter = plotter[0,0]\n",
    "# bg_p.export_gltf(scenes_output_path.joinpath('2025-10-21_3d_laps.gltf').as_posix())\n",
    "bg_p.export_obj(scenes_output_path.joinpath('2025-10-21_3d_laps.obj').as_posix())\n",
    "bg_p.export_vtkjs(scenes_output_path.joinpath('2025-10-21_3d_laps').as_posix())\n",
    "# bg_p.export_html(scenes_output_path.joinpath('2025-10-21_3d_laps.html'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## single_combined_plot == False mode (mode 2.):        \n",
    "p2, laps_pages2 = plot_lap_trajectories_3d(a_sess, single_combined_plot=False, curr_num_subplots=len(curr_active_pipeline.sess.laps.lap_id), active_page_index=1)\n",
    "p2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5da581",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b15c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out3.plots.artists['line_artists']\n",
    "\n",
    "a_linear_index: int = 0\n",
    "\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5(override_path=Path('2026-02-10_full_pipeline.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f23fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ddf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_exports_path = curr_active_pipeline.get_output_path().joinpath('EXPORTS').resolve()\n",
    "curr_exports_path.mkdir(exist_ok=True)\n",
    "\n",
    "curr_out_csv = curr_exports_path.joinpath('position.csv').resolve()\n",
    "\n",
    "pos_df: pd.DataFrame = curr_active_pipeline.sess.position.to_dataframe()\n",
    "pos_df.to_csv(curr_out_csv)\n",
    "print(f'saved: \"{curr_out_csv.as_uri()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eaf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for an_epoch_name, a_sess in curr_active_pipeline.filtered_sessions.items():\n",
    "    a_sess.position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subplots: int = len(out3.plots.artists['line_artists'])\n",
    "for a_linear_index in np.arange(num_subplots):\n",
    "    _out_markers =  out3.plots.artists['line_markers'][a_linear_index]\n",
    "    line = out3.plots.artists['line_artists'][a_linear_index]    \n",
    "    _out_markers.set_sizes(np.atleast_1d(np.full_like(_out_markers.get_sizes(), 0.1)))\n",
    "    \n",
    "p3.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d932d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = out3.plots.artists['line_artists'][0]\n",
    "# --- Extract x/y data back ---\n",
    "segments = line.get_segments()  # list of (N, 2) arrays\n",
    "xdata = np.concatenate([seg[:, 0] for seg in segments])\n",
    "ydata = np.concatenate([seg[:, 1] for seg in segments])\n",
    "\n",
    "xdata, ydata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "line.get_color().shape # (342, 4)\n",
    "# line.get_colors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_markers = {}\n",
    "line_markers['start'] = _plot_helper_add_arrow(line, position=0, position_mode='index', direction='right', size=20, color='green') # start\n",
    "line_markers['end'] = _plot_helper_add_arrow(line, position=None, position_mode='index', direction='right', size=20, color='yellow') # middle\n",
    "# _plot_helper_add_arrow(line[0], position=curr_lap_num_points, position_mode='index', direction='right', size=20, color='red') # end\n",
    "line_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70767d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640038aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.media_output_helpers import save_array_as_video\n",
    "\n",
    "video_out_path = save_array_as_video(array=active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'], video_filename='output/videos/snapshot_occupancy_weighted_tuning_maps.avi', isColor=False)\n",
    "print(f'video_out_path: {video_out_path}')\n",
    "reveal_in_system_file_manager(video_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.active_embedded_track_pyqtgraph_time_sync_widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('out', _out_container_new.plots, max_depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ccdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_container_new.plots.parent_root_widget\n",
    "print_keys_if_possible('', _out_container_new, max_depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, plotter in _out_container_new.ui.sync_plotters.items():\n",
    "    plotter.plots_data\n",
    "    # export_pyqtgraph_plot(plotter.ui.root_graphics_layout_widget)\n",
    "\n",
    "    plotter.export_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ad4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_position_decoder_plotter.on_window_changed(5.0, 15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b51df",
   "metadata": {},
   "source": [
    "# ⚓✅💾 Export ALL tracks (both plotting backends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e682ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.dock_manager_widget.get_leaf_only_flat_dock_identifiers_list() # ['interval_overview', 'intervals', 'rasters[raster_overview]', 'rasters[raster_window]', 'new_curves_separate_plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyphoplacecellanalysis.External.pyqtgraph_extensions.exporters.CustomImageExporter import CustomImageExporter\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.exporters.ImageExporter import ImageExporter as CustomImageExporter\n",
    "\n",
    "# exporter = ImageExporter(graphics_item)\n",
    "\n",
    "# Export an active_2d_plot timeline to a multi-page PDF\n",
    "exporter = CustomImageExporter(active_2d_plot)\n",
    "saved_pdf_path = exporter.export('output.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c06169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.pyqtgraph.exporters.ImageExporter import ImageExporter\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.Qt import QtWidgets\n",
    "\n",
    "def export_all_scenes_in_widget(parent_widget: QtWidgets.QWidget, base_filename: str):\n",
    "    \"\"\"\n",
    "    Export all QGraphicsScene objects found in QGraphicsView widgets \n",
    "    within the parent widget.\n",
    "\n",
    "    #TODO 2025-12-18 05:59: - [ ] This works correctly for each track within the current viewport window, but does not export for all time.\n",
    "    \n",
    "    Args:\n",
    "        parent_widget: The parent QWidget containing QGraphicsView widgets\n",
    "        base_filename: Base filename (will append _1, _2, etc. for multiple scenes)\n",
    "    \"\"\"\n",
    "    # Find all QGraphicsView widgets recursively\n",
    "    views = parent_widget.findChildren(QtWidgets.QGraphicsView)\n",
    "    \n",
    "    if not views:\n",
    "        print(\"No QGraphicsView widgets found in parent widget\")\n",
    "        return\n",
    "    \n",
    "    exported_files = []\n",
    "    for i, view in enumerate(views):\n",
    "        scene = view.scene()\n",
    "        if scene is None:\n",
    "            continue\n",
    "            \n",
    "        # Create exporter for this scene\n",
    "        exporter = ImageExporter(scene)\n",
    "        \n",
    "        # Generate filename\n",
    "        if len(views) == 1:\n",
    "            filename = f\"{base_filename}.png\"\n",
    "        else:\n",
    "            name, ext = base_filename.rsplit('.', 1) if '.' in base_filename else (base_filename, 'png')\n",
    "            filename = f\"{name}_{i+1}.{ext}\"\n",
    "        \n",
    "        # Export\n",
    "        try:\n",
    "            exporter.export(fileName=filename)\n",
    "            exported_files.append(filename)\n",
    "            print(f\"Exported scene {i+1} to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting scene {i+1}: {e}\")\n",
    "    \n",
    "    return exported_files\n",
    "\n",
    "\n",
    "export_all_scenes_in_widget(parent_widget=active_2d_plot, base_filename='test_export_all_scenes_in_widget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accba350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.PyqtgraphTimeSynchronizedWidget import PyqtgraphTimeSynchronizedWidget\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.MatplotlibTimeSynchronizedWidget import MatplotlibTimeSynchronizedWidget\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.image as mimage\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureToImageHelpers\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n",
    "\n",
    "# relative_data_output_parent_folder = Path('data').resolve()\n",
    "\n",
    "relative_data_output_parent_folder = curr_active_pipeline.get_output_path().resolve()\n",
    "Assert.path_exists(relative_data_output_parent_folder)\n",
    "\n",
    "## INPUTS: im_posterior_x_stack, track_labels, \n",
    "output_pdf_path: Path = relative_data_output_parent_folder.joinpath('2025-12-18_all_timeline_tracks_exported_stack.pdf')\n",
    "\n",
    "# included_track_dock_identifiers = None\n",
    "# included_track_dock_identifiers = [\n",
    "# \t# 'interval_overview',\n",
    "#     'intervals',\n",
    "#     # 'rasters[raster_overview]',\n",
    "#     'rasters[raster_window]',\n",
    "#      'new_curves_separate_plot',\n",
    "#     # 'ContinuousDecode_long_LR - t_bin_size: 0.025',\n",
    "#     # 'ContinuousDecode_long_RL - t_bin_size: 0.025',\n",
    "#     # 'ContinuousDecode_short_LR - t_bin_size: 0.025',\n",
    "#     # 'ContinuousDecode_short_RL - t_bin_size: 0.025',\n",
    "#     # 'marginal_over_track_ID_ContinuousDecode - t_bin_size: 0.025',\n",
    "\n",
    "#  'ContinuousDecode_long_LR - t_bin_size: 0.05',\n",
    "#  'ContinuousDecode_long_RL - t_bin_size: 0.05',\n",
    "#  'ContinuousDecode_short_LR - t_bin_size: 0.05',\n",
    "#  'ContinuousDecode_short_RL - t_bin_size: 0.05',\n",
    "#  'marginal_over_track_ID_ContinuousDecode - t_bin_size: 0.05'\n",
    "# ]\n",
    "included_track_dock_identifiers = [\n",
    "    # 'interval_overview',\n",
    "    'intervals',\n",
    "    # 'rasters[raster_overview]',\n",
    "    'rasters[raster_window]',\n",
    "    'new_curves_separate_plot',\n",
    "    # 'mpl_position_curves',\n",
    "    #  'MenuCommand_display_plot_marginal_1D_most_likely_position_comparisons',\n",
    "    #  'global context',\n",
    "    #  'global context (overview),\n",
    "]\n",
    "\n",
    "\n",
    "# track_labels: List[str] = list(included_track_dock_identifiers_to_track_labels_dict.values())\n",
    "track_labels = None\n",
    "saved_output_pdf_path = FigureToImageHelpers.export_wrapped_tracks_to_paged_df(active_2d_plot, output_pdf_path=output_pdf_path, included_track_dock_identifiers=included_track_dock_identifiers, track_labels=track_labels,\n",
    "                                                                                debug_max_num_pages=5, dpi=600, debug_print=True)\n",
    "\n",
    "## OUTPUTS: output_pdf_path, included_track_dock_identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52892a94",
   "metadata": {},
   "source": [
    "## 2025-12-02 - Matplotlib-based plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "widget, fig, ax, dDisplayItem = active_2d_plot.add_new_matplotlib_render_plot_widget(name='mpl_position_curves')\n",
    "pos_obj = curr_active_pipeline.sess.position\n",
    "pos_df: pd.DataFrame = pos_obj.to_dataframe()\n",
    "pos_df\n",
    "ax = ax[0]\n",
    "\n",
    "## INPUTS: pos_df, ax\n",
    "print(list(pos_df.columns)) # ['t', 'x', 'y', 'z', 'lin_pos', 'dt', 'velocity_x', 'acceleration_x', 'velocity_y', 'acceleration_y', 'velocity_z', 'acceleration_z', 'x_smooth', 'y_smooth', 'z_smooth', 'velocity_x_smooth', 'acceleration_x_smooth', 'velocity_y_smooth', 'acceleration_y_smooth', 'velocity_z_smooth', 'acceleration_z_smooth', 'approx_head_dir_degrees', 'head_dir_angle_binned', 'lap', 'lap_dir', 'lap_dir_2D', 'lap_dir_1D', 'speed', 'speed_xy'\n",
    "\n",
    "time_col_name: str = 't'\n",
    "# pos_col_series_list = ['x_smooth', 'y_smooth', 'z_smooth', 'approx_head_dir_degrees', 'speed', 'speed_xy']\n",
    "pos_col_series_list = ['x_smooth', 'y_smooth', 'z_smooth', 'lin_pos']\n",
    "\n",
    "_out_series_dict = {}\n",
    "for a_pos_col_name in pos_col_series_list:\n",
    "    # _out_series_dict[a_pos_col_name] = ax.scatter(pos_df[time_col_name], pos_df[a_pos_col_name], label=a_pos_col_name)\n",
    "    _out_series_dict[a_pos_col_name] = ax.plot(pos_df[time_col_name], pos_df[a_pos_col_name], label=a_pos_col_name)\n",
    "a_legend = ax.legend()\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget(identifier='mpl_position_curves', sync_mode=SynchronizedPlotMode.TO_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a83ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79259ae0",
   "metadata": {},
   "source": [
    "# 💯🎯 2025-12-04 - Predictive Coding Test?\n",
    "\n",
    "Integrate using a sliding window with the last 30 seconds as inputtttt\n",
    "\n",
    "For each actual position, see if it was predicted from the preceeding decoded position\n",
    " \n",
    "Kamran suspects that replay will occur of places that the animal is NOT CURRENTLY GOING OR AT. TO \"keep em fresh\" maybe, or \"normalize them in the brain\", or \"consolidate representation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff5c82",
   "metadata": {},
   "source": [
    "# 2025-12-11 - Predictive Coding Follow-up: Locality\n",
    "\n",
    "- Want a measaure of how close a given decoded posterior (for a specific time bin or range of time bins) is to the animal's actual measured position. -- \"locality\"\n",
    "- What if instead a given decoded posterior predicts (e.g. preceeds position) or replays (e.g. follows a previous position with a delay) instead of being synced to the current time.\n",
    "    - Find all non-running epochs and only decode those\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c89a29",
   "metadata": {},
   "source": [
    "### Add `non_local_locality_measures_epochs_df` to timeline as interval epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add to the timeline\n",
    "# a_rect_item, non_local_locality_measures_epochs_df = decoding_locality_measures.add_non_local_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot) ## NO NOT THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eab22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_locality_measures = _container_container.masked_container.decoding_locality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1670296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding\n",
    "\n",
    "identifier='non-local-stationary-pbes'\n",
    "visualization_update_dict = None\n",
    "# visualization_update_dict = {\n",
    "#     'non-local': dict(y_location=-2.3, height=0.3, pen_color=\"#d8db06\", pen_opacity=0.4843137254901961, brush_color=\"#bbae00\", brush_opacity=0.3078431372549019),\n",
    "#     'non-local-pbe': dict(y_location=-2.0, height=0.6, pen_color=\"#d7db06c1\", pen_opacity=0.7843137254901961, brush_color=\"#bb8900\", brush_opacity=0.6078431372549019),\n",
    "# }\n",
    "\n",
    "\n",
    "a_rect_item_pbe, _ = decoding_locality_measures.add_non_local_PBE_non_moving_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot, identifier=identifier)\n",
    "\n",
    "# a_rect_item, _ = decoding_locality_measures.add_non_local_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot, identifier='non-local', non_local_epochs_df=non_local_locality_measures_epochs_df, visualization_update_dict=visualization_update_dict)\n",
    "# a_rect_item_pbe, _ = decoding_locality_measures.add_non_local_PBE_non_moving_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot, identifier=identifier, non_local_epochs_df=overlap_included_only_df_dict['non_moving_PBE'], visualization_update_dict=visualization_update_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eafc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.remove_rendered_intervals(name='non-local-pbes', debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.remove_rendered_intervals(name='non-local', debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f266e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding\n",
    "\n",
    "visualization_update_dict = None\n",
    "# visualization_update_dict = {\n",
    "#     'non-local': dict(y_location=-2.3, height=0.3, pen_color=\"#d8db06\", pen_opacity=0.4843137254901961, brush_color=\"#bbae00\", brush_opacity=0.3078431372549019),\n",
    "#     'non-local-pbe': dict(y_location=-2.0, height=0.6, pen_color=\"#d7db06c1\", pen_opacity=0.7843137254901961, brush_color=\"#bb8900\", brush_opacity=0.6078431372549019),\n",
    "# }\n",
    "\n",
    "# a_rect_item, _ = decoding_locality_measures.add_non_local_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot, identifier='non-local', non_local_epochs_df=non_local_locality_measures_epochs_df, visualization_update_dict=visualization_update_dict)\n",
    "a_rect_item_pbe, _ = decoding_locality_measures.add_non_local_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot, identifier='non-local-pbes', non_local_epochs_df=overlap_included_only_df_dict['is_in_pbes'], visualization_update_dict=visualization_update_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualization_update_dict is None:\n",
    "    visualization_update_dict = {\n",
    "        # 'non-local': dict(y_location=-2.0, height=0.3, pen_color=\"#d8db06\", pen_opacity=0.4843137254901961, brush_color=\"#bbae00\", brush_opacity=0.3078431372549019),\n",
    "        # 'non-local-pbe': dict(y_location=-2.3, height=0.6, pen_color=\"#d7db06c1\", pen_opacity=0.7843137254901961, brush_color=\"#bb8900\", brush_opacity=0.6078431372549019),\n",
    "        'non-local-pbes': dict(y_location=-2.0, height=0.9, pen_color=\"#d8db06\", pen_opacity=0.7843137254901961, brush_color=\"#bbae00\", brush_opacity=0.6078431372549019),\n",
    "    }\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties(visualization_update_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_local_locality_measures_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ec30c",
   "metadata": {},
   "source": [
    "### Pickle the `DecodingLocalityMeasures` obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, PredictiveDecodingComputationsContainer\n",
    "\n",
    "## INPUTS: decoding_locality_measures\n",
    "# decoding_locality_measures: DecodingLocalityMeasures = DecodingLocalityMeasures._reload_class(decoding_locality_measures)\n",
    "\n",
    "## INPUTS: _obj\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-16_DecodingLocalityMeasures_result.pkl')\n",
    "decoding_locality_measures.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de6d9b",
   "metadata": {},
   "source": [
    "### Load the `DecodingLocalityMeasures` obj from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures\n",
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-16_DecodingLocalityMeasures_result.pkl')\n",
    "\n",
    "decoding_locality_measures: DecodingLocalityMeasures = DecodingLocalityMeasures.from_file(pkl_output_path)\n",
    "decoding_locality_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffaab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Jupyter.AsyncExecutionHelper import run_async\n",
    "\n",
    "def load_data_DecodingLocalityMeasures(curr_active_pipeline, pkl_path):\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures\n",
    "    return DecodingLocalityMeasures.from_file(pkl_path=pkl_path)\n",
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-16_DecodingLocalityMeasures_result.pkl')\n",
    "future_load_DecodingLocalityMeasures = run_async(\n",
    "    load_data_DecodingLocalityMeasures,\n",
    "    curr_active_pipeline,\n",
    "    pkl_output_path,\n",
    "    on_success=lambda result: globals().update({'decoding_locality_measures': result}),\n",
    "    # on_success=lambda result: setattr(curr_active_pipeline.global_computation_results.computed_data, 'DirectionalDecodersDecoded', result),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_locality_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91ac4f",
   "metadata": {},
   "source": [
    "### Manually compute adjacent epochs (obsoliting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7086f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute adjacent epochs\n",
    "_temp_non_local_locality_measures_df = deepcopy(non_local_locality_measures_df)\n",
    "## find the size of diff between sequential remaining timestamps, and compare them to the input base timestamp diff: 0.25\n",
    "initial_timestamp_diff: float = 0.25 # 250ms\n",
    "\n",
    "_temp_non_local_locality_measures_df['dt'] = _temp_non_local_locality_measures_df['t'].diff()\n",
    "_temp_non_local_locality_measures_df\n",
    "\n",
    "## find entries equal to initial_timestamp_diff so we can merge them:\n",
    "is_adjacent_epoch_bin = (_temp_non_local_locality_measures_df['dt'] == initial_timestamp_diff)\n",
    "_temp_non_local_locality_measures_df['is_adjacent_epoch_bin'] = is_adjacent_epoch_bin\n",
    "_temp_non_local_locality_measures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dced2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "_temp_non_local_locality_measures_epochs_df = deepcopy(_temp_non_local_locality_measures_df)\n",
    "_temp_non_local_locality_measures_epochs_df['start'] = _temp_non_local_locality_measures_epochs_df['t']\n",
    "_temp_non_local_locality_measures_epochs_df['stop'] = _temp_non_local_locality_measures_epochs_df['start'] + _temp_non_local_locality_measures_epochs_df['dt']\n",
    "_temp_non_local_locality_measures_epochs_df\n",
    "\n",
    "# quiescent_periods['start'] = spikes_df['t_rel_seconds'].shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa536912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mathutil import contiguous_regions\n",
    "\n",
    "idnan = contiguous_regions(_temp_non_local_locality_measures_df['is_adjacent_epoch_bin'].to_numpy())  # identify missing data points\n",
    "idnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch.from_boolean_array(_temp_non_local_locality_measures_df['is_adjacent_epoch_bin'].to_numpy(), t=_temp_non_local_locality_measures_df['t'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848420f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _split_into_consequitive_sequences\n",
    "\n",
    "\n",
    "column_name: str = 'is_adjacent_epoch_bin'\n",
    "accumulated_run_tuples = _split_into_consequitive_sequences(df=_temp_non_local_locality_measures_df, column_name=column_name)\n",
    "accumulated_run_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df026135",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_indicies = accumulated_run_tuples['Index'].to_numpy()\n",
    "split_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = np.split(deepcopy(_temp_non_local_locality_measures_df), split_indicies)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fce7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "render_scrollable_colored_table_from_dataframe(_temp_non_local_locality_measures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e482769",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_scrollable_colored_table_from_dataframe(accumulated_run_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_scrollable_colored_table_from_dataframe(accumulated_run_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_run_tuples = accumulated_run_tuples[accumulated_run_tuples[column_name]] ## get only the seqeuences of repeated run epochs, not the non-run epochs\n",
    "accumulated_run_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a021bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: _temp_non_local_locality_measures_df\n",
    "long_accumulated_run_tuples = accumulated_run_tuples[accumulated_run_tuples['num_timesteps'] > 3]\n",
    "\n",
    "flat_point_times = _temp_non_local_locality_measures_df['t'].to_numpy()\n",
    "\n",
    "final_accumulated_contiguous_epochs = []\n",
    "\n",
    "for row in long_accumulated_run_tuples.itertuples(index=False):\n",
    "    row = row._asdict()\n",
    "    # is_adjacent_value: bool = row[column_name]\n",
    "    is_adjacent_value: bool = row['value']\n",
    "    if is_adjacent_value:\n",
    "        a_start_idx = row['start_idx']\n",
    "        an_end_idx = row['end_idx']\n",
    "        a_value = row['value']\n",
    "\n",
    "        # _temp_non_local_locality_measures_df.iloc[a_start_idx]\n",
    "        final_accumulated_contiguous_epochs.append((flat_point_times[a_start_idx], flat_point_times[an_end_idx]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_accumulated_contiguous_epochs: pd.DataFrame = pd.DataFrame.from_records(final_accumulated_contiguous_epochs, columns=['start', 'stop'])\n",
    "final_accumulated_contiguous_epochs['duration'] = final_accumulated_contiguous_epochs['stop'] - final_accumulated_contiguous_epochs['start']\n",
    "final_accumulated_contiguous_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00916a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "_temp_non_local_locality_measures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[“event”] = ((df.frame - df.frame.shift() - 1) != 0).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_data[a_decoder_name] = _out_data[a_decoder_name].position.adding_lap_info(laps_df=laps_df, inplace=False)\n",
    "# _out_data[a_decoder_name] = _out_locality_measures_df.time_point_event.adding_true_decoder_identifier(t_start=t_start, t_delta=t_delta, t_end=t_end) ## ensures ['maze_id', 'is_LR_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99384848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2e37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_df = curr_active_pipeline.sess.epochs.to_dataframe()\n",
    "active_epochs_df = active_epochs_df[np.isin(epochs_df['label'], ['roam', 'sprinkle'])]\n",
    "active_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b3287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f37734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.External.peak_prominence2d import compute_prominence_contours\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "\n",
    "a_p_x_given_n = deepcopy(_obj.p_x_given_n_dict['roam'])\n",
    "num_timestamps: int = np.shape(a_p_x_given_n)[-1]\n",
    "\n",
    "np.shape(a_p_x_given_n) # (41, 63, 103948)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample_factor: int = 2\n",
    "\n",
    "# np.shape(a_p_x_given_n) # (41, 63, 103948)\n",
    "# H, W, T = np.shape(a_p_x_given_n)\n",
    "# a_p_x_given_n_small = a_p_x_given_n.reshape(H//downsample_factor, downsample_factor, W//downsample_factor, downsample_factor, T).mean(axis=(1, 3))\n",
    "# np.shape(a_p_x_given_n_small)\n",
    "\n",
    "\n",
    "def approx_downsample_pdf(a_p_x_given_n, downsample_factor = 4):\n",
    "    H, W, T = a_p_x_given_n.shape\n",
    "    k = downsample_factor\n",
    "\n",
    "    # compute padding needed (0..k-1)\n",
    "    pad_h = (k - (H % k)) % k\n",
    "    pad_w = (k - (W % k)) % k\n",
    "\n",
    "    # pad only spatial dims; replicate edge values to avoid introducing new mass\n",
    "    a_padded = np.pad(a_p_x_given_n,\n",
    "                    pad_width=((0, pad_h), (0, pad_w), (0, 0)),\n",
    "                    mode='edge').astype(float)\n",
    "\n",
    "    Hc, Wc = a_padded.shape[0], a_padded.shape[1]\n",
    "\n",
    "    # block-sum over k x k blocks, preserving total mass\n",
    "    a_blocksum = a_padded.reshape(Hc//k, k, Wc//k, k, T).sum(axis=(1, 3))\n",
    "\n",
    "    # renormalize each time bin to ensure it's a PDF (sums to 1)\n",
    "    mass = a_blocksum.sum(axis=(0, 1), keepdims=True)  # shape (1,1,T)\n",
    "    # avoid division by zero: if mass==0 leave as-is\n",
    "    mass_nonzero = mass.copy()\n",
    "    mass_nonzero[mass_nonzero == 0] = 1.0\n",
    "    a_p_x_given_n_small = a_blocksum / mass_nonzero\n",
    "\n",
    "    # shapes:\n",
    "    # original: (H, W, T)\n",
    "    # small: (ceil(H/k), ceil(W/k), T)\n",
    "    return a_p_x_given_n_small\n",
    "\n",
    "\n",
    "a_p_x_given_n_small = approx_downsample_pdf(a_p_x_given_n=a_p_x_given_n, downsample_factor=4)\n",
    "np.shape(a_p_x_given_n_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import approx_downsample_pdf\n",
    "\n",
    "a_p_x_given_n_small, xbin_centers_small, ybin_centers_small = approx_downsample_pdf(a_p_x_given_n=a_p_x_given_n, xbin_centers = _obj.xbin_centers, ybin_centers = _obj.ybin_centers, downsample_factor=4)\n",
    "np.shape(a_p_x_given_n_small)\n",
    "xbin_centers_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbin_centers = _obj.xbin_centers\n",
    "ybin_centers = _obj.ybin_centers\n",
    "\n",
    "# positions = \n",
    "# distances = np.min(np.sqrt(((np.argwhere(a_p_x_given_n == a_p_x_given_n.max())[:, None, :] - positions[None, :, :])**2).sum(axis=2)), axis=0)\n",
    "# distances = np.linalg.norm(_obj.gaussian_volume.reshape(-1, _obj.gaussian_volume.shape[-1]) - a_p_x_given_n.reshape(-1, a_p_x_given_n.shape[-1]), axis=0)\n",
    "# assume:\n",
    "# _obj.gaussian_volume: (H, W, T)\n",
    "# a_p_x_given_n: (H, W, T)\n",
    "# xbin_centers: shape (H,)\n",
    "# ybin_centers: shape (W,)\n",
    "\n",
    "X, Y = np.meshgrid(xbin_centers, ybin_centers, indexing='ij')  # shape (H, W)\n",
    "\n",
    "# Flatten spatial dims for weighted sum\n",
    "X_flat = X[:, :, None]  # (H, W, 1)\n",
    "Y_flat = Y[:, :, None]\n",
    "\n",
    "# Expected positions of _obj.gaussian_volume\n",
    "mean_X_obj = (X_flat * _obj.gaussian_volume).sum(axis=(0,1))  # shape (T,)\n",
    "mean_Y_obj = (Y_flat * _obj.gaussian_volume).sum(axis=(0,1))  # shape (T,)\n",
    "\n",
    "# Expected positions of a_p_x_given_n\n",
    "mean_X_a = (X_flat * a_p_x_given_n).sum(axis=(0,1))  # shape (T,)\n",
    "mean_Y_a = (Y_flat * a_p_x_given_n).sum(axis=(0,1))  # shape (T,)\n",
    "\n",
    "distances_spatial = np.sqrt((mean_X_obj - mean_X_a)**2 + (mean_Y_obj - mean_Y_a)**2)  # shape (T,)\n",
    "\n",
    "distances_spatial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d085ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_p_x_given_n = deepcopy(a_p_x_given_n_small)\n",
    "xbin_centers = xbin_centers_small\n",
    "ybin_centers = ybin_centers_small\n",
    "\n",
    "\n",
    "# Extract your 2D posterior slice at a specific timestamp\n",
    "for a_timestamp_idx in np.arange(num_timestamps):\n",
    "    if a_timestamp_idx < 10:\n",
    "        posterior_slice = a_p_x_given_n[:, :, a_timestamp_idx]  # Shape: (n_x_bins, n_y_bins) or similar\n",
    "\n",
    "        # Compute prominence - note the transpose!\n",
    "        xx, yy, slab, peaks_dict, id_map, prominence_map, parent_map = compute_prominence_contours(\n",
    "            xbin_centers=xbin_centers,\n",
    "            ybin_centers=ybin_centers,\n",
    "            slab=posterior_slice.T,  # IMPORTANT: transpose the slice\n",
    "            step=0.05,                # smaller = finer but slower (0.01-0.1 typical)\n",
    "            min_depth=0.1,            # minimum prominence threshold\n",
    "            min_area=None,\n",
    "            include_edge=True,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Sort peaks by prominence to get top 3\n",
    "        peaks_by_prominence = sorted(peaks_dict.items(), \n",
    "                                    key=lambda x: x[1]['prominence'], \n",
    "                                    reverse=True)\n",
    "        top_3_peaks = peaks_by_prominence[:3]\n",
    "\n",
    "        # Get the top peak\n",
    "        top_peak_id, top_peak_info = top_3_peaks[0]\n",
    "\n",
    "        # Create a mask for the region around the top peak\n",
    "        # The outermost contour defines the peak's region\n",
    "        top_peak_contour = top_peak_info['contour']  # or top_peak_info['contours'][-1]\n",
    "\n",
    "        # Create a boolean mask from the contour\n",
    "        mask = np.zeros(posterior_slice.T.shape, dtype=bool)  # Note: transposed shape\n",
    "        yy_grid, xx_grid = np.meshgrid(ybin_centers, xbin_centers, indexing='ij')\n",
    "        points = np.vstack([xx_grid.ravel(), yy_grid.ravel()]).T\n",
    "\n",
    "        # Check which points are inside the contour\n",
    "        inside = top_peak_contour.contains_points(points)\n",
    "        mask = inside.reshape(posterior_slice.T.shape)\n",
    "\n",
    "        # If you want the mask in the original orientation:\n",
    "        mask = mask.T\n",
    "\n",
    "        # Summary of results\n",
    "        print(f\"Found {len(peaks_dict)} peaks\")\n",
    "        print(f\"\\nTop 3 peaks by prominence:\")\n",
    "        for i, (peak_id, peak_info) in enumerate(top_3_peaks):\n",
    "            print(f\"  {i+1}. Peak {peak_id}: prominence={peak_info['prominence']:.3f}, \"\n",
    "                f\"height={peak_info['height']:.3f}, center={peak_info['center']}\")\n",
    "\n",
    "        # Access peak properties:\n",
    "        # top_peak_info['center']      - (x, y) center coordinates\n",
    "        # top_peak_info['prominence']  - prominence value\n",
    "        # top_peak_info['height']      - peak height\n",
    "        # top_peak_info['area']        - contour area\n",
    "        # top_peak_info['contours']    - list of contours from peak to col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layers, config_widgets_dict_dict, dock_window = _obj.add_all_layers(sync_plotters=sync_plotters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d273f",
   "metadata": {},
   "source": [
    "### Pickle the `PredictiveDecoding` obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, PredictiveDecodingComputationsContainer\n",
    "\n",
    "_obj: PredictiveDecoding = PredictiveDecoding._reload_class(_obj)\n",
    "\n",
    "## INPUTS: _obj\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-15_PredictiveDecoding_result.pkl')\n",
    "_obj.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding\n",
    "\n",
    "_obj: PredictiveDecoding = PredictiveDecoding._reload_class(_obj)\n",
    "\n",
    "## INPUTS: _obj\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-15_PredictiveDecoding_result.pkl')\n",
    "_obj.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70c4d9",
   "metadata": {},
   "source": [
    "### Load the `PredictiveDecoding` obj from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding\n",
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-18_PredictiveDecoding_result.pkl')\n",
    "\n",
    "_obj: PredictiveDecoding = PredictiveDecoding.from_file(pkl_output_path)\n",
    "_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685786a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_possible_bins: int = len(_obj.xbin_centers) * len(_obj.ybin_centers)\n",
    "total_num_possible_bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2146a",
   "metadata": {},
   "source": [
    "### Plot locality measure over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "## INPUTS: _out_locality_measures_df\n",
    "px.line(_out_locality_measures_df, x=\"t\", y=[\"roam\", \"sprinkle\"]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(_out_locality_measures_df, x=\"t\", y=[\"dist_to_highest_peak_roam\", \"mask_overlap_roam\"]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e552059",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(_out_locality_measures_df, x=\"t\", y=[\"dist_to_highest_peak_sprinkle\", \"mask_overlap_sprinkle\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952591e",
   "metadata": {},
   "source": [
    "### Add `_out_locality_measures_df` as timeline track so we can see them synced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ax.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "## INPUTS: _obj.locality_measures_df\n",
    "_out_locality_measures_df = _obj.locality_measures_df\n",
    "_out_locality_measures_df\n",
    "\n",
    "spike_raster_plt_2d: Spike2DRaster = spike_raster_window.spike_raster_plt_2d\n",
    "\n",
    "spike_raster_plt_2d: Spike2DRaster = spike_raster_window.spike_raster_plt_2d\n",
    "\n",
    "sync_mode = SynchronizedPlotMode.TO_WINDOW\n",
    "# sync_mode = SynchronizedPlotMode.TO_GLOBAL_DATA\n",
    "\n",
    "# track_dock_identifier: str = f'LocalityComputations'\n",
    "# ts_widget, fig, ax_list, dDisplayItem = spike_raster_plt_2d.add_new_matplotlib_render_plot_widget(name=track_dock_identifier, sync_mode=sync_mode)\n",
    "# track_ax = ax_list[0]\n",
    "\n",
    "added_track_widgets = {}\n",
    "track_ax_dict = {}\n",
    "\n",
    "all_computation_names = ['dist_to_highest_peak', 'mask_overlap', 'earthmovers']\n",
    "for a_computation_measure_name in all_computation_names:\n",
    "    track_dock_identifier: str = f'Locality[{a_computation_measure_name}]'\n",
    "    ts_widget, fig, ax_list, dDisplayItem = spike_raster_plt_2d.add_new_matplotlib_render_plot_widget(name=track_dock_identifier, sync_mode=sync_mode)\n",
    "    track_ax = ax_list[0]\n",
    "\n",
    "    track_ax_dict[a_computation_measure_name] = track_ax\n",
    "    added_track_widgets[track_dock_identifier] = ts_widget\n",
    "\n",
    "\n",
    "all_epoch_names = [\"roam\", \"sprinkle\"]\n",
    "# Predefine distinct colors (extend as needed)\n",
    "epoch_colors = {\n",
    "    \"roam\": \"tab:blue\",\n",
    "    \"sprinkle\": \"tab:orange\",\n",
    "}\n",
    "\n",
    "\n",
    "for an_epoch_name in _obj.epoch_names:\n",
    "    color = epoch_colors.get(an_epoch_name, None)\n",
    "    \n",
    "    # track_dock_identifier: str = f'LocalityEarthMov[{an_epoch_name}]'\n",
    "    # ts_widget, fig, ax_list, dDisplayItem = spike_raster_plt_2d.add_new_matplotlib_render_plot_widget(name=track_dock_identifier, sync_mode=SynchronizedPlotMode.TO_WINDOW)\n",
    "    # track_ax = ax_list[0]\n",
    "    # an_artist = track_ax.scatter(_out_locality_measures_df['t'], _out_locality_measures_df[an_epoch_name], label=track_dock_identifier)\n",
    "    \n",
    "    for a_computation_measure_name in all_computation_names:\n",
    "        track_ax = track_ax_dict[a_computation_measure_name]\n",
    "        a_key = f\"{a_computation_measure_name}_{an_epoch_name}\"\n",
    "        markerline, stemlines, baseline = track_ax.scatter(_out_locality_measures_df['t'], _out_locality_measures_df[a_key], color, label=a_key, \n",
    "                                                # color=color,\n",
    "                                                # use_line_collection=True,\n",
    "                                            )\n",
    "        # markerline, stemlines, baseline = track_ax.stem(_out_locality_measures_df['t'], _out_locality_measures_df[a_key], color, label=a_key, \n",
    "        #                                         # color=color,\n",
    "        #                                         # use_line_collection=True,\n",
    "        #                                     )\n",
    "        # plt.setp(stemlines, 'color', 'white')\n",
    "        # plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "        # # Optional aesthetics\n",
    "        # markerline.set_markersize(4)\n",
    "        # markerline.set_color(color) \n",
    "        # baseline.set_visible(False)\n",
    "        track_ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # --- STEM PLOT instead of scatter ---\n",
    "\n",
    "    # a_computation_measure_name: str = 'dist_to_highest_peak'\n",
    "    # a_key = f\"{a_computation_measure_name}_{an_epoch_name}\"\n",
    "    # markerline, stemlines, baseline = track_ax.stem(_out_locality_measures_df['t'], _out_locality_measures_df[a_key], color, label=a_key, \n",
    "    #                                         # color=color,\n",
    "    #                                         # use_line_collection=True,\n",
    "    #                                     )\n",
    "    # # stemlines.set_Color(color)\n",
    "    # # plt.setp(stemlines, 'color', plt.getp(markerline,'color'))\n",
    "    # plt.setp(stemlines, 'color', 'white')\n",
    "    # plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "\n",
    "\n",
    "    # # Optional aesthetics\n",
    "    # markerline.set_markersize(4)\n",
    "    # markerline.set_color(color) \n",
    "    # baseline.set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "    # a_computation_measure_name: str = 'mask_overlap'\n",
    "    # a_key = f\"{a_computation_measure_name}_{an_epoch_name}\"\n",
    "    # markerline, stemlines, baseline = track_ax.stem(_out_locality_measures_df['t'], _out_locality_measures_df[a_key], label=a_key, \n",
    "    #                                         # use_line_collection=True,\n",
    "    #                                     )\n",
    "    # # Optional aesthetics\n",
    "    # markerline.set_markersize(4)\n",
    "    # markerline.set_color(color)\n",
    "    # baseline.set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# added_track_widgets[track_dock_identifier] = ts_widget\n",
    "\n",
    "# track_ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_locality_measures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d0fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f27373",
   "metadata": {},
   "source": [
    "### Add to curr_active_pipeline.global_computation_results to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'] = _obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4cb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60342a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_out_locality_measures_df.plot.scatter(x='t', y='roam', ax=ax)\n",
    "_out_locality_measures_df.plot.scatter(x='t', y='sprinkle', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for an_epoch_name, v in _out_earthmovers_dist.items():\n",
    "    plt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8e5bf",
   "metadata": {},
   "source": [
    "# 📍 ⚓🟢 2025-12-19 - Precise forward/backward prediction from the non-local events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2e6b2",
   "metadata": {
    "tags": [
     "2025-12-24_run_locality_prediction_quality",
     "2026-01-08_Position-likePosteriorsOnlyFilteredResult",
     "2026-01-14_new-reasonable-future-and-past",
     "2026-01-16_fixing_compute_memory",
     "🧩⛓️🔶 2026-02-02 - required compute steps"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer, PredictiveDecodingComputationsGlobalComputationFunctions\n",
    "\n",
    "# curr_active_pipeline.register_computation(computation_function=PredictiveDecodingComputationsGlobalComputationFunctions.perform_predictive_decoding_analysis, is_global=True, registered_name='predictive_decoding_analysis')\n",
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab91f9",
   "metadata": {
    "tags": [
     "2025-12-24_run_locality_prediction_quality",
     "2026-01-14_new-reasonable-future-and-past",
     "🧩⛓️🔶 2026-02-02 - required compute steps"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer\n",
    "\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "# 🧩⛓️🔶 2026-02-02 - Required Compute Steps\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-PredictiveDecoding.predictive_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['predictive_decoding_analysis'], computation_kwargs_list=[{'window_size': 2, 'extant_decoded_time_bin_size': 0.250, 'enable_masked_filtered_container_before_any_comps': True,\n",
    "                                                                                                                                                    'should_perform_first_pass_compute_future_and_past_analysis': True, 'enable_filter_and_final_result_processing': True}],\n",
    "                                                                                                                                                    #  enabled_filter_names=['roam',],\n",
    "                                                                                                                                                    enabled_filter_names=['roam', 'sprinkle'],\n",
    "                                                                                                                                                    fail_on_exception=True, debug_print=True)\n",
    "\n",
    "## 7m 16.2 seconds\n",
    "## 12m \n",
    "## 24m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5a39d",
   "metadata": {
    "tags": [
     "2026-01-21_postcomputefilter",
     "end-run",
     "2026-02-09_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "_container_container: PredictiveDecodingComputationsContainerContainer = curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding']\n",
    "assert _container_container is not None\n",
    "container: PredictiveDecodingComputationsContainer = _container_container.container\n",
    "masked_container: PredictiveDecodingComputationsContainer = _container_container.masked_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa82ae",
   "metadata": {
    "tags": [
     "2026-01-21_postcomputefilter",
     "2026-02-02_resume_after_break",
     "2026-02-09_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "fine_time_bin_size: float = masked_container.most_recent_decoding_time_bin_size\n",
    "epoch_names = ['roam', 'sprinkle']\n",
    "print(f'fine_time_bin_size: {fine_time_bin_size}')\n",
    "\n",
    "if (container.pf1D_Decoder_dict is None) or (len(container.pf1D_Decoder_dict) == 0):\n",
    "    ## initialize it\n",
    "    directional_decoders_decode_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "    assert directional_decoders_decode_result is not None\n",
    "    container.pf1D_Decoder_dict = deepcopy(directional_decoders_decode_result.pf1D_Decoder_dict) ## copy the independent decoders\n",
    "    print(f'container: assigning pf1D_Decoder_dict: {list(container.pf1D_Decoder_dict.keys())}')\n",
    "    \n",
    "\n",
    "if (masked_container.pf1D_Decoder_dict is None) or (len(masked_container.pf1D_Decoder_dict) == 0):\n",
    "    ## initialize it\n",
    "    directional_decoders_decode_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "    assert directional_decoders_decode_result is not None\n",
    "    masked_container.pf1D_Decoder_dict = deepcopy(directional_decoders_decode_result.pf1D_Decoder_dict) ## copy the independent decoders\n",
    "    print(f'masked_container: assigning pf1D_Decoder_dict: {list(masked_container.pf1D_Decoder_dict.keys())}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('masked_container', masked_container, max_depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c90a44",
   "metadata": {
    "tags": [
     "2026-01-21_postcomputefilter",
     "2026-02-09_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.time_slicing import TimePointEventAccessor, TimeSliceAccessor\n",
    "from neuropy.core.flattened_spiketrains import FlattenedSpiketrains, SpikesAccessor\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer, MatchingPastFuturePositionsResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# active_epoch_names = epoch_names\n",
    "active_epoch_names = ['roam', 'sprinkle']\n",
    "# active_epoch_names = ['sprinkle']\n",
    "# active_epoch_names = ['roam']\n",
    "fine_decoding_time_bin_size: float = 0.025\n",
    "for an_epoch_name in active_epoch_names:    \n",
    "    try:\n",
    "        print(f'\\ttrying `.masked_container.final_refine_single_decoder_result_masks(...)` for an_epoch_name: \"{an_epoch_name}\"...')\n",
    "        if an_epoch_name not in masked_container.debug_computed_dict:\n",
    "            masked_container.debug_computed_dict[an_epoch_name] = {}\n",
    "        \n",
    "        # active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = a_masked_container.final_refine_single_decoder_result_masks(curr_active_pipeline=owning_pipeline_reference, decoding_time_bin_size=time_bin_size, an_epoch_name=an_epoch_name)\n",
    "        active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = masked_container.final_refine_single_decoder_result_masks(curr_active_pipeline=curr_active_pipeline, fine_decoding_t_bin_size=fine_decoding_time_bin_size, a_decoder_name=an_epoch_name,\n",
    "                                                                                                                                                              use_parallel=False, max_workers=1,\n",
    "                                                                                                                                                             )\n",
    "        masked_container.debug_computed_dict[an_epoch_name].update({'active_epochs_result': active_epochs_result, 'custom_results_df_list': custom_results_df_list, 'decoded_epoch_t_bins_promenence_result_obj': decoded_epoch_t_bins_promenence_result_obj})\n",
    "    except (ValueError, AttributeError, IndexError, KeyError, TypeError) as e:\n",
    "        print(f'\\t\\tWARN: the `enable_filter_and_final_result_processing` part of `perform_predictive_decoding_analysis(...) failed with error: {e}. Skipping.')\n",
    "        raise # pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise\n",
    "## END for an_epoch_name in epoch_names...\n",
    "\n",
    "# step: 0.0004381917184223964\n",
    "# 68m 40.4s - FAILED\n",
    "\n",
    "# 49.5s -- roam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb90152",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8672dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "## INPUTS: masked_container\n",
    "# \"W:\\Data\\Bapun\\RatN\\Day4OpenField\\output\\2026-02-02_PredictiveDecodingComputationsContainer_masked_split\"\n",
    "\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-04_PredictiveDecodingComputationsContainer_masked_split') ## includes most of 'sprinkle' in debug dict as well\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-05_PredictiveDecodingComputationsContainer_masked_split') ## includes most of 'sprinkle' in debug dict as well\n",
    "pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-16_PredictiveDecodingComputationsContainer_masked_split')\n",
    "pkl_split_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer_masked_only.pkl')\n",
    "# _container_container: PredictiveDecodingComputationsContainerContainer = PredictiveDecodingComputationsContainerContainer.from_file(pkl_output_path=pkl_output_path)\n",
    "\n",
    "## Inputs: _container_container\n",
    "\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-09_PredictiveDecodingComputationsContainer.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-17_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-17_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-19_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-20_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-21_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer_masked_manual_update.pkl')\n",
    "# _container_container.masked_container.save(pkl_output_path=pkl_output_path)\n",
    "\n",
    "pkl_output_path: Path = pkl_split_output_path.joinpath(f'Split_debug_computed_dict').resolve()\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "saveData(pkl_output_path, masked_container.debug_computed_dict, should_append=False, safe_save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320a5a8",
   "metadata": {},
   "source": [
    "### Update `masked_container.debug_computed_dict` from the partial split debug data so we have all the `debug_computed_dict` entires we need for 2026-02-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce6845",
   "metadata": {
    "tags": [
     "🟢 active-2026-02-11",
     "active-2026-02-16",
     "end-run"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "## INPUTS: masked_container\n",
    "# \"W:\\Data\\Bapun\\RatN\\Day4OpenField\\output\\2026-02-1602_PredictiveDecodingComputationsContainer_masked_split\"\n",
    "\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer_masked_split')\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-04_PredictiveDecodingComputationsContainer_masked_split') ## includes most of 'sprinkle' in debug dict as well\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-09_PredictiveDecodingComputationsContainer_masked_split')\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-10_PredictiveDecodingComputationsContainer_masked_split')\n",
    "pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-16_PredictiveDecodingComputationsContainer_masked_split')\n",
    "## Inputs: _container_container\n",
    "pkl_output_path: Path = pkl_split_output_path.joinpath(f'Split_debug_computed_dict').resolve()\n",
    "print(f'loading from pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "debug_computed_dict = loadData(pkl_output_path)\n",
    "# debug_computed_dict\n",
    "assert debug_computed_dict is not None\n",
    "debug_computed_dict\n",
    "\n",
    "masked_container.debug_computed_dict = masked_container.debug_computed_dict | debug_computed_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3eae67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06120f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-09_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "masked_container.save(pkl_output_path=pkl_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa006df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### 🔃⚙️⏲️ Async/background execution - loading `directional_decoders_decode_result` ⏳🔄🔶\n",
    "import time\n",
    "import concurrent.futures\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "out = widgets.Output(layout={\"border\":\"1px solid gray\"})\n",
    "display(out)\n",
    "\n",
    "def background_compute_work(out_widget, curr_active_pipeline):\n",
    "    # Use out_widget.append_stdout() so output is appended to this widget\n",
    "    try:\n",
    "        from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer\n",
    "\n",
    "        out_widget.append_stdout(f\"[bg] starting compute with 'predictive_decoding_analysis'...\\n\")\n",
    "        \n",
    "        curr_active_pipeline.reload_default_computation_functions()\n",
    "        \n",
    "        curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['predictive_decoding_analysis'], computation_kwargs_list=[{'window_size': 2, 'extant_decoded_time_bin_size': 0.250, 'enable_masked_filtered_container_before_any_comps': True,\n",
    "                                                                                                                                                                'should_perform_first_pass_compute_future_and_past_analysis': True, 'enable_filter_and_final_result_processing': True}],\n",
    "                                                                                                                                                                #  enabled_filter_names=['roam',],\n",
    "                                                                                                                                                                enabled_filter_names=['roam', 'sprinkle'],\n",
    "                                                                                                                                                                fail_on_exception=True, debug_print=True)\n",
    "\n",
    "\n",
    "        out_widget.append_stdout(f\"[bg] compute finished!\\n\")\n",
    "        _container_container: PredictiveDecodingComputationsContainerContainer = curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding']\n",
    "        assert _container_container is not None\n",
    "        # container: PredictiveDecodingComputationsContainer = _container_container.container\n",
    "        # masked_container: PredictiveDecodingComputationsContainer = _container_container.masked_container\n",
    "        return _container_container\n",
    "    \n",
    "    except Exception as e:\n",
    "        out_widget.append_stdout(f\"[bg] ERROR: {e}\\n\")\n",
    "        raise\n",
    "\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)\n",
    "future = executor.submit(background_compute_work, out, curr_active_pipeline)\n",
    "\n",
    "# main cell remains interactive immediately\n",
    "print(\"This cell finished without waiting for the background job\")\n",
    "\n",
    "# later you can check:\n",
    "# future.done(); directional_decoders_decode_result = future.result(timeout=0)\n",
    "# curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded'] = directional_decoders_decode_result ## assign to curr_active_pipeline's global result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "future.done(); _container_container: PredictiveDecodingComputationsContainerContainer = future.result(timeout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c78201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "if container.decoding_locality is None:\n",
    "    container.decoding_locality = container.predictive_decoding.locality_measures\n",
    "decoding_locality: DecodingLocalityMeasures = container.decoding_locality\n",
    "non_local_PBE_non_moving_epochs_df: pd.DataFrame = container.decoding_locality.non_local_PBE_non_moving_epochs_df\n",
    "predictive_decoding: PredictiveDecoding = container.predictive_decoding\n",
    "non_local_PBE_non_moving_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.epochs_decoded_result_cache_dict[0.025]['roam']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b999118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import SingleArtistMultiEpochBatchHelpers\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "track_name: str = 'SingleArtistMultiEpochBatchTrack'\n",
    "spike_raster_plt_2d: Spike2DRaster = spike_raster_window.spike_raster_plt_2d\n",
    "track_ts_widget, track_fig, track_ax_list, track_dock = spike_raster_plt_2d.add_new_matplotlib_render_plot_widget(name=track_name)\n",
    "track_ax = track_ax_list[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ts_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd687f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.pf1D_Decoder_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import multi_trajectory_color_plotter\n",
    "\n",
    "# With fixed maze extent:\n",
    "maze_extent = (a_decoder.xbin[0], a_decoder.xbin[-1], a_decoder.ybin[0], a_decoder.ybin[-1])\n",
    "plot_widget, plot_items = multi_trajectory_color_plotter(position_dfs=dfs_list, maze_extent=maze_extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02139cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MatchingPastFuturePositionsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MaskDataSource\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingDisplayWidget\n",
    "\n",
    "a_flat_matching_results_list_ds: MaskDataSource = MaskDataSource.init_from_list_of_MatchingPastFuturePositionsResult(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, filter_epochs=a_decoded_filter_epochs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8cd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import SingleArtistMultiEpochBatchHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.EpochComputationFunctions import DecodingResultND\n",
    "\n",
    "results2D: DecodingResultND = DecodingResultND(ndim=2, \n",
    "    test_epoch_results=masked_container.epochs_decoded_result_cache_dict,\n",
    "    continuous_results=None,\n",
    "    decoders=masked_container.pf1D_Decoder_dict, pfs=new_pf2Ds_dict,\n",
    "    frame_divided_epochs_results=frame_divided_epochs_specific_decoded_results2D_dict, \n",
    "    frame_divided_epochs_df=deepcopy(global_frame_divided_epochs_df),\n",
    "    pos_df=,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd47285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2D\n",
    "\n",
    "# Unpack all fields in order\n",
    "ndim, pos_df, pfs, decoders, test_epoch_results, continuous_results, frame_divided_epochs_df, frame_divided_epochs_results = results2D\n",
    "# *test_args = results2D\n",
    "# print(len(test_args))\n",
    "# anUPDATED_TUPLE_2D, UPDATED_frame_divided_epochs_specific_decoded_results2D_dict = results2D\n",
    "ndim\n",
    "\n",
    "\n",
    "# Assuming you have:\n",
    "# - results2D: DecodingResultND object with decoded data\n",
    "# - active_plot: matplotlib axes (your timeline axes)\n",
    "# - frame_divide_bin_size: float (e.g., 0.5 seconds per epoch)\n",
    "# - global_session: session object\n",
    "\n",
    "# Create the helper\n",
    "batch_plot_helper: SingleArtistMultiEpochBatchHelpers = SingleArtistMultiEpochBatchHelpers(\n",
    "    results2D=masked_container.epochs_decoded_result_cache_dict[0.025]['roam'], \n",
    "    active_ax=track_ax,  # Your timeline axes\n",
    "    frame_divide_bin_size=frame_divide_bin_size,\n",
    "    desired_epoch_start_idx=0,  # Optional: start epoch index\n",
    "    desired_epoch_end_idx=None  # Optional: end epoch index (None = all)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185719a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot everything (posteriors, positions, track shapes)\n",
    "plots_data = batch_plot_helper.add_all_track_plots(\n",
    "    global_session=global_session,\n",
    "    posterior_masking_value=0.0025,  # Mask low probability values\n",
    "    debug_print=False\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Build the data first\n",
    "batch_plot_helper.shared_build_flat_stacked_data(force_recompute=True, debug_print=True)\n",
    "\n",
    "# 2. Add track shapes (optional, requires global_session)\n",
    "track_shape_artists = batch_plot_helper.add_track_shapes(global_session=global_session)\n",
    "\n",
    "# 3. Add decoded position posteriors (heatmaps)\n",
    "curr_artist_dict, image_extent, plots_data = batch_plot_helper.add_position_posteriors(\n",
    "    posterior_masking_value=0.0025,\n",
    "    debug_print=True\n",
    ")\n",
    "\n",
    "# 4. Add measured positions\n",
    "measured_pos_line_artist, frame_division_epoch_separator_vlines = batch_plot_helper.add_track_positions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _container_container.masked_container.scoring_results_df\n",
    "\n",
    "_container_container.masked_container.active_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c1899",
   "metadata": {},
   "source": [
    "## 💾 Pickle Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0663251",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs: _container_container\n",
    "\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-09_PredictiveDecodingComputationsContainer.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-17_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-17_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-19_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-20_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-21_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer_masked_manual_update.pkl')\n",
    "_container_container.masked_container.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs: _container_container\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-09_PredictiveDecodingComputationsContainer.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-17_PredictiveDecodingComputationsContainer_non_masked.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-17_PredictiveDecodingComputationsContainer_non_masked_later.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-20_PredictiveDecodingComputationsContainer_non_masked_later.pkl')\n",
    "_container_container.container.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12915562",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs: _container_container\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-19_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-20_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-23_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-06_PredictiveDecodingComputationsContainer.pkl')\n",
    "_container_container.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f550a9",
   "metadata": {},
   "source": [
    "### 🔷 From Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-19_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-20_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-21_PredictiveDecodingComputationsContainer_masked_later.pkl')\n",
    "masked_container: PredictiveDecodingComputationsContainer = PredictiveDecodingComputationsContainer.from_file(pkl_output_path)\n",
    "# _container_container.masked_container = masked_container\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9adc6c",
   "metadata": {
    "tags": [
     "2026-02-02_resume_after_break"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-19_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-20_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-21_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-22_PredictiveDecodingComputationsContainer_FULL.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer.pkl')\n",
    "_container_container: PredictiveDecodingComputationsContainerContainer = PredictiveDecodingComputationsContainerContainer.from_file(pkl_output_path)\n",
    "assert _container_container is not None\n",
    "container: PredictiveDecodingComputationsContainer = _container_container.container\n",
    "masked_container: PredictiveDecodingComputationsContainer = _container_container.masked_container\n",
    "# 1m 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b84aa5",
   "metadata": {
    "tags": [
     "2026-02-02_resume_after_break"
    ]
   },
   "outputs": [],
   "source": [
    "assert _container_container is not None\n",
    "assert _container_container.container is not None\n",
    "assert _container_container.masked_container is not None\n",
    "curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'] = _container_container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298406bf",
   "metadata": {},
   "source": [
    "### Update `masked_container.debug_computed_dict` from the partial split debug data so we have all the `debug_computed_dict` entires we need for 2026-02-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b073326",
   "metadata": {
    "tags": [
     "2026-02-02_resume_after_break"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "## INPUTS: masked_container\n",
    "# \"W:\\Data\\Bapun\\RatN\\Day4OpenField\\output\\2026-02-02_PredictiveDecodingComputationsContainer_masked_split\"\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-02_PredictiveDecodingComputationsContainer_masked_split')\n",
    "pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-04_PredictiveDecodingComputationsContainer_masked_split') ## includes most of 'sprinkle' in debug dict as well\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-06_PredictiveDecodingComputationsContainer_masked_split')\n",
    "pkl_output_path: Path = pkl_split_output_path.joinpath(f'Split_debug_computed_dict').resolve()\n",
    "print(f'loading from pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "debug_computed_dict = loadData(pkl_output_path)\n",
    "# debug_computed_dict\n",
    "assert debug_computed_dict is not None\n",
    "# masked_container.debug_computed_dict = masked_container.debug_computed_dict | debug_computed_dict\n",
    "masked_container.debug_computed_dict.update(masked_container.debug_computed_dict | debug_computed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = pkl_split_output_path.joinpath(f'decoder_flat_matching_results_list_ds_dict').resolve()\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "decoder_flat_matching_results_list_ds_dict = loadData(pkl_output_path)\n",
    "assert decoder_flat_matching_results_list_ds_dict is not None\n",
    "## #TODO 2026-02-05 09:55: - [ ] update continaer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "container.active_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.active_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57a458",
   "metadata": {},
   "source": [
    "#### 2026-01-19 - Manual Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991c1ff",
   "metadata": {
    "tags": [
     "2026-01-16_fixing_compute_memory"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "## do the: `enable_filter_and_final_result_processing` part:\n",
    "# Validate container exists\n",
    "# container = global_computation_results.computed_data.get('PredictiveDecoding', None)\n",
    "## INPUTS: container\n",
    "assert container is not None\n",
    "masked_container = container.build_masked_container(curr_active_pipeline=curr_active_pipeline, should_filter_directional_decoders_decode_result=True, should_compute_future_and_past_analysis=True, should_compute_peak_prom_analysis=False) ## 3m now\n",
    "# 12m 17s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6adedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_container.active_epochs_df\n",
    "masked_container.epochs_decoded_result_cache_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "a_t_bin_size: float = 0.025\n",
    "epoch_names: List[str] = list(masked_container.epochs_decoded_result_cache_dict[a_t_bin_size].keys())\n",
    "epoch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_container = _container_container.masked_container\n",
    "_container_container.masked_container = masked_container\n",
    "# _container_container.container.pf1D_Decoder_dict\n",
    "# masked_container.pf1D_Decoder_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.epochs_decoded_result_cache_dict[0.025]['roam'].filter_epochs\n",
    "\n",
    "masked_container.epochs_decoded_result_cache_dict[0.025]['sprinkle'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6997169",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.predictive_decoding.locality_measures.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60774bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.pf1D_Decoder_dict = container.pf1D_Decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import PositionLikePosteriorScoring\n",
    "\n",
    "position_like_kwargs = dict(position_like_score_cutoff=0.42, num_min_position_like_t_bins=3)\n",
    "\n",
    "an_active_t_bin_size = 0.025\n",
    "a_decoder_name = 'roam'\n",
    "\n",
    "# a_decoder = masked_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "an_extant_result = masked_container.epochs_decoded_result_cache_dict[an_active_t_bin_size][a_decoder_name]\n",
    "print(f\"an_extant_result.n_epochs: {an_extant_result.n_epochs}\")\n",
    "print(f\"len(masked_container.active_epochs_df): {len(masked_container.active_epochs_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e12e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_decoded_local_epochs_result, scoring_results = PositionLikePosteriorScoring.filter_to_position_like_epochs_only(decoded_local_epochs_result=an_extant_result,\n",
    "                                                                                                                        #    xbin=a_decoder.xbin, ybin=a_decoder.ybin,\n",
    "                                                                                                                        xbin=masked_container.predictive_decoding.locality_measures.xbin, ybin=masked_container.predictive_decoding.locality_measures.ybin,\n",
    "                                                                                                                          **position_like_kwargs)\n",
    "print(f'filtered_decoded_local_epochs_result.n_epochs: {filtered_decoded_local_epochs_result.n_epochs}')\n",
    "scoring_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d02d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.epochs_decoded_result_cache_dict[an_active_t_bin_size][a_decoder_name] = filtered_decoded_local_epochs_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733eedd",
   "metadata": {},
   "source": [
    "#### Manually do `masked_container.compute_future_and_past_analysis(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3a218",
   "metadata": {
    "tags": [
     "2026-01-16_fixing_compute_memory"
    ]
   },
   "outputs": [],
   "source": [
    "a_t_bin_size: float = 0.025\n",
    "for a_decoder_name in epoch_names:\n",
    "    if a_decoder_name not in masked_container.debug_computed_dict:\n",
    "        masked_container.debug_computed_dict[a_decoder_name] = {}\n",
    "\n",
    "    assert a_t_bin_size in masked_container.epochs_decoded_result_cache_dict, f'we created it above!!'\n",
    "    a_masked_result = masked_container.epochs_decoded_result_cache_dict[a_t_bin_size].get(a_decoder_name, None) ## already masked in previously in `_subfn_update_internal_results`\n",
    "    a_decoder: BayesianPlacemapPositionDecoder = masked_container.pf1D_Decoder_dict.get(a_decoder_name, None)\n",
    "    assert a_masked_result is not None, f\"a_masked_result is None for masked_container.epochs_decoded_result_cache_dict[a_t_bin_size: {a_t_bin_size}][a_decoder_name: '{a_decoder_name}']\"\n",
    "    # if a_masked_result is None:\n",
    "    # a_masked_result, a_decoder, active_epochs_df = masked_container.update_active_epochs_and_decode_posteriors_if_needed(curr_active_pipeline, an_epoch_name=a_decoder_name, decoding_time_bin_size=a_t_bin_size, \n",
    "    #                                             **_decode_kwargs,\n",
    "    #                                             override_included_analysis_epochs=None, ## because it will use self.active_epochs if it exists.\n",
    "    #                                             epoch_id_key_name='non_local_PBE_non_moving_epoch', force_recompute_epoch_df_columns=False,\n",
    "    #                                         )\n",
    "\n",
    "\n",
    "    override_included_analysis_epochs: pd.DataFrame = ensure_dataframe(masked_container.active_epochs_df)\n",
    "    # override_included_analysis_epochs: pd.DataFrame = ensure_dataframe(a_masked_result.filter_epochs)\n",
    "    print(f'\\tlen(override_included_analysis_epochs): {len(override_included_analysis_epochs)}, \\n\\toverride_included_analysis_epochs: {override_included_analysis_epochs}')\n",
    "    \n",
    "    # 2025-01-08 - Mask based on position-like bins only _________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    _out = masked_container.compute_future_and_past_analysis(an_epoch_name=a_decoder_name, decoding_time_bin_size=a_t_bin_size, enable_updating_instance_states=True, \n",
    "                                                                override_included_analysis_epochs=override_included_analysis_epochs, ## is this right?\n",
    "                                                                \n",
    "                                                                )\n",
    "    # epoch_high_prob_pos_masks, epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list = _out\n",
    "    epoch_high_prob_pos_masks, epoch_t_bins_high_prob_pos_masks, epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list, _out_processed_items_list_dict = _out\n",
    "    # masked_container.debug_computed_dict[an_epoch_name] = {'epoch_high_prob_pos_masks': epoch_high_prob_pos_masks, 'epoch_t_bins_high_prob_pos_masks': epoch_t_bins_high_prob_pos_masks, 'epoch_matching_positions': epoch_matching_positions, 'past_future_info_dict': past_future_info_dict}\n",
    "    masked_container.debug_computed_dict[a_decoder_name].update({'epoch_high_prob_pos_masks': epoch_high_prob_pos_masks, 'epoch_t_bins_high_prob_pos_masks': epoch_t_bins_high_prob_pos_masks, 'epoch_matching_positions': epoch_matching_positions, 'past_future_info_dict': past_future_info_dict})\n",
    "\n",
    "\n",
    "## END for an_epoch_name in epoch_names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b0bcd",
   "metadata": {
    "tags": [
     "2026-01-16_fixing_compute_memory"
    ]
   },
   "outputs": [],
   "source": [
    "decoding_time_bin_size: float = 0.025 # 25ms, input \n",
    "assert masked_container is not None\n",
    "epoch_names = ['roam', 'sprinkle']\n",
    "for an_epoch_name in epoch_names:\n",
    "    try:\n",
    "        print(f'\\ttrying `.masked_container.final_refine_single_decoder_result_masks(...)` for an_epoch_name: \"{an_epoch_name}\"...')\n",
    "        if an_epoch_name not in masked_container.debug_computed_dict:\n",
    "            masked_container.debug_computed_dict[an_epoch_name] = {}\n",
    "        active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = masked_container.final_refine_single_decoder_result_masks(curr_active_pipeline=curr_active_pipeline, decoding_time_bin_size=decoding_time_bin_size, an_epoch_name=an_epoch_name)\n",
    "        masked_container.debug_computed_dict[an_epoch_name].update({'active_epochs_result': active_epochs_result, 'custom_results_df_list': custom_results_df_list, 'decoded_epoch_t_bins_promenence_result_obj': decoded_epoch_t_bins_promenence_result_obj})\n",
    "    except (ValueError, AttributeError, IndexError, KeyError, TypeError) as e:\n",
    "        print(f'\\t\\tWARN: the last part of `perform_predictive_decoding_analysis(...) failed with error: {e}. Skipping.')\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        raise\n",
    "## END for an_epoch_name in epoch_names...\n",
    "\n",
    "# \ttrying `.masked_container.final_refine_single_decoder_result_masks(...)` for an_epoch_name: \"roam\"...\n",
    "# step: 0.0004381917184223964\n",
    "# \ttrying `.masked_container.final_refine_single_decoder_result_masks(...)` for an_epoch_name: \"sprinkle\"...\n",
    "# step: 0.0006922956556940814\n",
    "# \t\tWARN: the last part of `perform_predictive_decoding_analysis(...) failed with error: epoch 0: posterior shape (41, 62) does not match x/y bin centers (41, 63). Skipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bac567",
   "metadata": {
    "tags": [
     "2026-01-16_fixing_compute_memory"
    ]
   },
   "outputs": [],
   "source": [
    "_container_container.container = masked_container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5099f",
   "metadata": {},
   "source": [
    "### Save to Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins.pkl')\n",
    "simplified_obj.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af835dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import PeakPromenence, PeakCounts, SlabResult, PeakPromenenceMetrics, PosteriorPeaksPeakProminence2dResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, safeSaveData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77251c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5809038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, safeSaveData\n",
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins.pkl')\n",
    "saveData(pkl_output_path, (simplified_obj.xx, simplified_obj.yy, simplified_obj.results, simplified_obj.flat_peaks_df, simplified_obj.filtered_flat_peaks_df, simplified_obj.peak_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_obj: PosteriorPeaksPeakProminence2dResult = PosteriorPeaksPeakProminence2dResult._reload_class(an_instance=simplified_obj)\n",
    "simplified_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified_obj = PosteriorPeaksPeakProminence2dResult.convert_paths_to_vertices(result_obj=posterior_peaks, in_place=False)\n",
    "\n",
    "simplified_obj = posterior_peaks.convert_paths_to_vertices(in_place=False)\n",
    "simplified_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins.hdf')\n",
    "simplified_obj.to_hdf(file_path=hdf_output_path, key='posterior_peaks', debug_print=True, enable_hdf_testing_mode=True)\n",
    "print(f'hdf_output_path: \"{hdf_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f63980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins_SPLIT_EVERYTHING_ELSE.pkl')\n",
    "saveData(pkl_output_path, (simplified_obj.xx, simplified_obj.yy, simplified_obj.flat_peaks_df, simplified_obj.filtered_flat_peaks_df, simplified_obj.peak_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ab7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_results_dict = {k:v for i, (k, v) in enumerate(simplified_obj.results.items()) if i < 2}\n",
    "sample_results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import SlabResult\n",
    "# debug_dump_object_member_shapes\n",
    "\n",
    "## INPUTS: simplified_obj.results\n",
    "\n",
    "slab_results_dict: Dict[Tuple, SlabResult] = {k:SlabResult(**a_slab_result_dict) for k, a_slab_result_dict in simplified_obj.results.items()}\n",
    "slab_results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_slab_results_dict: Dict[Tuple, SlabResult] = {k:SlabResult(**a_slab_result_dict) for k, a_slab_result_dict in simplified_obj.results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slab_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14736db9",
   "metadata": {},
   "source": [
    "## Back to earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29dbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins_SPLIT_results.pkl')\n",
    "saveData(pkl_output_path, (slab_results_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288580d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slab_results_dict[(0, 0)].peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5165bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(simplified_obj.results.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('simplified_obj.results', simplified_obj.results, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_slab_result_dict: Dict = simplified_obj.results[(0, 5)]\n",
    "slab_result: SlabResult = SlabResult(**a_slab_result_dict)\n",
    "\n",
    "# epoch_idx, t_idx, posterior_peaks_df, slab_result_dict\n",
    "# print_keys_if_possible('a_slab_result_dict', a_slab_result_dict, max_depth=2)\n",
    "print_keys_if_possible('slab_result.peaks', slab_result.peaks, max_depth=2)\n",
    "# debug_dump_object_member_shapes(slab_result.peaks)\n",
    "# a_slab_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f11366",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_peaks: int = len(slab_result.peaks)\n",
    "num_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "slab_result.peaks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c68b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('slab_result.peaks[1]', slab_result.peaks[1], max_depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_dump_object_member_shapes(slab_result.peaks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins_SPLIT_TESTING_SINGLE.pkl')\n",
    "saveData(pkl_output_path, (slab_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(slab_result.peaks[1], enable_print=True) # 3417.3662490844727 nearly halved memory usage\n",
    "\n",
    "print_keys_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(simplified_obj.results, enable_print=False) # 6717.631065368652 -> 3421.3314695358276 nearly halved memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(slab_results_dict, enable_print=False) # 3417.3662490844727 nearly halved memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6679c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d616b278",
   "metadata": {},
   "source": [
    "### Load from pickle file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b354b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer\n",
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-06_PosteriorPeaksPeakProminence2dResult_all_t_bins.pkl')\n",
    "posterior_peaks: PosteriorPeaksPeakProminence2dResult = PosteriorPeaksPeakProminence2dResult.from_file(pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rel_time_bin_idx' not in posterior_peaks.flat_peaks_df.columns:\n",
    "    posterior_peaks.flat_peaks_df['rel_time_bin_idx'] = deepcopy(posterior_peaks.flat_peaks_df['time_bin_idx'])\n",
    "    posterior_peaks.flat_peaks_df['time_bin_idx'] = posterior_peaks.flat_peaks_df['rel_time_bin_idx'] + start_idx\n",
    "\n",
    "posterior_peaks.flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec37dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_peaks.filtered_flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_peaks.filtered_flat_peaks_df[posterior_peaks.filtered_flat_peaks_df['peak_prominence'] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import PeakPromenence, PeakPromenenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc389e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(p_x_given_n_list)\n",
    "# len(posterior_peaks)\n",
    "list(posterior_peaks.results.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9601079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: p_x_given_n_list, posterior_peaks\n",
    "\n",
    "\n",
    "# Plot single time bin\n",
    "a_plotter, _update_plot_for_time_bin = PeakPromenenceDisplay.plot_prominence_peaks_3d_pyvista(\n",
    "    posterior_peaks_result=posterior_peaks,\n",
    "    p_x_given_n_list=p_x_given_n_list,\n",
    "    epoch_idx=0,\n",
    "    # time_bin_idx=29699,\n",
    "    time_bin_idx=29931,\n",
    "    show_col_contours=True,\n",
    "    show_probe_level_contours=True,\n",
    "    # probe_level_to_show=0.5,  # Only show 0.5x height contours\n",
    "    opacity=0.7,\n",
    "    z_axis_scale=1.0,\n",
    "    # z_axis_scale=100.0,\n",
    ")\n",
    "\n",
    "a_plotter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4451ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_update_plot_for_time_bin(29931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or plot multiple time bins in a grid\n",
    "plotter_grid = PeakPromenenceDisplay.plot_prominence_peaks_3d_pyvista_grid(\n",
    "    posterior_peaks_result=posterior_peaks,\n",
    "    p_x_given_n_list=p_x_given_n_list,\n",
    "    epoch_idx=0,\n",
    "    time_bin_indices=[0, 5, 10, 15],\n",
    "    n_cols=2\n",
    ")\n",
    "\n",
    "plotter_grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_contexts.roam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter\n",
    "# from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "\n",
    "# a_time_bin_size: float = 0.025\n",
    "# an_epoch_name: str = 'roam'\n",
    "# # container.decoding_locality\n",
    "# a_result: DecodedFilterEpochsResult = container.epochs_decoded_result_cache_dict[a_time_bin_size][an_epoch_name]\n",
    "# np.shape(container.decoding_locality.p_x_given_n)\n",
    "# p_x_given_n = container.decoding_locality.p_x_given_n_dict[an_epoch_name]\n",
    "# np.shape(p_x_given_n)\n",
    "\n",
    "# active_epoch_context = curr_active_pipeline.filtered_contexts.roam\n",
    "# curr_active_pipeline.prepare_for_display()\n",
    "# _out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=active_epoch_context,\n",
    "#                                     # params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "#                                     )\n",
    "# iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "# pActiveInteractiveLapsPlotter = _out['plotter']\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = DecodedTrajectoryPyVistaPlotter(a_result=a_result,\n",
    "#                                                                                              xbin = container.decoding_locality.xbin, ybin = container.decoding_locality.ybin,\n",
    "#                                                                                              xbin_centers = container.decoding_locality.xbin_centers, ybin_centers = container.decoding_locality.ybin_centers,\n",
    "#                                                                                             #  xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers,\n",
    "#                                                                                             p=iplapsDataExplorer.p)\n",
    "\n",
    "# a_decoded_trajectory_pyvista_plotter.build_ui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import PeakPromenence, PosteriorPeaksPeakProminence2dResult, PeakCounts, DecodedEpochIndex, DecodedEpochTimeBinIndex, DecodedEpochTimeBinIndexTuple\n",
    "\n",
    "a_time_bin_size: float = 0.025\n",
    "an_epoch_name: str = 'roam'\n",
    "# container.decoding_locality\n",
    "a_result: DecodedFilterEpochsResult = container.epochs_decoded_result_cache_dict[a_time_bin_size][an_epoch_name]\n",
    "a_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(container.decoding_locality.p_x_given_n)\n",
    "p_x_given_n = container.decoding_locality.p_x_given_n_dict[an_epoch_name]\n",
    "np.shape(p_x_given_n)\n",
    "\n",
    "# decoded_epochs_result = some_decoder_result  # has .p_x_given_n_list\n",
    "p_x_given_n_list = a_result.p_x_given_n_list\n",
    "# p_x_given_n_list = [container.decoding_locality.p_x_given_n_dict[an_epoch_name][:, :, time_bin_subset]] ## single element\n",
    "# p_x_given_n_list = [container.decoding_locality.p_x_given_n_dict[an_epoch_name][:, :, a_time_bin_subset] for a_time_bin_subset in time_bin_subset]\n",
    "\n",
    "default_kwargs = dict(step=0.02,\n",
    "                         minimum_included_peak_height=0.2,\n",
    "                         min_considered_promenence=0.2,\n",
    "                    )\n",
    "\n",
    "\n",
    "custom_kwargs = dict(step=0.001,\n",
    "                    minimum_included_peak_height=0.0002,\n",
    "                    min_considered_promenence=0.0002,\n",
    "                    )\n",
    "\n",
    "active_kwargs = {k:v for k, v in custom_kwargs.items()}\n",
    "# active_kwargs = default_kwargs\n",
    "# posterior_peaks = None\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-PeakPromenence._perform_find_posterior_peaks_peak_prominence2d_computation_optimized.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "a_result_posterior_peaks: PosteriorPeaksPeakProminence2dResult = PeakPromenence._perform_find_posterior_peaks_peak_prominence2d_computation(\n",
    "     p_x_given_n_list = p_x_given_n_list,\n",
    "     xbin_centers = container.decoding_locality.xbin_centers, ybin_centers = container.decoding_locality.ybin_centers,\n",
    "     peak_height_multiplier_probe_levels=(0.5, 0.9),\n",
    "     # \n",
    "     **active_kwargs,\n",
    "     uniform_blur_size=3,\n",
    "     gaussian_blur_sigma=3,\n",
    "     debug_print=False,\n",
    "     parallel=True, max_workers=4,\n",
    "     )\n",
    "# posterior_peaks.flat_peaks_df\n",
    "\n",
    "# 2m 13.2s - 500 entries\n",
    "# 50.4s - 500 entries after optimization\n",
    "# @ 8.5m - 5000 entries after optimization\n",
    "# 3.2m - all entries for step=0.002, minimum_included_peak_height=0.002, min_considered_promenence=0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35701c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import PeakPromenence\n",
    "\n",
    "all_epochs_all_t_bins_epoch_t_bin_idx_tuple_list, all_epochs_promenence_tuples_dict, all_epochs_masks = PeakPromenence.compute_posterior_peak_promenences(p_x_given_n_list=a_decoded_result.p_x_given_n_list, alpha=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e76f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs_promenence_tuples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad47f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import SlabResult\n",
    "# debug_dump_object_member_shapes\n",
    "\n",
    "\n",
    "a_slab_result_dict: Dict = a_result_posterior_peaks.results[(0, 1)]\n",
    "slab_result: SlabResult = SlabResult(**a_slab_result_dict)\n",
    "\n",
    "# epoch_idx, t_idx, posterior_peaks_df, slab_result_dict\n",
    "# print_keys_if_possible('a_slab_result_dict', a_slab_result_dict, max_depth=2)\n",
    "print_keys_if_possible('slab_result.peaks', slab_result.peaks, max_depth=2)\n",
    "# debug_dump_object_member_shapes(slab_result.peaks)\n",
    "# a_slab_result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epoch_t_bin_idx_tuples_list: List[Tuple] = list(a_result_posterior_peaks.results.keys())\n",
    "all_epoch_t_bin_idx_tuples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(a_result_posterior_peaks)\n",
    "Dict[DecodedEpochIndex, DecodedEpochTimeBinIndex= a_result_posterior_peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_result_posterior_peaks.peak_counts\n",
    "a_peaks_results: Dict[DecodedEpochIndex, DecodedEpochTimeBinIndex, Dict] = a_result_posterior_peaks.results\n",
    "an_epoch_idx: decoded_epoch_index = 0\n",
    "a_decoded_epoch_time_bin_idx: decoded_epoch_time_bin_index = 1\n",
    "a_epoch_t_bin_tuple: DecodedEpochIndex, DecodedEpochTimeBinIndex = (an_epoch_idx, a_decoded_epoch_time_bin_idx)\n",
    "an_epoch_t_bin_peaks_result: Dict = a_peaks_results[a_epoch_t_bin_tuple]\n",
    "peaks_dict = an_epoch_t_bin_peaks_result['peaks']\n",
    "# list(an_epoch_t_bin_peaks_result.keys())\n",
    "# print_keys_if_possible('an_epoch_t_bin_peaks_result', an_epoch_t_bin_peaks_result, max_depth=1)\n",
    "# list(a_peaks_results.keys())\n",
    "print_keys_if_possible('a_peaks', peaks_dict, max_depth=1)\n",
    "max_peak = list(peaks_dict.values())[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f914fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryNapariPlotter\n",
    "\n",
    "napari_plotter: DecodedTrajectoryNapariPlotter = DecodedTrajectoryNapariPlotter(a_result=a_result, \n",
    "                                                    xbin = container.decoding_locality.xbin, ybin = container.decoding_locality.ybin,\n",
    "                                                    xbin_centers = container.decoding_locality.xbin_centers, ybin_centers = container.decoding_locality.ybin_centers,\n",
    "                                                    create_logging_dock=True,\n",
    "                                                )\n",
    "viewer, layer = napari_plotter.build_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_layer, update_contours_for_current_indices = napari_plotter.add_peak_contours_layer(peak_prominence_result=a_result_posterior_peaks,\n",
    "                                                                                            #  edge_color = 'white', face_color = 'transparent', edge_width = 1.0,\n",
    "                                                                                            #  edge_color='#ff55ff9b', face_color = '#ffaaff78', edge_width = 0.0,\n",
    "                                                                                            edge_color='transparent', face_color = '#ffaaff78', edge_width = 0.001,\n",
    "                                                                                             )\n",
    "contours_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "napari_plotter._log_to_console('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12417f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.update_console('MEWS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa441359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# napari_plotter._log_to_console('test')\n",
    "# Send local variables to the console\n",
    "# viewer.update_console('test')\n",
    "\n",
    "# Send local variables to the console\n",
    "viewer.update_console(locals())\n",
    "\n",
    "# napari_plotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari.utils.notifications as napari_notify\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from napari.utils.notifications import (\n",
    "    notification_manager,\n",
    "    Notification,\n",
    "    NotificationSeverity,\n",
    "    show_console_notification,\n",
    ")\n",
    "\n",
    "# my_custom_plugin_name: str = \"napari-simple-reload\"\n",
    "my_custom_plugin_name: str = \"napari-DecodedTrajectoryNapariPlotter\"\n",
    "\n",
    "my_plugin_logger = logging.getLogger(my_custom_plugin_name)\n",
    "stdout_handler = logging.StreamHandler(sys.stderr)\n",
    "stdout_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        fmt=\"%(levelname)s: %(asctime)s %(message)s\",\n",
    "        datefmt=\"%d/%m/%Y %I:%M:%S %p\"\n",
    "    )\n",
    ")\n",
    "my_plugin_logger.addHandler(stdout_handler)\n",
    "my_plugin_logger.setLevel(logging.WARNING)\n",
    "\n",
    "def show_debug(message: str):\n",
    "    \"\"\"\n",
    "    Show a debug message in the notification manager.\n",
    "    \"\"\"\n",
    "    notification_ = Notification(message, severity=NotificationSeverity.DEBUG)\n",
    "    # Show message in the console only\n",
    "    show_console_notification(notification_)\n",
    "    # Show message in console and the napari GUI\n",
    "    notification_manager.dispatch(notification_)\n",
    "    # Control level of shown messages via napari preferences\n",
    "\n",
    "\n",
    "def example(input_string: str) -> str:\n",
    "    output_string = (\n",
    "        f\"You entered {input_string}!\"\n",
    "        if input_string\n",
    "        else \"Please enter something in the text box.\"\n",
    "    )\n",
    "    show_debug(f\"The input string was (napari): {input_string}\")\n",
    "    my_plugin_logger.debug(\n",
    "        f\"The input string was (logging): {input_string}\")\n",
    "    print(output_string)\n",
    "    return output_string\n",
    "\n",
    "\n",
    "example('pho-test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# napari_notify.show_console_notification('test')\n",
    "napari_notify.show_info('test')\n",
    "\n",
    "napari_notify.show_error('test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ac4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_notication.py\n",
    "import logging\n",
    "from napari.settings import get_settings\n",
    "from napari import run, Viewer\n",
    "\n",
    "settings = get_settings()\n",
    "settings.application.console_notification_level = \"debug\"\n",
    "settings.application.gui_notification_level = \"debug\"\n",
    "# viewer = Viewer()\n",
    "\n",
    "## INPUTS: viewer\n",
    "viewer_window = viewer.window\n",
    "viewer_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer_window.add_function_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# viewer.window.add_plugin_dock_widget(my_custom_plugin_name, \"Autogenerated\")\n",
    "logging.getLogger(my_custom_plugin_name).setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set edge_width after creation to ensure float values work properly\n",
    "# This is a workaround for Napari's rendering that sometimes rounds edge_width\n",
    "# contours_layer.edge_width = edge_width\n",
    "contours_layer.edge_width_is_relative = False\n",
    "contours_layer.edge_width = 0.05  # This will be relative to data units\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the above doesn't work, try making it relative to data scale:\n",
    "contours_layer.edge_width_is_relative = True\n",
    "contours_layer.edge_width = edge_width  # This will be relative to data units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_layer.editable = True  # Make non-editable since it's dynamically updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c500b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_contours_for_current_indices(0, 3)\n",
    "update_contours_for_current_indices(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eda5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_counts_layer = napari_plotter.add_peak_counts_layer(peak_prominence_result=a_result_posterior_peaks)\n",
    "peak_counts_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30424540",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_peaks.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db441484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible(curr_key='container.decoding_locality.debugging_dict_dict', curr_value=container.decoding_locality.debugging_dict_dict['roam'], max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005e965",
   "metadata": {},
   "source": [
    "### Save to Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import safeSaveData\n",
    "\n",
    "container = PredictiveDecodingComputationsContainer._reload_class(container)\n",
    "# decoding_locality = DecodingLocalityMeasures._reload_class(decoding_locality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2644861",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictiveDecodingComputationsContainer.rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results_df: pd.DataFrame = deepcopy(container.scoring_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.scoring_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eedbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-09_PredictiveDecodingComputationsContainer.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-15_PredictiveDecodingComputationsContainer.pkl')\n",
    "container.save(pkl_output_path=pkl_output_path)\n",
    "# container.save(pkl_output_path=pkl_output_path)\n",
    "# safeSaveData(pkl_path=pkl_output_path, db=container.to_dict())\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a87422",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(masked_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-13_PredictiveDecodingComputationsContainer.pkl')\n",
    "masked_container.save(pkl_output_path=pkl_output_path)\n",
    "# container.save(pkl_output_path=pkl_output_path)\n",
    "# safeSaveData(pkl_path=pkl_output_path, db=container.to_dict())\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-08_pred_decoding_result.pkl')\n",
    "# container.predictive_decoding.save(pkl_output_path=pkl_output_path)\n",
    "safeSaveData(pkl_path=pkl_output_path, db=container.predictive_decoding.to_dict())\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a645996",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-08_locality_measures.pkl')\n",
    "# container.predictive_decoding.locality_measures.save(pkl_output_path=pkl_output_path)\n",
    "safeSaveData(pkl_path=pkl_output_path, db=container.predictive_decoding.locality_measures.to_dict())\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be4591",
   "metadata": {},
   "source": [
    "### Load from pickle file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df5f07",
   "metadata": {
    "tags": [
     "2025-01-06-loadmain",
     "2026-01-07_pyvistaposteriorswithdualsliders_working",
     "2026-01-08_Position-likePosteriorsOnlyFilteredResult"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecoding, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, MatchingPastFuturePositionsResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-05_PredictiveDecodingComputationsContainer.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-08_PredictiveDecodingComputationsContainer.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-09_PredictiveDecodingComputationsContainer.pkl')\n",
    "container: PredictiveDecodingComputationsContainer = PredictiveDecodingComputationsContainer.from_file(pkl_output_path)\n",
    "# loaded_container: PredictiveDecodingComputationsContainer = PredictiveDecodingComputationsContainer.from_file(pkl_output_path)\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUTS: container\n",
    "assert container is not None, f\"container is None after loading from pickle!\"\n",
    "## Set pipeline data:\n",
    "curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'] = container\n",
    "if container.decoding_locality is None:\n",
    "    container.decoding_locality = container.predictive_decoding.locality_measures\n",
    "decoding_locality: DecodingLocalityMeasures = container.decoding_locality\n",
    "# non_local_PBE_non_moving_epochs_df: pd.DataFrame = decoding_locality.get_non_moving_PBE_non_local_epochs(curr_active_pipeline.sess, merging_adjacent_max_separation_sec=0.5)\n",
    "non_local_PBE_non_moving_epochs_df: pd.DataFrame = container.decoding_locality.non_local_PBE_non_moving_epochs_df\n",
    "predictive_decoding: PredictiveDecoding = container.predictive_decoding\n",
    "non_local_PBE_non_moving_epochs_df\n",
    "## OUTPUTS: container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b796bb",
   "metadata": {},
   "source": [
    "## ⚓🚧🎯🔷 After all computations, compute the fine-grained promenance/etc metrics for each posterior so we can see them (and assess how they're working) next to the posteriors themselves in the 3D Silx Volumetric viewer\n",
    "TODO 2025-12-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a84863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MatchingPastFuturePositionsResult, DecodingLocalityMeasures, PredictiveDecodingComputationsContainer, PredictiveDecodingComputationsContainerContainer, PredictiveDecoding\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import PositionLikePosteriorScoring\n",
    "from pyphoplacecellanalysis.External.peak_prominence2d import PeakPromenence, PosteriorPeaksPeakProminence2dResult\n",
    "\n",
    "# container = PredictiveDecodingComputationsContainer._reload_class(**get_dict_subset(container.__dict__, subset_excludelist=[]))\n",
    "\n",
    "## Get the computed result from the pipeline\n",
    "_container_container: PredictiveDecodingComputationsContainerContainer = curr_active_pipeline.global_computation_results.computed_data.get('PredictiveDecoding', None)\n",
    "container: PredictiveDecodingComputationsContainer = _container_container.container # curr_active_pipeline.global_computation_results.computed_data.get('PredictiveDecoding', None)\n",
    "if container is not None:\n",
    "    predictive_decoding: PredictiveDecoding = container.predictive_decoding\n",
    "    if predictive_decoding is not None:\n",
    "        print(f'PredictiveDecoding computed with window_size: {predictive_decoding.window_size}')\n",
    "        print(f'epoch_names: {predictive_decoding.epoch_names}')\n",
    "        if container.decoding_locality is None:\n",
    "            container.decoding_locality = container.predictive_decoding.locality_measures\n",
    "        decoding_locality: DecodingLocalityMeasures = container.decoding_locality\n",
    "    else:\n",
    "        print(f'PredictiveDecoding is None.')\n",
    "else:\n",
    "    print(f'PredictiveDecoding is not computed.')\n",
    "\n",
    "# epoch_high_prob_pos_masks = container.debug_computed_dict[an_epoch_name]['epoch_high_prob_pos_masks']\n",
    "# epoch_matching_positions = container.debug_computed_dict[an_epoch_name]['epoch_matching_positions']\n",
    "# past_future_info_dict = container.debug_computed_dict[an_epoch_name]['past_future_info_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ba472",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = PredictiveDecodingComputationsContainer._reload_class(container)\n",
    "\n",
    "type(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2610da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container = PredictiveDecodingComputationsContainer._reload_class(container) # TypeError: __init__() got an unexpected keyword argument 'locality_measures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b025407",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = deepcopy(masked_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8dfc5d",
   "metadata": {
    "tags": [
     "2026-01-07_pyvistaposteriorswithdualsliders_working",
     "2026-01-08_Position-likePosteriorsOnlyFilteredResult"
    ]
   },
   "outputs": [],
   "source": [
    "# INPUTS: container\n",
    "\n",
    "## OUTPUTS: active_epochs_result (masked result),  custom_results_df_list\n",
    "\n",
    "decoding_time_bin_size = 0.025\n",
    "an_epoch_name = 'roam'\n",
    "masked_container = container.build_masked_container(curr_active_pipeline=curr_active_pipeline, use_full_recompute_method=True,\n",
    "    should_filter_directional_decoders_decode_result=True, should_compute_future_and_past_analysis=True, should_compute_peak_prom_analysis=True,\n",
    "    window_size=8, use_parallel=True, max_workers=2, a_t_bin_size=decoding_time_bin_size,\n",
    ") ## 3m now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b7e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cec83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('masked_container.debug_computed_dict', masked_container.debug_computed_dict , max_depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e85f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "masked_container.debug_computed_dict[an_epoch_name] ## None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f82c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_time_bin_size = 0.025\n",
    "an_epoch_name = 'roam'\n",
    "active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = masked_container.final_refine_single_decoder_result_masks(curr_active_pipeline=curr_active_pipeline, decoding_time_bin_size=decoding_time_bin_size, an_epoch_name=an_epoch_name)\n",
    "\n",
    "# active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = final_refine_single_decoder_result_masks(container, decoding_time_bin_size = 0.025, an_epoch_name = 'roam')\n",
    "active_epochs_result\n",
    "## 2m 39s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_epoch_t_bins_promenence_result_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde35eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24976c43",
   "metadata": {},
   "source": [
    "### 💾 Save to Pickle file - Pickle `container`/`masked_container`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7317a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import safeSaveData, safeSaveSplitData\n",
    "## Pickle the result:\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-16_PredictiveDecodingComputationsContainer.pkl')\n",
    "safeSaveData(pkl_path=pkl_output_path, db=container)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "# split_save_folder, split_save_paths, split_save_output_types, failed_keys = safeSaveSplitData(pkl_output_path, masked_container, debug_print=True)\n",
    "# print(f'split_save_folder: \"{split_save_folder.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ad489",
   "metadata": {
    "tags": [
     "2025-01-14_pickling",
     "2026-01-14_new-reasonable-future-and-past"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import safeSaveData, safeSaveSplitData\n",
    "## Pickle the result:\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-14_PredictiveDecodingComputationsContainer.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-14_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-15_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-16_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "# masked_container.save(pkl_output_path=pkl_output_path)\n",
    "# container.save(pkl_output_path=pkl_output_path)\n",
    "# safeSaveData(pkl_path=pkl_output_path, db=container.to_dict())\n",
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = safeSaveSplitData(pkl_output_path, masked_container, debug_print=True)\n",
    "# split_save_folder, split_save_paths, split_save_output_types, failed_keys = safeSaveSplitData(pkl_output_path, container, debug_print=True)\n",
    "\n",
    "# safeSaveData(pkl_path=pkl_output_path, db=container.__dict__)\n",
    "# print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "print(f'split_save_folder: \"{split_save_folder.as_posix()}\"')\n",
    "# Path(\"w:/Data/Bapun/RatN/Day4OpenField/output/2026-01-15_PredictiveDecodingComputationsContainer_masked_split\")\n",
    "## 16m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import safeSaveData, safeSaveSplitData\n",
    "## Pickle the result:\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-15_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = safeSaveSplitData(pkl_output_path, masked_container, debug_print=True)\n",
    "print(f'split_save_folder: \"{split_save_folder.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b4d40",
   "metadata": {
    "tags": [
     "2026-01-14_new-reasonable-future-and-past"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadSplitData\n",
    "from neuropy.utils.mixins.indexing_helpers import get_dict_subset\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingComputationsContainer, PredictiveDecoding, DecodingLocalityMeasures\n",
    "# Load split data\n",
    "split_save_folder: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-14_PredictiveDecodingComputationsContainer_split')\n",
    "container = loadSplitData(split_save_folder, debug_print=True, target_cls=PredictiveDecodingComputationsContainer)\n",
    "# if isinstance(container, dict):\n",
    "#     container: PredictiveDecodingComputationsContainer = PredictiveDecodingComputationsContainer(**get_dict_subset(container, subset_excludelist=['_VersionedResultMixin_version']))\n",
    "type(container)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61fc77",
   "metadata": {
    "tags": [
     "2026-01-14_new-reasonable-future-and-past"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingComputationsContainer, PredictiveDecoding, DecodingLocalityMeasures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadSplitData\n",
    "\n",
    "# split_save_folder: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-14_PredictiveDecodingComputationsContainer_masked')\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-15_PredictiveDecodingComputationsContainer_masked.pkl')\n",
    "masked_container = loadSplitData(split_save_folder, debug_print=True, target_cls=PredictiveDecodingComputationsContainer, raise_on_exception=True)\n",
    "# if isinstance(masked_container, dict):\n",
    "#     masked_container: PredictiveDecodingComputationsContainer = PredictiveDecodingComputationsContainer(**get_dict_subset(masked_container, subset_excludelist=['_VersionedResultMixin_version']))\n",
    "\n",
    "type(masked_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingComputationsContainer\n",
    "from neuropy.utils.mixins.indexing_helpers import get_dict_subset\n",
    "\n",
    "if isinstance(loaded_data, dict):\n",
    "    container: PredictiveDecodingComputationsContainer = PredictiveDecodingComputationsContainer(**get_dict_subset(loaded_data, subset_excludelist=['_VersionedResultMixin_version']))\n",
    "\n",
    "\n",
    "type(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loaded_data)\n",
    "print_keys_if_possible('loaded_data', loaded_data, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38f22f",
   "metadata": {},
   "source": [
    "### Load from Pickle file -  `container`/`masked_container`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadSplitData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult, PredictiveDecodingComputationsContainer\n",
    "\n",
    "# split_save_folder: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-14_PredictiveDecodingComputationsContainer_split')\n",
    "# split_save_folder: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-14_PredictiveDecodingComputationsContainer_masked_split')\n",
    "split_save_folder: Path = curr_active_pipeline.get_output_path().joinpath('2026-01-15_PredictiveDecodingComputationsContainer_masked_split')\n",
    "# Load split data\n",
    "loaded_data = loadSplitData(split_save_folder, debug_print=True, target_cls=PredictiveDecodingComputationsContainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece3859",
   "metadata": {},
   "source": [
    "## 2026-01-21 - ⚓🟢 Finish compute manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult, PredictiveDecodingComputationsContainer\n",
    "\n",
    "decoding_time_bin_size = 0.025\n",
    "an_epoch_name = 'sprinkle'\n",
    "active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = masked_container.final_refine_single_decoder_result_masks(curr_active_pipeline=curr_active_pipeline, decoding_time_bin_size=decoding_time_bin_size, an_epoch_name=an_epoch_name)\n",
    "\n",
    "# active_epochs_result, custom_results_df_list, decoded_epoch_t_bins_promenence_result_obj = final_refine_single_decoder_result_masks(container, decoding_time_bin_size = 0.025, an_epoch_name = 'roam')\n",
    "active_epochs_result\n",
    "## 2m 39s\n",
    "\n",
    "## OUTPUTS: decoded_epoch_t_bins_promenence_result_obj\n",
    "\n",
    "## finished in 20.3m\n",
    "## 19m 52s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict[an_epoch_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if an_epoch_name not in masked_container.debug_computed_dict:\n",
    "    print(f'masked_container.debug_computed_dict[\"{an_epoch_name}\"]: does not exist for this epoch name... creating!')\n",
    "    masked_container.debug_computed_dict[an_epoch_name] = {}\n",
    "\n",
    "# if masked_container.debug_computed_dict.get('decoded_epoch_t_bins_promenence_result_obj', None) is None:\n",
    "    \n",
    "\n",
    "list(masked_container.debug_computed_dict.keys()) ## ['roam', 'sprinkle']\n",
    "\n",
    "list(masked_container.debug_computed_dict['roam'].keys()) # ['decoded_epoch_t_bins_promenence_result_obj'] ['prominence_future_past_analysis', 'active_epochs_result', 'custom_results_df_list', 'decoded_epoch_t_bins_promenence_result_obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(masked_container.debug_computed_dict[an_epoch_name])\n",
    "masked_container.debug_computed_dict) # [an_epoch_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict[an_epoch_name].update({'active_epochs_result': active_epochs_result, 'custom_results_df_list': custom_results_df_list, 'decoded_epoch_t_bins_promenence_result_obj': decoded_epoch_t_bins_promenence_result_obj}) ## missing 'prominence_future_past_analysis'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a62dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decoded_epoch_t_bins_promenence_result_obj) # PosteriorPeaksPeakProminence2dResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cc90f",
   "metadata": {
    "tags": [
     "2026-01-12_workingprediction",
     "2026-01-14_new-reasonable-future-and-past",
     "2026-01-16_fixing_compute_memory",
     "2026-01-21_postcomputefilter"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult\n",
    "from pyphoplacecellanalysis.External.peak_prominence2d import PosteriorPeaksPeakProminence2dResult\n",
    "\n",
    "## INPUTS: masked_container\n",
    "# non_local_PBE_non_moving_epochs_df: pd.DataFrame = decoding_locality.get_non_moving_PBE_non_local_epochs(curr_active_pipeline.sess, merging_adjacent_max_separation_sec=merging_adjacent_max_separation_sec)\n",
    "# non_local_PBE_non_moving_epochs_df: pd.DataFrame = container.decoding_locality.non_local_PBE_non_moving_epochs_df\n",
    "\n",
    "if masked_container.decoding_locality is not None:\n",
    "    masked_container.decoding_locality = DecodingLocalityMeasures._reload_class(masked_container.decoding_locality)\n",
    "\n",
    "measured_positions_df: pd.DataFrame = deepcopy(masked_container.decoding_locality.measured_positions_df)\n",
    "# measured_positions_df\n",
    "\n",
    "gaussian_volume = masked_container.predictive_decoding.gaussian_volume ## the volume for all time bins\n",
    "\n",
    "# Usage from Container:\n",
    "a_t_bin_size: float = 0.025\n",
    "# a_decoder_name: str = 'roam'\n",
    "a_decoder_name: str = 'sprinkle'\n",
    "slice_level_multipliers = [0.25, 0.5, 0.9]\n",
    "\n",
    "a_decoder = masked_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "a_decoded_result = masked_container.epochs_decoded_result_cache_dict[a_t_bin_size][a_decoder_name] # DecodedFilterEpochsResult\n",
    "a_decoded_filter_epochs_df: pd.DataFrame = a_decoded_result.filter_epochs\n",
    "# a_decoded_result.active_filter_epochs\n",
    "\n",
    "## INPUTS: decoded_epoch_t_bins_promenence_result_obj\n",
    "\n",
    "mask_included_bins_list, summit_slice_levels_list, mask_included_p_x_given_n_list_dict, epoch_prom_t_bin_high_prob_pos_masks_dict, epoch_prom_high_prob_pos_masks_dict, *extra_outs = decoded_epoch_t_bins_promenence_result_obj.compute_discrete_contour_masks(p_x_given_n_list=a_decoded_result.p_x_given_n_list, slice_level_multipliers=slice_level_multipliers)\n",
    "\n",
    "#TODO 2026-01-21 08:45: - [ ] `epoch_t_bin_high_prob_masks_dict ` or `epoch_high_prob_masks_dict` are used to update the final masks\n",
    "\n",
    "epoch_matching_past_future_positions, _an_out_tuple, a_decoded_filter_epochs_df = PredictiveDecoding.compute_specific_future_and_past_analysis(decoded_local_epochs_result=a_decoded_result,\n",
    "        measured_positions_df=measured_positions_df, gaussian_volume=gaussian_volume,\n",
    "        active_epochs_df=a_decoded_filter_epochs_df,\n",
    "        an_epoch_name=a_decoder_name, top_v_percent=None,\n",
    "        epoch_t_bin_high_prob_masks_dict=epoch_prom_t_bin_high_prob_pos_masks_dict,\n",
    "        epoch_high_prob_masks_dict=epoch_prom_high_prob_pos_masks_dict,\n",
    "        a_slice_multiplier=slice_level_multipliers[0],\n",
    "        progress_print=True,\n",
    "        merging_adjacent_max_separation_sec = 1e-9,\n",
    "        minimum_epoch_duration = 0.05,\n",
    "        # merging_adjacent_max_separation_sec=merging_adjacent_max_separation_sec, minimum_epoch_duration=minimum_epoch_duration,\n",
    "        should_defer_extended_computations=True, max_workers=4,\n",
    ")\n",
    "epoch_high_prob_pos_masks, epoch_t_bins_high_prob_pos_masks, epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list, _out_processed_items_list_dict = _an_out_tuple\n",
    "_out_epoch_flat_mask_future_past_result: List[MatchingPastFuturePositionsResult] = _out_processed_items_list_dict['_out_epoch_flat_mask_future_past_result']\n",
    "\n",
    "## OUTPUTS: _out_epoch_flat_mask_future_past_result\n",
    "\n",
    "## previous 53.8s\n",
    "## 23.5m\n",
    "## 13.2m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_epoch_flat_mask_future_past_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac79ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\tassigning the results to self.debug_computed_dict...')\n",
    "# At the end of _filter_single_epoch_result, add:\n",
    "if a_decoder_name not in masked_container.debug_computed_dict:\n",
    "    masked_container.debug_computed_dict[a_decoder_name] = {}\n",
    "\n",
    "if 'prominence_future_past_analysis' not in masked_container.debug_computed_dict[a_decoder_name]:\n",
    "    masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'] = {} ## init new\n",
    "\n",
    "masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'].update({\n",
    "    'epoch_high_prob_pos_masks': epoch_high_prob_pos_masks,\n",
    "    'epoch_t_bins_high_prob_pos_masks': epoch_t_bins_high_prob_pos_masks,\n",
    "    'epoch_matching_positions': epoch_matching_positions,\n",
    "    'past_future_info_dict': past_future_info_dict,\n",
    "    'matching_pos_dfs_list': matching_pos_dfs_list,\n",
    "    'matching_pos_epochs_dfs_list': matching_pos_epochs_dfs_list,\n",
    "    'decoded_epoch_t_bins_promenence_result_obj': decoded_epoch_t_bins_promenence_result_obj,\n",
    "    'slice_level_multiplier_used': slice_level_multipliers[0],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible(\"masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']\", masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'], max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('_out_processed_items_list_dict', _out_processed_items_list_dict, max_depth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62095aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the processed items to the dict too\n",
    "masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'].update(_out_processed_items_list_dict)\n",
    "### _out_epoch_flat_mask_future_past_result: List[MatchingPastFuturePositionsResult] = masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']['_out_epoch_flat_mask_future_past_result']\n",
    "\n",
    "## adds ['_out_epoch_flat_mask', '_out_processed_masks', '_out_epoch_flat_mask_future_past_result'] from `_out_processed_items_list_dict` to `masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']`\n",
    "\n",
    "# for k, v in _out_processed_items_list_dict.items():\n",
    "#     ## add the processed items to the dict too\n",
    "#     self.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'].update(_out_processed_items_list_dict)\n",
    "# _out_epoch_flat_mask_future_past_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df375833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible(\"masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']\", masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'], max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, v in _out_processed_items_list_dict.items():\n",
    "    print(f'removing {a_name}')\n",
    "    masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'].pop(a_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102545a",
   "metadata": {},
   "source": [
    "### 2026-01-20 - ⚓📈🚧 Test adding masked found non-local/position-like epochs to the SpikeRaster2D timeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc42f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_container.debug_computed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d3245",
   "metadata": {
    "tags": [
     "2026-01-21_plot_result",
     "2026-02-02_resume_after_break"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult, MatchingPastFuturePositionsResult, MaskDataSource\n",
    "\n",
    "# Usage from Container:\n",
    "a_t_bin_size: float = 0.025\n",
    "# a_decoder_name: str = 'roam'\n",
    "a_decoder_name: str = 'sprinkle'\n",
    "\n",
    "a_decoder = masked_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "a_decoded_result = masked_container.epochs_decoded_result_cache_dict[a_t_bin_size][a_decoder_name] # DecodedFilterEpochsResult\n",
    "a_decoded_filter_epochs_df: pd.DataFrame = a_decoded_result.filter_epochs\n",
    "_out_epoch_flat_mask_future_past_result: List[MatchingPastFuturePositionsResult] = masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']['_out_epoch_flat_mask_future_past_result']\n",
    "a_flat_matching_results_list_ds: MaskDataSource = MaskDataSource.init_from_list_of_MatchingPastFuturePositionsResult(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, filter_epochs=a_decoded_filter_epochs_df, \n",
    "                                                                                                                        xbin=a_decoder.xbin, ybin=a_decoder.ybin, xbin_centers=a_decoder.xbin_centers, ybin_centers=a_decoder.ybin_centers,\n",
    "                                                                                                                        curr_position_df=masked_container.decoding_locality.pos_df,\n",
    "                                                                                                                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_epoch_flat_mask_future_past_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098f82e",
   "metadata": {
    "tags": [
     "2026-02-03_hacking_roam_v_sprinkle_comparisons",
     "2026-02-09_sprinklevsroam",
     "2026-02-10_laps",
     "active-2026-02-16"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult, MatchingPastFuturePositionsResult, MaskDataSource\n",
    "\n",
    "# Usage from Container:\n",
    "a_t_bin_size: float = 0.025\n",
    "\n",
    "## INPUTS: masked_container\n",
    "decoder_epoch_flat_mask_future_past_result_dict: Dict[types.DecoderName, List[MatchingPastFuturePositionsResult]] = {} ## can't be serialized for some reason\n",
    "\n",
    "decoder_flat_matching_results_list_ds_dict: Dict[types.DecoderName, MaskDataSource] = {}\n",
    "\n",
    "included_epoch_names = ['roam', 'sprinkle']\n",
    "# included_epoch_names = ['roam']\n",
    "# included_epoch_names = ['sprinkle']\n",
    "\n",
    "for a_decoder_name in included_epoch_names:\n",
    "    a_decoder = masked_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "    a_decoded_result = masked_container.epochs_decoded_result_cache_dict[a_t_bin_size][a_decoder_name] # DecodedFilterEpochsResult\n",
    "    a_decoded_filter_epochs_df: pd.DataFrame = a_decoded_result.filter_epochs\n",
    "    decoder_epoch_flat_mask_future_past_result_dict[a_decoder_name] = masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']['_out_epoch_flat_mask_future_past_result']\n",
    "\n",
    "    _out_epoch_flat_mask_future_past_result: List[MatchingPastFuturePositionsResult] = decoder_epoch_flat_mask_future_past_result_dict[a_decoder_name]\n",
    "    ## updates: decoder_flat_matching_results_list_ds_dict\n",
    "    decoder_flat_matching_results_list_ds_dict[a_decoder_name] = MaskDataSource.init_from_list_of_MatchingPastFuturePositionsResult(epoch_flat_mask_future_past_result=decoder_epoch_flat_mask_future_past_result_dict[a_decoder_name],\n",
    "                                                                                                                        filter_epochs=a_decoded_filter_epochs_df, \n",
    "                                                                                                                        xbin=a_decoder.xbin, ybin=a_decoder.ybin, xbin_centers=a_decoder.xbin_centers, ybin_centers=a_decoder.ybin_centers,\n",
    "                                                                                                                        curr_position_df=masked_container.decoding_locality.pos_df,\n",
    "                                                                                                                     )\n",
    "\n",
    "## OUTPUTS: decoder_epoch_flat_mask_future_past_result_dict, decoder_flat_matching_results_list_ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('k', masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis'], max_depth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b196791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "## INPUTS: masked_container\n",
    "# \"W:\\Data\\Bapun\\RatN\\Day4OpenField\\output\\2026-02-02_PredictiveDecodingComputationsContainer_masked_split\"\n",
    "\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-04_PredictiveDecodingComputationsContainer_masked_split') ## includes most of 'sprinkle' in debug dict as well\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-05_PredictiveDecodingComputationsContainer_masked_split') ## includes most of 'sprinkle' in debug dict as well\n",
    "# pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-06_PredictiveDecodingComputationsContainer_masked_split') ## includes ALL of 'sprinkle' in debug dict as well\n",
    "pkl_split_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-02-10_PredictiveDecodingComputationsContainer_masked_split') ## includes ALL of 'sprinkle' in debug dict as well\n",
    "pkl_split_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "pkl_output_path: Path = pkl_split_output_path.joinpath(f'decoder_flat_matching_results_list_ds_dict').resolve()\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "saveData(pkl_output_path, decoder_flat_matching_results_list_ds_dict, should_append=False, safe_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19928a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_path: Path = pkl_split_output_path.joinpath(f'decoder_epoch_flat_mask_future_past_result_dict').resolve()\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "saveData(pkl_output_path, decoder_epoch_flat_mask_future_past_result_dict, should_append=False, safe_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb7bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_3d\n",
    "## single_combined_plot == True mode (mode 1.):\n",
    "plotter, laps_pages = plot_lap_trajectories_3d(curr_active_pipeline.sess, single_combined_plot=True)\n",
    "plotter.show()\n",
    "\n",
    "## single_combined_plot == False mode (mode 2.):        \n",
    "plotter, laps_pages = plot_lap_trajectories_3d(curr_active_pipeline.sess, single_combined_plot=False, curr_num_subplots=len(curr_active_pipeline.sess.laps.lap_id), active_page_index=1)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f49985",
   "metadata": {},
   "source": [
    "## Compare 'sprinkle' vs. 'roam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ae7c0",
   "metadata": {
    "tags": [
     "2026-02-05_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoder_name = 'sprinkle'\n",
    "a_ds: MaskDataSource = decoder_flat_matching_results_list_ds_dict[a_decoder_name]\n",
    "a_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae3532",
   "metadata": {
    "tags": [
     "2026-02-05_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "a_ds.num_epochs\n",
    "a_ds.num_epoch_time_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8427f",
   "metadata": {
    "tags": [
     "2026-02-05_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# decoded_occupancy = np.nansum([np.nansum(v, axis=-1) for v in a_ds.p_x_given_n_list], axis=0) # (n_epochs, n_x, n_y)\n",
    "# decoded_occupancy\n",
    "\n",
    "decoded_occupancy_dict = {a_decoder_name:np.nansum([np.nansum(v, axis=-1) for v in a_ds.p_x_given_n_list], axis=0) for a_decoder_name, a_ds in decoder_flat_matching_results_list_ds_dict.items()}\n",
    "decoded_occupancy_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6211726",
   "metadata": {
    "tags": [
     "2026-02-05_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "_an_out = {}\n",
    "for a_decoder_name, a_decoded_occupancy in decoded_occupancy_dict.items():\n",
    "    _an_out[a_decoder_name] = pg.ImageWindow(a_decoded_occupancy, title=f'{a_decoder_name}_decoded_occupancy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traja\n",
    "from traja.contrib import rdp\n",
    "from traja import TrajaCollection\n",
    "\n",
    "## build all data at once\n",
    "minimum_included_matching_sequence_length: int = 4\n",
    "\n",
    "trajCol1: TrajaCollection = None\n",
    "\n",
    "epoch_data_list_dict: Dict[str, List[Dict]] = {}\n",
    "# for a_decoder_name, a_decoded_occupancy in decoded_occupancy_dict.items():\n",
    "for a_decoder_name, a_ds in decoder_flat_matching_results_list_ds_dict.items():\n",
    "    n_epochs: int = a_ds.num_epochs\n",
    "    epoch_indicies = np.arange(n_epochs)\n",
    "    epoch_data_list: List[Dict] = [a_ds._prepare_epoch_data(an_epoch_idx=new_epoch_idx, minimum_included_matching_sequence_length=minimum_included_matching_sequence_length) for new_epoch_idx in epoch_indicies]\n",
    "    epoch_data_list_dict[a_decoder_name] = epoch_data_list\n",
    "    \n",
    "\n",
    "# epoch_data_list\n",
    "## OUTPUTS: epoch_data_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_matching_good_merged_segment_epochs_df_list = [v['curr_matching_good_merged_segment_epochs_df'] for i, v in enumerate(epoch_data_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c059a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_idx: int = 1\n",
    "curr_matching_good_merged_segment_epochs_df_list[test_epoch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a27d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25533478",
   "metadata": {},
   "source": [
    "# 2026-01-21 - ⚓💯🖌️ Main new plot `render_predictive_decoding_with_vispy` (`vispy`-based) display of the past/curr_epoch/future trajectories and masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c77c6",
   "metadata": {
    "tags": [
     "2026-01-21_postcomputefilter",
     "2026-01-21_plot_result",
     "2026-02-02_resume_after_break"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingVispyWidget, render_predictive_decoding_with_vispy\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.Testing.StackedDynamicTablesWidget import TableManager\n",
    "\n",
    "# matplotlib_configuration_update(is_interactive=True, backend='qt5'\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "## INPUTS: _out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df, container\n",
    "a_decoder_name: str = 'roam'\n",
    "# _out_epoch_flat_mask_future_past_result = decoder_flat_matching_results_list_ds_dict[a_decoder_name]\n",
    "viewer: PredictiveDecodingVispyWidget = render_predictive_decoding_with_vispy(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df=a_decoded_filter_epochs_df,\n",
    "                                                 curr_position_df = container.decoding_locality.pos_df,  ## container, nmot masked container\n",
    "                                                 pf_decoder = a_decoder, decoded_result = a_decoded_result, \n",
    "                                                 show_full_position_background = False, current_traj_seconds_pre_post_extension = 0.0, \n",
    "                                                #  past_future_trajectory_extension_seconds=0.750, \n",
    "                                                 past_future_trajectory_extension_seconds={'start': 0.25, 'end': 0.5}, start_end_extension_max_opacity=0.4, \n",
    "                                                 require_angle_match=False, color_matches_by_matching_angle=False,\n",
    "                                                #  require_angle_match=True, color_matches_by_matching_angle=True,\n",
    "                                                #  enable_debug_plot_trajectory_average_angle_arrows=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56481658",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.dock_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720149ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MatchingPastFuturePositionsResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import SingleEpochDecodedResult, DecodedFilterEpochsResult\n",
    "\n",
    "# _test_epoch_idx: int = 3\n",
    "_test_epoch_idx: int = 4\n",
    "_test_epoch_result = MatchingPastFuturePositionsResult._reload_class(_out_epoch_flat_mask_future_past_result[_test_epoch_idx])\n",
    "# _test_epoch_result.merged_segment_epochs\n",
    "# _test_epoch_result.recompute_all()\n",
    "\n",
    "# _test_epoch_result.matching_past_positions_df\n",
    "# _test_epoch_result.matching_future_positions_df\n",
    "relevant_positions_df: pd.DataFrame = deepcopy(_test_epoch_result.relevant_positions_df)\n",
    "matching_pos_epochs_df: pd.DataFrame = deepcopy(_test_epoch_result.matching_pos_epochs_df)\n",
    "merged_segment_epochs: pd.DataFrame = deepcopy(_test_epoch_result.merged_segment_epochs)\n",
    "\n",
    "relevant_positions_df\n",
    "matching_pos_epochs_df\n",
    "merged_segment_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_num_found_epoch_t_bins: int = 3\n",
    "\n",
    "## INPUTS: merged_segment_epochs, relevant_positions_df, matching_pos_epochs_df\n",
    "\n",
    "good_merged_segment_epochs: pd.DataFrame = merged_segment_epochs[merged_segment_epochs['num_epoch_t_bins'] >= min_num_found_epoch_t_bins]\n",
    "good_merged_segment_epochs\n",
    "\n",
    "\n",
    "# relevant_positions_df[relevant_positions_df['matching_found_relevant_merged_pos_epoch'] > -1]\n",
    "\n",
    "good_only_relevant_positions_df: pd.DataFrame = relevant_positions_df[np.logical_and(relevant_positions_df['matching_found_relevant_merged_pos_epoch'].isin(good_merged_segment_epochs['label']), (relevant_positions_df['matching_found_relevant_pos_epoch'] > -1))]\n",
    "good_only_relevant_positions_df\n",
    "\n",
    "\n",
    "good_only_included_epoch_labels: NDArray = np.unique(good_only_relevant_positions_df['matching_found_relevant_pos_epoch'].to_numpy())\n",
    "# good_only_included_epoch_labels\n",
    "## INPUTS: matching_pos_epochs_df\n",
    "good_only_matching_pos_epochs_df = deepcopy(matching_pos_epochs_df)[matching_pos_epochs_df['label'].isin(good_only_included_epoch_labels)]\n",
    "good_only_matching_pos_epochs_df\n",
    "\n",
    "## OUTPUTS: good_merged_segment_epochs, good_only_relevant_positions_df, good_only_matching_pos_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.indexing_helpers import flatten\n",
    "## INPUTS: good_merged_segment_epochs\n",
    "good_merged_included_epoch_t_idxs_list: List[List[int]] = good_merged_segment_epochs['epoch_t_idx'].map(lambda x: [int(v) for v in x.split('+')])\n",
    "good_merged_included_epoch_t_idxs_list\n",
    "total_good_merged_epochs: int = np.sum([len(v) for v in good_merged_included_epoch_t_idxs_list])\n",
    "total_good_merged_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(flatten(good_merged_included_epoch_t_idxs_list)) # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88a928",
   "metadata": {},
   "source": [
    "### 2026-01-23 - Good merged paths FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import EpochsAccessor, Epoch\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MatchingPastFuturePositionsResult\n",
    "\n",
    "## INPUTS: _test_epoch_result\n",
    "# matching_relevant_positions_df: pd.DataFrame = _test_epoch_result.filter_positions_to_epoch_mask_included_bins(a_pos_df=_test_epoch_result.relevant_positions_df.copy())\n",
    "# matching_past_positions_df: pd.DataFrame = _test_epoch_result.filter_positions_to_epoch_mask_included_bins(a_pos_df=_test_epoch_result.matching_past_positions_df.copy())\n",
    "# matching_future_positions_df: pd.DataFrame = _test_epoch_result.filter_positions_to_epoch_mask_included_bins(a_pos_df=_test_epoch_result.matching_future_positions_df.copy())\n",
    "\n",
    "merged_found_pos_epoch_id_key_name: str = 'matching_found_relevant_merged_pos_epoch'\n",
    "\n",
    "## OUTPUTS: matching_relevant_positions_df\n",
    "merged_segment_epochs, relevant_merged_positions_df = _test_epoch_result.compute_compilete_paths(max_allowed_trajectory_gap_seconds=2.5, merged_found_pos_epoch_id_key_name=merged_found_pos_epoch_id_key_name)\n",
    "## OUTPUTS: merged_segment_epochs, relevant_merged_positions_df\n",
    " \n",
    "merged_segment_epochs\n",
    "relevant_merged_positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_epoch_result.matching_pos_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assign sequence_id back to positions for partitioning\n",
    "# a_curr_matching_positions_df = df.copy()\n",
    "# a_curr_matching_positions_df = a_curr_matching_positions_df.time_point_event.adding_epochs_identity_column(epochs_df=new_pos_epochs, epoch_id_key_name=col_name, override_time_variable_name='t', epoch_label_column_name='label', no_interval_fill_value=-1, should_replace_existing_column=True, drop_non_epoch_events=True, overlap_behavior=OverlappingIntervalsFallbackBehavior.FALLBACK_TO_SLOW_SEARCH)\n",
    "\n",
    "# # Partition first, then segment each epoch separately so each trajectory gets its own segment_Vp_deg\n",
    "# curr_matching_positions_df_dict: Dict[types.epoch_index, pd.DataFrame] = a_curr_matching_positions_df.pho.partition_df_dict(col_name)\n",
    "\n",
    "# ## Segment trajectories per-position-trajectory-epoch (so each trajectory gets its own representative direction angle)\n",
    "# for epoch_idx, epoch_pos_df in curr_matching_positions_df_dict.items():\n",
    "#     curr_matching_positions_df_dict[epoch_idx] = epoch_pos_df.position.adding_segmented_trajectories_columns(disable_segmentation=disable_segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "## want to add merged_found_pos_epoch_id_key_name to the original path df so I can sort those too from the visualization\n",
    "# merged_found_pos_epoch_id_key_name\n",
    "min_num_spanning_bins: int = 3\n",
    "\n",
    "long_merged_segment_epochs: pd.DataFrame = merged_segment_epochs[(merged_segment_epochs['num_epoch_t_bins'] > min_num_spanning_bins)]\n",
    "long_merged_segment_epochs\n",
    "long_only_relevant_merged_positions_df: pd.DataFrame = relevant_merged_positions_df[np.isin(relevant_merged_positions_df[merged_found_pos_epoch_id_key_name], long_merged_segment_epochs['label'])]\n",
    "long_only_relevant_merged_positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21105034",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_merged_segment_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_epoch_flat_mask_future_past_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008974b",
   "metadata": {
    "tags": [
     "2026-02-02_resume_after_break"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingVispyWidget, render_predictive_decoding_with_vispy\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.Testing.StackedDynamicTablesWidget import TableManager\n",
    "\n",
    "long_found_paths_only_viewer: PredictiveDecodingVispyWidget = render_predictive_decoding_with_vispy(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df=a_decoded_filter_epochs_df,\n",
    "                                                 curr_position_df = masked_container.decoding_locality.pos_df, \n",
    "                                                 pf_decoder = a_decoder, decoded_result = a_decoded_result, \n",
    "                                                 show_full_position_background = False, current_traj_seconds_pre_post_extension = 0.0, \n",
    "                                                #  past_future_trajectory_extension_seconds=0.750, \n",
    "                                                 past_future_trajectory_extension_seconds={'start': 0.25, 'end': 0.5}, start_end_extension_max_opacity=0.4, \n",
    "                                                 require_angle_match=False, color_matches_by_matching_angle=False,\n",
    "                                                #  require_angle_match=True, color_matches_by_matching_angle=True,\n",
    "                                                #  enable_debug_plot_trajectory_average_angle_arrows=True,\n",
    "                                                minimum_included_matching_sequence_length = 4, ## this is what makes it used the filtered info\n",
    "                                                color_matches_by_merged_epoch_t_bin_idx=False,\n",
    "                                                enable_table_widgets=True,\n",
    "                                                active_epoch_idx=4,\n",
    "                                                enable_multi_epoch_overview_display_mode = False,\n",
    "                                                # enable_multi_epoch_overview_display_mode = True,\n",
    ")\n",
    "long_found_paths_only_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb0707",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_epoch_flat_mask_future_past_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: long_found_paths_only_viewer\n",
    "exported_middle_pane_files = long_found_paths_only_viewer.export_vispy_viewer_epochs(export_folder='./exports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingVispyWidget, render_predictive_decoding_with_vispy\n",
    "\n",
    "long_found_paths_only_batch_overview_viewer: PredictiveDecodingVispyWidget = render_predictive_decoding_with_vispy(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df=a_decoded_filter_epochs_df,\n",
    "                                                 curr_position_df = masked_container.decoding_locality.pos_df, \n",
    "                                                 pf_decoder = a_decoder, decoded_result = a_decoded_result, \n",
    "                                                 show_full_position_background = False, current_traj_seconds_pre_post_extension = 0.0, \n",
    "                                                #  past_future_trajectory_extension_seconds=0.750, \n",
    "                                                 past_future_trajectory_extension_seconds={'start': 0.25, 'end': 0.5}, start_end_extension_max_opacity=0.4, \n",
    "                                                 require_angle_match=False, color_matches_by_matching_angle=False,\n",
    "                                                #  require_angle_match=True, color_matches_by_matching_angle=True,\n",
    "                                                #  enable_debug_plot_trajectory_average_angle_arrows=True,\n",
    "                                                minimum_included_matching_sequence_length = 4, ## this is what makes it used the filtered info\n",
    "                                                color_matches_by_merged_epoch_t_bin_idx=False,\n",
    "                                                enable_table_widgets=True,\n",
    "                                                active_epoch_idx=0, enable_multi_epoch_overview_display_mode = False, enable_full_vispy_debug_mode=False,\n",
    "                                                # active_epoch_idx=None, enable_multi_epoch_overview_display_mode = True, MAX_NUM_OVERVIEW_EPOCHS_TO_RENDER=4, enable_full_vispy_debug_mode=False,\n",
    ")\n",
    "long_found_paths_only_batch_overview_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd202eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingVispyWidget, render_predictive_decoding_with_vispy\n",
    "\n",
    "long_found_paths_only_batch_overview_viewer: PredictiveDecodingVispyWidget = render_predictive_decoding_with_vispy(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df=a_decoded_filter_epochs_df,\n",
    "                                                 curr_position_df = masked_container.decoding_locality.pos_df, \n",
    "                                                 pf_decoder = a_decoder, decoded_result = a_decoded_result, \n",
    "                                                 show_full_position_background = False, current_traj_seconds_pre_post_extension = 0.0, \n",
    "                                                #  past_future_trajectory_extension_seconds=0.750, \n",
    "                                                 past_future_trajectory_extension_seconds={'start': 0.25, 'end': 0.5}, start_end_extension_max_opacity=0.4, \n",
    "                                                 require_angle_match=False, color_matches_by_matching_angle=False,\n",
    "                                                #  require_angle_match=True, color_matches_by_matching_angle=True,\n",
    "                                                #  enable_debug_plot_trajectory_average_angle_arrows=True,\n",
    "                                                minimum_included_matching_sequence_length = 4, ## this is what makes it used the filtered info\n",
    "                                                color_matches_by_merged_epoch_t_bin_idx=False,\n",
    "                                                enable_table_widgets=True,\n",
    "                                                active_epoch_idx=0, enable_multi_epoch_overview_display_mode = False, enable_full_vispy_debug_mode=False,\n",
    "                                                active_epoch_idx=None, enable_multi_epoch_overview_display_mode = True, MAX_NUM_OVERVIEW_EPOCHS_TO_RENDER=4, enable_full_vispy_debug_mode=True,\n",
    ")\n",
    "long_found_paths_only_batch_overview_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_epoch_idx: int = 3\n",
    "# a_multi_epoch_overview_container = long_found_paths_only_batch_overview_viewer.multi_epoch_overview_container_render_dict_list[an_epoch_idx]\n",
    "a_multi_epoch_overview_container = long_found_paths_only_viewer.multi_epoch_overview_container_render_dict_list[an_epoch_idx]\n",
    "\n",
    "an_update_dict = a_multi_epoch_overview_container['an_update_dict']\n",
    "\n",
    "posterior_mask_contours = an_update_dict['posterior_mask_contours']\n",
    "posterior_mask_contours\n",
    "# [<Line at 0x27a02ed3340>, <Line at 0x27a02ed33d0>, <Line at 0x27a02f0ffd0>, <Line at 0x27a03139970>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cded22",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mask_contours = long_found_paths_only_viewer.posterior_mask_contours\n",
    "posterior_mask_contours\n",
    "# [<Line at 0x2860ee1ac70>,\n",
    "#  <Line at 0x2860ee52520>,\n",
    "#  <Line at 0x2860ee7afd0>,\n",
    "#  <Line at 0x2860eea1c70>,\n",
    "#  <Line at 0x2860eed9520>,\n",
    "#  <Line at 0x2860eefffd0>,\n",
    "#  <Line at 0x28612f96130>,\n",
    "#  <Line at 0x28612f96df0>,\n",
    "#  <Line at 0x28612faeee0>,\n",
    "#  <Line at 0x2861300f430>,\n",
    "#  <Line at 0x28613038c70>,\n",
    "#  <Line at 0x2861305efd0>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pygwalker as pyg\n",
    "\n",
    "_out_epoch_flat_mask_future_past_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be111e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "walker = pyg.walk(a_decoded_filter_epochs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mask_contours[0].parent_chain()\n",
    "# [<Line at 0x27a02ed3340>,\n",
    "#  <SubScene at 0x27a022c1250>,\n",
    "#  <ViewBox at 0x27a022e2760>,\n",
    "#  <Grid at 0x279fffb9610:\n",
    "#  [[1 2 2 2 2]]>,\n",
    "#  <Grid at 0x279fd369c10:\n",
    "#  [[1 1 1 1 1 0 0]\n",
    "#   [2 2 2 2 2 0 0]\n",
    "#   [3 3 3 3 3 0 0]\n",
    "#   [4 4 4 4 4 0 0]\n",
    "#   [5 5 5 5 5 5 5]]>,\n",
    "#  <Widget at 0x279fd64e550>,\n",
    "#  <SubScene at 0x279d08e1130>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b8e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Loop through and clear all children\n",
    "while len(view.scene.children) > 0:\n",
    "    view.scene.children[0].parent = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69592883",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer._clear_epoch_visuals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer.canvas.update()\n",
    "long_found_paths_only_viewer.canvas.native.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf52793",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer._apply_trajectory_highlight_for_selected_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer.past_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer.update_epoch_display(new_epoch_idx=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ffc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer.canvas.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs: int = long_found_paths_only_viewer.a_flat_matching_results_list_ds.num_epochs\n",
    "epoch_indicies = np.arange(n_epochs)\n",
    "epoch_data_list: List[Dict] = [long_found_paths_only_viewer.a_flat_matching_results_list_ds._prepare_epoch_data(an_epoch_idx=new_epoch_idx, minimum_included_matching_sequence_length=long_found_paths_only_viewer.minimum_included_matching_sequence_length) for new_epoch_idx in epoch_indicies]\n",
    "epoch_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4055dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(long_found_paths_only_viewer.grid)\n",
    "type(long_found_paths_only_viewer.time_bin_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vispy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d59a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_grid: vispy.scene.widgets.grid.Grid = long_found_paths_only_viewer.time_bin_grid\n",
    "time_bin_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0945bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in list(time_bin_grid.children):\n",
    "    time_bin_grid.remove_widget(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e82fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_found_paths_only_viewer.combined_timeline_view\n",
    "all_views = [long_found_paths_only_viewer.past_view, long_found_paths_only_viewer.future_view, long_found_paths_only_viewer.posterior_2d_view, long_found_paths_only_viewer.combined_timeline_view, long_found_paths_only_viewer.colorbar_view]\n",
    "all_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_view in all_views:\n",
    "    # Method 1: Loop through and clear all children\n",
    "    while len(a_view.scene.children) > 0:\n",
    "        a_view.scene.children[0].parent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b357516",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer.update_epoch_display(new_epoch_idx=7)\n",
    "# long_found_paths_only_viewer.update_epoch_display(new_epoch_idx=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e13488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca717fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_epoch_idx: int = 4\n",
    "epoch_data = long_found_paths_only_viewer.a_flat_matching_results_list_ds._prepare_epoch_data(an_epoch_idx=new_epoch_idx, minimum_included_matching_sequence_length=long_found_paths_only_viewer.minimum_included_matching_sequence_length)\n",
    "print(f'epoch_data: {list(epoch_data.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_matching_past_future_positions_df_dict = {k: v for k, v in epoch_data['curr_matching_past_future_positions_df_dict'].items()}\n",
    "curr_matching_good_merged_segment_epochs_df: pd.DataFrame = epoch_data.get('curr_matching_good_merged_segment_epochs_df', None)\n",
    "curr_matching_epochs_df: pd.DataFrame = epoch_data.get('curr_matching_epochs_df', None)\n",
    "curr_matching_epochs_df_dict: pd.DataFrame = epoch_data.get('curr_matching_epochs_df_dict', None) \n",
    "assert curr_matching_good_merged_segment_epochs_df is not None\n",
    "## use this \n",
    "curr_matching_good_merged_segment_epochs_df = curr_matching_good_merged_segment_epochs_df.reset_index(drop=True, inplace=False)\n",
    "num_good_epochs: int = len(curr_matching_good_merged_segment_epochs_df)\n",
    "print(f'curr_matching_good_merged_segment_epochs_df - num_good_epochs: {num_good_epochs}')\n",
    "\n",
    "num_good_epochs_past_future = {k:len(v) for k, v in curr_matching_past_future_positions_df_dict.items()}\n",
    "num_total_good_epochs_past_future: int = np.sum(list(num_good_epochs_past_future.values()))\n",
    "print(f'\\tnum_good_epochs_past_future: {num_good_epochs_past_future},\\n\\tnum_total_good_epochs_past_future: {num_total_good_epochs_past_future}')            \n",
    "assert num_good_epochs == num_total_good_epochs_past_future, f'num_total_good_epochs_past_future: {num_total_good_epochs_past_future} != num_good_epochs: {num_good_epochs}'\n",
    "\n",
    "curr_matching_good_merged_segment_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50cd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look up their `pre_merged_epoch_label`\n",
    "\n",
    "a_pre_merged_epoch_labels_list: List[NDArray] = curr_matching_good_merged_segment_epochs_df['pre_merged_epoch_label'].map(lambda x: np.array([int(v) for v in x.split('+')]))\n",
    "a_pre_merged_epoch_labels_list\n",
    "\n",
    "\n",
    "# curr_matching_epochs_df.to_dict(orient='dict')\n",
    "\n",
    "# curr_matching_epochs_df.pho.partition_df('label')\n",
    "\n",
    "pre_merged_epochs_properties_map: Dict = {int(a_row.label):dict(start=a_row.start, stop=a_row.stop, label=int(a_row.label), duration=a_row.duration, is_future_present_past=a_row.is_future_present_past) for a_row in curr_matching_epochs_df.itertuples(index=True)}\n",
    "\n",
    "# a_pre_merged_epoch_labels_info_dict_list = [[pre_merged_epochs_properties_map.get(vv, None) for vv in v if (pre_merged_epochs_properties_map.get(vv, None) is not None)] for v in a_pre_merged_epoch_labels_list]\n",
    "a_pre_merged_epoch_labels_info_df_list = [pd.DataFrame([pre_merged_epochs_properties_map.get(vv, None) for vv in v if (pre_merged_epochs_properties_map.get(vv, None) is not None)]) for v in a_pre_merged_epoch_labels_list]\n",
    "# a_pre_merged_epoch_labels_info_dict_list\n",
    "a_pre_merged_epoch_labels_info_df_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# curr_matching_epochs_df['label'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_matching_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34eba1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_name: str = 'matching_found_relevant_pos_epoch'\n",
    "\n",
    "for a_past_future_str, df_dict in curr_matching_past_future_positions_df_dict.items():\n",
    "    # len(df_dict)\n",
    "    # type(df_dict)\n",
    "    a_new_epoch_df_key = list(df_dict.keys())[new_epoch_idx]\n",
    "    print(f'a_new_epoch_df_key: {a_new_epoch_df_key}')\n",
    "    # df = df_list[new_epoch_idx]\n",
    "    df = df_dict[a_new_epoch_df_key]\n",
    "    df[key_name] = df[key_name].astype(int)\n",
    "    df\n",
    "    \n",
    "    # ['past'][new_epoch_idx]\n",
    "    ## find epoch/rel start/stop times\n",
    "    a_grouped_df: pd.DataFrame = df.groupby([key_name]).agg(t_min=('t', 'min'), t_max=('t', 'max'), t_nunique=('t', 'nunique')).reset_index()\n",
    "    a_grouped_df['duration'] = a_grouped_df['t_max'] - a_grouped_df['t_min']\n",
    "    a_grouped_df = a_grouped_df.set_index(key_name, drop=True, inplace=False)\n",
    "    a_grouped_df\n",
    "    \n",
    "    ## ['past_future_matching_pos_epoch_id'], ['past_future_matching_pos_epoch_id']\n",
    "    # key_name: str = 'past_future_matching_pos_epoch_id'\n",
    "    \n",
    "    \n",
    "    # df[key_name].map(lambda x: a_grouped_df[a_grouped_df['matching_found_relevant_pos_epoch'] == x])\n",
    "    df[key_name].astype(int).map(lambda x: a_grouped_df.loc[int(x)].to_dict())\n",
    "    \n",
    "    ## now need to convert these to relative to the segment\n",
    "    \n",
    "    \n",
    "# curr_matching_past_future_positions_df_dict['past'][new_epoch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_matching_good_merged_segment_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_found_paths_only_viewer.canvas.update()\n",
    "long_found_paths_only_viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb918a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_id_key_name: str = deepcopy(_test_epoch_result.epoch_id_key_name)\n",
    "print(f'epoch_id_key_name: \"{epoch_id_key_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import SingleEpochDecodedResult, DecodedFilterEpochsResult\n",
    "\n",
    "_test_epoch_result.decoded_epoch_result.nbins\n",
    "_test_epoch_result.decoded_epoch_result.epoch_info_tuple\n",
    "\n",
    "decoded_epoch_result: SingleEpochDecodedResult = _test_epoch_result.decoded_epoch_result\n",
    "decoded_epoch_result.time_window_centers\n",
    "\n",
    "n_t_bins: int = len(decoded_epoch_result.time_window_centers)\n",
    "n_t_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2125556",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(_test_epoch_result.epoch_t_bins_high_prob_pos_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(relevant_positions_df['segment_idx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9902921",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(_test_epoch_result.relevant_positions_df['segment_Vp_deg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all epochs at 2x resolution\n",
    "exported_middle_pane_files = export_vispy_viewer_epochs(viewer, export_folder='./exports', resolution_scale=1.0)\n",
    "\n",
    "# Or export specific epochs at 3x resolution\n",
    "# exported_files = export_vispy_viewer_epochs(viewer, export_folder='./exports', resolution_scale=3.0, epoch_indices=[0, 5, 10, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg.ImageWindow(a_high_prob_mask_dt)\n",
    "\n",
    "pg.ImageWindow(np.any(a_high_prob_mask_dt, axis=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d165f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = pg.ImageWindow(a_p_x_given_n)\n",
    "_out_masked = pg.ImageWindow(a_high_prob_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iso_item: IsocurveItem = IsocurveItem(data=a_p_x_given_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3fb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iso_item.setParentItem(an_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab18504",
   "metadata": {},
   "source": [
    "### Testing masked epochs via `Epoch3DSceneTimeBinViewer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.Silx.EpochTimeBinViewerWidget import EpochTimeBinViewer, Epoch3DSceneTimeBinViewer\n",
    "from pyphoplacecellanalysis.GUI.Silx.EpochTimeBinViewerWidget import TextDataProviderDatasource\n",
    "\n",
    "a_text_data_provider: TextDataProviderDatasource = TextDataProviderDatasource(a_df=deepcopy(filtered_flat_peaks_df).rename(columns={'time_bin_idx': 't_bin_idx'}, inplace=False), \n",
    "    text_columns=['summit_slice_percent_area', 'peak_prominence', 'summit_slice_x_width', 'summit_slice_y_width'],  # Columns to display\n",
    ")\n",
    "\n",
    "\n",
    "# viewer = EpochTimeBinViewer(\n",
    "viewer = Epoch3DSceneTimeBinViewer(\n",
    "    decoded_result=a_decoded_result,\n",
    "    xbin_centers=a_decoder.xbin_centers,\n",
    "    ybin_centers=a_decoder.ybin_centers,\n",
    "    text_data_provider=a_text_data_provider,\n",
    ")\n",
    "viewer.setWindowTitle('viewer with text')\n",
    "viewer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_item = viewer.time_bin_items[2] # List[Scatter2D]\n",
    "an_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_item.getLabel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_item.setLabel('test NEW LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REORDER with promenence order\n",
    "\n",
    "# Define your desired epoch order (e.g., [2, 0, 1, 3] to reorder)\n",
    "desired_epoch_order = filtered_mean_percent_area_sorted_epochs_df['epoch_idx'].to_numpy() # [2, 0, 1, 3]  # Your custom order\n",
    "\n",
    "# Create a reordered copy of decoded_result\n",
    "import copy\n",
    "reordered_result = copy.deepcopy(a_decoded_result)\n",
    "\n",
    "# Reorder p_x_given_n_list\n",
    "reordered_result.p_x_given_n_list = [a_decoded_result.p_x_given_n_list[i] for i in desired_epoch_order]\n",
    "\n",
    "# Also reorder other epoch-specific lists if they exist\n",
    "if hasattr(a_decoded_result, 'time_bin_containers') and a_decoded_result.time_bin_containers is not None:\n",
    "    reordered_result.time_bin_containers = [a_decoded_result.time_bin_containers[i] for i in desired_epoch_order]\n",
    "\n",
    "# Now use the reordered result\n",
    "prom_ordered_viewer: Epoch3DSceneTimeBinViewer = Epoch3DSceneTimeBinViewer(decoded_result=reordered_result,\n",
    "                                    xbin_centers=a_decoder.xbin_centers,\n",
    "                                    ybin_centers=a_decoder.ybin_centers,\n",
    "                                    locality_measures_df=measures_df,  # DataFrame with 'start' and 'stop' columns\n",
    "                                    text_columns=text_columns,  # Columns to display\n",
    "    \n",
    ")\n",
    "prom_ordered_viewer.setWindowTitle('promenence-sort ordered Epochs')\n",
    "prom_ordered_viewer.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## INPUTS: a_result_posterior_peaks\n",
    "# # a_result_posterior_peaks is your PosteriorPeaksPeakProminence2dResult\n",
    "# _out_countours_overlay_items = viewer.add_peak_contours_overlays(peak_prominence_result=a_result_posterior_peaks,\n",
    "#                                   edge_color='#ffaaff78',\n",
    "#                                   line_width=1.0,\n",
    "#                                   z_offset=0.01)\n",
    "# _out_countours_overlay_items\n",
    "\n",
    "## INPUTS: prom_ordered_viewer\n",
    "\n",
    "import copy\n",
    "from pyphoplacecellanalysis.External.peak_prominence2d import PosteriorPeaksPeakProminence2dResult\n",
    "\n",
    "## INPUTS: desired_epoch_order\n",
    "# Your desired epoch order (same as what you used for decoded_result)\n",
    "# desired_epoch_order = [2, 0, 1, 3]  # Your custom order\n",
    "\n",
    "# Create a reordered copy of the peak prominence result\n",
    "reordered_peak_result = copy.deepcopy(decoded_epoch_t_bins_promenence_result_obj)\n",
    "\n",
    "# Create a mapping from old epoch_idx to new epoch_idx\n",
    "# desired_epoch_order[i] is the old epoch_idx that should appear at position i\n",
    "old_to_new_epoch_map = {old_idx: new_idx for new_idx, old_idx in enumerate(desired_epoch_order)}\n",
    "\n",
    "# Remap the results dictionary keys from (old_epoch_idx, t_bin_idx) to (new_epoch_idx, t_bin_idx)\n",
    "reordered_results = {}\n",
    "for (old_epoch_idx, t_bin_idx), value in reordered_peak_result.results.items():\n",
    "    if old_epoch_idx in old_to_new_epoch_map:\n",
    "        new_epoch_idx = old_to_new_epoch_map[old_epoch_idx]\n",
    "        reordered_results[(new_epoch_idx, t_bin_idx)] = value\n",
    "\n",
    "reordered_peak_result.results = reordered_results\n",
    "\n",
    "# Remap epoch_idx column in the dataframes\n",
    "if 'epoch_idx' in reordered_peak_result.flat_peaks_df.columns:\n",
    "    reordered_peak_result.flat_peaks_df = reordered_peak_result.flat_peaks_df.copy()\n",
    "    reordered_peak_result.flat_peaks_df['epoch_idx'] = reordered_peak_result.flat_peaks_df['epoch_idx'].map(old_to_new_epoch_map)\n",
    "\n",
    "if 'epoch_idx' in reordered_peak_result.filtered_flat_peaks_df.columns:\n",
    "    reordered_peak_result.filtered_flat_peaks_df = reordered_peak_result.filtered_flat_peaks_df.copy()\n",
    "    reordered_peak_result.filtered_flat_peaks_df['epoch_idx'] = reordered_peak_result.filtered_flat_peaks_df['epoch_idx'].map(old_to_new_epoch_map)\n",
    "\n",
    "# Now use the reordered peak result\n",
    "_prom_ordered_out_countours_overlay_items = prom_ordered_viewer.add_peak_contours_overlays(peak_prominence_result=reordered_peak_result,\n",
    "                                  edge_color='#ffaaff78',\n",
    "                                  line_width=1.0,\n",
    "                                  z_offset=0.01)\n",
    "_prom_ordered_out_countours_overlay_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2dbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a894f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the total possible area (enviornment area)\n",
    "environment_possible_size = (np.ptp(decoded_epoch_t_bins_promenence_result_obj.xx), np.ptp(decoded_epoch_t_bins_promenence_result_obj.yy))\n",
    "environment_possible_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_possible_area: float = np.product(environment_possible_size) ## cm^2\n",
    "environment_possible_area\n",
    "\n",
    "decoded_epoch_t_bins_promenence_result_obj.flat_peaks_df['summit_slice_percent_area'] = decoded_epoch_t_bins_promenence_result_obj.flat_peaks_df['summit_slice_area'] / environment_possible_area\n",
    "decoded_epoch_t_bins_promenence_result_obj.flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f35408",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_epoch_t_bins_promenence_result_obj.flat_peaks_df\n",
    "# decoded_epoch_t_bins_promenence_result_obj.filtered_flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e41e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to Pickle file\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2026-01_06_old_prom_2d_result.pkl')\n",
    "saveData(pkl_output_path, (old_prom_2d_result, custom_results_df_list, custom_results_df_list, decoded_local_epochs_result))\n",
    "# old_prom_2d_result.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-24_pred_decoding_result.pkl')\n",
    "# pred_decoding_obj.save(pkl_output_path=pkl_output_path)\n",
    "# print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: container\n",
    "pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-25_container.pkl')\n",
    "container.save(pkl_output_path=pkl_output_path)\n",
    "print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea196d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pkl_output_path: Path = curr_active_pipeline.get_output_path().joinpath('2025-12-24_masked_container.pkl')\n",
    "# masked_container.save(pkl_output_path=pkl_output_path)\n",
    "# print(f'pkl_output_path: \"{pkl_output_path.as_posix()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dedd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_prom_2d_result.filtered_flat_peaks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e636cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_flat_peaks_df = old_prom_2d_result.filtered_flat_peaks_df\n",
    "old_prom_2d_result.flat_peaks_df # 1344\n",
    "# old_prom_2d_result.peak_counts.raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_computation_results_dict\n",
    "custom_computation_results_df_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b63c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_track = decoding_locality.add_non_local_PBE_non_moving_epochs_to_intervals_timeline(active_2d_plot=active_2d_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a28e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.debug_computed_dict[an_epoch_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_high_prob_pos_masks = container.debug_computed_dict[an_epoch_name]['epoch_high_prob_pos_masks']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b39ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, PredictiveDecodingComputationsContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "# _obj: PredictiveDecoding = PredictiveDecoding._reload_class(_obj)\n",
    "\n",
    "container = PredictiveDecodingComputationsContainer._reload_class(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6107544",
   "metadata": {
    "tags": [
     "2026-01-14_new-reasonable-future-and-past"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingDisplayWidget\n",
    "\n",
    "# First, compute the results (if not already done)\n",
    "_out_tuple = container.compute_future_and_past_analysis(curr_active_pipeline, an_epoch_name='roam', decoding_time_bin_size=0.025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_high_prob_pos_masks, epoch_t_bins_high_prob_pos_masks, epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list, _out_processed_items_list_dict = _out_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_epoch_name = 'roam'\n",
    "curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'].debug_computed_dict[an_epoch_name] = {'epoch_high_prob_pos_masks': epoch_high_prob_pos_masks, 'epoch_matching_positions': epoch_matching_positions, 'past_future_info_dict': past_future_info_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_high_prob_pos_masks, epoch_t_bins_high_prob_pos_masks, epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list = container.compute_future_and_past_analysis(curr_active_pipeline, an_epoch_name='roam', decoding_time_bin_size=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd270e",
   "metadata": {
    "tags": [
     "2026-01-16_fixing_compute_memory"
    ]
   },
   "outputs": [],
   "source": [
    "include_includelist = None\n",
    "if include_includelist is None:\n",
    "    include_includelist = ['roam'] # , 'sprinkle'\n",
    "epoch_names: List[str] = include_includelist \n",
    "print(f'\\t processing will occur for epoch_names: {epoch_names}')\n",
    "for an_epoch_name in epoch_names:    \n",
    "    try:\n",
    "        print(f'\\ttrying `.compute_future_and_past_analysis(...)` for an_epoch_name: \"{an_epoch_name}\"...')\n",
    "        curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'].debug_computed_dict[an_epoch_name] = {}\n",
    "        _out = curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'].compute_future_and_past_analysis(curr_active_pipeline, an_epoch_name=an_epoch_name)\n",
    "        epoch_high_prob_pos_masks, epoch_t_bins_high_prob_pos_masks, epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list, processed_items_list_dict = _out\n",
    "        curr_active_pipeline.global_computation_results.computed_data['PredictiveDecoding'].debug_computed_dict[an_epoch_name] = {'epoch_high_prob_pos_masks': epoch_high_prob_pos_masks, 'epoch_matching_positions': epoch_matching_positions, 'past_future_info_dict': past_future_info_dict}\n",
    "    except (ValueError, AttributeError, IndexError, KeyError, TypeError) as e:\n",
    "        print(f'\\t\\tWARN: the last part of `perform_predictive_decoding_analysis(...) failed with error: {e}. Skipping.')\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create and display the widget\n",
    "a_widget = PredictiveDecodingDisplayWidget.init_from_container(container=container, decoding_time_bin_size=0.025, an_epoch_name='roam')\n",
    "a_widget  # Display the widget in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b44980",
   "metadata": {},
   "outputs": [],
   "source": [
    ", epoch_matching_positions, past_future_info_dict, matching_pos_dfs_list, matching_pos_epochs_dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingDisplayWidget\n",
    "\n",
    "# a_widget: PredictiveDecodingDisplayWidget = PredictiveDecodingDisplayWidget.init_from_container(container=container, decoding_time_bin_size=0.025, an_epoch_name='roam')\n",
    "a_widget: PredictiveDecodingDisplayWidget = PredictiveDecodingDisplayWidget.init_from_container(container=masked_container, decoding_time_bin_size=0.025, an_epoch_name='roam')\n",
    "a_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf46205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.peak_prominence2d import PosteriorPeaksPeakProminence2dResult\n",
    "\n",
    "slice_level_multipliers=[0.5, 0.9]\n",
    "mask_included_bins_list, summit_slice_levels_list, mask_included_p_x_given_n_list_dict, epoch_prom_t_bin_high_prob_pos_masks, epoch_prom_high_prob_pos_masks = decoded_epoch_t_bins_promenence_result_obj.compute_discrete_contour_masks(p_x_given_n_list=decoded_local_epochs_result.p_x_given_n_list, slice_level_multipliers=slice_level_multipliers)\n",
    "# mask_included_bins_list\n",
    "# summit_slice_levels_list\n",
    "epoch_prom_high_prob_pos_mask = epoch_prom_high_prob_pos_masks[0.9] ## high\n",
    "np.shape(epoch_prom_high_prob_pos_mask) # (74, 41, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de41375",
   "metadata": {},
   "source": [
    "# ⚓⛓️🟢 2025-01-08 - \"Position-like\" Posterior Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0859b",
   "metadata": {},
   "source": [
    "#### Test on actual posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa01222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(decoded_local_epochs_result.p_x_given_n_list)\n",
    "len(decoded_local_epochs_result.p_x_given_n_list)\n",
    "\n",
    "decoded_local_epochs_result.p_x_given_n_list # a List[NDArray]\n",
    "np.shape(decoded_local_epochs_result.p_x_given_n_list[0]) # (41, 63, 5)\n",
    "\n",
    "output_path = Path('2026-01-08_posteriors_data.npz').resolve()\n",
    "\n",
    "# Save compressed. \n",
    "# We use *data_list to unpack the list into separate arguments.\n",
    "# Numpy will automatically name them 'arr_0', 'arr_1', etc.\n",
    "np.savez_compressed(output_path, *decoded_local_epochs_result.p_x_given_n_list)\n",
    "\n",
    "print(f\"Successfully saved {len(decoded_local_epochs_result.p_x_given_n_list)} arrays to {output_path.as_posix()}\")\n",
    "\n",
    "\n",
    "# np.shape(decoded_local_epochs_result.p_x_given_n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_local_epochs_result.nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_local_epochs_result.nbins\n",
    "np.concatenate([v.centers for v in decoded_local_epochs_result.time_bin_containers])\n",
    "\n",
    "# decoded_local_epochs_result.nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_local_epochs_result.nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.indexing_helpers import flatten\n",
    "\n",
    "# (np.mean(np.diff(xbin_centers)), np.mean(np.diff(ybin_centers)))\n",
    "\n",
    "p_x_given_n_list: List[NDArray] = deepcopy(decoded_local_epochs_result.p_x_given_n_list) # a List[NDArray]\n",
    "# flat_p_x_given_n_list = flatten(p_x_given_n_list)\n",
    "\n",
    "# epoch_idx_list = [np.array([epoch_idx] * len(t_bin_values)) for epoch_idx, t_bin_values in enumerate(p_x_given_n_list)]\n",
    "epoch_idx_list = [np.array([epoch_idx] * a_n_bins) for epoch_idx, a_n_bins in enumerate(decoded_local_epochs_result.nbins)]\n",
    "epoch_idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## flatten all epochs across time bins\n",
    "flat_p_x_given_n_list = np.concatenate(p_x_given_n_list, axis=2) # (41, 63, 1508)\n",
    "\n",
    "np.shape(flat_p_x_given_n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import PositionLikePosteriorScoring\n",
    "\n",
    "## INPUTS: flat_p_x_given_n_list\n",
    "xbin = np.array([-85.7562, -80.9188, -76.0813, -71.2439, -66.4065, -61.569, -56.7316, -51.8942, -47.0568, -42.2193, -37.3819, -32.5445, -27.707, -22.8696, -18.0322, -13.1948, -8.35733, -3.5199, 1.31753, 6.15495, 10.9924, 15.8298, 20.6672, 25.5047, 30.3421, 35.1795, 40.017, 44.8544, 49.6918, 54.5292, 59.3667, 64.2041, 69.0415, 73.879, 78.7164, 83.5538, 88.3912, 93.2287, 98.0661, 102.904, 107.741, 112.578])\n",
    "ybin = np.array([-96.4477, -93.3514, -90.255, -87.1587, -84.0623, -80.966, -77.8697, -74.7733, -71.677, -68.5806, -65.4843, -62.3879, -59.2916, -56.1952, -53.0989, -50.0025, -46.9062, -43.8099, -40.7135, -37.6172, -34.5208, -31.4245, -28.3281, -25.2318, -22.1354, -19.0391, -15.9427, -12.8464, -9.75005, -6.6537, -3.55736, -0.46101, 2.63534, 5.73168, 8.82803, 11.9244, 15.0207, 18.1171, 21.2134, 24.3098, 27.4061, 30.5024, 33.5988, 36.6951, 39.7915, 42.8878, 45.9842, 49.0805, 52.1769, 55.2732, 58.3696, 61.4659, 64.5622, 67.6586, 70.7549, 73.8513, 76.9476, 80.044, 83.1403, 86.2367, 89.333, 92.4294, 95.5257, 98.6221])\n",
    "\n",
    "# Choose an epoch to visualize (e.g., the first one)\n",
    "scoring_results = PositionLikePosteriorScoring.compute_and_plot_posterior_stack(\n",
    "    flat_p_x_given_n_list,\n",
    "    x_edges=xbin,\n",
    "    y_edges=ybin, \n",
    "    should_plot_results=False, \n",
    ")\n",
    "\n",
    "scoring_results['t'] = np.concatenate([v.centers for v in decoded_local_epochs_result.time_bin_containers])\n",
    "scoring_results['epoch_idx'] = np.concatenate([np.array([epoch_idx] * a_n_bins) for epoch_idx, a_n_bins in enumerate(decoded_local_epochs_result.nbins)])\n",
    "\n",
    "scoring_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_position_like_list = []\n",
    "for epoch_idx, a_n_bins in enumerate(decoded_local_epochs_result.nbins):\n",
    "    curr_epoch_t_bins_is_position_like = scoring_results[scoring_results['epoch_idx'] == epoch_idx]['is_position_like'].to_numpy()\n",
    "    is_position_like_list.append(curr_epoch_t_bins_is_position_like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_position_like_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_idx: int = 7\n",
    "max_n_frames_to_plot = 60\n",
    "\n",
    "_ = PositionLikePosteriorScoring.compute_and_plot_posterior_stack(\n",
    "    flat_p_x_given_n_list[:, :, (max_n_frames_to_plot*page_idx):],\n",
    "    x_edges=xbin,\n",
    "    y_edges=ybin, \n",
    "    should_plot_results=True,\n",
    "    max_n_frames_to_plot = max_n_frames_to_plot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3127f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "## Finds the epochs containing a at least a minimum number of position-like bins and returns a filtered result object with only those.\n",
    "## INPUTS: scoring_results, decoded_local_epochs_result\n",
    "# 'num_position_like_t_bins'\n",
    "num_min_position_like_t_bins: int = 3\n",
    "epoch_overall_scoring_results_df = scoring_results.groupby(['epoch_idx']).agg(num_position_like_t_bins=('is_position_like', 'sum'), score_mean=('score', 'mean')).reset_index()\n",
    "is_epoch_idx_included = (epoch_overall_scoring_results_df['num_position_like_t_bins'] > num_min_position_like_t_bins)\n",
    "\n",
    "included_epoch_idxs = epoch_overall_scoring_results_df[is_epoch_idx_included]['epoch_idx'].to_numpy()\n",
    "included_epoch_idxs\n",
    "\n",
    "filtered_decoded_local_epochs_result: DecodedFilterEpochsResult = deepcopy(decoded_local_epochs_result).filtered_by_epochs(included_epoch_indicies=included_epoch_idxs)\n",
    "filtered_decoded_local_epochs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13373966",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_decoded_local_epochs_result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25023d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUTS: filtered_decoded_local_epochs_result\n",
    "\n",
    "## filter the epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923ca2e",
   "metadata": {},
   "source": [
    "# 2026-02-06 -- Compare 'sprinkle' and 'roam' - split lab sequences\n",
    "- [ ] display on matplotlib-style figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bceda9",
   "metadata": {
    "tags": [
     "2026-02-09_sprinklevsroam"
    ]
   },
   "outputs": [],
   "source": [
    "## 🖼️✅ Test Single Epoch/Axes `DecodedTrajectoryMatplotlibPlotter` with slider to choose epoch\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "hort_name=None, tags=['laps', 'segmentation'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2026-02-11 15:12', related_items=[])\n",
    "def compute_segmentations_and_decode_laps(curr_active_pipeline, active_container, epoch_names: List[types.DecoderName] = ['roam', 'sprinkle'], decoding_time_bin_size: float = 0.250, enable_plot: bool=True):\n",
    "    \"\"\" attempts to recompute the 2D-equivalent of 'laps' or 'sprints' where the animal is actually running. \n",
    "    \n",
    "    \"\"\"\n",
    "    from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.Mixins.LapsVisualizationMixin import LapsVisualizationMixin\n",
    "    \n",
    "\n",
    "    decoded_laps_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {}\n",
    "    _laps_out_dict: Dict[str, Dict[types.DecoderName, Any]] = {'laps_epoch_df': {}, 'curr_position_df': {}, 'epoch_specific_position_dfs': {}}\n",
    "    plot_trajectories_2d_kwargs = dict(should_include_trajectory_arrows=True, )\n",
    "    \n",
    "    for an_epoch_name in epoch_names:\n",
    "\n",
    "        # an_epoch_name: str = 'roam'\n",
    "        # global_session = deepcopy(curr_active_pipeline.sess)\n",
    "        # curr_session = deepcopy(curr_active_pipeline.sess)\n",
    "\n",
    "        # curr_session = deepcopy(curr_active_pipeline.sess)\n",
    "        curr_session = deepcopy(curr_active_pipeline.filtered_sessions[an_epoch_name])\n",
    "        curr_position_df, epoch_specific_position_dfs = LapsVisualizationMixin._compute_laps_specific_position_dfs(curr_session)\n",
    "\n",
    "        _laps_out_dict['curr_position_df'][an_epoch_name] = curr_position_df\n",
    "        _laps_out_dict['epoch_specific_position_dfs'][an_epoch_name] = epoch_specific_position_dfs\n",
    "        \n",
    "\n",
    "        a_decoder = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "        # a_result2D: DecodedFilterEpochsResult = decoded_local_epochs_result.frame_divided_epochs_results[an_epoch_name]\n",
    "        a_new_global_decoder2D = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "        # a_result2D = results2D.a_result2D\n",
    "        # a_new_global_decoder2D = results2D.a_new_global_decoder2D\n",
    "        ## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "        xbin = deepcopy(a_new_global_decoder2D.xbin)\n",
    "        xbin_centers = deepcopy(a_new_global_decoder2D.xbin_centers)\n",
    "        ybin_centers = deepcopy(a_new_global_decoder2D.ybin_centers)\n",
    "        ybin = deepcopy(a_new_global_decoder2D.ybin)\n",
    "        # num_filter_epochs: int = decoded_local_epochs_result.num_filter_epochs\n",
    "\n",
    "        ## get laps:\n",
    "        laps_obj: Laps = curr_session.laps\n",
    "        laps_epoch: Epoch = laps_obj.to_Epoch()\n",
    "        laps_epoch_df: pd.DataFrame = laps_epoch.to_dataframe()\n",
    "        ## Decode the laps\n",
    "        _laps_out_dict['laps_epoch_df'][an_epoch_name] = laps_epoch_df\n",
    "        \n",
    "        laps_decoded_result: DecodedFilterEpochsResult = a_decoder.decode_specific_epochs(spikes_df=curr_session.spikes_df, filter_epochs=laps_epoch_df, \n",
    "                                        decoding_time_bin_size=decoding_time_bin_size)\n",
    "        ## OUTPUTS: curr_position_df, laps_decoded_result\n",
    "        \n",
    "        if enable_plot:\n",
    "            _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "            curr_num_subplots: int = len(laps_epoch_df)\n",
    "            print(f'curr_num_subplots: {curr_num_subplots}')\n",
    "\n",
    "            max_num_columns: int = 10\n",
    "            a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=laps_decoded_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, rotate_to_vertical=True)\n",
    "            fig, axs, decoded_epochs_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(curr_position_df=curr_position_df, curr_num_subplots=curr_num_subplots, active_page_index=0, \n",
    "                                                                                                epoch_specific_position_dfs=epoch_specific_position_dfs, epoch_ids=None,\n",
    "                                                                                                plot_actual_lap_lines=True, use_theoretical_tracks_instead=False, fixed_columns=min(curr_num_subplots, max_num_columns), **plot_trajectories_2d_kwargs)\n",
    "\n",
    "            ax = axs[0][0]\n",
    "            ax.set_aspect('auto')  # Adjust automatically based on data limits\n",
    "            ax.set_adjustable('datalim')  # Ensure the aspect ratio respects the data limits\n",
    "            ax.autoscale()  # Autoscale the view to fit data\n",
    "            \n",
    "\n",
    "    ## END for an_epoch_name in epoch_names...\n",
    "    return decoded_laps_dict, _laps_out_dict\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# Perform Plotting                                                                                                                                                                                                                                                                     #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "decoding_time_bin_size = 0.025\n",
    "# decoding_time_bin_size = 0.250\n",
    "\n",
    "## INPUTS: container\n",
    "\n",
    "# active_container = _container_container.container\n",
    "active_container = _container_container.masked_container\n",
    "# decoded_laps_dict, _laps_out_dict = compute_segmentations_and_decode_laps(curr_active_pipeline, active_container=active_container, epoch_names = ['roam', 'sprinkle'], decoding_time_bin_size=decoding_time_bin_size, enable_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_container = _container_container.container\n",
    "active_container = _container_container.masked_container\n",
    "# decoded_local_epochs_result = container.epochs_decoded_result_cache_dict[decoding_time_bin_size][an_epoch_name]\n",
    "decoded_local_epochs_result = active_container.epochs_decoded_result_cache_dict[decoding_time_bin_size][an_epoch_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to decode for all laps at that fine resolution\n",
    "decoded_local_epochs_result = _container_container.container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc62ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# Usage from Container:\n",
    "a_t_bin_size: float = 0.025\n",
    "# a_t_bin_size: float = 0.250\n",
    "a_decoder_name: str = 'roam'\n",
    "active_container = _container_container.masked_container\n",
    "a_decoder = active_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "a_decoded_result = active_container.epochs_decoded_result_cache_dict[a_t_bin_size][a_decoder_name] # DecodedFilterEpochsResult\n",
    "\n",
    "\n",
    "decoded_local_epochs_result = None\n",
    "# decoded_local_epochs_result = a_decoded_result\n",
    "# p = None\n",
    "\n",
    "xbin = a_decoder.xbin\n",
    "xbin_centers = a_decoder.xbin_centers\n",
    "ybin = a_decoder.ybin\n",
    "ybin_centers = a_decoder.ybin_centers\n",
    "\n",
    "# curr_position_df = curr_active_pipeline.sess.position.to_dataframe()\n",
    "\n",
    "curr_position_df, epoch_specific_position_dfs = LapsVisualizationMixin._compute_laps_specific_position_dfs(curr_active_pipeline.sess)\n",
    "a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=decoded_local_epochs_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, rotate_to_vertical=True)\n",
    "fig, axs, decoded_epochs_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(curr_position_df=curr_position_df, curr_num_subplots=10, active_page_index=0, \n",
    "                                                                                     epoch_specific_position_dfs=epoch_specific_position_dfs, epoch_ids=None,\n",
    "                                                                                     plot_actual_lap_lines=True, use_theoretical_tracks_instead=False, fixed_columns=10)\n",
    "\n",
    "ax = axs[0][0]\n",
    "ax.set_aspect('auto')  # Adjust automatically based on data limits\n",
    "ax.set_adjustable('datalim')  # Ensure the aspect ratio respects the data limits\n",
    "ax.autoscale()  # Autoscale the view to fit data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85664053",
   "metadata": {},
   "outputs": [],
   "source": [
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fa60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# axs[0][0].set_aspect(1)\n",
    "# axs[0][0].set_aspect('equal')  # Preserve aspect ratio\n",
    "integer_slider = a_decoded_traj_plotter.plot_epoch_with_slider_widget(an_epoch_idx=0, include_most_likely_pos_line=False)\n",
    "integer_slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "_container_container.masked_container.epochs_decoded_result_cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b4057",
   "metadata": {},
   "source": [
    "# 2026-02-11 - Clean up all unused old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.vispy.vispy_helpers import VispyHelpers, ContourItem, contours_from_masks, create_contour_line_visuals\n",
    "from pyphoplacecellanalysis.Pho2D.vispy.vispy_helpers import TrajectorySegmentsVisual\n",
    "\n",
    "  \n",
    "from vispy import app\n",
    "\n",
    "t1 = np.linspace(0, 2 * np.pi, 80)\n",
    "df1 = pd.DataFrame({'x': 0.2 * np.cos(t1), 'y': 0.2 * np.sin(t1)})\n",
    "t2 = np.linspace(0, 2 * np.pi, 50)\n",
    "df2 = pd.DataFrame({'x': 0.15 * np.cos(t2) + 0.3, 'y': 0.15 * np.sin(t2)})\n",
    "df3 = pd.DataFrame({'x': np.linspace(-0.25, 0.25, 40), 'y': np.linspace(-0.2, 0.2, 40)})\n",
    "segments = [df1, df2, df3]\n",
    "canvas = scene.SceneCanvas(keys='interactive', size=(800, 600), show=True)\n",
    "view = canvas.central_widget.add_view()\n",
    "view.camera = 'panzoom'\n",
    "seg_visual = TrajectorySegmentsVisual(segments, parent=view.scene, colors=['red', 'green', 'blue'], line_width=2.0, order=10)\n",
    "if seg_visual.line is not None:\n",
    "    seg_visual.line.set_gl_state('translucent', depth_test=False)\n",
    "else:\n",
    "    for line in seg_visual.lines:\n",
    "        line.set_gl_state('translucent', depth_test=False)\n",
    "VispyHelpers.set_view_camera(view, np.vstack([df1[['x', 'y']].values, df2[['x', 'y']].values, df3[['x', 'y']].values]), padding=0.15)\n",
    "\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c589b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import OptimizedViewportRenderer, ThumbnailCacheEntry, Viewport\n",
    "\n",
    "renderer = OptimizedViewportRenderer(\n",
    "    results2D=results2D,\n",
    "    active_ax=track_ax,\n",
    "    base_frame_divide_bin_size=0.5,\n",
    "    min_thumbnail_width_px=50,\n",
    "    max_thumbnails_per_viewport=100\n",
    ")\n",
    "\n",
    "# Render for current viewport\n",
    "viewport = Viewport(start_time=10.0, end_time=20.0, width_pixels=800, height_pixels=200)\n",
    "artists, extent = renderer.render_viewport(viewport, posterior_masking_value=0.0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31217fc8",
   "metadata": {},
   "source": [
    "# ❎ 2026-02-12 - Segementation (Just use existing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bd659",
   "metadata": {
    "tags": [
     "🟢 active-2026-02-11"
    ]
   },
   "source": [
    "# 🚧🟢🎯 2026-02-12 - Past/Future Findings to Videos\n",
    "- [ ] get long paths likee I did before\n",
    "- [ ] extract the (start_t, end_t) from each\n",
    "- [ ] use these (start_t, end_t) epochs to view that segment of video (for animal position) \n",
    "    - [ ] e.g. control the attached posterior/trajectory renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50626f8",
   "metadata": {
    "tags": [
     "2026-02-13_render_past_future_to_cells🖌️🖼️💾",
     "active-2026-02-16"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult, MatchingPastFuturePositionsResult, MaskDataSource\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import do_plot_and_export_past_future_all, _build_attached_plotters_once\n",
    "\n",
    "import shutil\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter, RenderColoringMode\n",
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.Mixins.LapsVisualizationMixin import LapsVisualizationMixin\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.Mixins.AnimalTrajectoryPlottingMixin import AnimalTrajectoryPlottingMixin\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPositionDecoderPlotter import TimeSynchronizedPositionDecoderPlotter\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.PhoContainerTool import GenericPyQtGraphContainer\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import build_combined_time_synchronized_Bapun_decoders_window\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster, SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.SpecificDockWidgetManipulatingMixin import SpecificDockWidgetManipulatingMixin\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import PredictiveDecodingVispyWidget, render_predictive_decoding_with_vispy\n",
    "from qtpy import QtWidgets\n",
    "# import neuropy.utils.type_aliases as types # import neuropy.utils.type_aliases as types\n",
    "import pyphoplacecellanalysis.General.type_aliases as types\n",
    "DecodedEpochIndex: TypeAlias = int # an integer index that is an aclu\n",
    "DecodedEpochID: TypeAlias = int # an integer index that is an \n",
    "\n",
    "DecodedEpochTimeBinIndex: TypeAlias = int # an integer index that is an aclu\n",
    "\n",
    "# Define a new type as a tuple of the two above custom types\n",
    "DecodedEpochTimeBinIndexTuple: TypeAlias = Tuple[DecodedEpochIndex, DecodedEpochTimeBinIndex]\n",
    "\n",
    "\n",
    "active_container = _container_container.masked_container\n",
    "\n",
    "# an_epoch_name = 'roam'\n",
    "# a_decoder = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "# a_decoder = active_container.pf1D_Decoder_dict[an_epoch_name]\n",
    "a_decoder = list(active_container.pf1D_Decoder_dict.values())[0]\n",
    "# a_result2D: DecodedFilterEpochsResult = decoded_local_epochs_result.frame_divided_epochs_results[an_epoch_name]\n",
    "xbin = a_decoder.xbin\n",
    "xbin_centers = a_decoder.xbin_centers\n",
    "ybin = a_decoder.ybin\n",
    "ybin_centers = a_decoder.ybin_centers\n",
    "past_future_keys = ['future', 'past']\n",
    "    \n",
    "\n",
    "## INPUTS: curr_epoch_idx\n",
    "def plot_matching_trajectories(a_ds, curr_position_df, curr_epoch_idx: int, minimum_included_matching_sequence_length: int = 4):\n",
    "    \"\"\" plots the matching trajectories for a given PBE \n",
    "    \n",
    "    captures: xbin, ybin, xbin_Centers, ybin_centers\n",
    "    \"\"\"\n",
    "    epoch_data = a_ds._prepare_epoch_data(an_epoch_idx=curr_epoch_idx, minimum_included_matching_sequence_length=minimum_included_matching_sequence_length)\n",
    "    # curr_matching_past_future_positions_df_dict = epoch_data['curr_matching_past_future_positions_df_dict']\n",
    "    curr_matching_past_future_positions_df_list = epoch_data['curr_matching_past_future_positions_df_list']\n",
    "    ## OUTPUTS: curr_matching_past_future_positions_df_list\n",
    "    \n",
    "    past_future_all_found_path_epochs_df_dict = {}\n",
    "    # for a_past_future_key in curr_matching_past_future_positions_df_list:        \n",
    "    for a_past_future_key, found_item in curr_matching_past_future_positions_df_list.items():\n",
    "        # all_found_path_epochs_df = pd.DataFrame([(df['t'].min(), df['t'].max()) for df in curr_matching_past_future_positions_df_list[a_past_future_key]], columns=['start', 'end'])\n",
    "        all_found_path_epochs_df = pd.DataFrame([(df['t'].min(), df['t'].max()) for df in found_item], columns=['start', 'end'])\n",
    "        all_found_path_epochs_df['past_or_future'] = a_past_future_key\n",
    "        past_future_all_found_path_epochs_df_dict[a_past_future_key] = all_found_path_epochs_df\n",
    "\n",
    "    # all_found_path_epochs_df_merged: pd.DataFrame = PandasHelpers.safe_concat(past_future_all_found_path_epochs_df_dict.values())\n",
    "    # all_found_path_epochs_df_merged\n",
    "\n",
    "    curr_matching_past_future_positions_df_all_list: List[pd.DataFrame] = flatten([v for v in curr_matching_past_future_positions_df_list.values()])\n",
    "    # curr_matching_past_future_positions_df_all_list\n",
    "\n",
    "    ## OUTPUTS: curr_matching_past_future_positions_df_list, all_found_path_epochs_df_merged, curr_matching_past_future_positions_df_all_list\n",
    "    decoded_local_epochs_result = None ## NOT NEEDED\n",
    "\n",
    "    epoch_specific_position_dfs = curr_matching_past_future_positions_df_all_list\n",
    "    num_found_possible_path_matches: int = len(epoch_specific_position_dfs)\n",
    "    assert (num_found_possible_path_matches > 0), f\"num_found_possible_path_matches: {num_found_possible_path_matches} for curr_epoch_idx: {curr_epoch_idx}\"\n",
    "    ## INPUTS: epoch_specific_position_dfs\n",
    "    ## add an optional `arrow_opacity` to `arrow_concentration_kwargs` that can be used to further customize the arrows on their own. If it's not provided by the user, `line_opacity` is used\n",
    "    should_include_trajectory_arrows = True\n",
    "    # should_include_trajectory_arrows = False\n",
    "    arrow_concentration_kwargs = dict(\n",
    "                    arrow_skip = 30, time_cmap='magma', arrow_color_scheme = RenderColoringMode.TIME,\n",
    "                    mutation_scale_multiplier = 10, mutation_scale_constant = 1, arrow_length_multiplier = 0.2, arrow_length_constant = 0.05, arrow_lw = 0.5, \n",
    "                    arrow_opacity = 0.5,\n",
    "                )\n",
    "    _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "    a_decoded_traj_plotter: DecodedTrajectoryMatplotlibPlotter = DecodedTrajectoryMatplotlibPlotter(a_result=decoded_local_epochs_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, rotate_to_vertical=True)\n",
    "    fig, axs, decoded_epochs_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(curr_position_df=curr_position_df, curr_num_subplots=num_found_possible_path_matches, active_page_index=0, \n",
    "                                                                                        epoch_specific_position_dfs=epoch_specific_position_dfs, epoch_ids=None,\n",
    "                                                                                        plot_actual_lap_lines=True, should_include_trajectory_arrows=should_include_trajectory_arrows,\n",
    "                                                                                        arrow_concentration_kwargs=arrow_concentration_kwargs, line_opacity = 0.9,\n",
    "                                                                                        line_start_lw = 0.5, line_end_lw = 4.5, cmap='plasma', #cmap='magma', ## 'magma' is actually very important\n",
    "                                                                                        #  line_start_lw = 0.3, line_end_lw = 1.0,\n",
    "                                                                                        # line_start_lw = 0.3, line_end_lw = 25.0,\n",
    "                                                                                        use_theoretical_tracks_instead=False, fixed_columns=10)\n",
    "\n",
    "\n",
    "    figure_title: str = f'PBE[{curr_epoch_idx}]: num matching paths {num_found_possible_path_matches}'\n",
    "    perform_update_title_subtitle(fig=fig, ax=None, title_string=figure_title, #subtitle_string=\"TEST - SUBTITLE\",\n",
    "                                )\n",
    "\n",
    "    # ax = axs[0][0]\n",
    "    # ax.set_aspect('auto')  # Adjust automatically based on data limits\n",
    "    # ax.set_adjustable('datalim')  # Ensure the aspect ratio respects the data limits\n",
    "    # ax.autoscale()  # Autoscale the view to fit data\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    return a_decoded_traj_plotter, fig, axs, decoded_epochs_pages\n",
    "\n",
    "\n",
    "def render_for_epoch(a_ds, curr_epoch_idx: int, sync_plotters: Dict, curr_export_video_parent_folder: Path, minimum_included_matching_sequence_length=4):\n",
    "    \"\"\" renders all the past/future paths for a specific PBE idx and saves them out to video\n",
    "    \"\"\"\n",
    "    export_video_paths = []\n",
    "\n",
    "\n",
    "    epoch_data = a_ds._prepare_epoch_data(an_epoch_idx=curr_epoch_idx, minimum_included_matching_sequence_length=minimum_included_matching_sequence_length)\n",
    "    # curr_matching_past_future_positions_df_dict = epoch_data['curr_matching_past_future_positions_df_dict']\n",
    "    curr_matching_past_future_positions_df_list = epoch_data['curr_matching_past_future_positions_df_list']\n",
    "    ## OUTPUTS: curr_matching_past_future_positions_df_list\n",
    "\n",
    "    past_future_all_found_path_epochs_df_dict = {}\n",
    "    # for a_past_future_key in curr_matching_past_future_positions_df_list:        \n",
    "    for a_past_future_key, found_item in curr_matching_past_future_positions_df_list.items():\n",
    "        # all_found_path_epochs_df = pd.DataFrame([(df['t'].min(), df['t'].max()) for df in curr_matching_past_future_positions_df_list[a_past_future_key]], columns=['start', 'end'])\n",
    "        all_found_path_epochs_df = pd.DataFrame([(df['t'].min(), df['t'].max()) for df in found_item], columns=['start', 'end'])\n",
    "        all_found_path_epochs_df['past_or_future'] = a_past_future_key\n",
    "        past_future_all_found_path_epochs_df_dict[a_past_future_key] = all_found_path_epochs_df\n",
    "\n",
    "    all_found_path_epochs_df_merged: pd.DataFrame = PandasHelpers.safe_concat(past_future_all_found_path_epochs_df_dict.values())\n",
    "\n",
    "    ## OUTPUTS: all_found_path_epochs_df_merged\n",
    "    # all_found_path_epochs_df_merged\n",
    "    for a_row in all_found_path_epochs_df_merged.itertuples(index=True):\n",
    "        found_path_index: int = int(a_row.Index)\n",
    "        \n",
    "        for an_epoch_name, a_plotter in sync_plotters.items():\n",
    "            # export_extension = '.avi'\n",
    "            export_extension = '.gif'\n",
    "            an_export_video_path = curr_export_video_parent_folder.joinpath(f'found_{found_path_index}_decoder_{an_epoch_name}{export_extension}')\n",
    "            print(f'exporting to \"{an_export_video_path}\"')\n",
    "            # With custom settings\n",
    "            video_path = a_plotter.export_video(\n",
    "                output_path=an_export_video_path,\n",
    "                start_t=a_row.start,\n",
    "                end_t=a_row.end,\n",
    "                fps=24.0,\n",
    "                width=720,\n",
    "                height=720,\n",
    "                progress_print=True,\n",
    "                debug_print=False,\n",
    "            )\n",
    "            print(f'\\texport to video_path: \"{video_path.resolve().as_posix()}\" complete.')\n",
    "            export_video_paths.append(video_path)\n",
    "            # export_video_paths[an_epoch_name] = video_path\n",
    "\n",
    "    print(f'done exporting all videos.')\n",
    "\n",
    "    return export_video_paths\n",
    "\n",
    "\n",
    "def _build_attached_plotters_once(curr_active_pipeline, active_2d_plot=None):\n",
    "    \"\"\" builds the two attached time-synced plotters that are used to render 2D data\n",
    "    Usage:\n",
    "        _out_container_new, sync_plotters = _build_attached_plotters_once(curr_active_pipeline=curr_active_pipeline, active_2d_plot=None)\n",
    "    \"\"\"    \n",
    "    hardcoded_params: HardcodedProcessingParameters = BapunDataSessionFormatRegisteredClass._get_session_specific_parameters(session_context=curr_active_pipeline.get_session_context())\n",
    "    directional_decoders_decode_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "    assert directional_decoders_decode_result is not None\n",
    "    _out_container_new: GenericPyQtGraphContainer = build_combined_time_synchronized_Bapun_decoders_window(curr_active_pipeline, included_filter_names=hardcoded_params.non_global_activity_session_names, fixed_window_duration = 1.0,\n",
    "        directional_decoders_decode_result=directional_decoders_decode_result,\n",
    "        controlling_widget=active_2d_plot, create_new_controlling_widget=False, show_posteriors=False,\n",
    "        # controlling_widget=active_2d_plot, create_new_controlling_widget=False, show_posteriors=False,\n",
    "    )\n",
    "\n",
    "    # active_2d_plot: Spike2DRaster = _out_container_new.ui.controlling_widget\n",
    "    sync_plotters: Dict[str, TimeSynchronizedPositionDecoderPlotter] = _out_container_new.ui.sync_plotters\n",
    "    win: PhoDockAreaContainingWindow = _out_container_new.ui.root_dockAreaWindow\n",
    "\n",
    "    ## Disable debug print to speed up animation\n",
    "    for a_plotter_name, a_plotter in sync_plotters.items():\n",
    "        a_plotter.params.debug_print = False\n",
    "\n",
    "    ## INPUTS: _out_container, active_2d_plot, _out_container, sync_plotters, \n",
    "\n",
    "    for an_epoch_name, a_plotter in sync_plotters.items():\n",
    "        a_plotter.ui.root_plot.setTitle(f'PositionDecoder -  t = {a_plotter.last_window_time}')    \n",
    "        # a_plotter.params.drop_below_threshold = 1.0 ## DROP ALL SO NO POSTERIORS ARE SHOWN\n",
    "        a_plotter.ui.imv.setVisible(False) ## Hide posterior heatmap completely\n",
    "        QtWidgets.QApplication.processEvents()\n",
    "        \n",
    "    return _out_container_new, sync_plotters\n",
    "\n",
    "\n",
    "@function_attributes(short_name=None, tags=['MAIN'], input_requires=[], output_provides=[], uses=['render_for_epoch', 'plot_matching_trajectories', '_build_attached_plotters_once'], used_by=[], creation_date='2026-02-13 03:49', related_items=[])\n",
    "def do_plot_and_export_past_future_all(curr_active_pipeline, _container_container, decoder_flat_matching_results_list_ds_dict, included_epoch_ids, sync_plotters=None, enable_render_videos: bool = True, a_t_bin_size: float = 0.025, **kwargs):\n",
    "    \"\"\" main function, captures a lot\n",
    "    \"\"\"\n",
    "    # ==================================================================================================================================================================================================================================================================================== #\n",
    "    # BEGIN SINGLE `curr_epoch_idx`                                                                                                                                                                                                                                                        #\n",
    "    # ==================================================================================================================================================================================================================================================================================== #\n",
    "    ## OUTPUTS: curr_epoch_idx\n",
    "    # curr_epoch_idx: int = 8\n",
    "\n",
    "    # BUILD ALL OUTPUT FILE PATHS ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    # export_video_paths = {}\n",
    "\n",
    "    export_video_parent_folder = curr_active_pipeline.get_output_path().joinpath('videos').joinpath('past_future_matches').resolve()\n",
    "    if enable_render_videos:\n",
    "        export_video_parent_folder.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "    # ==================================================================================================================================================================================================================================================================================== #\n",
    "    # BEGIN RENDERING OUTPUT PATHS                                                                                                                                                                                                                                                         #\n",
    "    # ==================================================================================================================================================================================================================================================================================== #\n",
    "\n",
    "    # Usage from Container:\n",
    "    \n",
    "    # a_t_bin_size: float = 0.250\n",
    "    # a_decoder_name: str = 'roam'\n",
    "    active_container = _container_container.masked_container\n",
    "    \n",
    "    curr_position_df, _ = LapsVisualizationMixin._compute_laps_specific_position_dfs(curr_active_pipeline.sess)\n",
    "    paradigm_df = ensure_dataframe(curr_active_pipeline.sess.paradigm)\n",
    "    paradigm_df = paradigm_df[paradigm_df['label'].isin(['roam', 'sprinkle'])].reset_index(drop=True)\n",
    "    # paradigm_df\n",
    "    # Only get the positions on the relevant mazes:\n",
    "    curr_position_df = curr_position_df.position.adding_maze_id_if_needed(active_maze_epochs_df=paradigm_df, no_interval_fill_value=np.nan)\n",
    "    curr_position_df = curr_position_df[curr_position_df['maze_id'].notna()]\n",
    "    # curr_position_df\n",
    "\n",
    "    export_video_parent_folder = export_video_parent_folder.joinpath('past_future_matches')\n",
    "    all_fn_out = {}\n",
    "\n",
    "    for curr_epoch_idx in included_epoch_ids:\n",
    "        print(f'STARTING TO PROCESS curr_epoch_idx: {curr_epoch_idx} ___________________')    \n",
    "        ## need to run for both datasources/decoder names:\n",
    "\n",
    "        ## #TODO 2026-02-16 19:40: - [ ] get the right track for each PBE epoch        \n",
    "        try:\n",
    "            #TODO 2026-02-16 19:54: - [ ] Need to loop through the active decoder names here I guess:\n",
    "            a_decoder_name: str = 'roam'\n",
    "\n",
    "\n",
    "            a_decoder = active_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "            a_decoded_result = active_container.epochs_decoded_result_cache_dict[a_t_bin_size][a_decoder_name] # DecodedFilterEpochsResult\n",
    "            # epoch_specific_position_dfs =  curr_active_pipeline.filtered_sessions[a_decoder_name].position.to_dataframe().pho.partition_df('lap')\n",
    "            a_ds = decoder_flat_matching_results_list_ds_dict[a_decoder_name]  ## #TODO 2026-02-16 19:56: - [ ] Needs to be the correct datasource for this epoch\n",
    "\n",
    "            # curr_position_df = curr_position_df[curr_position_df['lap'] != np.nan]\n",
    "            # epoch_specific_position_dfs = [curr_position_df.groupby('lap').get_group(i)[['t','x','y','lin_pos']] for i in session.laps.lap_id] # dataframes split for each ID:\n",
    "            \n",
    "\n",
    "            # GET DATA FOR EPOCH _________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "            \n",
    "            # decoder_flat_matching_results_list_ds_dict\n",
    "\n",
    "            epoch_data = a_ds._prepare_epoch_data(an_epoch_idx=curr_epoch_idx)\n",
    "            # curr_matching_past_future_positions_df_dict = epoch_data['curr_matching_past_future_positions_df_dict']\n",
    "            curr_matching_past_future_positions_df_list = epoch_data['curr_matching_past_future_positions_df_list']\n",
    "            ## OUTPUTS: curr_matching_past_future_positions_df_list\n",
    "\n",
    "            ## INPUTS: all_found_path_epochs_df_merged\n",
    "            curr_epoch_export_parent_folder = export_video_parent_folder.joinpath(f'epoch_{curr_epoch_idx}')\n",
    "            curr_epoch_export_parent_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            print(f'curr_export_video_parent_folder: {curr_epoch_export_parent_folder}')\n",
    "            ## OUTPUTS: curr_export_video_parent_folder\n",
    "\n",
    "            curr_export_images_parent_folder = curr_epoch_export_parent_folder.joinpath('posteriors')\n",
    "            curr_export_images_parent_folder.mkdir(exist_ok=True)\n",
    "\n",
    "            active_out_middle_figure_path = curr_export_images_parent_folder.joinpath(f'plot_decoded_trajectories_2d_{curr_epoch_idx}') # curr_active_pipeline.output_figure(final_context, fig, debug_print=True) \n",
    "\n",
    "\n",
    "\n",
    "            past_future_all_found_path_epochs_df_dict = {}\n",
    "            for a_past_future_key in past_future_keys:\n",
    "                all_found_path_epochs_df = pd.DataFrame([(df['t'].min(), df['t'].max()) for df in curr_matching_past_future_positions_df_list[a_past_future_key]], columns=['start', 'end'])\n",
    "                all_found_path_epochs_df['past_or_future'] = a_past_future_key\n",
    "                past_future_all_found_path_epochs_df_dict[a_past_future_key] = all_found_path_epochs_df\n",
    "\n",
    "            all_found_path_epochs_df_merged: pd.DataFrame = pd.concat(past_future_all_found_path_epochs_df_dict.values())\n",
    "            curr_matching_past_future_positions_df_all_list: List[pd.DataFrame] = flatten([v for v in curr_matching_past_future_positions_df_list.values()])\n",
    "            num_found_possible_path_matches: int = len(curr_matching_past_future_positions_df_all_list)\n",
    "\n",
    "            # assert (num_found_possible_path_matches > 0), f\"curr_epoch_idx: {curr_epoch_idx} has {num_found_possible_path_matches} num_found_possible_path_matches... wtf?\"\n",
    "            if (num_found_possible_path_matches == 0):\n",
    "                raise ValueError(f\"curr_epoch_idx: {curr_epoch_idx} has {num_found_possible_path_matches} num_found_possible_path_matches... wtf?\")\n",
    "                \n",
    "            ## OUTPUTS: all_found_path_epochs_df_merged\n",
    "            # all_found_path_epochs_df_merged\n",
    "\n",
    "            # ==================================================================================================================================================================================================================================================================================== #\n",
    "            # 2D Grid of possible paths                                                                                                                                                                                                                                                            #\n",
    "            # ==================================================================================================================================================================================================================================================================================== #\n",
    "\n",
    "            ## OUTPUTS: curr_position_df\n",
    "            _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "            ## PBE-specific setup\n",
    "            ## INPUTS: curr_epoch_idx\n",
    "            a_decoded_traj_plotter, fig, axs, decoded_epochs_pages = plot_matching_trajectories(a_ds=a_ds, curr_position_df=curr_position_df, curr_epoch_idx=curr_epoch_idx, minimum_included_matching_sequence_length=minimum_included_matching_sequence_length)\n",
    "            final_context = curr_active_pipeline.build_display_context_for_filtered_session(filtered_session_name=a_decoder_name, display_fn_name='plot_decoded_trajectories_2d', curr_epoch_idx=curr_epoch_idx)\n",
    "            active_out_figure_paths = curr_active_pipeline.output_figure(final_context, fig, debug_print=True) \n",
    "\n",
    "            ## made in wrong place, so copy to the correct one (`active_out_middle_figure_path`):\n",
    "\n",
    "            ## INPUTS: active_out_middle_figure_path\n",
    "            active_out_figure_path: Path = active_out_figure_paths[0][0]\n",
    "            Assert.path_exists(active_out_figure_path)\n",
    "            # active_out_middle_figure_path = active_out_figure_path\n",
    "            active_out_middle_figure_path = active_out_middle_figure_path.with_suffix(active_out_figure_path.suffix) ## set suffix to same as the input image\n",
    "            print(f'copying active_out_figure_path: {active_out_figure_path} -> active_out_middle_figure_path: {active_out_middle_figure_path}')\n",
    "            shutil.copy2(active_out_figure_path, active_out_middle_figure_path)\n",
    "            print(f'\\tdone.')\n",
    "\n",
    "            # ==================================================================================================================================================================================================================================================================================== #\n",
    "            # Render out the posterior info/contours (middle pane from vispy app version)                                                                                                                                                                                                          #\n",
    "            # ==================================================================================================================================================================================================================================================================================== #\n",
    "\n",
    "            _out_epoch_flat_mask_future_past_result: List[MatchingPastFuturePositionsResult] = decoder_epoch_flat_mask_future_past_result_dict[a_decoder_name]\n",
    "\n",
    "            ## INPUTS: curr_epoch_idx\n",
    "            long_found_paths_only_batch_overview_viewer: PredictiveDecodingVispyWidget = render_predictive_decoding_with_vispy(epoch_flat_mask_future_past_result=_out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df=a_decoded_filter_epochs_df,\n",
    "                                                            curr_position_df = masked_container.decoding_locality.pos_df, \n",
    "                                                            pf_decoder = a_decoder, decoded_result = a_decoded_result, \n",
    "                                                            show_full_position_background = False, current_traj_seconds_pre_post_extension = 0.0, \n",
    "                                                            #  past_future_trajectory_extension_seconds=0.750, \n",
    "                                                            past_future_trajectory_extension_seconds={'start': 0.25, 'end': 0.5}, start_end_extension_max_opacity=0.4, \n",
    "                                                            require_angle_match=False, color_matches_by_matching_angle=False,\n",
    "                                                            #  require_angle_match=True, color_matches_by_matching_angle=True,\n",
    "                                                            #  enable_debug_plot_trajectory_average_angle_arrows=True,\n",
    "                                                            minimum_included_matching_sequence_length = minimum_included_matching_sequence_length, ## this is what makes it used the filtered info\n",
    "                                                            color_matches_by_merged_epoch_t_bin_idx=False,\n",
    "                                                            enable_table_widgets=False,\n",
    "                                                            active_epoch_idx=curr_epoch_idx, enable_multi_epoch_overview_display_mode = False, enable_full_vispy_debug_mode=False,\n",
    "                                                            # active_epoch_idx=None, enable_multi_epoch_overview_display_mode = True, MAX_NUM_OVERVIEW_EPOCHS_TO_RENDER=4, enable_full_vispy_debug_mode=False,\n",
    "            )\n",
    "            # long_found_paths_only_batch_overview_viewer\n",
    "\n",
    "            ## OUTPUTS: curr_export_video_parent_folder, curr_export_images_parent_folder\n",
    "            exported_middle_pane_files = long_found_paths_only_batch_overview_viewer.export_vispy_viewer_epochs(export_folder=curr_export_images_parent_folder, epoch_indices=[curr_epoch_idx]) # #TODO 2026-02-17 17:13: - [ ] curr_epoch_idx could be wrong here\n",
    "\n",
    "\n",
    "            # ==================================================================================================================================================================================================================================================================================== #\n",
    "            # Render the video in here:                                                                                                                                                                                                                                                            #\n",
    "            # ==================================================================================================================================================================================================================================================================================== #\n",
    "            an_epoch_export_video_paths = {}\n",
    "            \n",
    "            if enable_render_videos:\n",
    "                if sync_plotters is None:\n",
    "                    print(f'WARNING: have no attached synced plotters, so building them.')\n",
    "                    _out_container_new, sync_plotters = _build_attached_plotters_once(curr_active_pipeline=curr_active_pipeline, active_2d_plot=None)\n",
    "\n",
    "                an_epoch_export_video_paths = render_for_epoch(a_ds=a_ds, curr_epoch_idx=curr_epoch_idx, sync_plotters=sync_plotters, curr_export_video_parent_folder=curr_epoch_export_parent_folder)\n",
    "                # export_video_paths[an_epoch_idx] = an_epoch_export_video_paths\n",
    "                \n",
    "            else:\n",
    "                print(f'enable_render_videos is False, so skipping.')\n",
    "                \n",
    "\n",
    "            # sync_plotters = _outs[-1]\n",
    "            # print(f'\\tSUCCESSS {_outs}. done.')\n",
    "            print(f'\\t__________________________________________________')\n",
    "        except (TypeError, ValueError, AttributeError, AssertionError, IndexError) as e:\n",
    "            print(f'\\tencounterd error {e}. Skipping.')\n",
    "        except Exception as e:\n",
    "            raise\n",
    "    ## END for curr_epoch_idx in included_epoch_ids...\n",
    "    \n",
    "    print(f'\\t__________________________________________________')\n",
    "\n",
    "    return export_video_parent_folder, all_fn_out # exported_middle_pane_files, active_out_figure_path, a_decoded_traj_plotter, sync_plotters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neuropy.utils.type_aliases as types # import neuropy.utils.type_aliases as types\n",
    "import pyphoplacecellanalysis.General.type_aliases as types\n",
    "DecodedEpochIndex: TypeAlias = int # an integer index that is an aclu\n",
    "DecodedEpochID: TypeAlias = int # an integer index that is an \n",
    "\n",
    "DecodedEpochTimeBinIndex: TypeAlias = int # an integer index that is an aclu\n",
    "\n",
    "# Define a new type as a tuple of the two above custom types\n",
    "DecodedEpochTimeBinIndexTuple: TypeAlias = Tuple[DecodedEpochIndex, DecodedEpochTimeBinIndex]\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MatchingPastFuturePositionsResult\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# BEGIN FUNCTION BODY                                                                                                                                                                                                                                                                  #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "## INPUTS: _out_epoch_flat_mask_future_past_result, a_decoded_filter_epochs_df, container\n",
    "\n",
    "# Usage from Container:\n",
    "a_t_bin_size: float = 0.025\n",
    "minimum_included_matching_sequence_length: int = 4\n",
    "\n",
    "## INPUTS: masked_container\n",
    "decoder_epoch_flat_mask_future_past_result_dict: Dict[types.DecoderName, List[MatchingPastFuturePositionsResult]] = {} ## can't be serialized for some reason\n",
    "\n",
    "decoder_flat_matching_results_list_ds_dict: Dict[types.DecoderName, MaskDataSource] = {}\n",
    "\n",
    "included_epoch_names = ['roam', 'sprinkle']\n",
    "past_future_keys = ['future', 'past']\n",
    "\n",
    "\n",
    "for a_decoder_name in included_epoch_names:\n",
    "    a_decoder = masked_container.pf1D_Decoder_dict[a_decoder_name]\n",
    "    a_decoded_result = masked_container.epochs_decoded_result_cache_dict[a_t_bin_size][a_decoder_name] # DecodedFilterEpochsResult\n",
    "    a_decoded_filter_epochs_df: pd.DataFrame = a_decoded_result.filter_epochs\n",
    "    decoder_epoch_flat_mask_future_past_result_dict[a_decoder_name] = masked_container.debug_computed_dict[a_decoder_name]['prominence_future_past_analysis']['_out_epoch_flat_mask_future_past_result']\n",
    "    ## updates: decoder_flat_matching_results_list_ds_dict\n",
    "    decoder_flat_matching_results_list_ds_dict[a_decoder_name] = MaskDataSource.init_from_list_of_MatchingPastFuturePositionsResult(epoch_flat_mask_future_past_result=decoder_epoch_flat_mask_future_past_result_dict[a_decoder_name],\n",
    "                                                                                                                        filter_epochs=a_decoded_filter_epochs_df, \n",
    "                                                                                                                        xbin=a_decoder.xbin, ybin=a_decoder.ybin, xbin_centers=a_decoder.xbin_centers, ybin_centers=a_decoder.ybin_centers,\n",
    "                                                                                                                        curr_position_df=masked_container.decoding_locality.pos_df,\n",
    "                                                                                                                     )\n",
    "\n",
    "## OUTPUTS: decoder_epoch_flat_mask_future_past_result_dict, decoder_flat_matching_results_list_ds_dict\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# BEGIN GETTING THE DATA FOR THE PLOT                                                                                                                                                                                                                                                  #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "included_epoch_ids_dict: Dict[types.DecoderName, NDArray] = {}\n",
    "decoder_included_epoch_idx_ids_dict: Dict[types.DecoderName, Dict[DecodedEpochIndex, DecodedEpochID]] = {}\n",
    "\n",
    "# for a_decoder_name in included_epoch_names:\n",
    "for a_decoder_name, a_ds in decoder_flat_matching_results_list_ds_dict.items():\n",
    "    # a_ds: MaskDataSource  = decoder_flat_matching_results_list_ds_dict[a_decoder_name]\n",
    "    \n",
    "    # find included/excluded epochs ______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    included_epoch_ids: NDArray = []\n",
    "    included_epoch_idx_ids_dict: Dict[DecodedEpochIndex, DecodedEpochID] = {}\n",
    "\n",
    "    for curr_epoch_idx in np.arange(a_ds.num_epochs):\n",
    "\n",
    "        epoch_data = a_ds._prepare_epoch_data(an_epoch_idx=curr_epoch_idx, minimum_included_matching_sequence_length=minimum_included_matching_sequence_length)\n",
    "        # curr_matching_past_future_positions_df_dict = epoch_data['curr_matching_past_future_positions_df_dict']\n",
    "        curr_matching_past_future_positions_df_list = epoch_data['curr_matching_past_future_positions_df_list']\n",
    "        ## OUTPUTS: curr_matching_past_future_positions_df_list\n",
    "        original_epoch_id: int = epoch_data['curr_epoch_row']['original_epoch_idx'] #.get('original_epoch_id', None)\n",
    "        print(f'original_epoch_id[{curr_epoch_idx}]: {original_epoch_id}')\n",
    "\n",
    "        past_future_all_found_path_epochs_df_dict = {}\n",
    "        for a_past_future_key in past_future_keys:\n",
    "            if len(curr_matching_past_future_positions_df_list) > 0:\n",
    "                all_found_path_epochs_df = pd.DataFrame([(df['t'].min(), df['t'].max()) for df in curr_matching_past_future_positions_df_list[a_past_future_key]], columns=['start', 'end'])\n",
    "                all_found_path_epochs_df['past_or_future'] = a_past_future_key\n",
    "                past_future_all_found_path_epochs_df_dict[a_past_future_key] = all_found_path_epochs_df\n",
    "\n",
    "        ## END for a_past_future_key in past_future_keys...\n",
    "        all_found_path_epochs_df_merged: pd.DataFrame = PandasHelpers.safe_concat(past_future_all_found_path_epochs_df_dict.values())\n",
    "        curr_matching_past_future_positions_df_all_list: List[pd.DataFrame] = flatten([v for v in curr_matching_past_future_positions_df_list.values()])\n",
    "        num_found_possible_path_matches: int = len(curr_matching_past_future_positions_df_all_list)\n",
    "\n",
    "        if (num_found_possible_path_matches > 0):\n",
    "            # print(f'{curr_epoch_idx}: num_found_possible_path_matches: {num_found_possible_path_matches}')\n",
    "            # included_epoch_ids.append(curr_epoch_idx) ## WRONG\n",
    "            included_epoch_ids.append(original_epoch_id) \n",
    "            included_epoch_idx_ids_dict[curr_epoch_idx] = original_epoch_id\n",
    "\n",
    "    ## END for curr_epoch_idx in np.arange(a_...\n",
    "    \n",
    "    included_epoch_ids = np.array(included_epoch_ids)\n",
    "    included_epoch_ids_dict[a_decoder_name] = included_epoch_ids\n",
    "    decoder_included_epoch_idx_ids_dict[a_decoder_name] = included_epoch_idx_ids_dict\n",
    "    ## OUTPUTS: a_ds, included_epoch_ids\n",
    "\n",
    "## END for a_decoder_name in included_epoch_names...\n",
    "## Look only at the entries in BOTH epochs (roam and sprinkle)\n",
    "# included_epoch_ids = list(included_epoch_ids_dict.values())\n",
    "included_epoch_ids = np.unique((list(set(list(included_epoch_ids_dict.values())[0]).intersection(*list(included_epoch_ids_dict.values())[1:]))))\n",
    "\n",
    "## Build the shared sync plotters for all runs\n",
    "# _out_container_new, sync_plotters = _build_attached_plotters_once(curr_active_pipeline=curr_active_pipeline, active_2d_plot=None)\n",
    "## OUTPUTS: sync_plotters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import DecodingLocalityMeasures, PredictiveDecoding, MatchingPastFuturePositionsResult, MatchingPastFuturePositionsResult, MaskDataSource\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import do_plot_and_export_past_future_all\n",
    "\n",
    "\n",
    "## get appropriate datasource based on epoch\n",
    "do_plot_and_export_past_future_all(curr_active_pipeline=curr_active_pipeline, _container_container=_container_container, decoder_flat_matching_results_list_ds_dict=decoder_flat_matching_results_list_ds_dict, \n",
    "\t\t\t\t\t\t\t\t\t\t   included_epoch_ids=included_epoch_ids, sync_plotters=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76801121",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_decoder_name, a_ds in decoder_flat_matching_results_list_ds_dict.items():\n",
    "    # a_ds: MaskDataSource  = decoder_flat_matching_results_list_ds_dict[a_decoder_name]\n",
    "    \n",
    "    # find included/excluded epochs ______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    # included_epoch_ids = []\n",
    "\n",
    "    for a_row in a_ds.filter_epochs.iterrows()\n",
    "        \n",
    "    # for curr_epoch_idx in np.arange(a_ds.num_epochs):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f59f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t_bin_size: float = 0.025\n",
    "# 1/24.0\n",
    "slo_mo_multiplier: float = 10.0 \n",
    "a_ds.filter_epochs['computed_num_frames'] = (np.ceil(a_ds.filter_epochs['duration'] / a_t_bin_size).astype(int) * slo_mo_multiplier).astype(int)\n",
    "a_ds.filter_epochs\n",
    "\n",
    "\n",
    "# np.nanmin(a_ds.filter_epochs['duration']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83d4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f41818c9",
   "metadata": {},
   "source": [
    "# 3D peak-finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51382a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = dict()\n",
    "_out['_display_3d_interactive_spike_and_behavior_browser'] = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatN',session_name='Day4OpenField',filter_name='roam')) # _display_3d_interactive_spike_and_behavior_browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1592e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvistaqt\n",
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractivePlaceCellDataExplorer import InteractivePlaceCellDataExplorer\n",
    "\n",
    "ipspikesDataExplorer: InteractivePlaceCellDataExplorer = _out['_display_3d_interactive_spike_and_behavior_browser']['ipspikesDataExplorer']\n",
    "# plotter = _out['plotter']\n",
    "ipspikesDataExplorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6188ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = dict()\n",
    "_out['_display_3d_interactive_custom_data_explorer'] = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=IdentifyingContext(format_name='bapun',animal='RatN',session_name='Day4OpenField',filter_name='roam')) # _display_3d_interactive_custom_data_explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15bb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0c4de",
   "metadata": {
    "tags": [
     "2026-02-17_pyvista"
    ]
   },
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import pyvistaqt as pvqt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyphoplacecellanalysis.Pho3D.PyVista.spikeAndPositions import perform_plot_flat_arena\n",
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.Mixins.InteractivePlotterMixins import InteractivePyvistaPlotterBuildIfNeededMixin\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter, DecoderRenderingPyVistaMixin\n",
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.PredictiveDecodingComputations import MaskDataSource\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "\n",
    "# active_session_configuration_context = IdentifyingContext(format_name='bapun',animal='RatN',session_name='Day4OpenField',filter_name='roam')\n",
    "active_session_configuration_context = curr_active_pipeline.filtered_contexts['roam']\n",
    "active_session_configuration_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a14d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm_df: pd.DataFrame = ensure_dataframe(curr_active_pipeline.sess.paradigm)[ensure_dataframe(curr_active_pipeline.sess.paradigm)['label'].isin(['roam', 'sprinkle'])]\n",
    "paradigm_df\n",
    "# paradigm_df['start'].min(), paradigm_df['stop'].max()\n",
    "\n",
    "# paradigm_df[paradigm_df['label'] == 'sprinkle']['start']\n",
    "\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=active_session_configuration_context,\n",
    "                                    params_kwargs=dict(should_use_linear_track_geometry=False, **{'t_start': paradigm_df['start'].min(), 't_delta': paradigm_df[paradigm_df['label'] == 'sprinkle']['start'], 't_end': paradigm_df['stop'].max()}),\n",
    "                                    )\n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "pActiveInteractiveLapsPlotter = _out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d52048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter, DecoderRenderingPyVistaMixin\n",
    "\n",
    "a_result = None\n",
    "a_decoder = _container_container.masked_container.pf1D_Decoder_dict['roam']\n",
    "a_result = _container_container.masked_container.most_recent_continuously_decoded_dict['roam']\n",
    "## OUTPUTS: a_decoder, a_result\n",
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = DecodedTrajectoryPyVistaPlotter(a_result=a_result, xbin=a_decoder.xbin, xbin_centers=a_decoder.xbin_centers, ybin=a_decoder.ybin, ybin_centers=a_decoder.ybin_centers, p=iplapsDataExplorer.p)\n",
    "# a_decoded_trajectory_pyvista_plotter.build_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: decoder_flat_matching_results_list_ds_dict\n",
    "for a_decoder_name, a_ds in decoder_flat_matching_results_list_ds_dict.items():\n",
    "    # a_ds: MaskDataSource  = decoder_flat_matching_results_list_ds_dict[a_decoder_name]\n",
    "    \n",
    "    # find included/excluded epochs ______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ #\n",
    "    # included_epoch_ids = []\n",
    "    a_ds.filter_epochs\n",
    "\n",
    "    # for a_row in a_ds.filter_epochs.iterrows()\n",
    "        \n",
    "    # for curr_epoch_idx in np.arange(a_ds.num_epochs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.Mixins.LapsVisualizationMixin import LapsVisualizationMixin\n",
    "\n",
    "## INPUTS: a_ds.epoch_t_bins_high_prob_pos_masks\n",
    "\n",
    "an_epoch_name: types.DecoderName = 'roam'\n",
    "a_ds: MaskDataSource = decoder_flat_matching_results_list_ds_dict[an_epoch_name]\n",
    "a_ds.filter_epochs['original_epoch_idx'] = a_ds.filter_epochs['original_epoch_idx'].astype(int)\n",
    "# n_masks: int = len(a_ds.epoch_t_bins_high_prob_pos_masks)\n",
    "n_epochs: int = len(a_ds.epoch_t_bins_high_prob_pos_masks)\n",
    "print(f'n_epochs: {n_epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a_ds.epoch_high_prob_pos_masks) # (65, 41, 62)\n",
    "a_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c772a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecoderRenderingPyVistaMixin\n",
    "\n",
    "an_epoch_idx = 4\n",
    "plots_data, plots = iplapsDataExplorer.plot_decoded_PBE_matching_past_future_results(a_ds=a_ds, an_epoch_idx = an_epoch_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66afb8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd994a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a highly-transparent plane with opaque edges at z=PBE_start_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_any_spline_literal(p, curr_lap_points, name=None, line_scalars=None, plot_data=None, **kwargs):\n",
    "    \"\"\" plots the position line \n",
    "    \n",
    "    num_lap_samples = np.shape(curr_lap_position_traces)[1]\n",
    "    curr_lap_points = np.column_stack((curr_lap_position_traces[0,:], curr_lap_position_traces[1,:], curr_lap_position_traces[2, :])) # (N, 3)\n",
    "    plot_data, plot = plot_any_spline_literal(p, curr_lap_points, name=None, line_scalars=None, plot_data=None, )\n",
    "    \n",
    "    \"\"\"\n",
    "    line = LapsVisualizationMixin.lines_from_points(curr_lap_points)\n",
    "    if line_scalars is not None:\n",
    "        line[\"scalars\"] = line_scalars\n",
    "    else:      \n",
    "        # Color by time (index) as per original implementation\n",
    "        line[\"scalars\"] = np.arange(line.n_points)\n",
    "\n",
    "    plot_data = (plot_data or {}) | {'name': name, 'curr_lap_points': curr_lap_points}\n",
    "\n",
    "    tube = line.tube(radius=0.1)\n",
    "    plot_data['tube'] = tube\n",
    "    \n",
    "    # Note: 'show_scalar_bar': False is set, so you won't see the legend unless changed to True\n",
    "    plot = p.add_mesh(tube, **({'name': name, 'render_lines_as_tubes': False, 'show_scalar_bar': False, 'lighting': False, 'render': False} | kwargs))\n",
    "    return plot_data, plot\n",
    "\n",
    "\n",
    "def plot_any_spline(p, curr_lap_position_traces, lap_start_z=0.9, time_to_z_range: float=10.0, name=None, color_by_speed=True, render_kwargs_dict=None, **kwargs):\n",
    "    \"\"\" plots the position line \"\"\"\n",
    "    num_lap_samples = np.shape(curr_lap_position_traces)[1]\n",
    "    curr_lap_points = np.column_stack((curr_lap_position_traces[0,:], curr_lap_position_traces[1,:], curr_lap_position_traces[2, :])) # (N, 3)\n",
    "\n",
    "    ts = deepcopy(curr_lap_points[:, 2])\n",
    "    earliest_t: float = np.nanmin(ts)\n",
    "    time_range: float = np.ptp(ts)\n",
    "    print(f'time_range: {time_range}')\n",
    "    time_axis_scaling_factor: float = time_to_z_range / time_range\n",
    "    print(f'time_axis_scaling_factor: {time_axis_scaling_factor}')\n",
    "    times_to_z_pos_fn = lambda ts: ((ts - earliest_t) * time_axis_scaling_factor) + lap_start_z\n",
    "    curr_lap_points[:, 2] = ((ts - earliest_t) * time_axis_scaling_factor) + lap_start_z # np.array([0.9, 0.900204, 0.900409, ..., 100.9, 100.9, 100.9])\n",
    "\n",
    "    line = LapsVisualizationMixin.lines_from_points(curr_lap_points)\n",
    "    if color_by_speed:\n",
    "        # Compute Speed along the path\n",
    "        # 1. Calculate differences between consecutive points (dx, dy)\n",
    "        dx = np.diff(curr_lap_position_traces[0, :])\n",
    "        dy = np.diff(curr_lap_position_traces[1, :])\n",
    "        \n",
    "        # 2. Compute Euclidean distance (speed proxy, assuming constant sampling rate)\n",
    "        # If sampling rate is not constant, you would divide this by dt (time delta)\n",
    "        speed = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # 3. Pad the array to match the number of points (diff reduces length by 1)\n",
    "        # We repeat the last speed value to maintain shape\n",
    "        speed = np.hstack((speed, speed[-1]))\n",
    "        line[\"scalars\"] = speed\n",
    "    else:\n",
    "        # Color by time (index) as per original implementation\n",
    "        line[\"scalars\"] = np.arange(line.n_points)\n",
    "\n",
    "    # if name is None:\n",
    "    #     plot_name = 'lap_location_trail_spline[{}]'.format(int(curr_lap_id))\n",
    "    # else:\n",
    "    #     plot_name = name\n",
    "\n",
    "    # trail_fade_values = np.linspace(0.0, 0.6, num_lap_samples)\n",
    "    # size_values = np.linspace(0.2, 0.6, num_lap_samples) # fade from a scale of 0.2 to 0.6\n",
    "\n",
    "    plot_data = {'name': name, 'times_to_z_pos_fn': times_to_z_pos_fn, 'time_range': time_range, 'earliest_t': earliest_t, 'num_lap_samples':num_lap_samples, 'curr_lap_position_traces': curr_lap_position_traces, 'curr_lap_points': curr_lap_points}\n",
    "    tube = line.tube(radius=0.1)\n",
    "    plot_data['tube'] = tube\n",
    "    \n",
    "    if (render_kwargs_dict is None) or (len(render_kwargs_dict) == 0):\n",
    "        # tube.plot(smooth_shading=True)\n",
    "        # color_map_name = 'bmy' # old\n",
    "        color_map_name = 'cividis' # 2023-05-09 and newer\n",
    "        # 'cmap': color_map_name, \n",
    "        kwargs['cmap'] = color_map_name\n",
    "    else:\n",
    "        for k, v in render_kwargs_dict.items():\n",
    "            kwargs[k] = v\n",
    "\n",
    "\n",
    "    # Note: 'show_scalar_bar': False is set, so you won't see the legend unless changed to True\n",
    "    plot = p.add_mesh(tube, **({'name': name, 'render_lines_as_tubes': False, 'show_scalar_bar': False, 'lighting': False, 'render': False} | kwargs))\n",
    "    return plot_data, plot\n",
    "\n",
    "\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "# BEGIN FUNCTION BODY                                                                                                                                                                                                                                                                  #\n",
    "# ==================================================================================================================================================================================================================================================================================== #\n",
    "plots = {}\n",
    "plots_data = {}\n",
    "\n",
    "plotter = pvqt.BackgroundPlotter()\n",
    "# axes = pv.CubeAxesActor(camera=plotter.camera)\n",
    "# axes.bounds = mesh.bounds\n",
    "# plotter.add_actor(axes)\n",
    "# axes = plotter.add_axes_at_origin()\n",
    "plotter.background_color = pv.Color('paraview')\n",
    "\n",
    "## INPUTS: a_ds\n",
    "\n",
    "curr_position_df: pd.DataFrame = deepcopy(a_ds.curr_position_df)\n",
    "\n",
    "# linear track geometry is not used to build the arena model, meaning for linear tracks it won't look as good as the geometry version.\n",
    "## The track shape will be approximated from the positions and the positions of the spikes:\n",
    "# plotter = self.p\n",
    "## INPUTS: plotter\n",
    "# x = self.x\n",
    "# y = self.y\n",
    "x = curr_position_df['x'].to_numpy()\n",
    "y = curr_position_df['y'].to_numpy()\n",
    "\n",
    "plots['maze_bg'] = perform_plot_flat_arena(plotter, x, y, bShowSequenceTraversalGradient=False, smoothing=False)\n",
    "plots_data['maze_bg'] = {'track_dims': None, 'maze_pdata': None}\n",
    "\n",
    "lap_start_z = 0.9 \n",
    "time_to_z_range = 100.0\n",
    "position_stop_z: float = lap_start_z + time_to_z_range\n",
    "render_kwargs_dict = {'color': [0.1, 0.1, 0.1], 'pbr': True, 'metallic': 0.8, 'roughness': 0.5, 'diffuse': 1, 'render': True}\n",
    "\n",
    "# _out = LapsVisualizationMixin.plot_lap_trajectory_path_spline(plotter, curr_lap_position_traces=xyt, curr_lap_id=-1, lap_start_z=0.9, lap_id_dependent_z_offset=0.45, color_by_speed=False)\n",
    "xyt = curr_position_df[['x', 'y', 't']].to_numpy().T # np.shape(xyt): (3, 489104)\n",
    "plot_data, plot_tube = plot_any_spline(plotter, curr_lap_position_traces=xyt, name='all_positions', lap_start_z=lap_start_z, time_to_z_range=time_to_z_range, color_by_speed=False, render_kwargs_dict=render_kwargs_dict) # , color='black'\n",
    "plots_data['all_positions'] = plot_data\n",
    "plots['all_positions'] = plot_tube\n",
    "\n",
    "times_to_z_pos_fn = plots_data['all_positions']['times_to_z_pos_fn']\n",
    "times_to_z_pos_fn\n",
    "\n",
    "def _cleanup_epoch_linear_idx_actors(plotter, plots_data=None, plots=None):\n",
    "    \"\"\"Removes the actors added by _on_update_epoch_linear_idx (PosteriorFilledContours and PBE path segments).\n",
    "    plots_data, plots = _cleanup_epoch_linear_idx_actors(plotter=plotter, plots_data=plots_data, plots=plots)\n",
    "    \"\"\"\n",
    "    if plots_data is None:\n",
    "        plots_data = {}\n",
    "    if plots is None:\n",
    "        plots = {}\n",
    "\n",
    "    # 1) Remove PosteriorFilledContours (structure: {'contours': {sub_name: actor, ...}})\n",
    "    contour_plot = plots.pop('PosteriorFilledContours', None)\n",
    "    plots_data.pop('PosteriorFilledContours', None)\n",
    "    if contour_plot is not None and isinstance(contour_plot, dict) and 'contours' in contour_plot:\n",
    "        for sub_name, actor in contour_plot['contours'].items():\n",
    "            if actor is not None:\n",
    "                try:\n",
    "                    plotter.remove_actor(actor)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # 2) Remove all PBE path segment actors (keys like PBE[0][past][0], PBE[0][future][1])\n",
    "    keys_to_remove = [k for k in plots if isinstance(k, str) and k.startswith('PBE[')]\n",
    "    for key in keys_to_remove:\n",
    "        actor_or_struct = plots.pop(key, None)\n",
    "        plots_data.pop(key, None)\n",
    "        if actor_or_struct is None:\n",
    "            continue\n",
    "        try:\n",
    "            if isinstance(actor_or_struct, dict) and 'main' in actor_or_struct:\n",
    "                plotter.remove_actor(actor_or_struct['main'])\n",
    "            else:\n",
    "                plotter.remove_actor(actor_or_struct)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return plots_data, plots\n",
    "\n",
    "def _on_update_epoch_linear_idx(plotter, a_ds, an_epoch_idx: int, times_to_z_pos_fn, plots_data=None, plots=None):\n",
    "    \"\"\" called to plot for a given PBE -- plots all potentially matching paths and such \"\"\"\n",
    "    ## INPUTS: an_epoch_idx\n",
    "    if plots_data is None:\n",
    "        plots_data = {}\n",
    "    if plots is None:\n",
    "        plots = {}\n",
    "\n",
    "    an_active_PBE_epoch_row = a_ds.filter_epochs.iloc[an_epoch_idx]\n",
    "    # an_active_PBE_epoch_row.label\n",
    "    # an_active_PBE_epoch_row.original_epoch_idx\n",
    "    \n",
    "    plot_seg_key: str = f'PBE[{an_active_PBE_epoch_row.original_epoch_idx}]'\n",
    "    \n",
    "    an_epoch_mask = a_ds.epoch_t_bins_high_prob_pos_masks[an_epoch_idx] \n",
    "    # an_epoch_mask ## an_epoch_mask.shape (41, 62, 5) - (n_x_bins, n_y_bins, n_t_bins)\n",
    "\n",
    "    xbin_centers = a_ds.xbin_centers\n",
    "    ybin_centers = a_ds.ybin_centers\n",
    "\n",
    "    # self.perform_update_plot_epoch_time_bin_range(self.curr_time_bin_index)\n",
    "    # active_plot_key: str = f'PBE[{an_active_PBE_epoch_row.original_epoch_idx}].PosteriorFilledContours'\n",
    "    active_plot_key: str = f'PosteriorFilledContours'\n",
    "    plots_data[active_plot_key], plots[active_plot_key] = DecoderRenderingPyVistaMixin.perform_plot_filled_contours(p=plotter, \n",
    "                                                                                    xbin_centers=xbin_centers, ybin_centers=ybin_centers,\n",
    "                                                                                    posterior_p_x_given_n=an_epoch_mask, levels=2, cmap='viridis', opacity=0.95, contour_extrude_z=position_stop_z, name=f'PosteriorFilledContours')\n",
    "\n",
    "    \n",
    "    a_row_out_dict = a_ds._prepare_epoch_data(an_epoch_idx=an_active_PBE_epoch_row.original_epoch_idx, minimum_included_matching_sequence_length=4) ## an_active_PBE_epoch_row.original_epoch_idx == an_epoch_idx right?\n",
    "    curr_matching_past_future_positions_df_dict: Dict[types.PastFutureCategory, Dict[types.epoch_index, pd.DataFrame]] = a_row_out_dict['curr_matching_past_future_positions_df_dict']\n",
    "    if 'past' not in curr_matching_past_future_positions_df_dict:\n",
    "        curr_matching_past_future_positions_df_dict['past'] = {}\n",
    "    if 'future' not in curr_matching_past_future_positions_df_dict:\n",
    "        curr_matching_past_future_positions_df_dict['future'] = {}\n",
    "    \n",
    "    for a_past_future_key, past_matching_position_df_dict in curr_matching_past_future_positions_df_dict.items():\n",
    "        # past_matching_position_df_dict = curr_matching_past_future_positions_df_dict.get('past', {})\n",
    "        for a_matched_position_segment_idx, a_matched_pos_segment_df in past_matching_position_df_dict.items():\n",
    "            ## do all the past items\n",
    "            a_matched_pos_segment_df['z'] = a_matched_pos_segment_df['t'].apply(times_to_z_pos_fn)\n",
    "            render_kwargs_dict = {'color': [0.9, 0.9, 0.9], 'pbr': True, 'metallic': 0.1, 'roughness': 0.8, 'diffuse': 1, 'render': True}\n",
    "            plot_seg_key: str = f'PBE[{an_active_PBE_epoch_row.original_epoch_idx}][{a_past_future_key}][{a_matched_position_segment_idx}]'\n",
    "            print(f'plotting: \"{plot_seg_key}\"')\n",
    "            # _out = LapsVisualizationMixin.plot_lap_trajectory_path_spline(plotter, curr_lap_position_traces=xyt, curr_lap_id=-1, lap_start_z=0.9, lap_id_dependent_z_offset=0.45, color_by_speed=False)\n",
    "            xyz = a_matched_pos_segment_df[['x', 'y', 'z']].to_numpy().T # np.shape(xyt): (3, 489104)\n",
    "            # plots_data[plot_seg_key], plots[plot_seg_key] = plot_any_spline(plotter, curr_lap_position_traces=xyz, name=plot_seg_key, color_by_speed=False, render_kwargs_dict=render_kwargs_dict) \n",
    "            curr_lap_points = np.column_stack((xyz[0,:], xyz[1,:], xyz[2, :])) # (N, 3)\n",
    "            plots_data[plot_seg_key], plots[plot_seg_key] = plot_any_spline_literal(p=plotter, curr_lap_points=curr_lap_points, name=plot_seg_key, line_scalars=None, plot_data=dict(), **render_kwargs_dict)\n",
    "        ## END for a_matched_position_segment_idx, a_...\n",
    "        \n",
    "    ## END for a_past_future_key, past_matching_position_df_dict in curr_matching_past_future_positions_df_dict.items():        \n",
    "    return plots_data, plots\n",
    "\n",
    "an_epoch_idx: int = 4\n",
    "plots_data, plots = _cleanup_epoch_linear_idx_actors(plotter=plotter, plots_data=plots_data, plots=plots)\n",
    "plots_data, plots = _on_update_epoch_linear_idx(plotter=plotter, a_ds=a_ds, an_epoch_idx=an_epoch_idx, times_to_z_pos_fn=times_to_z_pos_fn, plots_data=plots_data, plots=plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b290ff",
   "metadata": {
    "tags": [
     "2026-02-17_pyvista"
    ]
   },
   "outputs": [],
   "source": [
    "# plots\n",
    "plotter.show_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de18071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# InteractivePyvistaPlotterBuildIfNeededMixin.build\n",
    "\n",
    "# an_epoch_idx: int = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_ds.epoch_high_prob_pos_masks ## add the posterior\n",
    "# a_ds.matching_pos_epochs_dfs_list\n",
    "# a_ds.matching_pos_dfs_list\n",
    "len(a_ds.matching_pos_merged_segment_epochs_dfs_list) ## 65\n",
    "a_ds.num_epochs ### 65\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "matching_pos_merged_segment_epochs_dfs_list = a_ds.matching_pos_merged_segment_epochs_dfs_list\n",
    "# matching_pos_merged_segment_epochs_dfs_list\n",
    "\n",
    "for a_row in a_ds.filter_epochs.itertuples(index=True):\n",
    "    \n",
    "\n",
    "\n",
    "    a_row_out_dict = a_ds._prepare_epoch_data(an_epoch_idx=a_row.Index, minimum_included_matching_sequence_length=4)\n",
    "    curr_matching_past_future_positions_df_dict: Dict[types.PastFutureCategory, Dict[types.epoch_index, pd.DataFrame]] = a_row_out_dict['curr_matching_past_future_positions_df_dict']\n",
    "    if 'past' not in curr_matching_past_future_positions_df_dict:\n",
    "        curr_matching_past_future_positions_df_dict['past'] = {}\n",
    "    if 'future' not in curr_matching_past_future_positions_df_dict:\n",
    "        curr_matching_past_future_positions_df_dict['future'] = {}\n",
    "    \n",
    "    for a_past_future_key, past_matching_position_df_dict in curr_matching_past_future_positions_df_dict.items():\n",
    "        # past_matching_position_df_dict = curr_matching_past_future_positions_df_dict.get('past', {})\n",
    "        for a_matched_position_segment_idx, a_matched_pos_segment_df in past_matching_position_df_dict.items():\n",
    "            ## do all the past items\n",
    "            a_matched_pos_segment_df['z'] = a_matched_pos_segment_df['t'].apply(times_to_z_pos_fn)\n",
    "            render_kwargs_dict = {'color': [0.9, 0.9, 0.9], 'pbr': True, 'metallic': 0.8, 'roughness': 0.5, 'diffuse': 1, 'render': True}\n",
    "            plot_seg_key: str = f'PBE[{a_row.Index}][{a_past_future_key}][{a_matched_position_segment_idx}]'\n",
    "            print(f'plotting: \"{plot_seg_key}\"')\n",
    "            # _out = LapsVisualizationMixin.plot_lap_trajectory_path_spline(plotter, curr_lap_position_traces=xyt, curr_lap_id=-1, lap_start_z=0.9, lap_id_dependent_z_offset=0.45, color_by_speed=False)\n",
    "            xyz = a_matched_pos_segment_df[['x', 'y', 'z']].to_numpy().T # np.shape(xyt): (3, 489104)\n",
    "            # plots_data[plot_seg_key], plots[plot_seg_key] = plot_any_spline(plotter, curr_lap_position_traces=xyz, name=plot_seg_key, color_by_speed=False, render_kwargs_dict=render_kwargs_dict) \n",
    "            num_lap_samples = np.shape(xyz)[1]\n",
    "            curr_lap_points = np.column_stack((xyz[0,:], xyz[1,:], xyz[2, :])) # (N, 3)\n",
    "            plots_data[plot_seg_key], plots[plot_seg_key] = plot_any_spline_literal(p=plotter, curr_lap_points=curr_lap_points, name=plot_seg_key, line_scalars=None, plot_data=dict(), **render_kwargs_dict)\n",
    "        ## END for a_matched_position_segment_idx, a_...\n",
    "        \n",
    "    ## END for a_past_future_key, past_matching_position_df_dict in curr_matching_past_future_positions_df_dict.items():        \n",
    "\n",
    "# for an_epoch_idx, a_line_to_plot in enumerate(matching_pos_merged_segment_epochs_dfs_list):\n",
    "\t## plot the line\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac04c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_matching_past_future_positions_df_dict['past']\n",
    "\n",
    "list(curr_matching_past_future_positions_df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "\n",
    "# --- Step 1: Generate a dummy 3D boolean mask ---\n",
    "# (You would replace this with your actual boolean mask array)\n",
    "shape = (50, 50, 50)\n",
    "x, y, z = np.indices(shape)\n",
    "center = np.array(shape) / 2\n",
    "radius = 15\n",
    "\n",
    "# Create a boolean sphere: True inside, False outside\n",
    "boolean_mask = (x - center[0])**2 + (y - center[1])**2 + (z - center[2])**2 < radius**2\n",
    "\n",
    "# --- Step 2: Convert to PyVista Grid ---\n",
    "# Create a UniformGrid with the same dimensions as the mask\n",
    "grid = pv.UniformGrid()\n",
    "grid.dimensions = shape\n",
    "\n",
    "# IMPORTANT: PyVista expects numeric data for contouring. \n",
    "# Cast boolean (True/False) to float (1.0/0.0).\n",
    "# .flatten(order='F') is often needed if your array is in Fortran order, \n",
    "# but for standard C-contiguous numpy arrays, the default often works. \n",
    "# Explicitly setting order='F' ensures x/y/z axes map correctly in PyVista.\n",
    "grid.point_data[\"values\"] = boolean_mask.astype(float).flatten(order=\"F\") \n",
    "\n",
    "# --- Step 3: Extract the Isosurface (Contour) ---\n",
    "# We use 0.5 as the threshold between False (0) and True (1)\n",
    "contours = grid.contour([0.5])\n",
    "\n",
    "# --- Step 4: Plot with BackgroundPlotter ---\n",
    "plotter = pv.BackgroundPlotter()\n",
    "\n",
    "# Add the contours\n",
    "plotter.add_mesh(contours, color=\"teal\", opacity=0.8, smooth_shading=True)\n",
    "\n",
    "# Optional: Add a bounding box to see the volume context\n",
    "plotter.add_mesh(grid.outline(), color=\"black\")\n",
    "\n",
    "plotter.add_text(\"Boolean Mask Contour (Isosurface @ 0.5)\", font_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319df9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_epoch_idx: int = 0\n",
    "an_epoch_mask = a_ds.epoch_t_bins_high_prob_pos_masks[an_epoch_idx] \n",
    "an_epoch_mask ## an_epoch_mask.shape (41, 62, 5) - (n_x_bins, n_y_bins, n_t_bins)\n",
    "\n",
    "# --- 1. Setup & Configuration ---\n",
    "# (Using your provided dimensions)\n",
    "n_x_bins, n_y_bins, n_t_bins = an_epoch_mask.shape\n",
    "\n",
    "# Define the specific Z-plane height where all contours will render\n",
    "specified_z_plane = 15.0  \n",
    "\n",
    "# --- 2. Iterate, Extract, and Plot ---\n",
    "for t_idx in range(n_t_bins):\n",
    "    # Extract the 2D boolean mask for the current time bin\n",
    "    mask_2d = an_epoch_mask[:, :, t_idx]\n",
    "    \n",
    "    # Skip if the mask is empty (no True values) to avoid errors\n",
    "    if not np.any(mask_2d):\n",
    "        continue\n",
    "\n",
    "    # Create a 2D PyVista Grid for this slice\n",
    "    # Dimensions: (nx, ny, 1) to make it flat\n",
    "    grid = pv.UniformGrid()\n",
    "    grid.dimensions = (n_x_bins, n_y_bins, 1)\n",
    "    \n",
    "    # Position the grid on the specified Z-plane\n",
    "    # The origin is (x0, y0, z0)\n",
    "    grid.origin = (0.0, 0.0, specified_z_plane)\n",
    "    \n",
    "    # Assign the boolean data as floats, flattened in Fortran order\n",
    "    grid.point_data[\"values\"] = mask_2d.astype(float).flatten(order=\"F\")\n",
    "    \n",
    "    # Extract the contour outlines at the 0.5 threshold\n",
    "    contours = grid.contour([0.5])\n",
    "    \n",
    "    # Skip plotting if the mask is entirely False (no contours generated)\n",
    "    if contours.n_points == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get a distinct color from a matplotlib palette (e.g., 'tab10')\n",
    "    # This automatically handles grabbing a new color for each t_idx\n",
    "    color = plt.colormaps[\"tab10\"](t_idx)[:3] \n",
    "    \n",
    "    # Plot the 2D contour lines\n",
    "    plots['contours'] = plotter.add_mesh(\n",
    "        contours,\n",
    "        color=color,\n",
    "        line_width=3,          # Make the 2D lines thicker and easier to see\n",
    "        label=f\"Time {t_idx}\"  # Label for the legend\n",
    "    )\n",
    "\n",
    "plots_data['contours'] = contours\n",
    "\n",
    "\n",
    "# --- 3. Finalize Plotter ---\n",
    "plotter.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21182e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer \n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = iplapsDataExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fa9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
