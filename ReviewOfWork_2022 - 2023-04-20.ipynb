{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = find_first_extant_path([Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')])\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "tags": [
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load",
     "REQUIRED"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-07_11-26-53'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-08_14-26-15'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_3-23-37'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-12_15-55-31'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-13_14-42-6'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ## Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13e68b4-03e0-4388-a35c-87a352a6e6b3",
   "metadata": {
    "tags": [
     "load",
     "single_session",
     "REQUIRED"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\loadedSessPickle.pkl.\n",
      "property already present in pickled version. No need to save.\n",
      "using provided computation_functions_name_whitelist: ['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_time_dependent_pf_sequential_surprise_computation_perform_two_step_position_decoding_computation']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[1] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                          skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e403d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sane_midpoint_x: 119.48022705218361, hardcoded_track_midpoint_x: None, track_min_max_x: (22.736279243974774, 261.696733348342)\n",
      "hardcoded_track_midpoint_x is None, falling back to sane_midpoint_x... 119.48022705218361\n",
      "desc_crossings_x: (20,), asc_crossings_x: (20,)\n",
      "setting new computation epochs because laps changed.\n"
     ]
    }
   ],
   "source": [
    "from neuropy.analyses.placefields import PfND\n",
    "from PendingNotebookCode import constrain_to_laps\n",
    "from PendingNotebookCode import compute_short_long_constrained_decoders\n",
    "\n",
    "curr_active_pipeline = constrain_to_laps(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda48261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16389,) should be less than time_window_edges: (35888,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16389,) should be less than time_window_edges: (35888,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "self will be re-binned to match target_pf...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16389,) should be less than time_window_edges: (35888,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16389,) should be less than time_window_edges: (35888,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "self will be re-binned to match target_pf...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19183,) should be less than time_window_edges: (25983,)!\n"
     ]
    }
   ],
   "source": [
    "(long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "# 3m 40.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a896c17",
   "metadata": {},
   "source": [
    "# Fix persistance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_time_dependent_placefield_computation'], enabled_filter_names=[long_epoch_name, short_epoch_name, global_epoch_name], fail_on_exception=False, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04466ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['_perform_time_dependent_placefield_computation'], config_names_whitelist=[long_epoch_name, short_epoch_name, global_epoch_name], debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) # AttributeError: 'PfND_TimeDependent' object has no attribute '_included_thresh_neurons_indx'\n",
    "# TypeError: cannot pickle 'MplMultiTab' object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2112c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a92c8",
   "metadata": {},
   "source": [
    "# 2023-03-16 - Explore passing in long/short decoders specifically to `perform_full_session_leave_one_out_decoding_analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f22b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    }
   ],
   "source": [
    "from neuropy.core.session.dataSession import DataSession\n",
    "\n",
    "long_replays, short_replays, global_replays = [DataSession.perform_replace_session_replays_with_estimates(a_session, require_intersecting_epoch=None) for a_session in [long_session, short_session, global_session]] # , require_intersecting_epoch=None\n",
    "# long_replays, short_replays, global_replays = [a_session.replace_session_replays_with_estimates(require_intersecting_epoch=None, debug_print=False) for a_session in [long_session, short_session, global_session]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8e03e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.588736</td>\n",
       "      <td>44.829950</td>\n",
       "      <td>0</td>\n",
       "      <td>0.241213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.528196</td>\n",
       "      <td>61.662074</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.884600</td>\n",
       "      <td>65.113925</td>\n",
       "      <td>2</td>\n",
       "      <td>0.229325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.777701</td>\n",
       "      <td>72.951699</td>\n",
       "      <td>3</td>\n",
       "      <td>0.173998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.735138</td>\n",
       "      <td>78.048298</td>\n",
       "      <td>4</td>\n",
       "      <td>0.313160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1789.761459</td>\n",
       "      <td>1790.004332</td>\n",
       "      <td>65</td>\n",
       "      <td>0.242872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1797.036320</td>\n",
       "      <td>1797.120186</td>\n",
       "      <td>66</td>\n",
       "      <td>0.083866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1807.339864</td>\n",
       "      <td>1807.476845</td>\n",
       "      <td>67</td>\n",
       "      <td>0.136980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1886.125503</td>\n",
       "      <td>1886.315445</td>\n",
       "      <td>68</td>\n",
       "      <td>0.189942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1889.340196</td>\n",
       "      <td>1889.456502</td>\n",
       "      <td>69</td>\n",
       "      <td>0.116306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          start         stop label  duration\n",
       "0     44.588736    44.829950     0  0.241213\n",
       "1     61.528196    61.662074     1  0.133878\n",
       "2     64.884600    65.113925     2  0.229325\n",
       "3     72.777701    72.951699     3  0.173998\n",
       "4     77.735138    78.048298     4  0.313160\n",
       "..          ...          ...   ...       ...\n",
       "65  1789.761459  1790.004332    65  0.242872\n",
       "66  1797.036320  1797.120186    66  0.083866\n",
       "67  1807.339864  1807.476845    67  0.136980\n",
       "68  1886.125503  1886.315445    68  0.189942\n",
       "69  1889.340196  1889.456502    69  0.116306\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26804fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_full_session_leave_one_out_decoding_analysis\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import SurpriseAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import build_neurons_color_map # for plot_short_v_long_pf1D_comparison\n",
    "\n",
    "# Get existing long/short decoders from the cell under \"# 2023-02-24 Decoders\"\n",
    "long_decoder, short_decoder = deepcopy(long_one_step_decoder_1D), deepcopy(short_one_step_decoder_1D)\n",
    "assert np.all(long_decoder.xbin == short_decoder.xbin)\n",
    "\n",
    "## backup existing replay objects\n",
    "# long_session.replay_backup, short_session.replay_backup, global_session.replay_backup = [deepcopy(a_session.replay) for a_session in [long_session, short_session, global_session]]\n",
    "# null-out the replay objects\n",
    "# long_session.replay, short_session.replay, global_session.replay = [None, None, None]\n",
    "\n",
    "# Compute/estimate replays if missing from session:\n",
    "if not global_session.has_replays:\n",
    "    print(f'Replays missing from sessions. Computing replays...')\n",
    "    long_session.replay, short_session.replay, global_session.replay = [a_session.estimate_replay_epochs(require_intersecting_epoch=None, min_epoch_included_duration=0.06, max_epoch_included_duration=None, maximum_speed_thresh=None, min_inclusion_fr_active_thresh=0.01, min_num_unique_aclu_inclusions=3).to_dataframe() for a_session in [long_session, short_session, global_session]]\n",
    "\n",
    "### Need to prune to only the cells active in both epochs ahead of time:\n",
    "# Prune to the shared aclus in both epochs (short/long):\n",
    "long_shared_aclus_only_decoder, short_shared_aclus_only_decoder = [BasePositionDecoder.init_from_stateful_decoder(a_decoder) for a_decoder in (long_decoder, short_decoder)]\n",
    "shared_aclus, (long_shared_aclus_only_decoder, short_shared_aclus_only_decoder), long_short_pf_neurons_diff = BasePositionDecoder.prune_to_shared_aclus_only(long_shared_aclus_only_decoder, short_shared_aclus_only_decoder)\n",
    "n_neurons = len(shared_aclus)\n",
    "# for plotting purposes, build colors only for the common (present in both, the intersection) neurons:\n",
    "neurons_colors_array = build_neurons_color_map(n_neurons, sortby=None, cmap=None)\n",
    "print(f'{n_neurons = }, {neurons_colors_array.shape =}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for violating epochs in long_session.replay, short_session.replay\n",
    "from neuropy.core.epoch import Epoch\n",
    "import portion as P # Required for interval search: portion~=2.3.0\n",
    "test_replay_epochs = deepcopy(global_session.replay)\n",
    "# test_replay_epochs = deepcopy(short_session.replay)\n",
    "test_replay_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb590ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_session.ripple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45653ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_session.pbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2023-02-23 - PortionInterval approach to ensuring uniqueness:\n",
    "from neuropy.utils.efficient_interval_search import convert_PortionInterval_to_epochs_df, _convert_start_end_tuples_list_to_PortionInterval\n",
    "\n",
    "before_num_rows = test_replay_epochs.epochs.n_epochs        \n",
    "_intermediate_portion_interval = _convert_start_end_tuples_list_to_PortionInterval(zip(test_replay_epochs.epochs.starts, test_replay_epochs.epochs.stops))\n",
    "filtered_epochs = convert_PortionInterval_to_epochs_df(_intermediate_portion_interval)\n",
    "after_num_rows = np.shape(filtered_epochs)[0]\n",
    "changed_num_rows = after_num_rows - before_num_rows\n",
    "print(f'Dataframe Changed from {before_num_rows} -> {after_num_rows} ({changed_num_rows = })')\n",
    "filtered_epochs\n",
    "\n",
    "# _intermediate_portion_interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4912e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IDEA: flatten all the start, end times for all epochs into a flat list. Test each time against all intervals and record the indicies of the intervals that it falls into. That's the number of overlaps\n",
    "n_epochs = test_replay_epochs.epochs.n_epochs\n",
    "t_starts = test_replay_epochs.epochs.starts\n",
    "t_stops = test_replay_epochs.epochs.stops\n",
    "\n",
    "t_all = np.hstack((t_starts, t_stops))\n",
    "\n",
    "singleton_t_all = [P.singleton(t) for t in t_all]\n",
    "epochs_portion_intervals = [P.closed(start, stop) for start, stop in zip(t_starts, t_stops)]\n",
    "\n",
    "# P.from_data([(P.CLOSED, start, stop, P.CLOSED) for start, stop in start_end_tuples_list])\n",
    "# P.\n",
    "print(f'{n_epochs = }') # 1426\n",
    "assert len(t_all) == (n_epochs * 2)\n",
    "# timepoint_inclusion_intervals_dict = {a_t:[] for a_t in t_all} # the indicies corresponding to each point in a_t\n",
    "timepoint_inclusion_intervals_list = [] # [[] for a_t in t_all] # the indicies corresponding to each point in a_t\n",
    "\n",
    "for a_t, a_t_singleton in zip(t_all, singleton_t_all):\n",
    "\tcurr_is_included_in_interval = np.nonzero(np.array([an_interval.contains(a_t) for an_interval in epochs_portion_intervals]))\n",
    "\t# timepoint_inclusion_intervals_dict[a_t].extend(curr_is_included_in_interval)\n",
    "\ttimepoint_inclusion_intervals_list.append(curr_is_included_in_interval)\n",
    "timepoint_inclusion_intervals_list \n",
    "# 1.5s\n",
    "\n",
    "## Split back into separate lists for the start and end indicies:\n",
    "# _temp_index_list = list(timepoint_inclusion_intervals_dict.values())\n",
    "_temp_index_list = timepoint_inclusion_intervals_list\n",
    "_starts_index_lists = _temp_index_list[:n_epochs] # the [:N] notation doesn't include N, so it should be this instead of `(n_epochs-1)`\n",
    "_stops_index_lists = _temp_index_list[n_epochs:]\n",
    "\n",
    "\n",
    "\n",
    "assert len(_starts_index_lists) == len(_stops_index_lists)\n",
    "## Update the dataframe:\n",
    "test_replay_epochs['start_epoch_indicies'] = _starts_index_lists\n",
    "test_replay_epochs['stop_epoch_indicies'] = _stops_index_lists\n",
    "test_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22399ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_test_replay_epochs = test_replay_epochs.epochs.get_non_overlapping_df(debug_print=True)\n",
    "cleaned_test_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0eef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "Epoch.filter_epochs(test_replay_epochs, min_epoch_included_duration=1.0, max_epoch_included_duration=10.0, maximum_speed_thresh=None, min_num_unique_aclu_inclusions=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from viztracer import VizTracer\n",
    "\n",
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "\tlong_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=long_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_long', perform_cache_load=False) # , perform_cache_load=False\n",
    "\tshort_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=short_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_short', perform_cache_load=False) # , perform_cache_load=False\n",
    "\n",
    "# 4m 22.4s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69eed8a",
   "metadata": {},
   "source": [
    "## 2023-04-13 - Shuffled Surprise\n",
    "\"\"\" \n",
    "Relevant Functions:\n",
    "`perform_full_session_leave_one_out_decoding_analysis`:\n",
    "\t`perform_leave_one_aclu_out_decoding_analysis`:\tfrom pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_leave_one_aclu_out_decoding_analysis\n",
    "\t`_analyze_leave_one_out_decoding_results`: from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions import _analyze_leave_one_out_decoding_results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PendingNotebookCode import _new_compute_surprise, TimebinnedNeuronActivity\n",
    "# Distance metrics used by `_new_compute_surprise`\n",
    "# from scipy.spatial import distance # for Jensen-Shannon distance in `_subfn_compute_leave_one_out_analysis`\n",
    "# import random # for random.choice(mylist)\n",
    "# # from PendingNotebookCode import _scramble_curve\n",
    "# from scipy.stats import wasserstein_distance\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# def _compute_multi_surprise_metric(pf, p_x_given_n):\n",
    "# \treturn {'correlation': distance.correlation(pf, p_x_given_n),\n",
    "# \t\t'JSD': distance.jensenshannon(pf, p_x_given_n)}\n",
    "\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: distance.jensenshannon(pf, p_x_given_n)\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: distance.correlation(pf, p_x_given_n)\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: distance.sqeuclidean(pf, p_x_given_n)\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: wasserstein_distance(pf, p_x_given_n) # Figure out the correct function for this, it's in my old notebooks\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: pearsonr(pf, p_x_given_n)[0] # this returns just the correlation coefficient (R), not the p-value due to the [0]\n",
    "# active_surprise_metric_fn = _compute_multi_surprise_metric\n",
    "# long_results_obj, result, result_df, result_df_grouped = _new_compute_surprise(long_results_obj, active_surprise_metric_fn=active_surprise_metric_fn)\n",
    "\n",
    "# 2023-04-18 - Refactored into decoder_result\n",
    "result, result_df, result_df_grouped = long_results_obj.result, long_results_obj.result_df, long_results_obj.result_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c61634",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b860dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData, saveData\n",
    "\n",
    "### Build a folder to store the temporary outputs:\n",
    "output_data_folder = sess.get_output_path()\n",
    "\n",
    "cache_suffix = '_long'\n",
    "leave_one_out_result_pickle_path = output_data_folder.joinpath(f'leave_one_out_surprise_results{cache_suffix}.pkl').resolve()\n",
    "print(f'leave_one_out_result_pickle_path: {leave_one_out_result_pickle_path}')\n",
    "saveData(leave_one_out_result_pickle_path, (long_results_obj))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline._persistance_state\n",
    "curr_active_pipeline.pipeline_compare_dict\n",
    "curr_active_pipeline.persistance_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import DiagnosticDistanceMetricFigure\n",
    "\n",
    "## Render the internactive slider that allows selecting the timebin index to debug\n",
    "n_timebins = np.sum(long_results_obj.all_epochs_num_epoch_time_bins)\n",
    "timebinned_neuron_info = long_results_obj.timebinned_neuron_info\n",
    "result = long_results_obj.new_result\n",
    "active_fig_obj, update_function = DiagnosticDistanceMetricFigure.build_interactive_diagnostic_distance_metric_figure(long_results_obj, timebinned_neuron_info, result)\n",
    "active_fig_obj.integer_slider(n_timebins=n_timebins, update_func=update_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get set of cells active in a given time bin, for each compute the surprise of its placefield with the leave-one-out decoded posterior.\n",
    "\n",
    "# 2. From the remainder of cells (those not active), randomly choose one to grab the placefield of and compute the surprise with that and the same posterior.\n",
    "\n",
    "\n",
    "# Expectation: The cells that are included in the time bin are expected to have a lower surprise (be less correlated with) the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff50bc",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import build_pyqtgraph_epoch_indicator_regions # Add session indicators to pyqtgraph plot\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.MultiContextComparingDisplayFunctions import _helper_add_long_short_session_indicator_regions\n",
    "\n",
    "from pyphocorehelpers.print_helpers import generate_html_string\n",
    "# Long Short\n",
    "# TODO 2023-04-18 - Can Refactor in terms of `plot_long_short_any_values`?\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "# 'time_bin_indices': valid_time_bin_indicies, 'posterior_to_pf_mean_surprise': one_left_out_posterior_to_pf_surprises_mean, 'posterior_to_scrambled_pf_mean_surprise': one_left_out_posterior_to_scrambled_pf_surprises_mean}\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.MultiContextComparingDisplayFunctions import plot_long_short_expected_vs_observed_firing_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "win, plots_tuple, legend = plot_long_short_expected_vs_observed_firing_rates(long_results_obj, short_results_obj)\n",
    "long_epoch_indicator_region_items, short_epoch_indicator_region_items = _helper_add_long_short_session_indicator_regions(win, long_epoch=curr_active_pipeline.filtered_epochs[long_epoch_name], short_epoch=curr_active_pipeline.filtered_epochs[short_epoch_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4efa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_long_short_surprise_difference_plot(long_results_obj, short_results_obj):\n",
    "\t\"\"\" captures `curr_active_pipeline` \"\"\"\n",
    "\t# Private Subfunctions _______________________________________________________________________________________________ #\n",
    "\tdef _subfn_add_difference_plot_series(win, plots, result_df_grouped, series_suffix, **kwargs):\n",
    "\t\t\"\"\" captures nothing\n",
    "\t\tmodifies `plots` \"\"\"\n",
    "\t\tx=result_df_grouped.time_bin_centers.to_numpy()\n",
    "\t\ty=result_df_grouped['surprise_diff'].to_numpy()\n",
    "\t\tseries_id_str = f'difference_{series_suffix}'\n",
    "\t\tplots[series_id_str] = win.plot(x=x, y=y, name=series_id_str, alpha=0.5, **kwargs) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "\n",
    "\t# make a separate symbol_brush color for each cell:\n",
    "\t# cell_color_symbol_brush = [pg.intColor(i,hues=9, values=3, alpha=180) for i, aclu in enumerate(long_results_obj.original_1D_decoder.neuron_IDs)] # maxValue=128\n",
    "\t# All properties in common:\n",
    "\twin = pg.plot() # PlotWidget\n",
    "\twin.setWindowTitle('Long Sanity Check - Leave-one-out Custom Surprise Plot')\n",
    "\t# legend_size = (80,60) # fixed size legend\n",
    "\tlegend_size = None # auto-sizing legend to contents\n",
    "\tlegend = pg.LegendItem(legend_size, offset=(-1,0)) # do this instead of # .addLegend\n",
    "\tlegend.setParentItem(win.graphicsItem())\n",
    "\n",
    "\tplots = {}\n",
    "\tlabel_prefix_list = ['normal', 'scrambled']\n",
    "\tlong_short_symbol_list = ['t', 'o'] # note: 's' is a square. 'o', 't1': triangle pointing upwards0\n",
    "\n",
    "\t# Use mean time_bin and surprise for each epoch\n",
    "\t# plots['normal'] = win.plot(x=valid_time_bin_indicies, y=one_left_out_posterior_to_pf_surprises_mean, pen=None, symbol='t', symbolBrush=pg.intColor(1,6,maxValue=128), name=f'normal', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\t# plots['scrambled'] = win.plot(x=valid_time_bin_indicies, y=one_left_out_posterior_to_scrambled_pf_surprises_mean, pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), name=f'scrambled', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\t# curr_surprise_difference = one_left_out_posterior_to_scrambled_pf_surprises_mean - one_left_out_posterior_to_pf_surprises_mean\n",
    "\n",
    "\t# x=valid_time_bin_indicies\n",
    "\t# y=curr_surprise_difference\n",
    "\t# x=result_df_grouped.time_bin_indices.to_numpy()\n",
    "\n",
    "\t\n",
    "\t_subfn_add_difference_plot_series(win, plots, long_results_obj.result_df_grouped, series_suffix='_long', **dict(pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), clickable=True, hoverable=True, hoverSize=7))\n",
    "\n",
    "\t_subfn_add_difference_plot_series(win, plots, short_results_obj.result_df_grouped, series_suffix='_short', **dict(pen=None, symbol='o', symbolBrush=pg.intColor(3,6,maxValue=128), clickable=True))\n",
    "\n",
    "\t# dict(pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128))\n",
    "\n",
    "\n",
    "\t# x=result_df_grouped.time_bin_centers.to_numpy()\n",
    "\t# y=result_df_grouped['surprise_diff'].to_numpy()\n",
    "\t# plots['difference'] = win.plot(x=x, y=y, pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), name=f'difference', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\t# long_results_obj.result, long_results_obj.result_df, long_results_obj.result_df_grouped\n",
    "\n",
    "\t# short_results_obj.result, short_results_obj.result_df, short_results_obj.result_df_grouped\n",
    "\n",
    "\n",
    "\tfor k, v in plots.items():\n",
    "\t\tlegend.addItem(v, f'{k}')\n",
    "\n",
    "\twin.graphicsItem().setLabel(axis='left', text='Normal v. Random - Surprise (Custom)')\n",
    "\twin.graphicsItem().setLabel(axis='bottom', text='time')\n",
    "\n",
    "\twin.showGrid(True, True)  # Show grid for reference\n",
    "\n",
    "\t# Emphasize the y=0 crossing by drawing a horizontal line at y=0\n",
    "\tvline = pg.InfiniteLine(pos=0, angle=0, movable=False, pen=pg.mkPen(color='w', width=2, style=pg.QtCore.Qt.DashLine))\n",
    "\twin.addItem(vline)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Add session indicators to pyqtgraph plot\n",
    "\tlong_epoch = curr_active_pipeline.filtered_epochs[long_epoch_name]\n",
    "\tshort_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "\tlong_epoch_indicator_region_items, short_epoch_indicator_region_items = _helper_add_long_short_session_indicator_regions(win, long_epoch, short_epoch)\n",
    "\n",
    "\t# epoch_linear_region, epoch_region_label = build_pyqtgraph_epoch_indicator_regions(win, t_start=curr_active_pipeline.filtered_epochs[long_epoch_name].t_start, t_stop=curr_active_pipeline.filtered_epochs[long_epoch_name].t_stop, epoch_label='long', **dict(pen=pg.mkPen('#0b0049'), brush=pg.mkBrush('#0099ff42'), hoverBrush=pg.mkBrush('#fff400'), hoverPen=pg.mkPen('#00ff00')))\n",
    "\t# epoch_linear_region, epoch_region_label = build_pyqtgraph_epoch_indicator_regions(win, t_start=curr_active_pipeline.filtered_epochs[short_epoch_name].t_start, t_stop=curr_active_pipeline.filtered_epochs[short_epoch_name].t_stop, epoch_label='short', **dict(pen=pg.mkPen('#490000'), brush=pg.mkBrush('#f5161659'), hoverBrush=pg.mkBrush('#fff400'), hoverPen=pg.mkPen('#00ff00')))\n",
    "\n",
    "\ti_str = generate_html_string('i', color='white', bold=True)\n",
    "\tj_str = generate_html_string('j', color='red', bold=True)\n",
    "\ttitle_str = generate_html_string(f'JSD(p_x_given_n, pf[{i_str}]) - JSD(p_x_given_n, pf[{j_str}]) where {j_str} non-firing')\n",
    "\twin.setTitle(title_str)\n",
    "\n",
    "\twin.setWindowTitle('Long Sanity Check - Leave-one-out Custom Surprise Plot - JSD')\n",
    "\n",
    "\treturn win, plots\n",
    "\n",
    "win, plots = plot_long_short_surprise_difference_plot(long_results_obj, short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d937f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _helper_make_scatterplot_clickable(main_scatter_plot, enable_hover:bool=False):\n",
    "\t# Highlights the hovered spikes white:\n",
    "\t# main_scatter_plot.addPoints(hoverable=True,\n",
    "\t# \t# hoverSymbol=vtick, # hoverSymbol='s',\n",
    "\t# \thoverSize=7, # default is 5\n",
    "\t# \t)\n",
    "\n",
    "\n",
    "\t## Clickable/Selectable Spikes:\n",
    "\t# Will make all plots clickable\n",
    "\tclickedPen = pg.mkPen('#DDD', width=2)\n",
    "\tlastClicked = []\n",
    "\tdef _test_scatter_plot_clicked(plot, points):\n",
    "\t\tglobal lastClicked\n",
    "\t\tfor p in lastClicked:\n",
    "\t\t\tp.resetPen()\n",
    "\t\tprint(\"clicked points\", points)\n",
    "\t\tfor p in points:\n",
    "\t\t\tp.setPen(clickedPen)\n",
    "\t\tlastClicked = points\n",
    "\n",
    "\tmain_scatter_clicked_connection = main_scatter_plot.sigClicked.connect(_test_scatter_plot_clicked)\n",
    "\n",
    "\t## Hoverable Spikes:\n",
    "\tif enable_hover:\n",
    "\t\tdef _test_scatter_plot_hovered(plt, points, ev):\n",
    "\t\t\t# sigHovered(self, points, ev)\n",
    "\t\t\tprint(f'_test_scatter_plot_hovered(plt: {plt}, points: {points}, ev: {ev})')\n",
    "\t\t\tif (len(points) > 0):\n",
    "\t\t\t\tcurr_point = points[0]\n",
    "\t\t\t\t# self.\n",
    "\t\t\t\t# curr_point.index\n",
    "\t\tmain_scatter_hovered_connection = main_scatter_plot.sigHovered.connect(_test_scatter_plot_hovered)\n",
    "\telse:\n",
    "\t\tmain_scatter_hovered_connection = None\n",
    "\n",
    "\treturn lastClicked, clickedPen, (main_scatter_hovered_connection, main_scatter_clicked_connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a84888",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot = plots['difference__long']  # PlotDataItem \n",
    "a_curve = a_plot.curve # PlotCurveItem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot.sigPointsClicked\n",
    "# a_plot.sigPointsHovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe350d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve.curve.setClickable(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will make all plots clickable\n",
    "clickedPen = pg.mkPen('#DDD', width=2)\n",
    "lastClicked = []\n",
    "def _test_scatter_plot_clicked(plot, points):\n",
    "\tglobal lastClicked\n",
    "\tfor p in lastClicked:\n",
    "\t\tp.resetPen()\n",
    "\tprint(\"clicked points\", points)\n",
    "\tfor p in points:\n",
    "\t\tp.setPen(clickedPen)\n",
    "\tlastClicked = points\n",
    "\n",
    "main_scatter_clicked_connection = a_plot.sigClicked.connect(_test_scatter_plot_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastClicked, clickedPen, (main_scatter_hovered_connection, main_scatter_clicked_connection) = _helper_make_scatterplot_clickable(a_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d54370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to map from time_bin_indicies to times\n",
    "a_plot = plots['difference'] # PlotDataItem \n",
    "# a_plot.setLa\n",
    "# win.graphicsItem().setLabel(axis='left', text='Normal v. Random - Surprise (Custom)')\n",
    "\n",
    "# # Set the plot title with a LaTeX formula\n",
    "# title = pg.LabelItem(justify='center')\n",
    "# title.setText(r'<font size=\"4\">JSD Surprise Diff: $\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$</font>')\n",
    "# win.addItem(title)\n",
    "\n",
    "# win.graphicsItem().setLabel(axis='top', text=r'$\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot # works pretty well seemingly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_pyqtgraph_plot(win)\n",
    "# pg.setConfigOption('leftTitle', 'MathText')\n",
    "# win.setTitle(r'JSD(p_x_given_n, pf[<font size=\"4\"><b><span style=\"color:red;\">i</span></b></font>]) - JSD(p_x_given_n, pf[<font size=\"4\"><b>j</b></font>]) where <font size=\"4\"><b>j</b></font> non-firing')\n",
    "\n",
    "\n",
    "# win.setTitle(generate_html_string(f'JSD(p_x_given_n, pf[{i_str}]) - JSD(p_x_given_n, pf[{j_str}]) where {j_str} non-firing', font_size=8))\n",
    "\n",
    "# r'$\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$'\n",
    "# title_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d68ce1",
   "metadata": {},
   "source": [
    "$\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "win.setBackground\n",
    "win.setWindowTitle\n",
    "win.setBackgroundBrush\n",
    "win.setXRange\n",
    "win.setYRange\n",
    "\n",
    "\"Show X Grid\"\n",
    "\"Show Y Grid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_config = dict(pen=pg.mkPen('#fff'), brush=pg.mkBrush('#f004'), hoverBrush=pg.mkBrush('#fff4'), hoverPen=pg.mkPen('#f00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_epochs[short_epoch_name].t_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_epochs[long_epoch_name].start_end_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f996fd9",
   "metadata": {},
   "source": [
    "## Pre 2023-04-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Fresh (don't load from cache)\n",
    "long_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=long_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_long', perform_cache_load=False, skip_cache_save=False)\n",
    "short_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=short_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_short', perform_cache_load=False, skip_cache_save=False)\n",
    "# # (time_bins, neurons), (epochs, neurons), (epochs)\n",
    "# all_epochs_computed_one_left_out_posterior_to_pf_surprises, all_epochs_computed_cell_one_left_out_posterior_to_pf_surprises_mean, all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import plot_kourosh_activity_style_figure\n",
    "\n",
    "from neuropy.core.neurons import NeuronType\n",
    "# # Include only pyramidal aclus:\n",
    "# print(f'all shared_aclus: {len(shared_aclus)}\\nshared_aclus: {shared_aclus}')\n",
    "# shared_aclu_neuron_type = long_session.neurons.neuron_type[np.isin(long_session.neurons.neuron_ids, shared_aclus)]\n",
    "# assert len(shared_aclu_neuron_type) == len(shared_aclus)\n",
    "# # Find only the aclus that are pyramidal:\n",
    "# is_shared_aclu_pyramidal = (shared_aclu_neuron_type == NeuronType.PYRAMIDAL)\n",
    "# pyramidal_only_shared_aclus = shared_aclus[is_shared_aclu_pyramidal]\n",
    "# print(f'num pyramidal_only_shared_aclus: {len(pyramidal_only_shared_aclus)}\\npyramidal_only_shared_aclus: {pyramidal_only_shared_aclus}')\n",
    "\n",
    "\n",
    "## Drop Pyramidal but don't use only shared aclus:\n",
    "all_aclus = deepcopy(long_session.neurons.neuron_ids)\n",
    "neuron_type = long_session.neurons.neuron_type\n",
    "assert len(neuron_type) == len(all_aclus)\n",
    "# Find only the aclus that are pyramidal:\n",
    "is_aclu_pyramidal = (neuron_type == NeuronType.PYRAMIDAL)\n",
    "pyramidal_only_all_aclus = all_aclus[is_aclu_pyramidal]\n",
    "print(f'num pyramidal_only_all_aclus: {len(pyramidal_only_all_aclus)}\\npyramidal_only_all_aclus: {pyramidal_only_all_aclus}')\n",
    "\n",
    "# app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, shared_aclus, epoch_idx=5, callout_epoch_IDXs=[0,1,2,3], skip_rendering_callouts=True)\n",
    "# app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_shared_aclus, epoch_idx=2, callout_epoch_IDXs=[0,4], skip_rendering_callouts=False)\n",
    "app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_all_aclus, epoch_idx=3, callout_epoch_IDXs=[2,4,6], skip_rendering_callouts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea983fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_all_aclus, epoch_idx=11, callout_epoch_IDXs=[0,1,2, 3, 4, 5], skip_rendering_callouts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519e392",
   "metadata": {},
   "source": [
    "# 2023-04-13 Show Surprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.MultiContextComparingDisplayFunctions import plot_long_short, plot_long_short_any_values, plot_long_short_expected_vs_observed_firing_rates, _helper_add_long_short_session_indicator_regions\n",
    "# plot_long_short(long_results_obj, short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_long_short_any_values(long_results_obj=long_results_obj, short_results_obj=short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_long_short_expected_vs_observed_firing_rates(long_results_obj=long_results_obj, short_results_obj=short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fn = lambda a_results_obj: a_results_obj.all_epochs_decoded_epoch_time_bins_mean[:,0]\n",
    "# y_fn = lambda a_results_obj: a_results_obj.all_epochs_all_cells_one_left_out_posterior_to_scrambled_pf_surprises_mean\n",
    "y_fn = lambda a_results_obj: a_results_obj.all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean\n",
    "# y_fn = lambda a_results_obj: a_results_obj.all_epochs_computed_one_left_out_posterior_to_pf_surprises\n",
    "\n",
    "# (time_bins, neurons), (epochs, neurons), (epochs)\n",
    "# all_epochs_computed_one_left_out_posterior_to_pf_surprises, all_epochs_computed_cell_one_left_out_posterior_to_pf_surprises_mean, all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean\n",
    "win, plots_tuple, legend = plot_long_short_any_values(long_results_obj, short_results_obj, x=x_fn, y=y_fn, limit_aclus=[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe80559",
   "metadata": {},
   "source": [
    "# 2023-04-13 - Find Good looking epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(long_results_obj.active_filter_epochs, long_results_obj.all_included_filter_epochs_decoder_result, global_pos_df=global_session.position.df, variable_name='lin_pos', xbin=long_results_obj.original_1D_decoder.xbin,\n",
    "                                                        name='stacked_epoch_slices_long_results_obj', debug_print=True, debug_test_max_num_slices=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[23, 27, 29, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pagination support for `plot_decoded_epoch_slices`\n",
    "from pyphocorehelpers.indexing_helpers import compute_paginated_grid_config\n",
    "\n",
    "n_epochs = long_results_obj.active_filter_epochs.n_epochs\n",
    "subplot_no_pagination_configuration, included_combined_indicies_pages, page_grid_sizes = compute_paginated_grid_config(n_epochs, max_num_columns=1, max_subplots_per_page=32, data_indicies=result_df_grouped.index.to_numpy(), last_figure_subplots_same_layout=True)\n",
    "num_pages = len(included_combined_indicies_pages)\n",
    "num_pages\n",
    "included_epoch_indicies_pages = [[curr_included_epoch_index for (a_linear_index, curr_row, curr_col, curr_included_epoch_index) in v] for page_idx, v in enumerate(included_combined_indicies_pages)] # a list of length `num_pages` containing up to 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdb485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveWrapper import batch_extended_computations\n",
    "\n",
    "batch_extended_computations(curr_active_pipeline, include_whitelist=[], include_global_functions=False, fail_on_exception=True, progress_print=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a65b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.session_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e6526e9",
   "metadata": {},
   "source": [
    "# 2023-03-28 - Playing around with older computations/visualizations from the `_display_short_long_firing_rate_index_comparison` era:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2e9eb6",
   "metadata": {},
   "source": [
    "\n",
    "2023-04-20 - Encountered issue with the replays in session '2006-6-08_14-26-15' where they are duplicated exactly twice, like the first half of the rows are legitimate entries and the second half are directly repeated versions of the first with the only difference appearing to be the 'epoch_id' column changes from 1 to 2. 'rel_id' column seems incorrect but different for some reason. It must be how the MATLAB script exports the values.\n",
    "\n",
    "Also when I'm looking at only the `short_session.replay` there are many non-2 'epoch_id' values, which is strange. \n",
    "\n",
    "TODO: It could have something to do with Jonathan's code maybe? Because the 'replay_r' and 'replay_p' columns he added are different. SEEMS FALSE. It's this way even without running Jonathan's code, although the values might have been saved later?\n",
    "\tAlso flat_replay_idx jumps from 689 to 1087 at the transition from epoch_id 1 to 2\n",
    "\n",
    "UPDATE: the 'replay_r' and 'replay_p' columns aren't from Joanthan, they're in the original .replay_info.mat that's imported!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd397788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import create_daily_programmatic_display_function_testing_folder_if_needed, session_context_to_relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61605d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-01-* - Call extended computations to build `_display_short_long_firing_rate_index_comparison` figures:\n",
    "extended_computations_include_whitelist=['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis'] # do only specifiedl\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_whitelist=extended_computations_include_whitelist, include_global_functions=True, fail_on_exception=True, progress_print=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0da65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_computed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_short_long_firing_rate_analyses'], fail_on_exception=True, debug_print=False) # fail_on_exception MUST be True or error handling is all messed up \n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "x_frs_index, y_frs_index = long_short_fr_indicies_analysis_results['x_frs_index'], long_short_fr_indicies_analysis_results['y_frs_index'] # use the all_results_dict as the computed data value\n",
    "active_context = long_short_fr_indicies_analysis_results['active_context']\n",
    "long_short_fr_indicies_analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09070ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_frs_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot long|short firing rate index:\n",
    "# fig_save_parent_path = Path(r'E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Results from 2023-01-20 - LongShort Firing Rate Indicies')\n",
    "## Get the output path (active_session_figures_out_path) for this session (and all of its filtered_contexts as well):\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "figures_parent_out_path = create_daily_programmatic_display_function_testing_folder_if_needed()\n",
    "active_session_figures_out_path = session_context_to_relative_path(figures_parent_out_path, active_identifying_session_ctx)\n",
    "print(f'curr_session_parent_out_path: {active_session_figures_out_path}')\n",
    "active_session_figures_out_path.mkdir(parents=True, exist_ok=True) # make folder if needed\n",
    "curr_active_pipeline.display('_display_short_long_firing_rate_index_comparison', curr_active_pipeline.sess.get_context(), fig_save_parent_path=active_session_figures_out_path)\n",
    "# plt.close() # closes the current figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_extended_programmatic_figures(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_programmatic_figures(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the 2D placefields for my presentation\n",
    "from neuropy.core.neuron_identities import PlotStringBrevityModeEnum\n",
    "from neuropy.plotting.ratemaps import BackgroundRenderingOptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# '_display_2d_placefield_result_plot_ratemaps_2D'\n",
    "# %matplotlib inline\n",
    "%matplotlib qt\n",
    "# curr_active_pipeline.display('_display_2d_placefield_result_plot_ratemaps_2D', long_epoch_context) # MatplotlibRenderPlots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48029c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# long_pf2D.plot_occupancy()\n",
    "_out = short_one_step_decoder_2D.pf.plot_ratemaps_2D(included_unit_neuron_IDs=[2,4,5], bg_rendering_mode=BackgroundRenderingOptions.EMPTY, use_special_overlayed_title=False, missing_aclu_string_formatter=None, debug_print=False, brev_mode=PlotStringBrevityModeEnum.NONE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
