{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler\n",
    "\n",
    "BATCH_DATE_TO_USE = '2023-09-28' # used for filenames throught the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : /media/MAX/Data/global_batch_result_2023-09-28.pkl... done.\n",
      "Failure loading /media/MAX/Data/global_batch_result_2023-09-28.pkl.\n",
      "creating new batch_run\n",
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n",
      "local_session_names_list: ['2006-6-07_16-40-19', '2006-6-08_15-46-47', '2006-6-08_21-16-25', '2006-6-09_22-24-40', '2006-6-12_16-53-46', '2006-6-13_15-22-3']\n",
      "local_session_names_list: ['2006-4-09_17-29-30', '2006-4-10_12-25-50', '2006-4-10_21-2-40', '2006-4-11_15-16-59', '2006-4-12_14-39-31', '2006-4-12_17-53-55', '2006-4-16_15-12-23', '2006-4-17_12-33-47', '2006-4-18_13-6-1', '2006-4-18_15-23-32', '2006-4-19_13-34-40', '2006-4-19_16-48-9', '2006-4-21_10-24-35', '2006-4-25_14-28-51', '2006-4-25_17-17-6', '2006-4-26_13-22-13', '2006-4-27_14-43-12', '2006-4-28_12-17-27', '2006-4-28_16-48-29']\n",
      "local_session_names_list: ['2006-4-09_16-40-54', '2006-4-10_12-58-3', '2006-4-10_19-11-57', '2006-4-11_12-48-38', '2006-4-11_16-2-46', '2006-4-12_14-59-23', '2006-4-12_15-25-59', '2006-4-16_14-49-24', '2006-4-16_18-47-52', '2006-4-17_12-52-15', '2006-4-18_13-28-57', '2006-4-18_15-38-2', '2006-4-19_13-50-7', '2006-4-19_16-37-40', '2006-4-21_11-19-2', '2006-4-25_13-20-55', '2006-4-25_17-33-28', '2006-4-26_13-51-50', '2006-4-27_18-21-57', '2006-4-28_12-38-13', '2006-4-28_17-6-14']\n",
      "local_session_names_list: ['11-02_17-46-44', '11-02_19-28-0', '11-03_12-3-25', '11-03_21-26-8', '11-05_19-26-43', '11-09_11-43-50', '11-09_12-15-3', '11-09_21-17-16', '11-09_22-4-5', '11-19_12-35-59', '11-19_13-2-0', '11-19_13-55-7', 'fet11-01_12-58-54', 'fet11-03_11-0-53', 'fet11-03_20-28-3', 'fet11-04_21-20-3', 'redundant', 'showclus', 'sleep', 'tmaze']\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/global_batch_result_2023-09-28.pkl... done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 11:26:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 03:23:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20-3</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-04 21:20:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/redundant</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/showclus</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43   \n",
       "3   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67   /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20-3   \n",
       "68          /media/MAX/Data/KDIBA/pin01/one/redundant   \n",
       "69           /media/MAX/Data/KDIBA/pin01/one/showclus   \n",
       "70              /media/MAX/Data/KDIBA/pin01/one/sleep   \n",
       "71              /media/MAX/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   None 2006-06-07 11:26:53            0   \n",
       "1   SessionBatchProgress.NOT_STARTED   None 2006-06-08 14:26:15            0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None 2006-06-09 01:22:43            0   \n",
       "3   SessionBatchProgress.NOT_STARTED   None 2006-06-09 03:23:37            0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None 2006-06-12 15:55:31            0   \n",
       "..                               ...    ...                 ...          ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   None 2009-11-04 21:20:03            0   \n",
       "68  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "69  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "70  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "71  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "0                0             0                0     False   \n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "3                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "67               0             0                0     False   \n",
       "68               0             0                0     False   \n",
       "69               0             0                0     False   \n",
       "70               0             0                0     False   \n",
       "71               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3                                                       \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3                                                       \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data')] # , Path(r'/home/halechr/FastData')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "# try to load an existing batch result:\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    # display(good_only_batch_progress_df)\n",
    "    display(batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "included_session_batch_progress_df = batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    included_session_batch_progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71584edc",
   "metadata": {},
   "source": [
    "# Execute Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff419df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_included_session_contexts, output_python_scripts, output_slurm_scripts = global_batch_run.generate_batch_slurm_jobs(included_session_contexts, Path('output/generated_slurm_scripts/').resolve(), \n",
    "                                                                                                                        use_separate_run_directories=True, should_force_reload_all=False, should_perform_figure_generation_to_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "\n",
    "button_defns = [(str(a_ctxt), lambda _: reveal_in_system_file_manager(a_script)) for a_ctxt, a_script in zip(output_included_session_contexts, output_python_scripts)]\n",
    "button_executor = JupyterButtonRowWidget(button_defns=button_defns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bab22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_in_system_file_manager(output_python_scripts[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c8c22",
   "metadata": {},
   "source": [
    "## Working on redirecting stdout and print statements to file for batch scripts:\n",
    "https://code.activestate.com/recipes/119404/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ac428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import builtins as __builtin__\n",
    "# __builtins__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.programming_helpers import override_function_context\n",
    "\n",
    "import builtins\n",
    "\n",
    "# def custom_print(*args, **kwargs):\n",
    "# \tbuiltins.print(f'custom_print(...)\\n\\t')\n",
    "# \tbuiltins.print(*args, **kwargs)\n",
    "\n",
    "# def custom_print(*args, **kwargs):\n",
    "# \tbuiltins.print(f'custom_print(...)\\n\\t')\n",
    "# \tbuiltins.print(*args, **kwargs)\n",
    "\t\n",
    "# with override_function_context(builtins, \"print\", custom_print):\n",
    "# \tprint(5)\n",
    "# \tprint('hi!') # RecursionError: maximum recursion depth exceeded\n",
    "\n",
    "\n",
    "def test_fn():\n",
    "\tdef custom_print(*args, **kwargs):\n",
    "\t\tbuiltins.print(f'custom_print(...)\\n\\t')\n",
    "\t\tbuiltins.print(*args, **kwargs)\n",
    "\t\t\n",
    "\n",
    "\tcustom_print(5)\n",
    "\tcustom_print('hi!') # RecursionError: maximum recursion depth exceeded\n",
    "\n",
    "\n",
    "test_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae401cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "__builtins__.print(f'custom_print(...)\\n\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "# https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stdout\n",
    "\n",
    "with open('help.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "\t\tprint(5)\n",
    "\t\tprint('hi!') # RecursionError: maximum recursion depth exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_task_logger = build_batch_task_logger(session_context=curr_session_context) # create logger , file_logging_dir=\n",
    "_line_sweep = '=========================='\n",
    "## REPLACES THE `print` function within this scope\n",
    "def new_print(*args):\n",
    "\t# Call both regular print and logger.info\n",
    "\tprint(*args)\n",
    "\tcurr_task_logger.info(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ded20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_python_scripts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6ae279",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(included_session_contexts): 15\n",
      "Beginning processing with len(included_session_contexts): 15\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.one.2006-6-12_15-55-31\"):build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.one.2006-6-08_14-26-15\"):build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.one.2006-6-09_1-22-43\"):\n",
      "\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-08_14-26-15 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-08_14-26-15.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-09_1-22-43 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-09_1-22-43.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-12_15-55-31 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-12_15-55-31.log\n",
      "\n",
      "\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "========================== runBatch STARTING ==========================\tsession_context: kdiba_gor01_one_2006-6-08_14-26-15\n",
      "========================== runBatch STARTING ==========================\n",
      "\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\tsession_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15\n",
      "\n",
      "\tsession_context: kdiba_gor01_one_2006-6-12_15-55-31\tsession_context: kdiba_gor01_one_2006-6-09_1-22-43__________________________________________________________________\n",
      "\n",
      "\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31\tsession_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15\n",
      "\n",
      "____________________________________________________________________________________________________________________________________\n",
      "\n",
      "active_data_mode_name: kdiba\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      "\n",
      "\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl...active_data_mode_name: kdiba\n",
      " Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl...Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl...  done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891000503675347\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910301258809114\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.4950167366005063\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:590: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 107)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 9 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:09.724049\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "ERROR: encountered exception !! HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5' ::::: (<class 'tables.exceptions.HDF5ExtError'>, HDF5ExtError(\"Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'\"), <traceback object at 0x7f3714c49600>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5' ::::: (<class 'tables.exceptions.HDF5ExtError'>, HDF5ExtError(\"Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'\"), <traceback object at 0x7f3668f687c0>) while trying to build the session HDF output for kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-07_16-40-19\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-07_16-40-19 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-07_16-40-19.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 9 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:538: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 9)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: This REALLY should not happen! peak_y_bin_idx: 7, matching_vertical_scan_y_idxs: [7]!!\n",
      "  warn(f'\\tWARNING: This REALLY should not happen! peak_y_bin_idx: {peak_y_bin_idx}, matching_vertical_scan_y_idxs: {matching_vertical_scan_y_idxs}!!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43. error: !!  ::::: (<class 'NotImplementedError'>, NotImplementedError(), <traceback object at 0x7f36c0a342c0>)\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:02:34.400990\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_1-22-43'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-09_1-22-43/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-09_1-22-43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:596: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-08_21-16-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-08_21-16-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-08_21-16-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-08_21-16-25\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910392074055536\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910421009683105\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.49502338942683916\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910034299840243\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.49500190554668017\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910056377995829\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:596: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 9 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:538: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 9)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910002198517705\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8911445602437015\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.4950803112465008\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:519: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 9)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 9 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:56.253674\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_14-26-15'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-08_14-26-15/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-08_14-26-15...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:35.430218\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-07_16-40-19'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-07_16-40-19/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-09_22-24-40\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-09_22-24-40 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-09_22-24-40.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl... \t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-07_16-40-19...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:30.309405\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-12_16-53-46\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-12_16-53-46 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-12_16-53-46.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-12_16-53-46\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_21-16-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-08_21-16-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-08_21-16-25...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.one.2006-4-09_17-29-30\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-09_17-29-30 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-09_17-29-30.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8917783085379002\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:519: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:590: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 112)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:596: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:538: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 12 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910047310008246\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8917783085379002\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910085242505384\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8930266370537583\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.4961259094743102\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:12.311239\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_16-53-46'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8920156429657817\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:519: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:590: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 112)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:596: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:538: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.one.2006-4-10_12-25-50\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-10_12-25-50 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-10_12-25-50.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-10_12-25-50\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:07.702016\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_17-29-30'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5, with key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-09_17-29-30/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-09_17-29-30...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.two.2006-4-09_16-40-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-09_16-40-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-09_16-40-54.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:08.417695\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-25-50'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5, with key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/one/2006-4-10_12-25-50/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-10_12-25-50...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.two.2006-4-10_12-58-3\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-10_12-58-3 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-10_12-58-3.log\n",
      "========================== runBatch STARTING ==========================\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:06.403355\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_16-40-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5, with key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-09_16-40-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-09_16-40-54...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.11-02_17-46-44\"):\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_17-46-44 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_17-46-44.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_17-46-44\n",
      "\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:53.399438\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_22-24-40'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-09_22-24-40/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910012069598315\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.4950006705332397\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:590: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 114)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-09_22-24-40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.11-02_19-28-0\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_19-28-0 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_19-28-0.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_19-28-0\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:590: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 114)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8911438639794357\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.4950799244330198\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:08.745980\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-58-3'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5, with key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/vvp01/two/2006-4-10_12-58-3/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-10_12-58-3...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.11-03_12-3-25\"):\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-03_12-3-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-03_12-3-25.log========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_11-03_12-3-25\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl... done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910594497554136\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:11.285071\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_17-46-44'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5, with key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_17-46-44/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8917123037163892\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:07.478705\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_19-28-0'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5, with key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-02_19-28-0/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:530: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 10 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_17-46-44...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_19-28-0...\n",
      "ERROR: encountered exception !! The input must be a list of SpikeTrain ::::: (<class 'TypeError'>, TypeError('The input must be a list of SpikeTrain'), <traceback object at 0x7f370f90ea40>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_11-02_19-28-0]\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.fet11-01_12-58-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.fet11-01_12-58-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.fet11-01_12-58-54.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_fet11-01_12-58-54\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl... \t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "done.\n",
      "Loading pickled pipeline success: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "done.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'ratemap_peaks_prominence2d', 'long_short_endcap_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "pfdt_computation, maze already computed.\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:04.132515\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-03_12-3-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5, with key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/11-03_12-3-25/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-03_12-3-25...\n",
      "ERROR: encountered exception !! The input must be a list of SpikeTrain ::::: (<class 'TypeError'>, TypeError('The input must be a list of SpikeTrain'), <traceback object at 0x7f36650ffa40>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_11-03_12-3-25]\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.8910000898043593\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:417: UserWarning: n_contours is 0 for level: 0.4950000498913107\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:525: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "extended_stats, maze already computed.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "long_short_decoding_analyses, maze already computed.\n",
      "short_long_pf_overlap_analyses, maze already computed.\n",
      "long_short_fr_indicies_analyses, maze already computed.\n",
      "jonathan_firing_rate_analysis, maze already computed.\n",
      "long_short_post_decoding, maze already computed.\n",
      "long_short_endcap_analysis, maze already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('ratemap_peaks_prominence2d', 'maze'), ('pf_dt_sequential_surprise', 'maze')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:16.162118\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'fet11-01_12-58-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:231: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:237: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5, with key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/pin01/one/fet11-01_12-58-54/global_computations/expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_fet11-01_12-58-54...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "\n",
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=1)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=3)\n",
    "\n",
    "enable_full_pipeline_in_ram = False\n",
    "# enable_full_pipeline_in_ram = True\n",
    "\n",
    "# Whether to output figures:\n",
    "should_perform_figure_generation_to_file=False\n",
    "# should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "## Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[3:6]\n",
    "\n",
    "# ALL\n",
    "included_session_contexts = included_session_contexts\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "\n",
    "# No Reloading\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "                                                enable_full_pipeline_in_ram=enable_full_pipeline_in_ram,\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "# # Forced Reloading:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "active_post_run_callback_fn = result_handler.on_complete_success_execution_session\n",
    "# active_post_run_callback_fn = _temp_on_complete_success_execution_session\n",
    "\n",
    "\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "active_computation_functions_name_includelist=['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'position_decoding_two_step', \n",
    "                                               'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                                # 'long_short_inst_spike_rate_groups',\n",
    "                                                'pf_dt_sequential_surprise', 'long_short_endcap_analysis']\n",
    "\n",
    "\n",
    "# active_computation_functions_name_includelist=['_perform_baseline_placefield_computation']\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=active_post_run_callback_fn,\n",
    "                             fail_on_exception=True, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': active_computation_functions_name_includelist,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0981cde1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 11:26:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>40</td>\n",
       "      <td>279</td>\n",
       "      <td>40</td>\n",
       "      <td>224</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>46</td>\n",
       "      <td>179</td>\n",
       "      <td>40</td>\n",
       "      <td>142</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 03:23:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>55</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20-3</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-04 21:20:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/redundant</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/showclus</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43   \n",
       "3   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67   /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20-3   \n",
       "68          /media/MAX/Data/KDIBA/pin01/one/redundant   \n",
       "69           /media/MAX/Data/KDIBA/pin01/one/showclus   \n",
       "70              /media/MAX/Data/KDIBA/pin01/one/sleep   \n",
       "71              /media/MAX/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   None 2006-06-07 11:26:53            0   \n",
       "1     SessionBatchProgress.COMPLETED   None 2006-06-08 14:26:15           40   \n",
       "2     SessionBatchProgress.COMPLETED   None 2006-06-09 01:22:43           46   \n",
       "3   SessionBatchProgress.NOT_STARTED   None 2006-06-09 03:23:37            0   \n",
       "4     SessionBatchProgress.COMPLETED   None 2006-06-12 15:55:31           40   \n",
       "..                               ...    ...                 ...          ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   None 2009-11-04 21:20:03            0   \n",
       "68  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "69  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "70  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "71  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "0                0             0                0     False   \n",
       "1              279            40              224      True   \n",
       "2              179            40              142      True   \n",
       "3                0             0                0     False   \n",
       "4               37            34               55      True   \n",
       "..             ...           ...              ...       ...   \n",
       "67               0             0                0     False   \n",
       "68               0             0                0     False   \n",
       "69               0             0                0     False   \n",
       "70               0             0                0     False   \n",
       "71               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3                                                       \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3                                                       \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Across Sessions After Batching Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85e7111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5'),\n",
       " PosixPath('/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Undocumented: HDF5 files containing links to other external .h5 files. These work!\n",
    "\n",
    "included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_h5_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dfee55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for file path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table. wth exception HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "failed for file path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/long_short_fr_indicies_analysis/table. wth exception HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "failed for file path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table. wth exception HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "a_name: neuron_identities_table\n",
      "writing /home/halechr/repos/Spike3D/output/across_session_results/neuron_identities_table.csv.\n",
      "a_name: long_short_fr_indicies_analysis_table\n",
      "writing /home/halechr/repos/Spike3D/output/across_session_results/long_short_fr_indicies_analysis_table.csv.\n",
      "a_name: neuron_replay_stats_table\n",
      "writing /home/halechr/repos/Spike3D/output/across_session_results/neuron_replay_stats_table.csv.\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "AcrossSessionTables.save_out_to_combined_file(included_session_contexts, included_h5_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c4e7b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for file path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table. wth exception HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>long_pf_peak_x</th>\n",
       "      <th>short_pf_peak_x</th>\n",
       "      <th>track_membership</th>\n",
       "      <th>long_non_replay_mean</th>\n",
       "      <th>short_non_replay_mean</th>\n",
       "      <th>non_replay_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>short_num_replays</th>\n",
       "      <th>neuron_type</th>\n",
       "      <th>is_long_peak_left_cap</th>\n",
       "      <th>is_long_peak_right_cap</th>\n",
       "      <th>is_long_peak_either_cap</th>\n",
       "      <th>LS_pf_peak_x_diff</th>\n",
       "      <th>aclu</th>\n",
       "      <th>session_uid</th>\n",
       "      <th>neuron_uid</th>\n",
       "      <th>session_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>212.160000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>0.419960</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>-0.305237</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|2</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>136.160000</td>\n",
       "      <td>198.160000</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>0.275226</td>\n",
       "      <td>1.227759</td>\n",
       "      <td>0.952532</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>3</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|3</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.160000</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>0.117397</td>\n",
       "      <td>0.834271</td>\n",
       "      <td>0.716874</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|4</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>98.160000</td>\n",
       "      <td>202.160000</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>2.090985</td>\n",
       "      <td>1.844885</td>\n",
       "      <td>-0.246099</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-104.0</td>\n",
       "      <td>5</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|5</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>26.129319</td>\n",
       "      <td>12.892497</td>\n",
       "      <td>-13.236821</td>\n",
       "      <td>...</td>\n",
       "      <td>216</td>\n",
       "      <td>intr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|6</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>179.403791</td>\n",
       "      <td>177.403791</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>0.892852</td>\n",
       "      <td>1.027642</td>\n",
       "      <td>0.134789</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|28</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>59.403791</td>\n",
       "      <td>107.403791</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>0.837484</td>\n",
       "      <td>0.766268</td>\n",
       "      <td>-0.071216</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>pyr</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>29</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|29</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>119.403791</td>\n",
       "      <td>127.403791</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>0.712330</td>\n",
       "      <td>0.577649</td>\n",
       "      <td>-0.134681</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>30</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|30</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>217.403791</td>\n",
       "      <td>33.403791</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>0.902182</td>\n",
       "      <td>1.016473</td>\n",
       "      <td>0.114291</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>pyr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>184.0</td>\n",
       "      <td>31</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|31</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>63.403791</td>\n",
       "      <td>95.403791</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>0.946397</td>\n",
       "      <td>0.529446</td>\n",
       "      <td>-0.416951</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>pyr</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>32</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|32</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    format_name animal exper_name        session_name  long_pf_peak_x  \\\n",
       "0         kdiba  gor01        one  2006-6-08_14-26-15      212.160000   \n",
       "1         kdiba  gor01        one  2006-6-08_14-26-15      136.160000   \n",
       "2         kdiba  gor01        one  2006-6-08_14-26-15             NaN   \n",
       "3         kdiba  gor01        one  2006-6-08_14-26-15       98.160000   \n",
       "4         kdiba  gor01        one  2006-6-08_14-26-15             NaN   \n",
       "..          ...    ...        ...                 ...             ...   \n",
       "830       kdiba  pin01        one   fet11-01_12-58-54      179.403791   \n",
       "831       kdiba  pin01        one   fet11-01_12-58-54       59.403791   \n",
       "832       kdiba  pin01        one   fet11-01_12-58-54      119.403791   \n",
       "833       kdiba  pin01        one   fet11-01_12-58-54      217.403791   \n",
       "834       kdiba  pin01        one   fet11-01_12-58-54       63.403791   \n",
       "\n",
       "     short_pf_peak_x track_membership  long_non_replay_mean  \\\n",
       "0                NaN        LEFT_ONLY              0.419960   \n",
       "1         198.160000           SHARED              0.275226   \n",
       "2         178.160000       RIGHT_ONLY              0.117397   \n",
       "3         202.160000           SHARED              2.090985   \n",
       "4                NaN           SHARED             26.129319   \n",
       "..               ...              ...                   ...   \n",
       "830       177.403791           SHARED              0.892852   \n",
       "831       107.403791           SHARED              0.837484   \n",
       "832       127.403791           SHARED              0.712330   \n",
       "833        33.403791           SHARED              0.902182   \n",
       "834        95.403791           SHARED              0.946397   \n",
       "\n",
       "     short_non_replay_mean  non_replay_diff  ...  short_num_replays  \\\n",
       "0                 0.114724        -0.305237  ...                  0   \n",
       "1                 1.227759         0.952532  ...                 52   \n",
       "2                 0.834271         0.716874  ...                 67   \n",
       "3                 1.844885        -0.246099  ...                 72   \n",
       "4                12.892497       -13.236821  ...                216   \n",
       "..                     ...              ...  ...                ...   \n",
       "830               1.027642         0.134789  ...                 39   \n",
       "831               0.766268        -0.071216  ...                 43   \n",
       "832               0.577649        -0.134681  ...                 32   \n",
       "833               1.016473         0.114291  ...                 35   \n",
       "834               0.529446        -0.416951  ...                 23   \n",
       "\n",
       "     neuron_type  is_long_peak_left_cap  is_long_peak_right_cap  \\\n",
       "0            pyr                  False                   False   \n",
       "1            pyr                  False                   False   \n",
       "2            pyr                  False                   False   \n",
       "3            pyr                  False                   False   \n",
       "4           intr                  False                   False   \n",
       "..           ...                    ...                     ...   \n",
       "830          pyr                  False                   False   \n",
       "831          pyr                   True                   False   \n",
       "832          pyr                  False                   False   \n",
       "833          pyr                  False                   False   \n",
       "834          pyr                   True                   False   \n",
       "\n",
       "     is_long_peak_either_cap  LS_pf_peak_x_diff  aclu  \\\n",
       "0                      False                NaN     2   \n",
       "1                      False              -62.0     3   \n",
       "2                      False                NaN     4   \n",
       "3                      False             -104.0     5   \n",
       "4                      False                NaN     6   \n",
       "..                       ...                ...   ...   \n",
       "830                    False                2.0    28   \n",
       "831                     True              -48.0    29   \n",
       "832                    False               -8.0    30   \n",
       "833                    False              184.0    31   \n",
       "834                     True              -32.0    32   \n",
       "\n",
       "                            session_uid                            neuron_uid  \\\n",
       "0    kdiba|gor01|one|2006-6-08_14-26-15  kdiba|gor01|one|2006-6-08_14-26-15|2   \n",
       "1    kdiba|gor01|one|2006-6-08_14-26-15  kdiba|gor01|one|2006-6-08_14-26-15|3   \n",
       "2    kdiba|gor01|one|2006-6-08_14-26-15  kdiba|gor01|one|2006-6-08_14-26-15|4   \n",
       "3    kdiba|gor01|one|2006-6-08_14-26-15  kdiba|gor01|one|2006-6-08_14-26-15|5   \n",
       "4    kdiba|gor01|one|2006-6-08_14-26-15  kdiba|gor01|one|2006-6-08_14-26-15|6   \n",
       "..                                  ...                                   ...   \n",
       "830   kdiba|pin01|one|fet11-01_12-58-54  kdiba|pin01|one|fet11-01_12-58-54|28   \n",
       "831   kdiba|pin01|one|fet11-01_12-58-54  kdiba|pin01|one|fet11-01_12-58-54|29   \n",
       "832   kdiba|pin01|one|fet11-01_12-58-54  kdiba|pin01|one|fet11-01_12-58-54|30   \n",
       "833   kdiba|pin01|one|fet11-01_12-58-54  kdiba|pin01|one|fet11-01_12-58-54|31   \n",
       "834   kdiba|pin01|one|fet11-01_12-58-54  kdiba|pin01|one|fet11-01_12-58-54|32   \n",
       "\n",
       "       session_datetime  \n",
       "0   2006-06-08 14:26:15  \n",
       "1   2006-06-08 14:26:15  \n",
       "2   2006-06-08 14:26:15  \n",
       "3   2006-06-08 14:26:15  \n",
       "4   2006-06-08 14:26:15  \n",
       "..                  ...  \n",
       "830 2009-11-01 12:58:54  \n",
       "831 2009-11-01 12:58:54  \n",
       "832 2009-11-01 12:58:54  \n",
       "833 2009-11-01 12:58:54  \n",
       "834 2009-11-01 12:58:54  \n",
       "\n",
       "[835 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "neuron_replay_stats_table = AcrossSessionTables.build_neuron_replay_stats_table(included_session_contexts, included_h5_paths)\n",
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d03d544c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for file path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/long_short_fr_indicies_analysis/table. wth exception HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>index</th>\n",
       "      <th>neuron_uid</th>\n",
       "      <th>session_uid</th>\n",
       "      <th>aclu</th>\n",
       "      <th>x_frs_index</th>\n",
       "      <th>y_frs_index</th>\n",
       "      <th>session_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>0</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|2</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.947513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>1</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|3</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.855006</td>\n",
       "      <td>-0.544194</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|4</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.972594</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>3</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|5</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.133418</td>\n",
       "      <td>-0.032454</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>4</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|6</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.433462</td>\n",
       "      <td>0.454120</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>26</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|28</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.567445</td>\n",
       "      <td>-0.172839</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>27</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|29</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>29</td>\n",
       "      <td>-0.176580</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>28</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|30</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.219618</td>\n",
       "      <td>-0.072261</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>29</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|31</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>31</td>\n",
       "      <td>0.314634</td>\n",
       "      <td>0.160301</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>30</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|32</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>32</td>\n",
       "      <td>0.467246</td>\n",
       "      <td>0.233702</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    format_name animal exper_name        session_name  index  \\\n",
       "0         kdiba  gor01        one  2006-6-08_14-26-15      0   \n",
       "1         kdiba  gor01        one  2006-6-08_14-26-15      1   \n",
       "2         kdiba  gor01        one  2006-6-08_14-26-15      2   \n",
       "3         kdiba  gor01        one  2006-6-08_14-26-15      3   \n",
       "4         kdiba  gor01        one  2006-6-08_14-26-15      4   \n",
       "..          ...    ...        ...                 ...    ...   \n",
       "830       kdiba  pin01        one   fet11-01_12-58-54     26   \n",
       "831       kdiba  pin01        one   fet11-01_12-58-54     27   \n",
       "832       kdiba  pin01        one   fet11-01_12-58-54     28   \n",
       "833       kdiba  pin01        one   fet11-01_12-58-54     29   \n",
       "834       kdiba  pin01        one   fet11-01_12-58-54     30   \n",
       "\n",
       "                               neuron_uid                         session_uid  \\\n",
       "0    kdiba|gor01|one|2006-6-08_14-26-15|2  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "1    kdiba|gor01|one|2006-6-08_14-26-15|3  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "2    kdiba|gor01|one|2006-6-08_14-26-15|4  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "3    kdiba|gor01|one|2006-6-08_14-26-15|5  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "4    kdiba|gor01|one|2006-6-08_14-26-15|6  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "..                                    ...                                 ...   \n",
       "830  kdiba|pin01|one|fet11-01_12-58-54|28   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "831  kdiba|pin01|one|fet11-01_12-58-54|29   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "832  kdiba|pin01|one|fet11-01_12-58-54|30   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "833  kdiba|pin01|one|fet11-01_12-58-54|31   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "834  kdiba|pin01|one|fet11-01_12-58-54|32   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "\n",
       "     aclu  x_frs_index  y_frs_index    session_datetime  \n",
       "0       2     0.947513     1.000000 2006-06-08 14:26:15  \n",
       "1       3    -0.855006    -0.544194 2006-06-08 14:26:15  \n",
       "2       4    -0.972594    -1.000000 2006-06-08 14:26:15  \n",
       "3       5     0.133418    -0.032454 2006-06-08 14:26:15  \n",
       "4       6     0.433462     0.454120 2006-06-08 14:26:15  \n",
       "..    ...          ...          ...                 ...  \n",
       "830    28    -0.567445    -0.172839 2009-11-01 12:58:54  \n",
       "831    29    -0.176580     0.089972 2009-11-01 12:58:54  \n",
       "832    30    -0.219618    -0.072261 2009-11-01 12:58:54  \n",
       "833    31     0.314634     0.160301 2009-11-01 12:58:54  \n",
       "834    32     0.467246     0.233702 2009-11-01 12:58:54  \n",
       "\n",
       "[835 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_short_fr_indicies_analysis_table = AcrossSessionTables.build_long_short_fr_indicies_analysis_table(included_session_contexts, included_h5_paths)\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d9b4a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for file path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table. wth exception HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_uid</th>\n",
       "      <th>session_uid</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>neuron_id</th>\n",
       "      <th>neuron_type</th>\n",
       "      <th>cluster_index</th>\n",
       "      <th>qclu</th>\n",
       "      <th>shank_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|2</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2</td>\n",
       "      <td>pyr</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|3</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>3</td>\n",
       "      <td>pyr</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|4</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>4</td>\n",
       "      <td>pyr</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|5</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>5</td>\n",
       "      <td>pyr</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15|6</td>\n",
       "      <td>kdiba|gor01|one|2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>6</td>\n",
       "      <td>intr</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|28</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>28</td>\n",
       "      <td>pyr</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|29</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>29</td>\n",
       "      <td>pyr</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|30</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>30</td>\n",
       "      <td>pyr</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|31</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>31</td>\n",
       "      <td>pyr</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54|32</td>\n",
       "      <td>kdiba|pin01|one|fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>32</td>\n",
       "      <td>pyr</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               global_uid                         session_uid  \\\n",
       "0    kdiba|gor01|one|2006-6-08_14-26-15|2  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "1    kdiba|gor01|one|2006-6-08_14-26-15|3  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "2    kdiba|gor01|one|2006-6-08_14-26-15|4  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "3    kdiba|gor01|one|2006-6-08_14-26-15|5  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "4    kdiba|gor01|one|2006-6-08_14-26-15|6  kdiba|gor01|one|2006-6-08_14-26-15   \n",
       "..                                    ...                                 ...   \n",
       "830  kdiba|pin01|one|fet11-01_12-58-54|28   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "831  kdiba|pin01|one|fet11-01_12-58-54|29   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "832  kdiba|pin01|one|fet11-01_12-58-54|30   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "833  kdiba|pin01|one|fet11-01_12-58-54|31   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "834  kdiba|pin01|one|fet11-01_12-58-54|32   kdiba|pin01|one|fet11-01_12-58-54   \n",
       "\n",
       "       session_datetime format_name animal exper_name        session_name  \\\n",
       "0   2006-06-08 14:26:15       kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "1   2006-06-08 14:26:15       kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2   2006-06-08 14:26:15       kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "3   2006-06-08 14:26:15       kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "4   2006-06-08 14:26:15       kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "..                  ...         ...    ...        ...                 ...   \n",
       "830 2009-11-01 12:58:54       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "831 2009-11-01 12:58:54       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "832 2009-11-01 12:58:54       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "833 2009-11-01 12:58:54       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "834 2009-11-01 12:58:54       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "     neuron_id neuron_type  cluster_index  qclu  shank_index  \n",
       "0            2         pyr              6     2            1  \n",
       "1            3         pyr              9     4            1  \n",
       "2            4         pyr             10     4            1  \n",
       "3            5         pyr             11     2            1  \n",
       "4            6        intr             12     5            1  \n",
       "..         ...         ...            ...   ...          ...  \n",
       "830         28         pyr             10     2            3  \n",
       "831         29         pyr             11     2            3  \n",
       "832         30         pyr             14     2            3  \n",
       "833         31         pyr              3     4            4  \n",
       "834         32         pyr              6     2            4  \n",
       "\n",
       "[835 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_identities_table = AcrossSessionTables.build_neuron_identities_table(included_session_contexts, included_h5_paths)\n",
    "neuron_identities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f51155d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_name: neuron_identities_table\n",
      "a_name: long_short_fr_indicies_analysis_table\n",
      "a_name: neuron_replay_stats_table\n"
     ]
    }
   ],
   "source": [
    "## Save converted back to .h5 file, .csv file, and several others\n",
    "\n",
    "across_session_outputs = {'output/across_session_results/neuron_identities_table': neuron_identities_table,\n",
    " 'output/across_session_results/long_short_fr_indicies_analysis_table': long_short_fr_indicies_analysis_table,\n",
    "'output/across_session_results/neuron_replay_stats_table': neuron_replay_stats_table}\n",
    "\n",
    "\n",
    "for k, v in across_session_outputs.items():\n",
    "    k = Path(k)\n",
    "    a_name = k.name\n",
    "    print(f'a_name: {a_name}')\n",
    "    v.to_csv(k.with_suffix(suffix='.csv'))\n",
    "    # v.to_hdf(k, key=f'/{a_name}')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08dea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_axes()\n",
    "ax0.plot(\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a3da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table = AcrossSessionTables.build_long_short_fr_indicies_analysis_table(included_session_contexts, included_h5_paths)\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4749ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neuron_identities_table = AcrossSessionTables.build_neuron_identities_table(included_session_contexts, included_h5_paths)\n",
    "neuron_identities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06675f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neuron_replay_stats_table = AcrossSessionTables.build_neuron_replay_stats_table(included_session_contexts, included_h5_paths)\n",
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09034c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013a234",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Alluvial Flow Diagrams with `floweaver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import floweaver as fw\n",
    "\n",
    "# How many of each track_membership type in each session?\n",
    "\n",
    "# TAKES `_out_value_counts_df`\n",
    "df = deepcopy(_out_value_counts_df)\n",
    "\n",
    "## Make a floweaver Dataset out of it\n",
    "dataset = fw.Dataset(df)\n",
    "\n",
    "partition_animal = dataset.partition('animal') # equivalent to `fw.Partition.Simple('animal', np.unique(_out_value_counts_df['animal']))`\n",
    "partition_session = dataset.partition(session_identifier_key)\n",
    "partition_track_membership = dataset.partition('track_membership')\n",
    "\n",
    "nodes = {\n",
    "    'animal': fw.ProcessGroup(['animal'], partition_animal, title='Animal'), # fw.ProcessGroup(['gor01', 'pin01', 'vvp01']\n",
    "    'track': fw.ProcessGroup(['track_membership'], partition_track_membership, title='TrackXclusivity'), # fw.ProcessGroup(['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED'], title='TrackXclusivity')\n",
    "}\n",
    "\n",
    "ordering = [\n",
    "    ['animal'],  # put \"animal\" on the left...\n",
    "    ['track'],   # ... and \"track\" on the right.\n",
    "]\n",
    "\n",
    "bundles = [\n",
    "    fw.Bundle('animal', 'track'),\n",
    "]\n",
    "\n",
    "# partitions = [\n",
    "#     fw.Partition.Simple('process', ['gor01', 'pin01', 'vvp01']),\n",
    "#     fw.Partition.Simple('process', ['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED']),\n",
    "# ]\n",
    "\n",
    "# nodes['animal'].partition = partitions[0]\n",
    "# nodes['track'].partition = partitions[1]\n",
    "\n",
    "sdd = fw.SankeyDefinition(nodes, bundles, ordering) # fw.SankeyDefinition(nodes, bundles, ordering, flow_partition=fw.Partition.Simple('process', []))\n",
    "\n",
    "\n",
    "## This is key where the ['source', 'target', 'value'] come in:\n",
    "# flow_data = df.groupby(['animal', 'track_membership'])['count'].sum().reset_index()\n",
    "# flow_data.columns = ['source', 'target', 'value']\n",
    "\n",
    "size_options = dict(width=500, height=400, margins=dict(left=100, right=100))\n",
    "# fw.weave(sdd, dataset, palette='Set2_8').to_widget().auto_save_png('alluvial_diagram.png')\n",
    "fw.weave(sdd, dataset, measures='count').to_widget(**size_options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is key where the ['source', 'target', 'value'] come in:\n",
    "flow_data = df.groupby(['animal', 'track_membership'])['count'].sum().reset_index()\n",
    "flow_data\n",
    "# flow_data.columns = ['source', 'target', 'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54097de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import floweaver as fw\n",
    "\n",
    "\n",
    "df = deepcopy(_out_value_counts_df)\n",
    "\n",
    "nodes = {\n",
    "    'animal': fw.ProcessGroup(['gor01', 'pin01', 'vvp01'], title='Animal'),\n",
    "    'track': fw.ProcessGroup(['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED'], title='TrackXclusivity'),\n",
    "}\n",
    "\n",
    "ordering = [\n",
    "    ['animal'],  # put \"animal\" on the left...\n",
    "    ['track'],   # ... and \"track\" on the right.\n",
    "]\n",
    "\n",
    "bundles = [\n",
    "    fw.Bundle('animal', 'track'),\n",
    "]\n",
    "\n",
    "partitions = [\n",
    "    fw.Partition.Simple('process', ['gor01', 'pin01', 'vvp01']),\n",
    "    fw.Partition.Simple('process', ['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED']),\n",
    "]\n",
    "\n",
    "nodes['animal'].partition = partitions[0]\n",
    "nodes['track'].partition = partitions[1]\n",
    "sdd = fw.SankeyDefinition(nodes, bundles, ordering, flow_partition=fw.Partition.Simple('process', []))\n",
    "\n",
    "flow_data = df.groupby(['animal', 'track_membership'])['count'].sum().reset_index()\n",
    "flow_data.columns = ['source', 'target', 'value']\n",
    "fw.weave(sdd, flow_data, palette='Set2_8').to_widget().auto_save_png('alluvial_diagram.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd585a",
   "metadata": {},
   "source": [
    "#### Three-item flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f496b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import floweaver as fw\n",
    "\n",
    "\n",
    "\n",
    "def plot_alluvial_diagram(df: pd.DataFrame, session_identifier_key:str='session_datetime'):\n",
    "\t\"\"\" 2023-08-24 - Renders a flow diagram showing the breakdown of LxC, SxC, Shared neurons in each session for each animal. \n",
    "\n",
    "\tUsage:\n",
    "\t\talluvial_widget, sdd, flow_data = plot_alluvial_diagram(_out_value_counts_df, session_identifier_key=session_identifier_key)\n",
    "\t\talluvial_widget\n",
    "\n",
    "\t\"\"\"\n",
    "\tdf = df.copy()\n",
    "\t\n",
    "\t## Somehow I want to get a 'type' key through this process and into the final flow_data DataFrame so that I can color based on it as mentioned in this demo: https://notebooks.gesis.org/binder/jupyter/user/ricklupton-floweaver-586o9ybx/notebooks/docs/tutorials/quickstart.ipynb#\n",
    "\t\n",
    "\t# Define the flow from \"animal\" to \"session_datetime\"\n",
    "\tflow_animal_to_session = df.groupby(['animal', session_identifier_key])['count'].sum().reset_index()\n",
    "\tflow_animal_to_session.columns = ['source', 'target', 'value']\n",
    "\n",
    "\t# Define the flow from \"session_datetime\" to \"track_membership\"\n",
    "\tflow_session_to_track = df.groupby([session_identifier_key, 'track_membership'])['count'].sum().reset_index()\n",
    "\tflow_session_to_track.columns = ['source', 'target', 'value']\n",
    "\n",
    "\t# Combine the two flows\n",
    "\tflow_data: pd.DataFrame = pd.concat([flow_animal_to_session, flow_session_to_track], ignore_index=True)\n",
    "\n",
    "\t# Define the unique session identifiers:\n",
    "\tunique_session_identifiers = df[session_identifier_key].unique().tolist()\n",
    "\n",
    "\tnodes = {\n",
    "\t\t'animal': fw.ProcessGroup(['gor01', 'pin01', 'vvp01'], title='Animal'),\n",
    "\t\t'session': fw.ProcessGroup(unique_session_identifiers, title=session_identifier_key),  # New node for session_datetime\n",
    "\t\t'track': fw.ProcessGroup(['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED'], title='TrackXclusivity'),\n",
    "\t}\n",
    "\n",
    "\tordering = [\n",
    "\t\t['animal'],\n",
    "\t\t['session'],  # Include session_datetime node\n",
    "\t\t['track'],\n",
    "\t]\n",
    "\n",
    "\tbundles = [\n",
    "\t\tfw.Bundle('animal', 'session'),  # Bundle from animal to session\n",
    "\t\tfw.Bundle('session', 'track'),   # Bundle from session to track\n",
    "\t]\n",
    "\n",
    "\tpartitions = [\n",
    "\t\tfw.Partition.Simple('process', ['gor01', 'pin01', 'vvp01']),\n",
    "\t\tfw.Partition.Simple('process', unique_session_identifiers),  # Partition for session_datetime\n",
    "\t\tfw.Partition.Simple('process', ['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED']),\n",
    "\t]\n",
    "\n",
    "\tnodes['animal'].partition = partitions[0]\n",
    "\tnodes['session'].partition = partitions[1]  # Assign partition for session_datetime\n",
    "\tnodes['track'].partition = partitions[2]\n",
    "\t\n",
    "\tflow_partition = fw.Partition.Simple('process', [])\n",
    "\t#flow_partition = fw.Partition.Simple('process', ['LEFT_ONLY', 'RIGHT_ONLY', 'SHARED']) # partitions[2]\n",
    "\tsdd = fw.SankeyDefinition(nodes, bundles, ordering, flow_partition=flow_partition) # \n",
    "\n",
    "\t## Weave the plot:\n",
    "\t_out_widget = fw.weave(sdd, flow_data).to_widget().auto_save_png('alluvial_diagram_with_session.png')\n",
    "\n",
    "\treturn _out_widget, sdd, flow_data\n",
    "\n",
    "\n",
    "\n",
    "alluvial_widget, sdd, flow_data = plot_alluvial_diagram(_out_value_counts_df, session_identifier_key=session_identifier_key)\n",
    "alluvial_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de182953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_global_computation_h5_paths = [a_dir.joinpath('output','global_computations.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']] \n",
    "included_h5_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc83c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-08-10\n",
    "# Add output modes for .pkl and .h5 files similar to figures. For GreatLakes Batch runs, allow output to the run folder.\n",
    "\n",
    "# like the figures, file naming conventions should change for the different types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb13a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphocorehelpers.indexing_helpers import partition, safe_pandas_get_group\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor, AcrossSessionsVisualizations\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import SingleBarResult\n",
    "\n",
    "common_file_path = Path('output/active_across_session_scatter_plot_results.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "\n",
    "# InstantaneousSpikeRateGroupsComputation, : pd.DataFrame\n",
    "_shell_obj, loaded_result_df = InstantaneousFiringRatesDataframeAccessor.load_and_prepare_for_plot(common_file_path)\n",
    "# Perform the actual plotting:\n",
    "AcrossSessionsVisualizations.across_sessions_bar_graphs(_shell_obj, num_sessions=13, save_figure=False, enable_tiny_point_labels=False, enable_hover_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c02c97-a566-4e24-9219-6ee09a4075b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "sessions_with_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e19412-2b21-4841-8207-952a04e1080b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_identifiers = included_session_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2e13-bbdf-4c82-9fd6-8d7ac702e114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to pickle:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21482b-23ac-44e6-bfb2-8e3d480d74bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine across .h5 files:\n",
    "import tables as tb\n",
    "import h5py\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import H5ExternalLinkBuilder\n",
    "\n",
    "# f'/kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table'\n",
    "\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths\n",
    "\n",
    "file_path = f'output/test_external_links.h5'\n",
    "a_global_computations_group_key: str = f\"{session_group_key}/global_computations\"\n",
    "with tb.open_file(file_path, mode='w') as f: # this mode='w' is correct because it should overwrite the previous file and not append to it.\n",
    "    a_global_computations_group = f.create_group(session_group_key, 'global_computations', title='the result of computations that operate over many or all of the filters in the session.', createparents=True)\n",
    "    an_external_link = f.create_external_link(f'', n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749849c-9f57-4d2c-aab8-5ee9353f6cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361385b-385d-4e12-997b-e2a68404858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8288dee-a41a-4640-a139-cb0a64f01814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdc211-56e3-4e38-b02f-ea71451766eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save output filelist:\n",
    "\n",
    "\n",
    "def save_filelist_to_text_file(hdf5_output_paths, filelist_path: Path):\n",
    "    _out_string = '\\n'.join([str(a_file) for a_file in hdf5_output_paths])\n",
    "    print(f'{_out_string}')\n",
    "    print(f'saving out to \"{filelist_path}\"...')\n",
    "    with open(filelist_path, 'w') as f:\n",
    "        f.write(_out_string)\n",
    "    return _out_string, filelist_path\n",
    "\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afc323-364c-4716-a30f-97a5e4fb2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e69a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0d19b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28828512",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2023-07-14 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "# inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-08-09_Test.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a11886",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_frs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, InstantaneousFiringRatesDataframeAccessor, InstantaneousSpikeRateGroupsComputation, trackMembershipTypesEnum, trackExclusiveToMembershipTypeDict, trackExclusiveToMembershipTypeReverseDict\n",
    "\n",
    "## Specify the output file:\n",
    "common_file_path = Path('output/test_across_session_scatter_plot_new.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "InstantaneousFiringRatesDataframeAccessor.add_results_to_inst_fr_results_table(curr_active_pipeline, common_file_path, file_mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the unique scatter plot dictionaries:\n",
    "across_session_contexts = list(across_sessions_instantaneous_fr_dict.keys())\n",
    "unique_animals = IdentifyingContext.find_unique_values(across_session_contexts)['animal'] # {'gor01', 'pin01', 'vvp01'}\n",
    "# Get number of animals to plot\n",
    "marker_list = [(5, i) for i in np.arange(len(unique_animals))] # [(5, 0), (5, 1), (5, 2)]\n",
    "scatter_props = [{'marker': mkr} for mkr in marker_list]  # Example, you should provide your own scatter properties\n",
    "scatter_props_dict = dict(zip(unique_animals, scatter_props))\n",
    "# {'pin01': {'marker': (5, 0)},\n",
    "#  'gor01': {'marker': (5, 1)},\n",
    "#  'vvp01': {'marker': (5, 2)}}\n",
    "scatter_props_dict\n",
    "\n",
    "# Pass a function that will return a set of kwargs for a given context\n",
    "def _return_scatter_props_fn(ctxt: IdentifyingContext):\n",
    "\t\"\"\" captures `scatter_props_dict` \"\"\"\n",
    "\tanimal_id = str(ctxt.animal)\n",
    "\treturn scatter_props_dict[animal_id]\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17b920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Single Session testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_out = global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, computation_functions_name_includelist =['_perform_baseline_placefield_computation'], active_session_computation_configs=None) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "_test_out\n",
    "\n",
    "# global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, **{'computation_functions_name_includelist': ['_perform_baseline_placefield_computation'], 'active_session_computation_configs': None}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 23.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2bb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "full_good_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is None]\n",
    "bad_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is not None]\n",
    "full_good_dirs\n",
    "bad_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f73f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status\n",
    "global_batch_run.session_batch_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf2bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf02efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the list of sessions that are completely ready to process:\n",
    "full_good_ready_to_process_sessions = list(good_only_batch_progress_df['context'].to_numpy())\n",
    "full_good_ready_to_process_sessions\n",
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run[\"good_sessions_list\"].extend(full_good_ready_to_process_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636e3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\",\\n\".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # List definitions\n",
    "\n",
    "# [IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\ncurr_context = \".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # Line definitions\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689d1d-6400-4f87-be0e-a184a1d5bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82aedf-b024-4f07-b1a9-350730b4db8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "save_time = datetime.now()\n",
    " \n",
    "print(\"save_time =\", save_time)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = save_time.strftime(\"%Y-%m-%d_%I-%M%p\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc6bd-5b34-41d0-b906-b0ad9927b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get output file paths:\n",
    "completed_pipeline_filename = 'loadedSessPickle.pkl'\n",
    "completed_global_computations_filename = 'outputs/global_computation_results.pkl'\n",
    "\n",
    "full_good_ready_to_process_session_paths = list(good_only_batch_progress_df['basedirs'].to_numpy())\n",
    "session_paths_output_folders = [sess_path.joinpath('outputs').resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "\n",
    "completed_pipeline_file_paths = [sess_path.joinpath(completed_pipeline_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths = [sess_path.joinpath(completed_global_computations_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab75122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countable Additivity \n",
    "# Any countable collections of points is size 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
