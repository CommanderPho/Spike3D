{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows collecting results across multiple pipeline runs. Concatenating across sessions and bin sizes. \n",
    "It takes CSVs, then determines the most recent one from the filename. \n",
    "\n",
    "```python\n",
    "\n",
    "# 2024-01-23 - \n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-fet11-01_12-58-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-03_12-3-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_17-46-44_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_19-28-0_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-58-3_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-25-50_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_16-40-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_22-24-40_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_16-53-46_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_17-29-30_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_14-26-15_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_1-22-43_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-07_16-40-19_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_21-16-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_15-55-31_time_bin_size_sweep_results.h5\n",
    "\n",
    "\n",
    "\n",
    "# found_session_export_paths = [Path(v).resolve() for v in  [\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "# matplotlib.use('Qt5Agg')\n",
    "%matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def find_csv_files(directory: str, recurrsive: bool=False):\n",
    "    directory_path = Path(directory) # Convert string path to a Path object\n",
    "    if recurrsive:\n",
    "        return list(directory_path.glob('**/*.csv')) # Return a list of all .csv files in the directory and its subdirectories\n",
    "    else:\n",
    "        return list(directory_path.glob('*.csv')) # Return a list of all .csv files in the directory and its subdirectories\n",
    "    \n",
    "\n",
    "def find_HDF5_files(directory: str):\n",
    "    directory_path = Path(directory) # Convert string path to a Path object\n",
    "    return list(directory_path.glob('**/*.h5')) # Return a list of all .h5 files in the directory and its subdirectories\n",
    "\n",
    "\n",
    "def parse_filename(path: Path, debug_print:bool=False) -> Tuple[datetime, str, str]:\n",
    "    \"\"\" \n",
    "    # from the found_session_export_paths, get the most recently exported laps_csv, ripple_csv (by comparing `export_datetime`) for each session (`session_str`)\n",
    "    a_export_filename: str = \"2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\"\n",
    "    export_datetime = \"2024-01-12_0420PM\"\n",
    "    session_str = \"kdiba_pin01_one_fet11-01_12-58-54\"\n",
    "    export_file_type = \"(laps_marginals_df)\" # .csv\n",
    "\n",
    "    # return laps_csv, ripple_csv\n",
    "    laps_csv = Path(\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\").resolve()\n",
    "    ripple_csv = Path(\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\").resolve()\n",
    "\n",
    "    \"\"\"\n",
    "    filename = path.stem   # Get filename without extension\n",
    "    decoding_time_bin_size_str = None\n",
    "    \n",
    "    pattern = r\"(?P<export_datetime_str>.*_\\d{2}\\d{2}[APMF]{2})-(?P<session_str>.*)-(?P<export_file_type>\\(?.+\\)?)(?:_tbin-(?P<decoding_time_bin_size_str>[^)]+))\"\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match is not None:\n",
    "        # export_datetime_str, session_str, export_file_type = match.groups()\n",
    "        export_datetime_str, session_str, export_file_type, decoding_time_bin_size_str = match.group('export_datetime_str'), match.group('session_str'), match.group('export_file_type'), match.group('decoding_time_bin_size_str')\n",
    "    \n",
    "        # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "        export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d_%I%M%p\")\n",
    "\n",
    "    else:\n",
    "        if debug_print:\n",
    "            print(f'did not match pattern with time.')\n",
    "        # day_date_only_pattern = r\"(.*(?:_\\d{2}\\d{2}[APMF]{2})?)-(.*)-(\\(.+\\))\"\n",
    "        day_date_only_pattern = r\"(\\d{4}-\\d{2}-\\d{2})-(.*)-(\\(?.+\\)?)\" # \n",
    "        day_date_only_match = re.match(day_date_only_pattern, filename) # '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'        \n",
    "        if day_date_only_match is not None:\n",
    "            export_datetime_str, session_str, export_file_type = day_date_only_match.groups()\n",
    "            # print(export_datetime_str, session_str, export_file_type)\n",
    "            # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "            export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d\")\n",
    "        \n",
    "        else:\n",
    "            # Try H5 pattern:\n",
    "            # matches '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'\n",
    "            day_date_with_variant_suffix_pattern = r\"(?P<export_datetime_str>\\d{4}-\\d{2}-\\d{2})_?(?P<variant_suffix>[^-_]*)-(?P<session_str>.+?)_(?P<export_file_type>[A-Za-z_]+)\"\n",
    "            day_date_with_variant_suffix_match = re.match(day_date_with_variant_suffix_pattern, filename) # '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'\n",
    "            if day_date_with_variant_suffix_match is not None:\n",
    "                export_datetime_str, session_str, export_file_type = day_date_with_variant_suffix_match.group('export_datetime_str'), day_date_with_variant_suffix_match.group('session_str'), day_date_with_variant_suffix_match.group('export_file_type')\n",
    "                # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "                export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d\")\n",
    "        \n",
    "            else:\n",
    "                print(f'ERR: Could not parse filename: \"{filename}\"') # 2024-01-18_GL_t_split_df\n",
    "                return None, None, None # used to return ValueError when it couldn't parse, but we'd rather skip unparsable files\n",
    "\n",
    "        \n",
    "    if export_file_type[0] == '(' and export_file_type[-1] == ')':\n",
    "        # Trim the brackets from the file type if they're present:\n",
    "        export_file_type = export_file_type[1:-1]\n",
    "\n",
    "    return export_datetime, session_str, export_file_type, decoding_time_bin_size_str\n",
    "\n",
    "\n",
    "def find_most_recent_files(found_session_export_paths: List[Path], debug_print: bool = False) -> Dict[str, Dict[str, Tuple[Path, datetime]]]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary representing the most recent files for each session type among a list of provided file paths.\n",
    "\n",
    "    Parameters:\n",
    "    found_session_export_paths (List[Path]): A list of Paths representing files to be checked.\n",
    "    debug_print (bool): A flag to trigger debugging print statements within the function. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[str, Tuple[Path, datetime]]]: A nested dictionary where the main keys represent \n",
    "    different session types. The inner dictionary's keys represent file types and values are the most recent \n",
    "    Path and datetime for this combination of session and file type.\n",
    "    \n",
    "    # now sessions is a dictionary where the key is the session_str and the value is another dictionary.\n",
    "    # This inner dictionary's key is the file type and the value is the most recent path for this combination of session and file type\n",
    "    # Thus, laps_csv and ripple_csv can be obtained from the dictionary for each session\n",
    "\n",
    "    \"\"\"\n",
    "    # Function 'parse_filename' should be defined in the global scope\n",
    "    parsed_paths = [(*parse_filename(p), p) for p in found_session_export_paths if (parse_filename(p)[0] is not None)]\n",
    "    parsed_paths.sort(reverse=True)\n",
    "\n",
    "    if debug_print:\n",
    "        print(f'parsed_paths: {parsed_paths}')\n",
    "\n",
    "    sessions = {}\n",
    "    for export_datetime, session_str, file_type, path, decoding_time_bin_size_str in parsed_paths:\n",
    "        if session_str not in sessions:\n",
    "            sessions[session_str] = {}\n",
    "\n",
    "        if (file_type not in sessions[session_str]) or (sessions[session_str][file_type][-1] < export_datetime):\n",
    "            sessions[session_str][file_type] = (path, decoding_time_bin_size_str, export_datetime)\n",
    "    \n",
    "    return sessions\n",
    "    \n",
    "\n",
    "def process_csv_file(file: str, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
    "    \"\"\" reads the CSV file and adds the 'session_name' column if it is missing. \n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    df['session_name'] = session_name \n",
    "    if curr_session_t_delta is not None:\n",
    "        df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
    "    return df\n",
    "\n",
    "\n",
    "debug_print: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR: Could not parse filename: \"2024-01-18_GL_t_split_df\"\n"
     ]
    }
   ],
   "source": [
    "## Load across session t_delta CSV, which contains the t_delta for each session:\n",
    "t_delta_csv_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs\\2024-01-18_GL_t_split_df.csv').resolve() # Apogee\n",
    "# t_delta_csv_path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-01-18_GL_t_split_df.csv').resolve() # GL\n",
    "\n",
    "# collected_outputs_directory = '/home/halechr/FastData/collected_outputs/'\n",
    "# collected_outputs_directory = r'C:\\Users\\pho\\Desktop\\collected_outputs'\n",
    "collected_outputs_directory = r'C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs' # APOGEE\n",
    "# collected_outputs_directory = '/home/halechr/cloud/turbo/Data/Output/collected_outputs' # GL\n",
    "\n",
    "## Find the files:\n",
    "csv_files = find_csv_files(collected_outputs_directory)\n",
    "h5_files = find_HDF5_files(collected_outputs_directory)\n",
    "\n",
    "csv_sessions = find_most_recent_files(found_session_export_paths=csv_files)\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "\n",
    "\n",
    "## The CSV containing the session delta time:\n",
    "t_delta_df = pd.read_csv(t_delta_csv_path, index_col=0) # Assuming that your CSV file has an index column\n",
    "t_delta_dict = t_delta_df.to_dict(orient='index')\n",
    "# t_delta_df\n",
    "\n",
    "# csv_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Extract each of the separate files from the sessions:\n",
    "    \n",
    "final_sessions = {}\n",
    "final_sessions_loaded_laps_dict = {}\n",
    "final_sessions_loaded_ripple_dict = {}\n",
    "final_sessions_loaded_laps_time_bin_dict = {}\n",
    "final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in csv_sessions.items():\n",
    "    # try:\n",
    "        final_sessions[session_str] = {}\n",
    "        for file_type, (an_decoding_time_bin_size_str, a_path, an_export_datetime) in session_dict.items():\n",
    "            final_sessions[session_str][file_type] = a_path\n",
    "            \n",
    "        session_name: str = str(session_str)  # Extract session name from the filename\n",
    "        if debug_print:\n",
    "            print(f'processing session_name: {session_name}')\n",
    "        curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "        if curr_session_t_delta is None:\n",
    "            print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "        # finds each of the four exports:\n",
    "        try:\n",
    "            laps_file = final_sessions[session_str]['laps_marginals_df']\n",
    "            final_sessions_loaded_laps_dict[session_str] = process_csv_file(laps_file, session_name, curr_session_t_delta, 'lap_start_t')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "                    \n",
    "        try:\n",
    "            ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "            final_sessions_loaded_ripple_dict[session_str] = process_csv_file(ripple_file, session_name, curr_session_t_delta, 'ripple_start_t')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "            \n",
    "\n",
    "        try:\n",
    "            laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "            final_sessions_loaded_laps_time_bin_dict[session_str] = process_csv_file(laps_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "\n",
    "\n",
    "        try:\n",
    "            ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "            final_sessions_loaded_ripple_time_bin_dict[session_str] = process_csv_file(ripple_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>P_LR</th>\n",
       "      <th>P_RL</th>\n",
       "      <th>P_Long</th>\n",
       "      <th>P_Short</th>\n",
       "      <th>epoch_idx</th>\n",
       "      <th>t_bin_center</th>\n",
       "      <th>session_name</th>\n",
       "      <th>delta_aligned_start_t</th>\n",
       "      <th>time_bin_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19018</th>\n",
       "      <td>0</td>\n",
       "      <td>0.491114</td>\n",
       "      <td>0.508886</td>\n",
       "      <td>3.876855e-01</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>0</td>\n",
       "      <td>6.907083</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>-876.990049</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19019</th>\n",
       "      <td>1</td>\n",
       "      <td>0.491114</td>\n",
       "      <td>0.508886</td>\n",
       "      <td>3.876855e-01</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>0</td>\n",
       "      <td>6.937083</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>-876.960049</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19020</th>\n",
       "      <td>2</td>\n",
       "      <td>0.491114</td>\n",
       "      <td>0.508886</td>\n",
       "      <td>3.876855e-01</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>0</td>\n",
       "      <td>6.967083</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>-876.930049</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19021</th>\n",
       "      <td>3</td>\n",
       "      <td>0.491114</td>\n",
       "      <td>0.508886</td>\n",
       "      <td>3.876855e-01</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>0</td>\n",
       "      <td>6.997083</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>-876.900049</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19022</th>\n",
       "      <td>4</td>\n",
       "      <td>0.491114</td>\n",
       "      <td>0.508886</td>\n",
       "      <td>3.876855e-01</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>0</td>\n",
       "      <td>7.027083</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>-876.870049</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627816</th>\n",
       "      <td>57452</td>\n",
       "      <td>0.993650</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>8.458879e-01</td>\n",
       "      <td>0.154112</td>\n",
       "      <td>79</td>\n",
       "      <td>2068.387436</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>856.829356</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627817</th>\n",
       "      <td>57453</td>\n",
       "      <td>0.875051</td>\n",
       "      <td>0.124949</td>\n",
       "      <td>6.168876e-07</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>79</td>\n",
       "      <td>2068.487436</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>856.929356</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627818</th>\n",
       "      <td>57454</td>\n",
       "      <td>0.703433</td>\n",
       "      <td>0.296567</td>\n",
       "      <td>2.924253e-01</td>\n",
       "      <td>0.707575</td>\n",
       "      <td>79</td>\n",
       "      <td>2068.587436</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>857.029356</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627819</th>\n",
       "      <td>57455</td>\n",
       "      <td>0.928085</td>\n",
       "      <td>0.071915</td>\n",
       "      <td>8.841508e-01</td>\n",
       "      <td>0.115849</td>\n",
       "      <td>79</td>\n",
       "      <td>2068.687436</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>857.129356</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627820</th>\n",
       "      <td>57456</td>\n",
       "      <td>0.998680</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>1.226342e-01</td>\n",
       "      <td>0.877366</td>\n",
       "      <td>79</td>\n",
       "      <td>2068.787436</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>857.229356</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601845 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      P_LR      P_RL        P_Long   P_Short  epoch_idx  \\\n",
       "19018            0  0.491114  0.508886  3.876855e-01  0.612315          0   \n",
       "19019            1  0.491114  0.508886  3.876855e-01  0.612315          0   \n",
       "19020            2  0.491114  0.508886  3.876855e-01  0.612315          0   \n",
       "19021            3  0.491114  0.508886  3.876855e-01  0.612315          0   \n",
       "19022            4  0.491114  0.508886  3.876855e-01  0.612315          0   \n",
       "...            ...       ...       ...           ...       ...        ...   \n",
       "627816       57452  0.993650  0.006350  8.458879e-01  0.154112         79   \n",
       "627817       57453  0.875051  0.124949  6.168876e-07  0.999999         79   \n",
       "627818       57454  0.703433  0.296567  2.924253e-01  0.707575         79   \n",
       "627819       57455  0.928085  0.071915  8.841508e-01  0.115849         79   \n",
       "627820       57456  0.998680  0.001320  1.226342e-01  0.877366         79   \n",
       "\n",
       "        t_bin_center                        session_name  \\\n",
       "19018       6.907083  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "19019       6.937083  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "19020       6.967083  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "19021       6.997083  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "19022       7.027083  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "...              ...                                 ...   \n",
       "627816   2068.387436  kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "627817   2068.487436  kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "627818   2068.587436  kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "627819   2068.687436  kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "627820   2068.787436  kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "\n",
       "        delta_aligned_start_t  time_bin_size  \n",
       "19018             -876.990049           0.03  \n",
       "19019             -876.960049           0.03  \n",
       "19020             -876.930049           0.03  \n",
       "19021             -876.900049           0.03  \n",
       "19022             -876.870049           0.03  \n",
       "...                       ...            ...  \n",
       "627816             856.829356           0.10  \n",
       "627817             856.929356           0.10  \n",
       "627818             857.029356           0.10  \n",
       "627819             857.129356           0.10  \n",
       "627820             857.229356           0.10  \n",
       "\n",
       "[601845 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Build across_sessions join dataframes:\n",
    "all_sessions_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# *_time_bin marginals:\n",
    "all_sessions_laps_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# Filter rows based on column: 'time_bin_size'\n",
    "all_sessions_laps_df = all_sessions_laps_df[all_sessions_laps_df['time_bin_size'].notna()]\n",
    "all_sessions_ripple_df = all_sessions_ripple_df[all_sessions_ripple_df['time_bin_size'].notna()]\n",
    "all_sessions_laps_time_bin_df = all_sessions_laps_time_bin_df[all_sessions_laps_time_bin_df['time_bin_size'].notna()]\n",
    "all_sessions_ripple_time_bin_df = all_sessions_ripple_time_bin_df[all_sessions_ripple_time_bin_df['time_bin_size'].notna()]\n",
    "\n",
    "all_sessions_laps_time_bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_time_bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sessions\n",
    "# {'kdiba_gor01_one_2006-6-08_14-26-15': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv')},\n",
    "#  'kdiba_gor01_one_2006-6-09_1-22-43': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(laps_marginals_df).csv')},\n",
    "#  'kdiba_pin01_one_fet11-01_12-58-54': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv')}}\n",
    "\n",
    "\n",
    "\n",
    "all_sessions_laps_df\n",
    "all_sessions_laps_time_bin_df\n",
    "\n",
    "final_output_path = Path(\"../output/\").resolve()\n",
    "\n",
    "# final_output_path.joinpath(\"2024-01-23_AcrossSession_fig_laps.html\").resolve()\n",
    "\n",
    "final_csv_export_path_laps = final_output_path.joinpath(\"2024-01-25_AcrossSession_Laps.csv\").resolve()\n",
    "# all_sessions_laps_df.to_csv(final_csv_export_path_laps)\n",
    "\n",
    "final_csv_export_path_ripple = final_output_path.joinpath(\"2024-01-25_AcrossSession_Ripple.csv\").resolve()\n",
    "# all_sessions_ripple_df.to_csv(final_csv_export_path_ripple)\n",
    "\n",
    "final_csv_export_path_laps, final_csv_export_path_ripple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_across_sessions_results(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name', size='time_bin_size')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name', size='time_bin_size')\n",
    "\n",
    "    # # Create a bubble chart for laps\n",
    "    # fig_laps = px.scatter(concatenated_laps_df, x='lap_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # # Create a bubble chart for ripples\n",
    "    # fig_ripples = px.scatter(concatenated_ripple_df, x='ripple_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{session_name}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{session_name}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures\n",
    "\n",
    "# Plot the time_bin marginals:\n",
    "\n",
    "def plot_across_sessions_results_with_histogram_gpt3(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    # Create a histogram for laps\n",
    "    fig_hist_laps = px.histogram(concatenated_laps_df, x='delta_aligned_start_t', nbins=50, title=f\"Laps - Session: {session_name}\")\n",
    "    \n",
    "    # Assign numerical values to session_name for color\n",
    "    session_name_to_color = {name: i for i, name in enumerate(concatenated_laps_df['session_name'].unique())}\n",
    "\n",
    "    # Create subplots with shared y-axis\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[f\"Laps - Session: {session_name}\", f\"Ripples - Session: {session_name}\"])\n",
    "    \n",
    "    # Add histogram to the left subplot\n",
    "    fig.add_trace(go.Histogram(x=concatenated_laps_df['delta_aligned_start_t'], nbinsx=50, name='Histogram'), row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    \n",
    "    # Add bubble chart to the right subplot\n",
    "    fig.add_trace(go.Scatter(x=concatenated_laps_df['delta_aligned_start_t'], y=concatenated_laps_df['P_Long'], mode='markers', marker=dict(color=concatenated_laps_df['session_name'].map(session_name_to_color))), row=1, col=2)\n",
    "    fig.update_xaxes(title_text='delta_aligned_start_t', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='P_Long', row=1, col=2)\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figure to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_name = Path(figures_folder, f\"{session_name}_combined_plot{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_name}\"...')\n",
    "        fig.write_image(fig_name)\n",
    "    \n",
    "    # Append the figure to the list\n",
    "    all_figures.append(fig)\n",
    "    \n",
    "    return all_figures\n",
    "\n",
    "\n",
    "def plot_across_sessions_results_with_histogram_new(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Your existing code (not modified)\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create subplot with 2 rows and 1 column\n",
    "    fig_laps = make_subplots(rows=2, cols=1)\n",
    "    # Add scatter plot to first row, first column\n",
    "    fig_laps.add_trace(\n",
    "        go.Scatter(x=concatenated_laps_df['delta_aligned_start_t'], y=concatenated_laps_df['P_Long'], mode='markers', name='Scatter'), \n",
    "        row=1, col=1\n",
    "    )\n",
    "    # add histogram to second row, first column\n",
    "    fig_laps.add_trace(\n",
    "        go.Histogram(x=concatenated_laps_df['delta_aligned_start_t'], name='Histogram'), \n",
    "        row=2, col=1\n",
    "    )\n",
    "    # Same for ripples\n",
    "    fig_ripples = make_subplots(rows=2, cols=1)\n",
    "    fig_ripples.add_trace(\n",
    "        go.Scatter(x=concatenated_ripple_df['delta_aligned_start_t'], y=concatenated_ripple_df['P_Long'], mode='markers', name='Scatter'), \n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_ripples.add_trace(\n",
    "        go.Histogram(x=concatenated_ripple_df['delta_aligned_start_t'], name='Histogram'), \n",
    "        row=2, col=1\n",
    "    )\n",
    "    # Your existing code continues from here (not modified)\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{session_name}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{session_name}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(all_sessions_ripple_df.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "all_session_figures = plot_across_sessions_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_df, concatenated_ripple_df=all_sessions_ripple_df, save_figures=True)\n",
    "\n",
    "# Show figures for all sessions\n",
    "for fig_laps, fig_ripples in all_session_figures:\n",
    "    fig_laps.show()\n",
    "    fig_ripples.show()\n",
    "\n",
    "    fig_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_laps.html\")\n",
    "    fig_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_ripples.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_plot_fn = plot_across_sessions_results\n",
    "# active_plot_fn = plot_across_sessions_results_with_histogram_gpt3\n",
    "# active_plot_fn = plot_across_sessions_results_with_histogram_new\n",
    "all_time_bin_session_figures = active_plot_fn(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df, save_figures=False)\n",
    "all_time_bin_session_figures\n",
    "\n",
    "for fig_time_bin_laps, fig_time_bin_ripples in all_time_bin_session_figures:\n",
    "    fig_time_bin_laps.show()\n",
    "    fig_time_bin_ripples.show()\n",
    "    # fig_laps.write_html(\"../output/2024-01-18_AcrossSession_fig_laps.html\")\n",
    "    # fig_ripples.write_html(\"../output/2024-01-18_AcrossSession_fig_ripples.html\")\n",
    "    fig_time_bin_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_laps.html\")\n",
    "    fig_time_bin_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_ripples.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_bin_session_figures[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histograms( data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str ) -> None:\n",
    "    # get the pre-delta epochs\n",
    "    pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "    post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "    descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "    # plot pre-delta histogram\n",
    "    pre_delta_df.hist(column='P_Long')\n",
    "    plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "    plt.show()\n",
    "\n",
    "    # plot post-delta histogram\n",
    "    post_delta_df.hist(column='P_Long')\n",
    "    plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "    plt.show()\n",
    "\n",
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'All Sessions', all_sessions_laps_time_bin_df, \"75 ms\")\n",
    "plot_histograms('Ripples', 'All Sessions', all_sessions_ripple_time_bin_df, \"75 ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histograms(data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str) -> None:\n",
    "    \"\"\" plots a stacked histogram of the many time-bin sizes \"\"\"\n",
    "    # get the pre-delta epochs\n",
    "    pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "    post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "    descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "    # plot pre-delta histogram\n",
    "    time_bin_sizes = pre_delta_df['time_bin_size'].unique()\n",
    "    \n",
    "    figure_identifier: str = f\"{descriptor_str}_preDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = pre_delta_df[pre_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot post-delta histogram\n",
    "    time_bin_sizes = post_delta_df['time_bin_size'].unique()\n",
    "    figure_identifier: str = f\"{descriptor_str}_postDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = post_delta_df[post_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'All Sessions', all_sessions_laps_time_bin_df, \"several\")\n",
    "plot_histograms('Ripples', 'All Sessions', all_sessions_ripple_time_bin_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Laps'\n",
    "session_spec = 'All Sessions'\n",
    "data_results_df = all_sessions_laps_time_bin_df\n",
    "time_bin_duration_str = \"75 ms\"\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "pre_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "plt.show()\n",
    "\n",
    "post_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Ripples'\n",
    "session_spec = 'All Sessions'\n",
    "data_results_df = all_sessions_ripple_time_bin_df\n",
    "time_bin_duration_str = \"75 ms\"\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "pre_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "plt.show()\n",
    "\n",
    "post_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_time_bin_df.hist(column='P_Long')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "from attrs import define, field, Factory\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "@define(slots=False)\n",
    "class TwoCSV:\n",
    "\t\"\"\" simple class wrapper to emulate the object that holds the other dfs \"\"\"\n",
    "\tlaps_all_epoch_bins_marginals_df = field()\n",
    "\tripple_all_epoch_bins_marginals_df = field()\n",
    "\n",
    "save_figure = True\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\tprint(f'final_context: {final_context}')\n",
    "\t# if save_figure:\n",
    "\t# \tfig.save_fig(\n",
    "\t# \t# return owning_pipeline_reference.output_figure(final_context, fig)\n",
    "\t# else:\n",
    "\t# \tpass # do nothing, don't save\n",
    "\t\n",
    "# all_sessions_laps_time_bin_df\n",
    "\n",
    "collector = plot_all_epoch_bins_marginal_predictions(TwoCSV(laps_all_epoch_bins_marginals_df=all_sessions_laps_df, ripple_all_epoch_bins_marginals_df=all_sessions_ripple_df), t_start=None, t_split=0.0, t_end=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=IdentifyingContext(), perform_write_to_file_callback=_perform_write_to_file_callback)\n",
    "collector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tuple(collector.figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import H5FileAggregator\n",
    "\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "h5_sessions\n",
    "\n",
    "final_sessions = {}\n",
    "time_bin_size_sweep_results_files = {}\n",
    "\n",
    "# final_sessions_loaded_laps_dict = {}\n",
    "# final_sessions_loaded_ripple_dict = {}\n",
    "# final_sessions_loaded_laps_time_bin_dict = {}\n",
    "# final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in h5_sessions.items():\n",
    "    final_sessions[session_str] = {}\n",
    "    for file_type, (a_path, an_export_datetime) in session_dict.items():\n",
    "        final_sessions[session_str][file_type] = a_path\n",
    "        \n",
    "    session_name: str = str(session_str)  # Extract session name from the filename\n",
    "    if debug_print:\n",
    "        print(f'processing session_name: {session_name}')\n",
    "    curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "    if curr_session_t_delta is None:\n",
    "        print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "    # finds each of the four exports:\n",
    "    time_bin_size_sweep_results_file = final_sessions[session_str]['time_bin_size_sweep_results']\n",
    "    time_bin_size_sweep_results_files[session_str] = Path(time_bin_size_sweep_results_file).resolve()\n",
    "    # ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "    # laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "    # ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "    \n",
    "\n",
    "\n",
    "session_identifiers = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "# Keys are like: \"/kdiba|gor01|two|2006-6-12_16-53-46\"\n",
    "included_h5_paths = list(time_bin_size_sweep_results_files.values())\n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"|\", include_property_names=False)) for a_ctxt in session_identifiers] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "laps_decoding_accuracy_results_table_keys = [f\"{session_group_key}/laps_decoding_accuracy_results/table\" for session_group_key in session_group_keys]\n",
    "a_loader = H5FileAggregator.init_from_file_lists(file_list=included_h5_paths, table_key_list=laps_decoding_accuracy_results_table_keys)\n",
    "_out_table = a_loader.load_and_consolidate()\n",
    "_out_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_yellow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
