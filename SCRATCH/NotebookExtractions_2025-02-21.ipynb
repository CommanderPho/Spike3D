{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024-03-01 - Get the active user-annotated epoch times from the `UserAnnotationsManager` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.misc import numpyify_array\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.epoch import EpochsAccessor\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "## Get from UserAnnotations directly instead of the intermediate viewer\n",
    "\n",
    "## # inputs: any_good_selected_epoch_times, any_good_selected_epoch_times, any_good_selected_epoch_indicies \n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies\n",
    "# Add user-selection columns to df\n",
    "a_df = deepcopy(filtered_ripple_simple_pf_pearson_merged_df)\n",
    "# a_df = deepcopy(ripple_weighted_corr_merged_df)\n",
    "a_df['is_user_annotated_epoch'] = False\n",
    "# any_good_selected_epoch_indicies = a_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, np.squeeze(any_good_selected_epoch_times[:,0]), t_column_names=['ripple_start_t',])\n",
    "# any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, any_good_selected_epoch_times, t_column_names=['ripple_start_t',])\n",
    "any_good_selected_epoch_indicies\n",
    "# a_df['is_user_annotated_epoch'] = np.isin(a_df.index.to_numpy(), any_good_selected_epoch_indicies)\n",
    "a_df['is_user_annotated_epoch'].loc[any_good_selected_epoch_indicies] = True # Here's another .iloc issue! Changing to .loc\n",
    "a_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DecoderDecodedEpochsResult.filter_epochs_dfs_by_annotation_times(curr_active_pipeline, any_good_selected_epoch_times, ripple_decoding_time_bin_size, filtered_ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚧 2024-12-16 - Look at the distributions of the various df columns conditioned on whether 'is_user_selected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filter_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_filter_epochs_df['is_included_by_heuristic_criteria'] = False # default to False\n",
    "all_filter_epochs_df.loc[all_filter_epochs_df.epochs.find_data_indicies_from_epoch_times(included_heuristic_ripple_start_times), 'is_included_by_heuristic_criteria'] = True\n",
    "all_filter_epochs_df\n",
    "\n",
    "# all_filter_epochs_df, (included_heuristic_ripple_start_times, excluded_heuristic_ripple_start_times) = HeuristicThresholdFiltering.add_columns(df=all_filter_epochs_df, start_col_name='start')\n",
    "# high_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_heuristic_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# low_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(excluded_heuristic_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "# included_filter_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_decoder_name = 'long_LR'\n",
    "all_epoch_result: DecodedFilterEpochsResult = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict[example_decoder_name])\n",
    "included_filter_epoch_result: DecodedFilterEpochsResult = deepcopy(high_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict[example_decoder_name])\n",
    "all_epoch_result, included_filter_epoch_result, (_, included_filter_epoch_times_to_all_epoch_index_arr) = HeuristicThresholdFiltering.get_filtered_result(all_epoch_result=all_epoch_result,\n",
    "                                                                                                         included_filter_epoch_result=included_filter_epoch_result, start_col_name='start'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[993.868, 994.185]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[473.423, 473.747], [624.226, 624.499], [637.785, 638.182], [1136.192, 1136.453], [1348.890, 1349.264], [1673.437, 1673.920], [1693.342, 1693.482]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[534.584, 534.939], [564.149, 564.440], [760.981, 761.121], [1131.640, 1131.887], [1161.001, 1161.274], [1332.283, 1332.395], [1707.712, 1707.919]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[438.267, 438.448], [637.785, 638.182], [1085.596, 1086.046], [1117.650, 1118.019], [1252.562, 1252.739], [1262.523, 1262.926], [1316.056, 1316.270], [1348.890, 1349.264], [1440.852, 1441.328], [1729.689, 1730.228], [1731.111, 1731.288]]\n",
    "\n",
    "## INPUTS: all_filter_epochs_df, paginated_multi_decoder_decoded_epochs_window\n",
    "## Updates 'is_user_annotated_epoch'\n",
    "# user_selected_start_times = paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times[:, 0]\n",
    "\n",
    "all_filter_epochs_df['is_user_annotated_epoch'] = False # default to False\n",
    "all_filter_epochs_df.loc[all_filter_epochs_df.epochs.find_data_indicies_from_epoch_times(user_selected_start_times), 'is_user_annotated_epoch'] = True\n",
    "all_filter_epochs_df\n",
    "\n",
    "# for a pd.DataFrame named `all_filter_epochs_df`, plot the distributions of the dataframe columns ['mseq_len_ignoring_intrusions', 'mseq_len_ratio_ignoring_intrusions_and_repeats', 'mseq_tcov', 'mseq_dtrav', 'is_included_by_heuristic_criteria'] as histograms for the two values of the boolean column ['is_user_annotated_epoch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_false_negative = NumpyHelpers.logical_and(all_filter_epochs_df['is_user_annotated_epoch'], np.logical_not(all_filter_epochs_df['is_included_by_heuristic_criteria']))\n",
    "# is_false_positive = NumpyHelpers.logical_and(np.logical_not(all_filter_epochs_df['is_user_annotated_epoch']), all_filter_epochs_df['is_included_by_heuristic_criteria'])\n",
    "\n",
    "# is_true_positive = NumpyHelpers.logical_and(all_filter_epochs_df['is_user_annotated_epoch'], all_filter_epochs_df['is_included_by_heuristic_criteria'])\n",
    "# is_true_negative = NumpyHelpers.logical_and(np.logical_not(all_filter_epochs_df['is_user_annotated_epoch']), np.logical_not(all_filter_epochs_df['is_included_by_heuristic_criteria']))\n",
    "\n",
    "assessment_dict = {'is_false_negative': NumpyHelpers.logical_and(all_filter_epochs_df['is_user_annotated_epoch'], np.logical_not(all_filter_epochs_df['is_included_by_heuristic_criteria'])),\n",
    "                   'is_false_positive': NumpyHelpers.logical_and(np.logical_not(all_filter_epochs_df['is_user_annotated_epoch']), all_filter_epochs_df['is_included_by_heuristic_criteria']),\n",
    "                   'is_true_positive': NumpyHelpers.logical_and(all_filter_epochs_df['is_user_annotated_epoch'], all_filter_epochs_df['is_included_by_heuristic_criteria']), \n",
    "                   'is_true_negative': NumpyHelpers.logical_and(np.logical_not(all_filter_epochs_df['is_user_annotated_epoch']), np.logical_not(all_filter_epochs_df['is_included_by_heuristic_criteria'])),\n",
    "}\n",
    "assessment_dict_counts = {k:np.sum(v) for k, v in assessment_dict.items()}\n",
    "assessment_dict_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cols = ['mseq_len', 'mseq_len_ignoring_intrusions','mseq_len_ratio_ignoring_intrusions_and_repeats','mseq_tcov','mseq_dtrav','is_included_by_heuristic_criteria']\n",
    "fig, axes = plt.subplots(len(cols), 1, figsize=(8, len(cols)*3))\n",
    "for ax, c in zip(axes, cols):\n",
    "    sns.histplot(data=all_filter_epochs_df, x=c, hue='is_user_annotated_epoch', kde=True, ax=ax)\n",
    "    ax.set_title(c)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🖼️ Plot specific `PhoPaginatedMultiDecoderDecodedEpochsWindow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter by included_epoch_idxs\n",
    "from neuropy.utils.indexing_helpers import flatten, NumpyHelpers, PandasHelpers\n",
    "\n",
    "##INPUTS: filtered_decoder_filter_epochs_decoder_result_dict, ripple_merged_complete_epoch_stats_df, included_epoch_indicies\n",
    "if included_epoch_indicies is not None:\n",
    "    ## filter by `included_epoch_indicies`\n",
    "    filter_thresholds_dict = {'mseq_len_ignoring_intrusions_and_repeats': 4, 'mseq_tcov': 0.35}\n",
    "    df_is_included_criteria_fn = lambda df: NumpyHelpers.logical_and(*[(df[f'overall_best_{a_col_name}'] >= a_thresh) for a_col_name, a_thresh in filter_thresholds_dict.items()])\n",
    "    included_heuristic_ripple_start_times = ripple_merged_complete_epoch_stats_df[df_is_included_criteria_fn(ripple_merged_complete_epoch_stats_df)]['ripple_start_t'].values\n",
    "    high_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_heuristic_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "\n",
    "## OUTPUTS: high_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_merged_complete_epoch_stats_df\n",
    "\n",
    "# ['P_LR', 'P_RL', 'P_Long', 'P_Short', 'P_Long_LR', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import get_heatmap_cmap\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers, ColorFormatConverter\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import FixedCustomColormaps\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow, DecodedEpochSlicesPaginatedFigureController, EpochSelectionsObject, ClickActionCallbacks\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.PaginationCtrl.PaginationControlWidget import PaginationControlWidget, PaginationControlWidgetState\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.Resources import GuiResources, ActionIcons, silx_resources_rc\n",
    "from neuropy.utils.indexing_helpers import flatten, NumpyHelpers, PandasHelpers\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicThresholdFiltering\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _plot_heuristic_evaluation_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "\n",
    "# ## pseudo-sorted `sorted_included_filter_epoch_times_to_all_epoch_index_arr`\n",
    "# sorted_included_filter_epoch_times_to_all_epoch_index_arr = deepcopy(included_filter_epoch_times_to_all_epoch_index_arr) ## unsorted\n",
    "# sorted_included_filter_epoch_times_to_all_epoch_index_arr = sorted_included_filter_epoch_times_to_all_epoch_index_arr[::-1]\n",
    "# sorted_included_filter_epoch_times_to_all_epoch_index_arr\n",
    "# reversed(sorted_included_filter_epoch_times_to_all_epoch_index_arr)\n",
    "\n",
    "# type(active_cmap) # matplotlib.colors.LinearSegmentedColormap\n",
    "# active_cmap.to_list()\n",
    "\n",
    "# color_list = [mcolors.rgb2hex(active_cmap(i)) for i in range(active_cmap.N)]\n",
    "# color_list\n",
    "\n",
    "# cmap = cm.get_cmap('viridis',nlevels)\n",
    "# active_cmap.set_under((1,1,1,0)) # Completely hide the underflow\n",
    "# active_cmap.colors[:,3] = np.linspace(0.3,0.9,13) # Choose a gradient for transparency\n",
    "\n",
    "# active_cmap.set_extremes(bad=None, under=None, over=None)\n",
    "\n",
    "\n",
    "# high_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict, high_continuous_seq_sort_only_filter_epochs_df\n",
    "\n",
    "# app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict = _plot_heuristic_evaluation_epochs(curr_active_pipeline, track_templates, filtered_decoder_filter_epochs_decoder_result_dict, ripple_merged_complete_epoch_stats_df=ripple_merged_complete_epoch_stats_df)\n",
    "\n",
    "\n",
    "app, (high_heuristic_paginated_multi_decoder_decoded_epochs_window, high_heuristic_pagination_controller_dict), (low_heuristic_paginated_multi_decoder_decoded_epochs_window, low_heuristic_pagination_controller_dict) = _plot_heuristic_evaluation_epochs(curr_active_pipeline, track_templates, filtered_decoder_filter_epochs_decoder_result_dict, ripple_merged_complete_epoch_stats_df=ripple_merged_complete_epoch_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[93.795, 125.088, 109.441, 144.645, 160.292, 164.203] # false-negative, although not great one\n",
    "[83.6663, 79.9392, 76.2121, 83.6663, 87.3934, 98.5748, 117.21], # false-negative, pretty good\n",
    "[76.2121, 83.6663, 91.1205, 94.8476, 109.756, 109.756], ## false-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_heuristic_paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline, enable_export_combined_img=True)\n",
    "# low_heuristic_paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_heuristic_paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_heuristic_paginated_multi_decoder_decoded_epochs_window.selec\n",
    "\n",
    "## Low-Heuristic Filtered: False Negative Selections\n",
    "with Ctx(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='heuristic_false_negative') as ctx:\n",
    "    user_annotations[ctx + Ctx(decoder='long_LR')] = []\n",
    "    user_annotations[ctx + Ctx(decoder='long_RL')] = [[438.267, 438.448], [826.546, 826.823], [1136.192, 1136.453]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_LR')] = []\n",
    "    user_annotations[ctx + Ctx(decoder='short_RL')] = [[1348.890, 1349.264]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_heuristic_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(included_columns=[#'P_decoder', #'ratio_jump_valid_bins', 'wcorr', 'avg_jump_cm', 'max_jump_cm',\n",
    "    'mseq_len_ignoring_intrusions', 'mseq_tcov', 'mseq_tdist', # , 'mseq_len_ratio_ignoring_intrusions_and_repeats', 'mseq_len_ignoring_intrusions_and_repeats'\n",
    "], defer_refresh=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_merged_complete_epoch_stats_df[NumpyHelpers.logical_and(ripple_merged_complete_epoch_stats_df['is_valid_epoch'], ripple_merged_complete_epoch_stats_df['is_included_by_heuristic_criteria'])] ## 136, 71 included requiring both\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "# Column Names _______________________________________________________________________________________________________ #\n",
    "basic_df_column_names = ['start', 'stop', 'label', 'duration']\n",
    "selection_col_names = ['is_user_annotated_epoch', 'is_valid_epoch']\n",
    "session_identity_col_names = ['session_name', 'time_bin_size', 'delta_aligned_start_t', 'pre_post_delta_category', 'maze_id']\n",
    "\n",
    "# Score Columns (one value for each decoder) _________________________________________________________________________ #\n",
    "decoder_bayes_prob_col_names = ['P_decoder']\n",
    "\n",
    "# radon_transform_col_names = ['score', 'velocity', 'intercept', 'speed']\n",
    "# weighted_corr_col_names = ['wcorr']\n",
    "# pearson_col_names = ['pearsonr']\n",
    "\n",
    "heuristic_score_col_names = HeuristicReplayScoring.get_all_score_computation_col_names()\n",
    "# print(f'heuristic_score_col_names: {heuristic_score_col_names}') # ['avg_jump_cm', 'travel', 'coverage', 'total_distance_traveled', 'track_coverage_score', 'mseq_len', 'mseq_len_ignoring_intrusions', 'mseq_len_ignoring_intrusions_and_repeats', 'mseq_len_ratio_ignoring_intrusions_and_repeats', 'mseq_tcov', 'mseq_dtrav']\n",
    "active_heuristic_col_names = ['avg_jump_cm', 'total_distance_traveled', 'track_coverage_score', 'mseq_len', 'mseq_len_ignoring_intrusions', 'mseq_tcov', 'mseq_dtrav']\n",
    "# print(f'active_heuristic_col_names: {active_heuristic_col_names}')\n",
    "\n",
    "## All included columns:\n",
    "all_df_shared_column_names: List[str] = basic_df_column_names + selection_col_names + session_identity_col_names # these are not replicated for each decoder, they're the same for the epoch\n",
    "all_df_score_column_names: List[str] = active_heuristic_col_names # decoder_bayes_prob_col_names + radon_transform_col_names + weighted_corr_col_names + pearson_col_names + \n",
    "all_df_column_names: List[str] = all_df_shared_column_names + all_df_score_column_names ## All included columns, includes the score columns which will not be replicated\n",
    "\n",
    "## OUTPUT: all_df_column_names\n",
    "## Add in the 'wcorr' metrics:\n",
    "merged_conditional_prob_column_names = ['P_LR', 'P_RL', 'P_Long', 'P_Short']\n",
    "merged_wcorr_column_names = ['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']\n",
    "\n",
    "\n",
    "active_column_names = deepcopy(all_df_column_names) # [*all_df_shared_column_names, *all_df_score_column_names]\n",
    "\n",
    "\n",
    "## INPUTS: ripple_merged_complete_epoch_stats_df\n",
    "df = deepcopy(ripple_merged_complete_epoch_stats_df)\n",
    "\n",
    "# [v for v in list(df.columns) if v.startswith()]\n",
    "\n",
    "all_columns = [*all_df_shared_column_names]\n",
    "# for a_decoder_name in ['long_LR', 'long_RL', 'short_LR', 'short_RL']:\n",
    "# \tall_columns.extend([f\"{a_col_name}_{a_decoder_name}\" for a_col_name in active_heuristic_col_names])\n",
    "    \n",
    "## keeps columns grouped together in sets of 4\n",
    "for a_col_name in active_heuristic_col_names:\n",
    "    all_columns.extend([f\"{a_col_name}_{a_decoder_name}\" for a_decoder_name in ['long_LR', 'long_RL', 'short_LR', 'short_RL']])\n",
    "    \n",
    "all_columns = [v for v in all_columns if v in df.columns]\n",
    "active_df: pd.DataFrame = deepcopy(df[all_columns])\n",
    "\n",
    "## OUTPUTS: all_columns, active_df\n",
    "print(all_columns)\n",
    "active_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts with some non-grouped columns: `['start', 'stop', 'label', 'duration', 'is_user_annotated_epoch', 'is_valid_epoch', 'session_name', 'time_bin_size', 'delta_aligned_start_t', 'pre_post_delta_category', 'maze_id']`. Then after these, all columns are to be grouped in groups of 4, and they have a shared prefix: `['avg_jump_cm_long_LR', 'avg_jump_cm_long_RL', 'avg_jump_cm_short_LR', 'avg_jump_cm_short_RL', 'total_distance_traveled_long_LR', 'total_distance_traveled_long_RL', 'total_distance_traveled_short_LR', 'total_distance_traveled_short_RL', 'track_coverage_score_long_LR', 'track_coverage_score_long_RL', 'track_coverage_score_short_LR', 'track_coverage_score_short_RL', 'mseq_len_long_LR', 'mseq_len_long_RL', 'mseq_len_short_LR', 'mseq_len_short_RL', 'mseq_len_ignoring_intrusions_long_LR', 'mseq_len_ignoring_intrusions_long_RL', 'mseq_len_ignoring_intrusions_short_LR', 'mseq_len_ignoring_intrusions_short_RL', 'mseq_tcov_long_LR', 'mseq_tcov_long_RL', 'mseq_tcov_short_LR', 'mseq_tcov_short_RL', 'mseq_dtrav_long_LR', 'mseq_dtrav_long_RL', 'mseq_dtrav_short_LR', 'mseq_dtrav_short_RL']` so for example one group would be `(e.g. `['avg_jump_cm_long_LR', 'avg_jump_cm_long_RL', 'avg_jump_cm_short_LR', 'avg_jump_cm_short_RL']` has the shared prefix `'avg_jump_cm_short'`. Please write the code so that it works for my dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Qt.pandas_model import SimplePandasModel, create_tabbed_table_widget\n",
    "\n",
    "\n",
    "## INPUTS: active_df\n",
    "\n",
    "ctrl_layout = pg.LayoutWidget()\n",
    "ctrl_widgets_dict = dict()\n",
    "                                                                                    \n",
    "# Tabbled table widget:\n",
    "tab_widget, views_dict, models_dict = create_tabbed_table_widget(dataframes_dict={'epochs': active_df.copy(), # active_epochs_df.copy(),\n",
    "                                                                                    # 'spikes': global_spikes_df.copy(), \n",
    "                                                                                    # 'combined_epoch_stats': pd.DataFrame(),\n",
    "                                                                                    })\n",
    "ctrl_widgets_dict['tables_tab_widget'] = tab_widget\n",
    "ctrl_widgets_dict['views_dict'] = views_dict\n",
    "ctrl_widgets_dict['models_dict'] = models_dict\n",
    "\n",
    "# Add the tab widget to the layout\n",
    "ctrl_layout.addWidget(tab_widget, row=2, rowspan=1, col=1, colspan=1)\n",
    "\n",
    "\n",
    "ctrl_layout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, QtGui\n",
    "# import pyqtgraph as pg\n",
    "\n",
    "class TableExample(QtWidgets.QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"PyQtGraph Table Example\")\n",
    "        self.resize(600, 400)\n",
    "\n",
    "        # Create the table widget\n",
    "        self.table = pg.TableWidget()\n",
    "        self.setCentralWidget(self.table)\n",
    "\n",
    "        # Populate the table with data\n",
    "        data = {'Column A': [1, 2, 3, 4, 5], \n",
    "                'Column B': ['A', 'B', 'C', 'D', 'E']}\n",
    "        self.table.setData(data)\n",
    "\n",
    "        # Highlight and scroll to row buttons\n",
    "        control_layout = QtWidgets.QVBoxLayout()\n",
    "        highlight_button = QtWidgets.QPushButton(\"Highlight Row 2\")\n",
    "        highlight_button.clicked.connect(self.highlight_row)\n",
    "        scroll_button = QtWidgets.QPushButton(\"Scroll to Row 4\")\n",
    "        scroll_button.clicked.connect(self.scroll_to_row)\n",
    "        control_layout.addWidget(highlight_button)\n",
    "        control_layout.addWidget(scroll_button)\n",
    "\n",
    "        # Add control buttons below table\n",
    "        container = QtWidgets.QWidget()\n",
    "        container_layout = QtWidgets.QVBoxLayout(container)\n",
    "        container_layout.addWidget(self.table)\n",
    "        container_layout.addLayout(control_layout)\n",
    "        self.setCentralWidget(container)\n",
    "\n",
    "    def highlight_row(self):\n",
    "        row_index = 2  # Row to highlight (0-based index)\n",
    "        for col in range(self.table.columnCount()):\n",
    "            item = self.table.item(row_index, col)\n",
    "            if item:\n",
    "                item.setBackground(QtGui.QBrush(QtGui.QColor('yellow')))\n",
    "\n",
    "    def scroll_to_row(self):\n",
    "        row_index = 4  # Row to scroll to (0-based index)\n",
    "        self.table.scrollToItem(self.table.item(row_index, 0), QtWidgets.QAbstractItemView.PositionAtCenter)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app = QtWidgets.QApplication([])\n",
    "#     window = TableExample()\n",
    "#     window.show()\n",
    "#     app.exec_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "global_laps_epochs_df = global_laps.to_dataframe()\n",
    "\n",
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "_out_laps_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, global_laps_epochs_df, track_templates, rank_order_results, RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_laps_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "from pyphocorehelpers.print_helpers import render_scrollable_colored_table_from_dataframe\n",
    "\n",
    "## INPUTS: active_df\n",
    "\n",
    "# df = deepcopy(df[all_columns])\n",
    "df = deepcopy(active_df)\n",
    "\n",
    "# ## Move the \"height\" columns to the end\n",
    "# df = reorder_columns_relative(df, column_names=list(filter(lambda column: column.startswith('mseq_dtrav_'), df.columns)), relative_mode='start')\n",
    "# df\n",
    "render_scrollable_colored_table_from_dataframe(df=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to [0, 1] range\n",
    "normalized_df = (df - df.min().min()) / (df.max().max() - df.min().min())\n",
    "# normalized_df\n",
    "render_scrollable_colored_table_from_dataframe(df=normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_decoder_filter_epochs_decoder_result_dict # Dict[types.DecoderName, DecodedFilterEpochsResult]\n",
    "\n",
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_cmap = FixedCustomColormaps.get_custom_orange_with_low_values_dropped_cmap()\n",
    "# active_cmap = FixedCustomColormaps.get_custom_black_with_low_values_dropped_cmap(low_value_cutoff=0.05)\n",
    "# active_cmap = ColormapHelpers.create_colormap_transparent_below_value(active_cmap, low_value_cuttoff=0.1)\n",
    "active_cmap = FixedCustomColormaps.get_custom_greyscale_with_low_values_dropped_cmap(low_value_cutoff=0.01, full_opacity_threshold=0.25)\n",
    "# active_cmap = FixedCustomColormaps.get_custom_orange_with_low_values_dropped_cmap()\n",
    "app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                decoder_decoded_epochs_result_dict=filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=filtered_ripple_simple_pf_pearson_merged_df, epochs_name='ripple',\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple', title='Long-like post-Delta Ripples Only', ## RIPPLE\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=high_heuristic_only_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple', title='High-sequence Score Ripples Only', ## RIPPLE\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', ## LAPS\n",
    "                                                                                                included_epoch_indicies=None,\n",
    "                                                                                                # included_epoch_indicies=included_filter_epoch_times_to_all_epoch_index_arr, ## unsorted\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=sorted_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple',  ## SORTED\n",
    "                                                                                                # included_epoch_indicies=sorted_included_filter_epoch_times_to_all_epoch_index_arr, ## SORTED\n",
    "                                                                                                debug_print=False,\n",
    "                                                                                                params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': False, \n",
    "                                                                                                    'enable_decoded_sequence_and_heuristics_curve': True, 'show_pre_merged_debug_sequences': False, 'show_heuristic_criteria_filter_epoch_inclusion_status': False, \n",
    "                                                                                                    'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True, 'enable_weighted_corr_data_provider_modify_axes_rect': False,\n",
    "                                                                                                    # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 9,\n",
    "                                                                                                    'scrollable_figure': False,\n",
    "                                                                                                    # 'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    # 'use_AnchoredCustomText': True, \n",
    "                                                                                                    'use_AnchoredCustomText': False, ## DEFAULT\n",
    "                                                                                                    'should_suppress_callback_exceptions': False,\n",
    "                                                                                                    # 'build_fn': 'insets_view',\n",
    "                                                                                                    'track_length_cm_dict': deepcopy(track_templates.get_track_length_dict()),\n",
    "                                                                                                    'posterior_heatmap_imshow_kwargs': dict(cmap=active_cmap), # , vmin=0.1, vmax=1.0\n",
    "                                                                                                    \n",
    "                                                                                                })\n",
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(included_columns=[#'P_decoder', #'ratio_jump_valid_bins', 'wcorr', 'avg_jump_cm', 'max_jump_cm',\n",
    "    'mseq_len_ignoring_intrusions', 'mseq_tcov', 'mseq_tdist', # , 'mseq_len_ratio_ignoring_intrusions_and_repeats', 'mseq_len_ignoring_intrusions_and_repeats'\n",
    "], defer_refresh=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(included_columns=[#'P_decoder', #'ratio_jump_valid_bins', 'wcorr', 'avg_jump_cm', 'max_jump_cm',\n",
    "    'mseq_len_ignoring_intrusions', 'mseq_tcov', 'mseq_tdist', # , 'mseq_len_ratio_ignoring_intrusions_and_repeats', 'mseq_len_ignoring_intrusions_and_repeats'\n",
    "], defer_refresh=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_paths, (_out_combined_img, combined_img_out_path) = paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline, enable_export_combined_img=True, combined_image_basename=f'{DAY_DATE_TO_USE}_combined_All_Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "annotations_man = UserAnnotationsManager()\n",
    "user_annotations = annotations_man.get_user_annotations()\n",
    "\n",
    "user_annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# children_is_selected_values = paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='is_selected')\n",
    "# children_is_selected_values_dict = deepcopy(paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='params.is_selected'))\n",
    "# children_is_selected_values_dict\n",
    "\n",
    "# self.plots_data.epoch_slices[self.is_selected]\n",
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "# [deepcopy(a_pagination_controller.is_selected) for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()]\n",
    "\n",
    "# [deepcopy(a_pagination_controller.plots_data.epoch_slices[a_pagination_controller.is_selected]) for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()]\n",
    "\n",
    "defer_render: bool = False\n",
    "children_is_epoch_selected = np.vstack([deepcopy(a_pagination_controller.is_selected) for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()]) #.shape (4, 136)\n",
    "any_child_epoch_is_selected = np.any(children_is_epoch_selected, axis=0) # (136,)\n",
    "\n",
    "# any_child_epoch_is_selected.shape\n",
    "# np.logical_or(children_is_selected, axis=1)\n",
    "\n",
    "## assign to all \n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.is_selected = deepcopy(any_child_epoch_is_selected) ## make it independent  # params.update(**updated_values)\n",
    "        # a_pagination_controller.params.is_selected\n",
    "\n",
    "    # Replace values in the dictionary with new_values\n",
    "    Assert.same_length(a_pagination_controller.params.is_selected, any_child_epoch_is_selected)\n",
    "    # a_pagination_controller.params.is_selected = {k: v for k, v in zip(a_pagination_controller.params.is_selected.keys(), any_child_epoch_is_selected)}\n",
    "\n",
    "    selected_epoch_times = a_pagination_controller.plots_data.epoch_slices[any_child_epoch_is_selected] # returns an S x 2 array of epoch start/end times that are currently selected.\n",
    "    # a_pagination_controller.perform_update_selections(defer_render=defer_render)\n",
    "\n",
    "    # a_start_stop_arr = self.selected_epoch_times # NOPE, these are the current selections\n",
    "    a_start_stop_arr = deepcopy(selected_epoch_times) # \n",
    "    if (a_start_stop_arr is not None) and (len(a_start_stop_arr) > 0):\n",
    "        assert np.shape(a_start_stop_arr)[1] == 2, f\"input should be start, stop times as a numpy array\"\n",
    "        new_selections = a_pagination_controller.restore_selections_from_epoch_times(a_start_stop_arr, defer_render=defer_render) # TODO: only accepts epoch_times specifications\n",
    "        \n",
    "\n",
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.perform_update_selections(defer_render=False)\n",
    "\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[105.400, 105.563], [132.511, 132.791], [149.959, 150.254], [154.499, 154.853], [191.609, 191.949], [251.417, 251.812], [438.267, 438.448], [473.423, 473.747], [534.584, 534.939], [564.149, 564.440], [599.710, 599.905], [624.226, 624.499], [637.785, 638.182], [670.216, 670.418], [675.972, 676.153], [722.678, 722.933], [745.811, 746.097], [760.981, 761.121], [783.916, 784.032], [808.799, 808.948], [993.868, 994.185], [1085.080, 1085.184], [1085.596, 1086.046], [1117.650, 1118.019], [1131.640, 1131.887], [1136.192, 1136.453], [1161.001, 1161.274], [1191.563, 1191.864], [1233.968, 1234.181], [1244.038, 1244.176], [1252.562, 1252.739], [1262.523, 1262.926], [1267.918, 1268.235], [1302.651, 1302.801], [1316.056, 1316.270], [1317.977, 1318.181], [1332.283, 1332.395], [1348.890, 1349.264], [1440.852, 1441.328], [1450.894, 1451.024], [1673.437, 1673.920], [1693.342, 1693.482], [1705.053, 1705.141], [1707.712, 1707.919], [1725.279, 1725.595], [1729.689, 1730.228], [1731.111, 1731.288]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[105.400, 105.563], [132.511, 132.791], [149.959, 150.254], [154.499, 154.853], [191.609, 191.949], [251.417, 251.812], [438.267, 438.448], [473.423, 473.747], [534.584, 534.939], [564.149, 564.440], [599.710, 599.905], [624.226, 624.499], [637.785, 638.182], [670.216, 670.418], [675.972, 676.153], [722.678, 722.933], [745.811, 746.097], [760.981, 761.121], [783.916, 784.032], [808.799, 808.948], [993.868, 994.185], [1085.080, 1085.184], [1085.596, 1086.046], [1117.650, 1118.019], [1131.640, 1131.887], [1136.192, 1136.453], [1161.001, 1161.274], [1191.563, 1191.864], [1233.968, 1234.181], [1244.038, 1244.176], [1252.562, 1252.739], [1262.523, 1262.926], [1267.918, 1268.235], [1302.651, 1302.801], [1316.056, 1316.270], [1317.977, 1318.181], [1332.283, 1332.395], [1348.890, 1349.264], [1440.852, 1441.328], [1450.894, 1451.024], [1673.437, 1673.920], [1693.342, 1693.482], [1705.053, 1705.141], [1707.712, 1707.919], [1725.279, 1725.595], [1729.689, 1730.228], [1731.111, 1731.288]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[105.400, 105.563], [132.511, 132.791], [149.959, 150.254], [154.499, 154.853], [191.609, 191.949], [251.417, 251.812], [438.267, 438.448], [473.423, 473.747], [534.584, 534.939], [564.149, 564.440], [599.710, 599.905], [624.226, 624.499], [637.785, 638.182], [670.216, 670.418], [675.972, 676.153], [722.678, 722.933], [745.811, 746.097], [760.981, 761.121], [783.916, 784.032], [808.799, 808.948], [993.868, 994.185], [1085.080, 1085.184], [1085.596, 1086.046], [1117.650, 1118.019], [1131.640, 1131.887], [1136.192, 1136.453], [1161.001, 1161.274], [1191.563, 1191.864], [1233.968, 1234.181], [1244.038, 1244.176], [1252.562, 1252.739], [1262.523, 1262.926], [1267.918, 1268.235], [1302.651, 1302.801], [1316.056, 1316.270], [1317.977, 1318.181], [1332.283, 1332.395], [1348.890, 1349.264], [1440.852, 1441.328], [1450.894, 1451.024], [1673.437, 1673.920], [1693.342, 1693.482], [1705.053, 1705.141], [1707.712, 1707.919], [1725.279, 1725.595], [1729.689, 1730.228], [1731.111, 1731.288]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[105.400, 105.563], [132.511, 132.791], [149.959, 150.254], [154.499, 154.853], [191.609, 191.949], [251.417, 251.812], [438.267, 438.448], [473.423, 473.747], [534.584, 534.939], [564.149, 564.440], [599.710, 599.905], [624.226, 624.499], [637.785, 638.182], [670.216, 670.418], [675.972, 676.153], [722.678, 722.933], [745.811, 746.097], [760.981, 761.121], [783.916, 784.032], [808.799, 808.948], [993.868, 994.185], [1085.080, 1085.184], [1085.596, 1086.046], [1117.650, 1118.019], [1131.640, 1131.887], [1136.192, 1136.453], [1161.001, 1161.274], [1191.563, 1191.864], [1233.968, 1234.181], [1244.038, 1244.176], [1252.562, 1252.739], [1262.523, 1262.926], [1267.918, 1268.235], [1302.651, 1302.801], [1316.056, 1316.270], [1317.977, 1318.181], [1332.283, 1332.395], [1348.890, 1349.264], [1440.852, 1441.328], [1450.894, 1451.024], [1673.437, 1673.920], [1693.342, 1693.482], [1705.053, 1705.141], [1707.712, 1707.919], [1725.279, 1725.595], [1729.689, 1730.228], [1731.111, 1731.288]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "children_is_selected_values_dict['long_LR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.array(list(v.values())) for k, v in children_is_selected_values_dict.items()]\n",
    "\n",
    "\n",
    "# children_is_selected_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='params.is_selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.set_children_props(prop_path='params.is_selected', value=children_is_selected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attached_yellow_blue_marginals_viewer_widget.plots.axs\n",
    "attached_yellow_blue_marginals_viewer_widget.plots.data_keys\n",
    "\n",
    "# attached_yellow_blue_marginals_viewer_widget.plots['secondary_yaxes'][ax]\n",
    "# list(attached_yellow_blue_marginals_viewer_widget.plots_data.keys())\n",
    "\n",
    "# attached_yellow_blue_marginals_viewer_widget.plots_data['epoch_slices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers, long_short_display_config_manager, DisplayColorsEnum, LongShortDisplayConfigManager, DisplayConfig\n",
    "\n",
    "# long_epoch_config = long_short_display_config_manager.long_epoch_config.as_pyqtgraph_kwargs()\n",
    "# short_epoch_config = long_short_display_config_manager.short_epoch_config.as_pyqtgraph_kwargs()\n",
    "\n",
    "# long_epoch_matplotlib_config = long_short_display_config_manager.long_epoch_config.as_matplotlib_kwargs()\n",
    "# short_epoch_matplotlib_config = long_short_display_config_manager.short_epoch_config.as_matplotlib_kwargs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "output_dict = {}\n",
    "for i, ax in enumerate(attached_yellow_blue_marginals_viewer_widget.plots.axs):\n",
    "    output_dict[ax] = PlottingHelpers.helper_matplotlib_add_pseudo2D_marginal_labels(ax, y_bin_labels=['long_LR', 'long_RL', 'short_LR', 'short_RL'], enable_draw_decoder_colored_lines=False)\n",
    "    # output_dict[ax] = PlottingHelpers.helper_matplotlib_add_pseudo2D_marginal_labels(ax, y_bin_labels=['long', 'short'], enable_draw_decoder_colored_lines=False)\n",
    "    # output_dict[ax] = PlottingHelpers.helper_matplotlib_add_pseudo2D_marginal_labels(ax, y_bin_labels=['LR', 'RL'], enable_draw_decoder_colored_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ID_line_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for decoder_name, inner_output_dict in output_dict.items():\n",
    "    for a_name, an_artist in inner_output_dict.items():\n",
    "        an_artist.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax.set_ylim(37.0773897438341, 253.98616538463315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dict = {}\n",
    "## Get the track configs for the colors:\n",
    "long_short_display_config_manager = LongShortDisplayConfigManager()\n",
    "long_epoch_config = long_short_display_config_manager.long_epoch_config.as_matplotlib_kwargs()\n",
    "short_epoch_config = long_short_display_config_manager.short_epoch_config.as_matplotlib_kwargs()\n",
    "\n",
    "# Highlight the two epochs with their characteristic colors ['r','b'] - ideally this would be at the very back\n",
    "if ((t_start is None) or (t_end is None)):\n",
    "    x_start_ax, x_stop_ax = ax.get_xlim()\n",
    "    t_start= (t_start or x_start_ax)\n",
    "    t_end = (t_end or x_stop_ax)\n",
    "output_dict[\"long_region\"] = ax.axvspan(t_start, t_split, color=long_epoch_config['facecolor'], alpha=0.2, zorder=0)\n",
    "output_dict[\"short_region\"] = ax.axvspan(t_split, t_end, color=short_epoch_config['facecolor'], alpha=0.2, zorder=0)\n",
    "# Update the xlimits with the new bounds\n",
    "ax.set_xlim(t_start, t_end)\n",
    "\n",
    "# Draw the vertical epoch splitter line:\n",
    "required_epoch_bar_height = ax.get_ylim()[-1]\n",
    "output_dict[\"divider_line\"] = ax.vlines(t_split, ymin=0, ymax=required_epoch_bar_height, color=(0,0,0,.25), zorder=25) # divider should be in very front"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug DecodedSequenceAndHeuristicsPlotData in the PhoPaginatedMultiDecoderDecodedEpochsWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import DecodedSequenceAndHeuristicsPlotData\n",
    "np.set_printoptions(formatter={'all': lambda x: f\"{x},\"})\n",
    "\n",
    "decoded_sequence_and_heuristics_curves_data_dict: Dict[types.DecoderName, Dict[float, DecodedSequenceAndHeuristicsPlotData]] = paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='plots_data.decoded_sequence_and_heuristics_curves_data')\n",
    "decoded_sequence_and_heuristics_partition_results_dict: Dict[types.DecoderName, Dict[float, SubsequencesPartitioningResult]] = {a_name:{k:v.partition_result for k, v in a_data_dict.items()} for a_name, a_data_dict in decoded_sequence_and_heuristics_curves_data_dict.items()}\n",
    "# decoded_sequence_and_heuristics_curves_data_dict\n",
    "# decoded_sequence_and_heuristics_partition_results_dict\n",
    "\n",
    "## OUTPUTS: decoded_sequence_and_heuristics_partition_results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.get_track_length_dict()\n",
    "\n",
    "track_templates.short_LR_decoder.xbin\n",
    "\n",
    "# track_templates.decoder_dict\n",
    "\n",
    "pos_bounds = [np.min([track_templates.long_LR_decoder.xbin, track_templates.short_LR_decoder.xbin]),\n",
    "              np.max([track_templates.long_LR_decoder.xbin, track_templates.short_LR_decoder.xbin])] # [37.0773897438341, 253.98616538463315]\n",
    "pos_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions()  # Reset to default\n",
    "np.set_printoptions(formatter={'all': lambda x: f\"{x},\"})\n",
    "\n",
    "a_start_time = 1707.918\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_t_list = [113.37414696447586, ]\n",
    "decoder_name: types.DecoderName = 'long_LR'\n",
    "# decoder_name: types.DecoderName = 'long_RL'\n",
    "# decoder_name: types.DecoderName = 'short_LR'\n",
    "absolute_epoch_idx: int = 132\n",
    "a_partition_result: SubsequencesPartitioningResult = list(decoded_sequence_and_heuristics_partition_results_dict[decoder_name].values())[absolute_epoch_idx]\n",
    "# a_partition_result.max_jump_distance_cm\n",
    "\n",
    "\n",
    "## CUSTOM\n",
    "# arr = np.array([240.66720547686478, 236.86178836035953, 229.25095412734905, 229.25095412734905, 224.64011989433857, 221.64011989433857, 187.3913658457913, 149.3371946807389, 153.14261179724411, 126.50469198170738, 134.11552621471787, 149.3371946807389, 126.50469198170738, 202.61303431181233, 38.980098302086716, 50.39634965160246, 84.64510370014968, 38.980098302086716, 252.08345682638054, 46.59093253509721, 38.980098302086716,])\n",
    "# arr[6:10] -= 50\n",
    "arr = [236.86178836035953, 122.69927486520214, 248.27803970987526, 244.47262259337003, 252.08345682638054, 244.47262259337003, 252.08345682638054, 252.08345682638054, 160.75344603025462, 187.3913658457913, 96.06135504966542, 115.08844063219165,]\n",
    "a_partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=arr, pos_bin_edges=a_partition_result.pos_bin_edges, max_ignore_bins=a_partition_result.max_ignore_bins, same_thresh=a_partition_result.same_thresh, max_jump_distance_cm=a_partition_result.max_jump_distance_cm,\n",
    "                                                                            #   flat_time_window_centers=a_partition_result.flat_time_window_centers, flat_time_window_edges=a_partition_result.flat_time_window_edges,\n",
    "                                                                             )\n",
    "\n",
    "\n",
    "a_partition_result.compute(debug_print=True)\n",
    "with pd.option_context('display.max_rows', 35):\n",
    "    a_partition_result.subsequences_df\n",
    "    a_partition_result.position_bins_info_df\n",
    "    a_partition_result.position_changes_info_df\n",
    "\n",
    "a_partition_result.merged_split_positions_arrays\n",
    "a_partition_result.longest_sequence_subsequence\n",
    "a_partition_result._plot_step_by_step_subsequence_partition_process(non_main_sequence_alpha_multiplier=0.2, should_show_non_main_sequence_hlines=True, debug_print=False)\n",
    "# partition_result\n",
    "a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False)\n",
    "\n",
    "# subsequence_lengths: [6 1 1 1 1 2 3 1 3 8], subsequence_len_sort_indicies: [9 0 8 6 5 7 4 3 2 1]\n",
    "# subsequence_lengths: [6 1 1 1 1 2 1 2 1 3 0 8], subsequence_len_sort_indicies: [11  0  9  7  5  8  6  4  3  2  1 10]\n",
    "\n",
    "print(f'\"Epoch[{decoder_name}][{absolute_epoch_idx}]\": {a_partition_result.flat_positions},') # \"Epoch[long_LR][1]\": [58.00718388461296, 38.980098302086716, 50.39634965160246, 58.00718388461296, 137.92094333122313, 126.50469198170738, 126.50469198170738, 149.3371946807389, 160.75344603025462, 217.8347027778333,],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_partition_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([240.66720547686478, 236.86178836035953, 229.25095412734905, 229.25095412734905, 221.64011989433857, 69.42343523412869, 187.3913658457913, 149.3371946807389, 153.14261179724411, 126.50469198170738, 134.11552621471787, 149.3371946807389, 126.50469198170738, 202.61303431181233, 38.980098302086716, 50.39634965160246, 84.64510370014968, 38.980098302086716, 252.08345682638054, 46.59093253509721, 38.980098302086716,])\n",
    "arr[6:10] -= 50\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_test_epoch_pos_sequence_dict = {\n",
    "    \"Epoch[long_LR][1]\": [58.00718388461296, 38.980098302086716, 50.39634965160246, 58.00718388461296, 137.92094333122313, 126.50469198170738, 126.50469198170738, 149.3371946807389, 160.75344603025462, 217.8347027778333,],\n",
    "    \"Epoch[short_LR][0]\": [84.64510370014968, 92.25593793316017, 77.03426946713918, 38.980098302086716, 38.980098302086716, 38.980098302086716, 77.03426946713918, 38.980098302086716, 217.8347027778333, 137.92094333122313, 206.41845142831755, 210.2238685448228, 195.00220007880182, 191.1967829622966, 88.45052081665493, 141.72636044772838, 38.980098302086716, 38.980098302086716, 38.980098302086716, 80.83968658364444, 202.61303431181233, 214.02928566132806, 217.8347027778333, 210.2238685448228, 210.2238685448228, 206.41845142831755, 187.3913658457913,],\n",
    "    \"Epoch[long_RL][11]\": [240.66720547686478, 236.86178836035953, 229.25095412734905, 229.25095412734905, 221.64011989433857, 69.42343523412869, 187.3913658457913, 149.3371946807389, 153.14261179724411, 126.50469198170738, 134.11552621471787, 149.3371946807389, 126.50469198170738, 202.61303431181233, 38.980098302086716, 50.39634965160246, 84.64510370014968, 38.980098302086716, 252.08345682638054, 46.59093253509721, 38.980098302086716,],\n",
    "    \"Epoch[long_RL][11]_IntroducedJump\": [240.66720547686478, 236.86178836035953, 229.25095412734905, 229.25095412734905, 221.64011989433857, 69.42343523412869, 187.3913658457913, 149.3371946807389, 153.14261179724411, 126.50469198170738, 134.11552621471787, 149.3371946807389, 126.50469198170738, 202.61303431181233, 38.980098302086716, 50.39634965160246, 84.64510370014968, 38.980098302086716, 252.08345682638054, 46.59093253509721, 38.980098302086716,],\n",
    "}\n",
    "# print(f'decoder_name: \"{decoder_name}\", absolute_epoch_idx: {absolute_epoch_idx}, flat_positions: {a_partition_result.flat_positions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_seq_length_dict = {'neither': a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=False, should_use_no_repeat_values=False),\n",
    "    'ignoring_intru': a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False),\n",
    "    '+no_repeat': a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=True),\n",
    "}\n",
    "\n",
    "longest_seq_length_multiline_label_str: str = '\\n'.join([': '.join([k, str(v)]) for k, v in longest_seq_length_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=SubsequenceDetectionSamples.most_likely_positions_bad_single_main_seq, **SubsequencesPartitioningResult_common_init_kwargs,)\n",
    "# Access the partitioned subsequences\n",
    "subsequences = partition_result.split_positions_arrays\n",
    "merged_subsequences = partition_result.merged_split_positions_arrays\n",
    "print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "subsequences\n",
    "merged_subsequences\n",
    "\n",
    "position_bins_info_df = deepcopy(partition_result.position_bins_info_df)\n",
    "position_changes_info_df = deepcopy(partition_result.position_changes_info_df)\n",
    "position_bins_info_df\n",
    "position_changes_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_data_index: 8\n",
    "data_x: 154.699102134922\n",
    "data_y: 206.9536589448834\n",
    "pixel_x: 260\n",
    "pixel_y: 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Ctx(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "    user_annotations[ctx + Ctx(decoder='long_LR')] = [[149.959, 150.254], [191.609, 191.949], [670.216, 670.418], [808.799, 808.948], [993.868, 994.185]]\n",
    "    user_annotations[ctx + Ctx(decoder='long_RL')] = [[251.417, 251.812], [624.226, 624.499], [637.785, 638.182], [1085.080, 1085.184], [1117.650, 1118.019], [1252.562, 1252.739], [1348.890, 1349.264], [1440.852, 1441.328]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_LR')] = [[105.400, 105.563], [564.149, 564.440], [1085.596, 1086.046], [1161.001, 1161.274], [1302.651, 1302.801], [1332.283, 1332.395], [1705.053, 1705.141], [1707.712, 1707.919], [1729.689, 1730.228]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_RL')] = [[132.511, 132.791], [154.499, 154.853], [1085.596, 1086.046], [1117.650, 1118.019], [1244.038, 1244.176], [1252.562, 1252.739], [1262.523, 1262.926], [1316.056, 1316.270], [1317.977, 1318.181], [1348.890, 1349.264], [1731.111, 1731.288]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "    user_annotations[ctx + Ctx(decoder='long_LR')] = []\n",
    "    user_annotations[ctx + Ctx(decoder='long_RL')] = [[223.204, 223.514], [235.576, 235.744]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_LR')] = [[223.204, 223.514]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_RL')] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Ctx(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "    user_annotations[ctx + Ctx(decoder='long_LR')] = [[993.868, 994.185]]\n",
    "    user_annotations[ctx + Ctx(decoder='long_RL')] = [[473.423, 473.747], [624.226, 624.499], [637.785, 638.182], [1136.192, 1136.453], [1348.890, 1349.264], [1673.437, 1673.920], [1693.342, 1693.482]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_LR')] = [[534.584, 534.939], [564.149, 564.440], [760.981, 761.121], [1131.640, 1131.887], [1161.001, 1161.274], [1332.283, 1332.395], [1707.712, 1707.919]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_RL')] = [[438.267, 438.448], [637.785, 638.182], [1085.596, 1086.046], [1117.650, 1118.019], [1252.562, 1252.739], [1262.523, 1262.926], [1316.056, 1316.270], [1348.890, 1349.264], [1440.852, 1441.328], [1729.689, 1730.228], [1731.111, 1731.288]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_name = 'short_LR'\n",
    "a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers[a_name]\n",
    "a_plots = a_pagination_controller.plots.decoded_sequence_and_heuristics_curves\n",
    "# plots = a_plots.plots\n",
    "# plots\n",
    "\n",
    "list(a_pagination_controller.plots_data.keys())\n",
    "\n",
    "# a_pagination_controller.filter_epochs_decoder_result\n",
    "a_pagination_controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filter_epochs_decoder_result: DecodedFilterEpochsResult = a_pagination_controller.plots_data.filter_epochs_decoder_result # .filter_epochs_decoder_result\n",
    "a_single_epoch_decoded_result = a_filter_epochs_decoder_result.get_result_for_epoch_at_time(epoch_start_time=105.40014315512963) # SingleEpochDecodedResult\n",
    "a_single_epoch_decoded_result.epoch_info_tuple\n",
    "\n",
    "a_single_epoch_decoded_result.p_x_given_n\n",
    "a_single_epoch_decoded_result.most_likely_positions\n",
    "# decoded_sequence_and_heuristics_curves_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_waterfall_chart(values, categories=None, ax=None, color_positive='green', color_negative='red', color_total='blue', edgecolor='black'):\n",
    "    \"\"\"Create a waterfall chart in Matplotlib with optional custom colors and axes.\"\"\"\n",
    "    if categories is None:\n",
    "        categories = [f'Item {i}' for i in range(len(values))]\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # Calculate cumulative starting points\n",
    "    cumulative = np.cumsum([0] + values[:-1])\n",
    "\n",
    "    # Determine bar colors\n",
    "    colors = []\n",
    "    for i, val in enumerate(values):\n",
    "        if i == len(values) - 1:\n",
    "            colors.append(color_total)\n",
    "        elif val >= 0:\n",
    "            colors.append(color_positive)\n",
    "        else:\n",
    "            colors.append(color_negative)\n",
    "\n",
    "    # Plot bars\n",
    "    x_positions = np.arange(len(values))\n",
    "    for i, val in enumerate(values):\n",
    "        bottom = cumulative[i] if val >= 0 else cumulative[i] + val\n",
    "        ax.bar(x_positions[i], abs(val), bottom=bottom, color=colors[i], edgecolor=edgecolor)\n",
    "\n",
    "    # Annotate values\n",
    "    for i, val in enumerate(values):\n",
    "        label_y = cumulative[i] + val if val >= 0 else cumulative[i] + val\n",
    "        ax.text(x_positions[i], label_y, f'{val:+.2f}', ha='center', va='bottom' if val >= 0 else 'top')\n",
    "\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.axhline(0, color='black', linewidth=1)\n",
    "    ax.set_title('Waterfall Chart')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "values = [100, 40, -30, 50, 60, -10, 210]\n",
    "categories = ['Start', 'Inc A', 'Dec B', 'Inc C', 'Inc D', 'Dec E', 'Total']\n",
    "\n",
    "# Create an Axes object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Call the waterfall function (assume it's already defined or imported)\n",
    "create_waterfall_chart(values=values, categories=categories, ax=ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-12-03 Constrains existing axes using the figure's layout_engine. Used to create a right margin to render text in (but already factored out)\n",
    "\n",
    "# for an_ax in a_pagination_controller.matplotlib_widget.axes:\n",
    "    \n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    a_fig = a_pagination_controller.matplotlib_widget.fig\n",
    "    # Get current subplots_adjust positions\n",
    "    current_adjust = deepcopy(a_fig.subplotpars) # type(current_adjust): matplotlib.figure.SubplotParams\n",
    "    current_adjust_dict = deepcopy(current_adjust.__dict__) # {'left': 0.125, 'bottom': 0.11, 'right': 0.9, 'top': 0.88, 'wspace': 0.2, 'hspace': 0.2}\n",
    "    print(\"Left:\", current_adjust.left)\n",
    "    print(\"Right:\", current_adjust.right)\n",
    "    print(\"Bottom:\", current_adjust.bottom)\n",
    "    print(\"Top:\", current_adjust.top)\n",
    "    print(\"Wspace:\", current_adjust.wspace)\n",
    "    print(\"Hspace:\", current_adjust.hspace)\n",
    "\n",
    "    # Get the current layout engine\n",
    "    layout_engine = a_fig.get_layout_engine()\n",
    "    print(\"Current layout engine:\", layout_engine) # Current layout engine: <matplotlib.layout_engine.ConstrainedLayoutEngine object at 0x00000176E18FB700>\n",
    "\n",
    "    if isinstance(layout_engine, matplotlib.layout_engine.ConstrainedLayoutEngine):\n",
    "        print(\"Constrained layout is active.\")\n",
    "        # Get the current constrained layout pads\n",
    "        pads = a_fig.get_constrained_layout_pads() # (0.04167, 0.04167, 0.02, 0.02)\n",
    "        pads_dict = dict(zip(['w_pad', 'h_pad', 'wspace', 'hspace'], pads))\n",
    "        pads_dict\n",
    "        print(\"w_pad:\", pads_dict['w_pad'])\n",
    "        print(\"h_pad:\", pads_dict['h_pad'])\n",
    "        print(\"wspace:\", pads_dict['wspace'])\n",
    "        print(\"hspace:\", pads_dict['hspace'])\n",
    "        # Adjust the right margin\n",
    "        # a_fig.set_constrained_layout_pads(right=0.8) # wspace=0.05, hspace=0.05,\n",
    "        curr_layout_rect = deepcopy(layout_engine.__dict__['_params'].get('rect', None)) # {'_params': {'h_pad': 0.04167, 'w_pad': 0.04167, 'hspace': 0.02, 'wspace': 0.02, 'rect': (0, 0, 1, 1)}, '_compress': False}\n",
    "        curr_layout_rect\n",
    "        # layout_engine.get('rect')\n",
    "        layout_engine.set(rect=(0.0, 0.0, 0.8, 1.0)) # ConstrainedLayoutEngine uses rect = (left, bottom, width, height)\n",
    "        \n",
    "    else:\n",
    "        print(\"Other layout engine or none is active.\")\n",
    "\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()\n",
    "\n",
    "# Left: 0.125\n",
    "# Right: 0.9\n",
    "# Bottom: 0.11\n",
    "# Top: 0.88\n",
    "# Wspace: 0.2\n",
    "# Hspace: 0.2\n",
    "\n",
    "# Adjust the right margin\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.indexing_helpers import NumpyHelpers, PandasHelpers\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import get_track_length_dict\n",
    "from neuropy.utils.indexing_helpers import ListHelpers\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring, HeuristicScoresTuple, SubsequencesPartitioningResult, is_valid_sequence_index\n",
    "from pyphocorehelpers.DataStructure.general_parameter_containers import RenderPlots\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {a_name:idealized_track_length_dict[a_name.split('_', maxsplit=1)[0]] for a_name, a_result in a_decoded_filter_epochs_decoder_result_dict.items()} # \n",
    "decoder_track_length_dict # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "## OUTPUTS: decoder_track_length_dict\n",
    "\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "# same_thresh_fraction_of_track: float = 0.15 ## up to 15% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "# same_thresh_n_bin_units: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "max_jump_distance_cm: float = (decoder_track_length_dict['short_LR'] * 0.1) # can't jump more than 45$ of the track\n",
    "print(f'max_jump_distance_cm: {max_jump_distance_cm}')\n",
    "\n",
    "a_result: DecodedFilterEpochsResult = filtered_decoder_filter_epochs_decoder_result_dict['long_LR']\n",
    "# a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "an_epoch_idx: int = 36\n",
    "\n",
    "# intrusion_example_epoch0 = np.array([624.225748876459, 624.4987573765684])\n",
    "# intrusion_example_epoch1 = np.array([637.7847819341114, 638.1821449307026])\n",
    "# a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR']\n",
    "# a_result = a_pagination_controller.plots_data['filter_epochs_decoder_result'] # DecodedFilterEpochsResult\n",
    "# a_single_epoch_decoded_result = a_result.get_result_for_epoch_at_time(epoch_start_time=intrusion_example_epoch0[0]) # SingleEpochDecodedResult\n",
    "# a_single_epoch_decoded_result.epoch_info_tuple\n",
    "\n",
    "## INPUTS: a_result: DecodedFilterEpochsResult, an_epoch_idx: int = 1, a_decoder_track_length: float\n",
    "a_most_likely_positions_list = a_result.most_likely_positions_list[an_epoch_idx]\n",
    "a_p_x_given_n = a_result.p_x_given_n_list[an_epoch_idx] # np.shape(a_p_x_given_n): (62, 9)\n",
    "n_time_bins: int = a_result.nbins[an_epoch_idx]\n",
    "n_pos_bins: int = np.shape(a_p_x_given_n)[0]\n",
    "time_window_centers = a_result.time_window_centers[an_epoch_idx]\n",
    "# a_track_length_cm: float = same_thresh_cm['long_LR']\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "# a_same_thresh_cm: float = 0.0\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "\n",
    "print(f'n_time_bins: {n_time_bins}')\n",
    "\n",
    "# INPUTS: a_most_likely_positions_list, n_pos_bins\n",
    "\n",
    "a_first_order_diff = np.diff(a_most_likely_positions_list, n=1, prepend=[a_most_likely_positions_list[0]])\n",
    "assert len(a_first_order_diff) == len(a_most_likely_positions_list), f\"the prepend above should ensure that the sequence and its first-order diff are the same length.\"\n",
    "\n",
    "## 2024-05-09 Smarter method that can handle relatively constant decoded positions with jitter:\n",
    "# partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.partition_subsequences_ignoring_repeated_similar_positions(a_first_order_diff, same_thresh=same_thresh)  # Add 1 because np.diff reduces the index by 1\n",
    "# not_ignoring_similar_partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list, flat_time_window_centers=time_window_centers, n_pos_bins=n_pos_bins, max_ignore_bins=2, same_thresh=0.0)\n",
    "partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list, flat_time_window_centers=time_window_centers, n_pos_bins=n_pos_bins, max_ignore_bins=2, same_thresh=a_same_thresh_cm, max_jump_distance_cm=max_jump_distance_cm, debug_print=True)\n",
    "\n",
    "# Split the array at each index where a sign change occurs\n",
    "relative_indicies_arr = np.arange(n_pos_bins)\n",
    "\n",
    "# active_split_indicies = deepcopy(partition_result.split_indicies) ## this is what it should be, but all the splits are +1 later than they should be\n",
    "active_split_indicies = deepcopy(partition_result.diff_split_indicies) ## this is what it should be, but all the splits are +1 later than they should be\n",
    "\n",
    "split_relative_indicies = np.split(relative_indicies_arr, active_split_indicies)\n",
    "split_most_likely_positions_arrays = np.split(a_most_likely_positions_list, active_split_indicies)\n",
    "split_most_likely_positions_arrays\n",
    "\n",
    "# split_first_order_diff_arrays = np.split(a_first_order_diff, partition_result.split_indicies)\n",
    "split_first_order_diff_arrays = np.split(a_first_order_diff, partition_result.diff_split_indicies)\n",
    "\n",
    "# longest_sequence\n",
    "split_diff_index_subsequence_index_arrays = np.split(np.arange(partition_result.n_diff_bins), partition_result.diff_split_indicies) # subtract 1 again to get the diff_split_indicies instead\n",
    "no_low_magnitude_diff_index_subsequence_indicies = [v[np.isin(v, partition_result.low_magnitude_change_indicies, invert=True)] for v in split_diff_index_subsequence_index_arrays] # get the list of indicies for each subsequence without the low-magnitude ones\n",
    "num_subsequence_bins = np.array([len(v) for v in split_diff_index_subsequence_index_arrays]) # np.array([4, 6])\n",
    "num_subsequence_bins_no_repeats = np.array([len(v) for v in no_low_magnitude_diff_index_subsequence_indicies]) # np.array([1, 1])\n",
    "\n",
    "# num_subsequence_bins: number of tbins in each split sequence\n",
    "# num_subsequence_bins_no_repeats\n",
    "\n",
    "total_num_subsequence_bins = np.sum(num_subsequence_bins)\n",
    "total_num_subsequence_bins_no_repeats = np.sum(num_subsequence_bins_no_repeats)\n",
    "\n",
    "longest_sequence_length_no_repeats: int = int(np.nanmax(num_subsequence_bins_no_repeats)) # Now find the length of the longest non-changing sequence\n",
    "longest_sequence_no_repeats_start_idx: int = int(np.nanargmax(num_subsequence_bins_no_repeats)) ## the actual start index of the longest sequence!\n",
    "# longest_sequence_no_repeats_start_idx\n",
    "\n",
    "\n",
    "# _tmp_merge_split_positions_arrays, final_out_subsequences, (subsequence_replace_dict, subsequences_to_add, subsequences_to_remove, final_intrusion_idxs) = partition_result.merge_over_ignored_intrusions(max_ignore_bins=2, debug_print=True)\n",
    "# subsequence_replace_dict\n",
    "# print(subsequences_to_remove)\n",
    "# print(subsequences_to_add)\n",
    "# final_intrusion_idxs\n",
    "# final_out_subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays\n",
    "# partition_result.bridged_intrusion_bin_indicies\n",
    "partition_result.sequence_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out: MatplotlibRenderPlots = _plot_step_by_step_subsequence_partition_process(partition_result=partition_result)\n",
    "# out\n",
    "\n",
    "out: MatplotlibRenderPlots = partition_result._plot_step_by_step_subsequence_partition_process()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.partition_subsequences()\n",
    "partition_result.merge_intrusions()\n",
    "partition_result.merged_split_positions_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out2: MatplotlibRenderPlots = partition_result._plot_step_by_step_subsequence_partition_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example Sequences\n",
    "partition_result.list_parts\n",
    "\n",
    "with pd.option_context('display.max_rows', 100):\n",
    "    partition_result.sequence_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_info_df: pd.DataFrame = partition_result.rebuild_sequence_info_df()\n",
    "with pd.option_context('display.max_rows', 100):\n",
    "    display(sequence_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_result.longest_subsequence_length\n",
    "partition_result.longest_sequence_length_no_repeats\n",
    "partition_result.longest_sequence_no_repeats_start_idx\n",
    "partition_result.longest_sequence_subsequence\n",
    "\n",
    "partition_result.first_order_diff_lst\n",
    "partition_result.low_magnitude_change_indicies\n",
    "partition_result.diff_split_indicies\n",
    "partition_result.split_indicies\n",
    "\n",
    "# get_longest_sequence_length_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "if isinstance(a_most_likely_positions_list, (list, tuple, )):\n",
    "    a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "\n",
    "(sub_change_equivalency_groups, sub_change_equivalency_group_values), (list_parts, list_split_indicies, sub_change_threshold_change_indicies) = SubsequencesPartitioningResult.detect_repeated_similar_positions(a_most_likely_positions_list, same_thresh=a_same_thresh_cm)\n",
    "sub_change_equivalency_groups\n",
    "# list_parts\n",
    "sub_change_equivalency_group_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sequence_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "longest_sequence_subsequence_partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.init_from_positions_list(longest_sequence_subsequence, n_pos_bins=n_pos_bins, max_ignore_bins=2, same_thresh=a_same_thresh_cm)\n",
    "longest_sequence_subsequence_partition_result.merged_split_positions_arrays ## makes things worse\n",
    "longest_sequence_subsequence_partition_result.list_parts\n",
    "longest_sequence_subsequence_partition_result.split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos_bins: int = 57\n",
    "a_same_thresh_cm: float = 32.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import function_attributes\n",
    "\n",
    "\n",
    "@function_attributes(short_name=None, tags=['split'], 'UNUSED', 'UNVALIDATED', 'fresh-reimpleemntation-attempt', input_requires=[], output_provides=[], uses=[], used_by=['_compute_should_split_arr'], creation_date='2024-12-04 04:02', related_items=[])\n",
    "def _compute_is_change_point_split_arr(first_order_diff_lst):\n",
    "    ## INPUTS: prev_accum_dir, first_order_diff_lst\n",
    "    prev_accum_dir = None # sentinal value\n",
    "    is_change_point_arr = []\n",
    "    did_accum_dir_change_arr = []\n",
    "    for i, v in enumerate(first_order_diff_lst):\n",
    "        curr_dir = np.sign(v)\n",
    "        did_accum_dir_change: bool = (prev_accum_dir != curr_dir)# and (prev_accum_dir is not None) and (prev_accum_dir != 0)\n",
    "        did_accum_dir_change_arr.append(did_accum_dir_change)\n",
    "        is_change_point: bool = True # (prev_accum_dir is None)\n",
    "        if did_accum_dir_change: \n",
    "            ## Exceeds the `same_thresh` indicating we want to use the change\n",
    "            ## sign changed, split here.\n",
    "            is_change_point = True\n",
    "            if (curr_dir != 0):\n",
    "                # only for non-zero directions should we set the prev_accum_dir, otherwise leave it what it was (or blank)\n",
    "                if prev_accum_dir is None:\n",
    "                    is_change_point = False # don't split for the first direction change (since it's a change from None/0.0\n",
    "                else:\n",
    "                    is_change_point = True\n",
    "                # ## either way update the prev_accum_dir\n",
    "                # prev_accum_dir = curr_dir\n",
    "            else:\n",
    "                print(f'debug: iteration[{i}] - v: {v} - curr_dir == 0')\n",
    "                ## #TODO 2024-12-04 04:15: - [ ] if it's 0, we consider it a change point\n",
    "                is_change_point = False\n",
    "\n",
    "            ## return should_split\n",
    "            # is_change_point_arr.append(is_change_point)\n",
    "            # END if (np.abs(v) > same_thresh)\n",
    "        else:\n",
    "            is_change_point = False # no change, shouldn't split\n",
    "            # is_change_point_arr.append(is_change_point)\n",
    "            ## normally continue accumulating without splitting\n",
    "        # END if did_accum_dir_change ...\n",
    "        is_change_point_arr.append(is_change_point) ## now `is_change_point` should be correct\n",
    "        ## either way update the prev_accum_dir\n",
    "        prev_accum_dir = curr_dir\n",
    "    \n",
    "\n",
    "    # end for i, v \n",
    "    is_change_point_arr = np.array(is_change_point_arr)\n",
    "    did_accum_dir_change_arr = np.array(did_accum_dir_change_arr)\n",
    "    return is_change_point_arr, did_accum_dir_change_arr\n",
    "    # return should_split_arr, did_accum_dir_change_arr\n",
    "\n",
    "\n",
    "\n",
    "@function_attributes(short_name=None, tags=['split', 'UNUSED', 'UNVALIDATED', 'fresh-reimpleemntation-attempt'], input_requires=[], output_provides=[], uses=['_compute_is_change_point_split_arr'], used_by=[], creation_date='2024-12-04 04:02', related_items=[])\n",
    "def _compute_should_split_arr(a_most_likely_positions_list, same_thresh: float):\n",
    "    \"\"\" \n",
    "    should_split = _compute_should_split_arr(first_order_diff_lst)\n",
    "    should_split\n",
    "    \n",
    "    \"\"\"\n",
    "    ## INPUTS: prev_accum_dir, first_order_diff_lst\n",
    "    ## INPUTS: a_most_likely_positions_list, same_thresh, \n",
    "    if isinstance(a_most_likely_positions_list, list):\n",
    "        a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "\n",
    "    first_order_diff_lst = np.diff(a_most_likely_positions_list, n=1, prepend=[a_most_likely_positions_list[0]])\n",
    "    assert len(first_order_diff_lst) == len(a_most_likely_positions_list), f\"the prepend above should ensure that the sequence and its first-order diff are the same length.\"\n",
    "    is_change_point_arr, did_accum_dir_change_arr = _compute_is_change_point_split_arr(first_order_diff_lst)\n",
    "    is_subthreshold = (np.abs(first_order_diff_lst) <= same_thresh)\n",
    "    # np.logical_and(did_accum_dir_change_arr, np.logical_not(is_subthreshold))\n",
    "    should_split = np.logical_and(is_change_point_arr, np.logical_not(is_subthreshold))    \n",
    "    return should_split, (is_change_point_arr, is_subthreshold)\n",
    "\n",
    "\n",
    "# longest_sequence_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "# a_most_likely_positions_list = longest_sequence_subsequence\n",
    "a_most_likely_positions_list = deepcopy(partition_result.flat_positions)\n",
    "same_thresh = a_same_thresh_cm\n",
    "should_split, (is_change_point_arr, is_subthreshold) = _compute_should_split_arr(a_most_likely_positions_list, same_thresh=same_thresh)\n",
    "should_split\n",
    "is_change_point_arr\n",
    "is_subthreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "longest_sequence_subsequence\n",
    "(sub_change_equivalency_groups, sub_change_equivalency_group_values), (list_parts, list_split_indicies, sub_change_threshold_change_indicies) = SubsequencesPartitioningResult.detect_repeated_similar_positions(longest_sequence_subsequence, same_thresh=a_same_thresh_cm)\n",
    "sub_change_equivalency_groups\n",
    "# list_parts\n",
    "sub_change_equivalency_group_values\n",
    "sub_change_threshold_change_indicies\n",
    "list_split_indicies \n",
    "list_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_ignoring_similar_partition_result\n",
    "not_ignoring_similar_partition_result.merged_split_positions_arrays # subsequence_index_lists_omitting_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.total_num_subsequence_bins\n",
    "partition_result.total_num_subsequence_bins_no_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.low_magnitude_change_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_sequence_info_df: pd.DataFrame = partition_result.rebuild_sequence_info_df()\n",
    "longest_subsequence_df: pd.DataFrame = rebuild_sequence_info_df[rebuild_sequence_info_df['subsequence_idx'] == 1]\n",
    "longest_subsequence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays\n",
    "split_arr_lengths = [len(v) for v in partition_result.split_positions_arrays]\n",
    "split_arr_lengths = flatten([[i] * len(v) for i, v in enumerate(partition_result.split_positions_arrays)])\n",
    "\n",
    "split_arr_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.low_magnitude_change_indicies\n",
    "partition_result.split_indicies\n",
    "partition_result.num_merged_subsequence_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ignore_bins: int = 2\n",
    "_tmp_merge_split_positions_arrays, final_out_subsequences, (subsequence_replace_dict, subsequences_to_add, subsequences_to_remove) = partition_result.merge_over_ignored_intrusions(max_ignore_bins=max_ignore_bins)\n",
    "_tmp_merge_split_positions_arrays\n",
    "subsequence_replace_dict\n",
    "final_out_subsequences\n",
    "\n",
    "# subsequences = deepcopy(partition_result.split_positions_arrays)\n",
    "# subsequences\n",
    "\n",
    "# remaining_subsequence_list= [[119.191, 142.107, 180.3, 191.757, 245.227], [84.8181, 84.8181, 84.8181]]\n",
    "# remaining_subsequence_list.reverse()\n",
    "# remaining_subsequence_list\n",
    "# curr_subsequence, remaining_subsequence_list = merge_subsequences([138.288, 134.469], remaining_subsequence_list=remaining_subsequence_list)\n",
    "\n",
    "# merged_subsequences = merge_subsequences(subsequences, max_ignore_bins=1)\n",
    "# merged_subsequences\n",
    "fig2, ax2 = _debug_plot_time_bins_multiple(positions_list=final_out_subsequences, num='debug_plot_merged_time_binned_positions')\n",
    "\n",
    "# array([138.288, 134.469]), array([69.5411]), array([249.046, 249.046, 249.046])\n",
    "# array([138.288, 134.469, 69.5411, 249.046, 249.046, 249.046])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (left_congruent_flanking_sequence, left_congruent_flanking_index), (right_congruent_flanking_sequence, right_congruent_flanking_index) = _compute_sequences_spanning_ignored_intrusions(split_first_order_diff_arrays, continuous_sequence_lengths, longest_sequence_start_idx=longest_sequence_start_idx, max_ignore_bins=max_ignore_bins)\n",
    "(left_congruent_flanking_sequence, left_congruent_flanking_index), (right_congruent_flanking_sequence, right_congruent_flanking_index) = _compute_sequences_spanning_ignored_intrusions(split_first_order_diff_arrays, num_subsequence_bins_no_repeats,\n",
    "                                                                                                                                                                                        target_subsequence_idx=longest_sequence_no_repeats_start_idx, max_ignore_bins=max_ignore_bins)\n",
    "print(f\"{left_congruent_flanking_sequence}: {left_congruent_flanking_sequence}\")\n",
    "print(f\"{right_congruent_flanking_sequence}: {right_congruent_flanking_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.low_magnitude_change_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.list_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_debug_plot_time_binned_positions(a_most_likely_positions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sequence_length_ratio = HeuristicReplayScoring.bin_wise_continuous_sequence_sort_score_fn(a_result=a_result, an_epoch_idx=an_epoch_idx, a_decoder_track_length=170.0, same_thresh=same_thresh)\n",
    "longest_sequence_length_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_filter_epochs_df = deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "active_filter_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test :🔷🚧💯  `SubsequencesPartitioningResult` until it's FULLY WORKING - 2024-12-04 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResult\n",
    "from neuropy.utils.indexing_helpers import PandasHelpers\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackInstance\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequenceDetectionSamples, GroundTruthData, ComputedPartitioningData, desired_selected_indicies_dict\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import classify_pos_bins\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import InteractivePlot\n",
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "## HARDCODED:\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "a_decoder_track_length: float = 214.0\n",
    "pos_bin_edges = np.array([37.0774, 40.8828, 44.6882, 48.4936, 52.2991, 56.1045, 59.9099, 63.7153, 67.5207, 71.3261, 75.1316, 78.937, 82.7424, 86.5478, 90.3532, 94.1586, 97.9641, 101.769, 105.575, 109.38, 113.186, 116.991, 120.797, 124.602, 128.407, 132.213, 136.018, 139.824, 143.629, 147.434, 151.24, 155.045, 158.851, 162.656, 166.462, 170.267, 174.072, 177.878, 181.683, 185.489, 189.294, 193.099, 196.905, 200.71, 204.516, 208.321, 212.127, 215.932, 219.737, 223.543, 227.348, 231.154, 234.959, 238.764, 242.57, 246.375, 250.181, 253.986])\n",
    "\n",
    "\n",
    "# Parameters _________________________________________________________________________________________________________ #\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "# max_jump_distance_cm: float = (same_thresh_fraction_of_track * a_decoder_track_length)\n",
    "# same_thresh = 0.0\n",
    "# same_thresh = 10.0\n",
    "# same_thresh = 60.0\n",
    "\n",
    "\n",
    "max_jump_distance_cm: float = 60.0 # Hardcoded\n",
    "\n",
    "# max_ignore_bins = 0\n",
    "max_ignore_bins = 2\n",
    "\n",
    "\n",
    "# Computed ___________________________________________________________________________________________________________ #\n",
    "a_same_thresh_cm: float = (same_thresh_fraction_of_track * a_decoder_track_length) # ALWAYS USE THE SAME FOR ALL TRACKS/DECODERS\n",
    "print(f'same_thresh_cm: {a_same_thresh_cm}') # a_same_thresh_cm: float = 10.700000000000001\n",
    "\n",
    "# hard_y_lims = (pos_bin_edges[0], pos_bin_edges[-1])\n",
    "# SubsequenceDetectionSamples.get_all_examples()\n",
    "test_dict = SubsequenceDetectionSamples.get_all_example_dict()\n",
    "\n",
    "test_dict, partitioned_results, _new_scores_df, _all_examples_scores_dict, _comparison_with_desired_dict = SubsequenceDetectionSamples.build_test_results_dict(test_dict=test_dict, same_thresh=a_same_thresh_cm, max_ignore_bins=max_ignore_bins, max_jump_distance_cm=max_jump_distance_cm, pos_bin_edges=deepcopy(pos_bin_edges),\n",
    "                                                                                                                                desired_selected_indicies_dict=desired_selected_indicies_dict)\n",
    "_new_scores_df\n",
    "_comparison_with_desired_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax_dict = SubsequenceDetectionSamples.plot_test_subsequences_as_ax_stack(partitioned_results=partitioned_results, desired_selected_indicies_dict=desired_selected_indicies_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_name: str = 'true_positives[5]' ## good for showing partitioning\n",
    "a_name: str = 'true_positives[4]'\n",
    "# subsequence_partitioning_result: SubsequencesPartitioningResult = partitioned_results['intrusion[0]']\n",
    "subsequence_partitioning_result: SubsequencesPartitioningResult = partitioned_results[a_name]\n",
    "# subsequence_partitioning_result: SubsequencesPartitioningResult = partitioned_results['chose_incorrect_subsequence_as_main_list[0]']\n",
    "\n",
    "# desired_partition_indicies = desired_selected_indicies_dict[a_name]\n",
    "# _out = subsequence_partitioning_result.plot_time_bins_multiple()\n",
    "out: MatplotlibRenderPlots = subsequence_partitioning_result._plot_step_by_step_subsequence_partition_process(should_show_non_main_sequence_hlines=True)\n",
    "\n",
    "# Pass the existing ax to the InteractivePlot\n",
    "# interactive_plot = InteractivePlot(_out.axes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot.selected_indicies # [11, 10, 9, 8, 7, 6, 5, 3, 2]\n",
    "# interactive_plot.selected_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequence_partitioning_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsequence_partitioning_result.flat_positions\n",
    "subsequence_partitioning_result.flat_positions[interactive_plot.selected_indicies]\n",
    "# array([183.586, 160.753, 134.116, 96.0614, 80.8397, 77.0343, 187.391])\n",
    "\n",
    "desired_pos_bins_dict = {'chose_incorrect_subsequence_as_main_list[0]':[183.586, 160.753, 134.116, 96.0614, 80.8397, 77.0343, 187.391],\n",
    "                         \n",
    "                        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ui, figures_dict, axs_dict), all_examples_plot_data_positions_arr_dict = SubsequenceDetectionSamples.plot_all_tabbled_figure(decoder_track_length_dict=decoder_track_length_dict, pos_bin_edges=deepcopy(pos_bin_edges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pos_bin_edges = deepcopy(track_templates.get_decoders_dict()['long_LR'].xbin_centers)\n",
    "pos_classification_df = classify_pos_bins(x=pos_bin_edges)\n",
    "pos_classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(ui, figures_dict, axs_dict), all_examples_plot_data_positions_arr_dict = SubsequenceDetectionSamples.plot_all_tabbled_figure(decoder_track_length_dict=decoder_track_length_dict, pos_bin_edges=pos_bin_edges)\n",
    "\n",
    "# test_dict\n",
    "# get_all_example_dict\n",
    "ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResultScoringComputations\n",
    "\n",
    "decoder_track_length: float = 214.0\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "# pos_bin_edges=deepcopy(track_templates.long_LR_decoder.xbin)\n",
    "pos_bin_edges = np.array([37.0774, 40.8828, 44.6882, 48.4936, 52.2991, 56.1045, 59.9099, 63.7153, 67.5207, 71.3261, 75.1316, 78.937, 82.7424, 86.5478, 90.3532, 94.1586, 97.9641, 101.769, 105.575, 109.38, 113.186, 116.991, 120.797, 124.602, 128.407, 132.213, 136.018, 139.824, 143.629, 147.434, 151.24, 155.045, 158.851, 162.656, 166.462, 170.267, 174.072, 177.878, 181.683, 185.489, 189.294, 193.099, 196.905, 200.71, 204.516, 208.321, 212.127, 215.932, 219.737, 223.543, 227.348, 231.154, 234.959, 238.764, 242.57, 246.375, 250.181, 253.986])\n",
    "\n",
    "SubsequencesPartitioningResult_common_init_kwargs = dict(same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=60.0, pos_bin_edges=deepcopy(pos_bin_edges), debug_print=False)\n",
    "test_dict = SubsequenceDetectionSamples.get_all_example_dict()\n",
    "partitioned_results, _new_scores_df, _all_examples_scores_dict = SubsequenceDetectionSamples.build_test_results_dict(test_dict=test_dict, **SubsequencesPartitioningResult_common_init_kwargs)\n",
    "_new_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: partitioner_kwargs\n",
    "# max_ignore_bins: int = 2, same_thresh: float = 4, max_jump_distance_cm: Optional[float]=None\n",
    "\n",
    "partitioned_results = {}\n",
    "\n",
    "for a_group_name, group_items in test_dict.items():\n",
    "    for i, (a_pos_tuple, a_ground_truth) in enumerate(group_items.items()):\n",
    "        print(f'a_group_name: {a_group_name}')\n",
    "        a_partitioner: SubsequencesPartitioningResult = SubsequenceDetectionSamples.build_partition_sequence(a_pos_tuple, **SubsequencesPartitioningResult_common_init_kwargs)\n",
    "        a_ground_truth.arr = deepcopy(a_partitioner.flat_positions)\n",
    "        partitioned_results[f\"{a_group_name}[{i}]\"] = a_partitioner\n",
    "\n",
    "## OUTPUTS: partitioned_results\n",
    "\n",
    "\n",
    "## INPUTS: partitioned_results, decoder_track_length\n",
    "all_subseq_partitioning_score_computations_fn_dict = SubsequencesPartitioningResultScoringComputations.build_all_bin_wise_subseq_partitioning_computation_fn_dict()\n",
    "\n",
    "\n",
    "# for an_epoch_idx, (a_example_name, a_partition_result) in enumerate(partitioned_results.items()):\n",
    "    \n",
    "#     {score_computation_name:computation_fn(partition_result=a_partition_result, a_result=None, an_epoch_idx=an_epoch_idx, a_decoder_track_length=decoder_track_length, pos_bin_edges=a_partition_result.pos_bin_edges) for score_computation_name, computation_fn in all_subseq_partitioning_score_computations_fn_dict.items()}\n",
    "_all_examples_scores_dict = {a_example_name: {score_computation_name:computation_fn(partition_result=a_partition_result, a_result=None, an_epoch_idx=an_epoch_idx, a_decoder_track_length=decoder_track_length, pos_bin_edges=a_partition_result.pos_bin_edges) for score_computation_name, computation_fn in all_subseq_partitioning_score_computations_fn_dict.items()} for an_epoch_idx, (a_example_name, a_partition_result) in enumerate(partitioned_results.items())}\n",
    "_all_examples_scores_dict\n",
    "# _all_epochs_scores_dict[unique_full_decoder_score_column_name] = [computation_fn(partition_result=a_partition_result, a_result=a_result, an_epoch_idx=an_epoch_idx, a_decoder_track_length=a_decoder_track_length, pos_bin_edges=xbin_edges) for an_epoch_idx, a_partition_result in enumerate(partition_result_dict[a_name])]\n",
    "## OUTPUTS: _all_examples_scores_dict\n",
    "\n",
    "# once done with all scores for this decoder, have `_a_separate_decoder_new_scores_dict`:\n",
    "_new_scores_df =  pd.DataFrame(_all_examples_scores_dict)\n",
    "_new_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define(slots=False)\n",
    "class ComputedPartitioningData:\n",
    "    detection_quality: float = field()\n",
    "    is_good: bool = field(default=None)\n",
    "    note: str = field(default='')\n",
    "    arr: NDArray = field(default=None)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        if self.is_good is None:\n",
    "            self.is_good = (self.detection_quality > 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResult\n",
    "\n",
    "# n_pos_bins: int = 57\n",
    "# max_ignore_bins: int = 2\n",
    "# a_same_thresh_cm: float = 10.700000000000001\n",
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "# pos_bin_edges=deepcopy(track_templates.long_LR_decoder.xbin)\n",
    "pos_bin_edges = np.array([37.0774, 40.8828, 44.6882, 48.4936, 52.2991, 56.1045, 59.9099, 63.7153, 67.5207, 71.3261, 75.1316, 78.937, 82.7424, 86.5478, 90.3532, 94.1586, 97.9641, 101.769, 105.575, 109.38, 113.186, 116.991, 120.797, 124.602, 128.407, 132.213, 136.018, 139.824, 143.629, 147.434, 151.24, 155.045, 158.851, 162.656, 166.462, 170.267, 174.072, 177.878, 181.683, 185.489, 189.294, 193.099, 196.905, 200.71, 204.516, 208.321, 212.127, 215.932, 219.737, 223.543, 227.348, 231.154, 234.959, 238.764, 242.57, 246.375, 250.181, 253.986])\n",
    "\n",
    "SubsequencesPartitioningResult_common_init_kwargs = dict(same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=60.0, pos_bin_edges=deepcopy(pos_bin_edges), debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=list(SubsequenceDetectionSamples.false_positive_list.values())[0], **SubsequencesPartitioningResult_common_init_kwargs,)\n",
    "\n",
    "# partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=SubsequenceDetectionSamples.most_likely_positions_bad_single_main_seq, **SubsequencesPartitioningResult_common_init_kwargs,)\n",
    "# Access the partitioned subsequences\n",
    "subsequences = partition_result.split_positions_arrays\n",
    "merged_subsequences = partition_result.merged_split_positions_arrays\n",
    "print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "subsequences\n",
    "merged_subsequences\n",
    "\n",
    "longest_seq_length_dict = {'neither': partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=False, should_use_no_repeat_values=False),\n",
    "    'ignoring_intru': partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False),\n",
    "    '+no_repeat': partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=True),\n",
    "}\n",
    "\n",
    "longest_seq_length_multiline_label_str: str = '\\n'.join([': '.join([k, str(v)]) for k, v in longest_seq_length_dict.items()])\n",
    "print(longest_seq_length_multiline_label_str)\n",
    "_out = partition_result._plot_step_by_step_subsequence_partition_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = partition_result._plot_step_by_step_subsequence_partition_process()\n",
    "\n",
    "# from matplotlib.patheffects import withStroke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.position_bins_info_df\n",
    "partition_result.position_changes_info_df\n",
    "partition_result.subsequences_df['is_main']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import partition_df_dict\n",
    "\n",
    "## INPUTS: partition_result\n",
    "main_subsequence_only_df = partition_result.subsequences_df[partition_result.subsequences_df['is_main']]\n",
    "main_subsequence_idx = main_subsequence_only_df['subsequence_idx'].to_numpy()[0]\n",
    "main_subsequence_idx\n",
    "\n",
    "# main_subsequence_only_df['flat_idx_first', 'flat_idx_last', 'flat_idx_first']\n",
    "\n",
    "non_intrusion_position_bins_info_df: pd.DataFrame = partition_result.position_bins_info_df[np.logical_not(partition_result.position_bins_info_df['is_intrusion'])]\n",
    "# non_intrusion_position_bins_info_df.groupby(['subsequence_idx'])\n",
    "\n",
    "partitioned_df_dict = partition_df_dict(non_intrusion_position_bins_info_df, partitionColumn='subsequence_idx') ## WARNING: some subsequences are ALL INTRUSIONS, meaning they get eliminated here\n",
    "non_intrusion_main_subsequence_df = partitioned_df_dict[main_subsequence_idx]\n",
    "non_intrusion_main_subsequence_df\n",
    "\n",
    "non_intrusion_main_subsequence_changes_info_df = SubsequencesPartitioningResult._compute_position_changes_info_df(position_bins_info_df=non_intrusion_main_subsequence_df,\n",
    "                                                                                             same_thresh=partition_result.same_thresh, max_jump_distance_cm=partition_result.max_jump_distance_cm, additional_split_on_exceeding_jump_distance=False)\n",
    "\n",
    "non_intrusion_main_subsequence_changes_info_df\n",
    "\n",
    "\n",
    "# subsequence_idx\n",
    "## find where additional splits are needed:\n",
    "exceeding_jump_distance_df = non_intrusion_main_subsequence_changes_info_df[non_intrusion_main_subsequence_changes_info_df['exceeds_jump_distance']] ## want to insert the split after `next_bin_flat_idxs` --> at index 4\n",
    "exceeding_jump_distance_split_indicies = exceeding_jump_distance_df['next_bin_flat_idxs'].to_numpy()\n",
    "exceeding_jump_distance_split_indicies\n",
    "\n",
    "# partition_result.split_indicies\n",
    "\n",
    "new_merged_split_indicies = deepcopy(partition_result.merged_split_indicies).tolist()\n",
    "new_merged_split_indicies.extend(exceeding_jump_distance_split_indicies)\n",
    "new_merged_split_indicies = np.unique(new_merged_split_indicies) ## sort them and get only the uniques\n",
    "new_merged_split_indicies\n",
    "\n",
    "new_merged_split_positions_arrays = np.split(deepcopy(partition_result.flat_positions), new_merged_split_indicies)\n",
    "# new_merged\n",
    "new_merged_split_positions_arrays\n",
    "\n",
    "partition_result.merged_split_positions_arrays = new_merged_split_positions_arrays\n",
    "# merged_split_position_flatindicies_arrays = merged_split_position_flatindicies_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays\n",
    "longest_sequence_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "\n",
    "are_all_jumps_less_than_max = (np.abs(np.diff(longest_sequence_subsequence, n=1)) <= partition_result.max_jump_distance_cm)\n",
    "assert np.all(are_all_jumps_less_than_max), f\"all jumps should be less than the max, but they are not! longest_sequence_subsequence: {longest_sequence_subsequence}\\n\\tare_all_jumps_less_than_max: {are_all_jumps_less_than_max}\"\n",
    "are_all_jumps_less_than_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_equiv_group_list, value_equiv_group_idxs_list = SubsequencesPartitioningResult.find_value_equiv_groups(longest_sequence_subsequence, same_thresh_cm=same_thresh_cm)\n",
    "\n",
    "# a_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "a_subsequence = deepcopy([38.9801, 225.446, 225.446])\n",
    "## INPUTS: a_subsequence\n",
    "total_num_values: int = len(a_subsequence)\n",
    "_, value_equiv_group_idxs_list = SubsequencesPartitioningResult.find_value_equiv_groups(a_subsequence, same_thresh_cm=partition_result.same_thresh)\n",
    "total_num_values_excluding_repeats: int = len(value_equiv_group_idxs_list) ## the total number of non-repeated values\n",
    "total_num_repeated_values: int = total_num_values - total_num_values_excluding_repeats\n",
    "print(f'value_equiv_group_idxs_list: {value_equiv_group_idxs_list}')\n",
    "print(f'total_num_values: {total_num_values}')\n",
    "print(f'total_num_values_excluding_repeats: {total_num_values_excluding_repeats}')\n",
    "print(f'total_num_repeated_values: {total_num_repeated_values}')\n",
    "\n",
    "num_items_per_equiv_list: List[int] = [len(v) for v in value_equiv_group_idxs_list] ## number of items in each equiv-list\n",
    "num_equiv_values: int = len(value_equiv_group_idxs_list) # the number of equivalence value sets in the longest subsequence\n",
    "\n",
    "\n",
    "num_equiv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_bins_info_df = deepcopy(partition_result.position_bins_info_df)\n",
    "position_changes_info_df = deepcopy(partition_result.position_changes_info_df)\n",
    "subsequences_df = deepcopy(partition_result.subsequences_df)\n",
    "position_bins_info_df\n",
    "position_changes_info_df\n",
    "subsequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(subsequences_df.columns)) # ['subsequence_idx', 'positions', 'total_distance_traveled', 'track_coverage_score', 'main_rank', 'is_main', 'flat_idx_first', 'flat_idx_last', 'start_t', 'end_t', 'n_intrusion_bins', 'len', 'len_excluding_repeats', 'len_excluding_intrusions', 'len_excluding_both']\n",
    "\n",
    "\n",
    "all_subsequence_values_count_dict = subsequences_df[['n_intrusion_bins', 'len', 'len_excluding_repeats', 'len_excluding_intrusions', 'len_excluding_both']].sum(axis='index').to_dict()\n",
    "all_len = all_subsequence_values_count_dict['len']\n",
    "all_len = all_subsequence_values_count_dict['len_excluding_repeats']\n",
    "all_len = all_subsequence_values_count_dict['len_excluding_intrusions']\n",
    "all_len = all_subsequence_values_count_dict['len_excluding_both']\n",
    "n_intrusion_bins = all_subsequence_values_count_dict['n_intrusion_bins']\n",
    "\n",
    "# ['total_distance_traveled', 'track_coverage_score', 'main_rank', 'is_main', 'n_intrusion_bins', 'len', 'len_excluding_repeats', 'len_excluding_intrusions', 'len_excluding_both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_bins_info_df\n",
    "\n",
    "# Performed 3 aggregations grouped on column: 'decoder_epoch_id'\n",
    "computed_subseq_properties_df = deepcopy(position_bins_info_df).groupby(['subsequence_idx']).agg(flat_idx_first=('flat_idx', 'first'), flat_idx_last=('flat_idx', 'last'),\n",
    "                                                                                            start_t=('t_bin_start', 'first'), end_t=('t_bin_end', 'last'),\n",
    "                                                                                             n_intrusion_bins=('is_intrusion', 'sum')).reset_index()\n",
    "\n",
    "## merge into `subsequences_df`:\n",
    "subsequences_df = subsequences_df.merge(computed_subseq_properties_df, on='subsequence_idx') ## drops missing entries unfortunately\n",
    "# subsequences_df = subsequences_df.merge(computed_subseq_properties_df, how='left', on='subsequence_idx') ## requires that the output dataframe has all rows that were in `subsequences_df`, filling in NaNs when no corresponding values are found in the right df\n",
    "subsequences_df['len_excluding_intrusions'] = subsequences_df['len'] - subsequences_df['n_intrusion_bins']\n",
    "subsequences_df['len_excluding_both'] = subsequences_df['len_excluding_repeats'] - subsequences_df['n_intrusion_bins']\n",
    "subsequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_multitab import MplMultiTab, MplMultiTab2D, MplTabbedFigure\n",
    "from neuropy.utils.matplotlib_helpers import TabbedMatplotlibFigures\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.CustomMatplotlibTabbedWidget import CustomMplMultiTab\n",
    "\n",
    "all_examples_plot_data_positions_arr_dict = SubsequenceDetectionSamples.get_all_examples()\n",
    "# num_tabs: int = len(SubsequenceDetectionSamples.intrusion_example_positions_list)\n",
    "# plot_subplot_mosaic_dict = {f\"intrusion_example[{i}]\":dict(sharex=True, sharey=True, mosaic=[[\"ax_ungrouped_seq\"],[\"ax_grouped_seq\"],[\"ax_merged_grouped_seq\"],], gridspec_kw=dict(wspace=0, hspace=0.15)) for i, idx in enumerate(np.arange(num_tabs))}\n",
    "\n",
    "num_tabs: int = len(all_examples_plot_data_positions_arr_dict)\n",
    "plot_subplot_mosaic_dict = {a_name:dict(sharex=True, sharey=True, mosaic=[[\"ax_ungrouped_seq\"],[\"ax_grouped_seq\"],[\"ax_merged_grouped_seq\"],], gridspec_kw=dict(wspace=0, hspace=0.15)) for i, (a_name, arr) in enumerate(all_examples_plot_data_positions_arr_dict.items())}\n",
    "\n",
    "# ui = MplMultiTab()\n",
    "# ui, figures_dict, axs_dict = TabbedMatplotlibFigures.build_tabbed_multi_figure(plot_subplot_mosaic_dict, obj_class=MplMultiTab)\n",
    "ui, figures_dict, axs_dict = TabbedMatplotlibFigures.build_tabbed_multi_figure(plot_subplot_mosaic_dict, obj_class=None)\n",
    "\n",
    "# ui.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "\n",
    "SubsequencesPartitioningResult_common_init_kwargs = dict(same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=60.0, pos_bin_edges=deepcopy(pos_bin_edges), debug_print=False)\n",
    "\n",
    "## rebuild\n",
    "all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, **SubsequencesPartitioningResult_common_init_kwargs) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "# all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=15.0, n_pos_bins=57, debug_print=False) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "\n",
    "all_examples_plot_data_name_keys = list(all_examples_plot_data_dict.keys())\n",
    "\n",
    "# ui.show()\n",
    "\n",
    "# def _test():\n",
    "# for i, a_most_likely_positions_list in enumerate(SubsequenceDetectionSamples.intrusion_example_positions_list):\n",
    "# for i, (a_name, a_most_likely_positions_list) in enumerate(all_examples_plot_data_positions_arr_dict.items()):\n",
    "for i, (a_name, a_partition_result) in enumerate(all_examples_plot_data_dict.items()):\n",
    "    # a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "    # a_partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, **SubsequencesPartitioningResult_common_init_kwargs)\n",
    "    # Access the partitioned subsequences\n",
    "    subsequences = a_partition_result.split_positions_arrays\n",
    "    merged_subsequences = a_partition_result.merged_split_positions_arrays\n",
    "    print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "    print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "    print(a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False))\n",
    "    a_fig = figures_dict[a_name] # list(figures_dict.values())[i]\n",
    "    an_ax_dict = axs_dict[a_name] # list(axs_dict.values())[i]    \n",
    "    _out = a_partition_result._plot_step_by_step_subsequence_partition_process(extant_ax_dict=an_ax_dict)\n",
    "\n",
    "\n",
    "partition_result = deepcopy(a_partition_result) ## just copy the first one\n",
    "\n",
    "# create plotting function\n",
    "def _perform_plot(fig, indices):\n",
    "    \"\"\" captures: all_examples_plot_data_name_keys, \n",
    "    \"\"\"\n",
    "    print('Doing plot:', indices)\n",
    "    # i, j = indices\n",
    "    i = indices\n",
    "    a_name = all_examples_plot_data_name_keys[i]\n",
    "    print(f'\\t a_name: \"{a_name}\"')\n",
    "    # ax = fig.subplots()\n",
    "    a_partition_result = all_examples_plot_data_dict[a_name]\n",
    "    if isinstance(fig, (MplTabbedFigure, )): # , TabNode\n",
    "        fig = fig.figure ## get the real figure\n",
    "    _out = a_partition_result._plot_step_by_step_subsequence_partition_process(extant_fig=fig, extant_ax_dict=None)\n",
    "    return _out\n",
    "    # return ax.scatter(*np.random.randn(2, n), color=colours[i],  marker=f'${markers[j]}$')\n",
    "\n",
    "# ui.add_task(_perform_plot)   # add your plot worker\n",
    "\n",
    "# ui.set_focus(0)      # this will trigger the plotting for group 0 tab 0\n",
    "ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_partition_result: SubsequencesPartitioningResult = all_examples_plot_data_dict['intrusion[2]']\n",
    "a_partition_result: SubsequencesPartitioningResult = all_examples_plot_data_dict['jump[3]']\n",
    "position_info_df, position_changes_info_df = a_partition_result.rebuild_sequence_info_df()\n",
    "position_info_df\n",
    "position_changes_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_partition_result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_split_indicies = position_changes_info_df[position_changes_info_df['should_split']].index # [1, 4, 5, 6, 8]\n",
    "diff_split_indicies\n",
    "\n",
    "split_indicies = position_changes_info_df[position_changes_info_df['should_split']]['prev_bin_flat_idx'].to_numpy()\n",
    "split_indicies\n",
    "\n",
    "active_split_indicies = deepcopy(split_indicies) ## this is what it should be, but all the splits are +1 later than they should be\n",
    "split_most_likely_positions_arrays = np.split(a_partition_result.flat_positions, active_split_indicies)\n",
    "# a_partition_result.split_positions_arrays = split_most_likely_positions_arrays\n",
    "split_most_likely_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_partition_result.diff_split_indicies\n",
    "a_partition_result.split_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_positions_arrays = deepcopy(a_partition_result.split_positions_arrays)\n",
    "merged_split_positions_arrays = deepcopy(a_partition_result.merged_split_positions_arrays)\n",
    "\n",
    "split_positions_arrays\n",
    "merged_split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attrs\n",
    "from qtpy.QtWidgets import QWidget, QToolBar, QFormLayout, QSpinBox, QDoubleSpinBox, QAction\n",
    "\n",
    "included_field_names = ['max_ignore_bins', 'same_thresh', 'max_jump_distance_cm']\n",
    "\n",
    "def create_controls(obj, callback):\n",
    "    \"\"\" tries to build UI controls to dynamically explore the effect of changing the parameters on the detected sequences \"\"\"\n",
    "    w = QWidget()\n",
    "    layout = QFormLayout(w)\n",
    "    for a in attrs.fields(obj.__class__):\n",
    "        desc = a.metadata.get('desc', '')\n",
    "        if a.name in included_field_names:\n",
    "            if a.type == int:\n",
    "                ctrl = QSpinBox()\n",
    "            else:\n",
    "                ctrl = QDoubleSpinBox()\n",
    "\n",
    "            ctrl.setToolTip(desc)\n",
    "            val = getattr(obj, a.name)\n",
    "            if val is not None:\n",
    "                ctrl.setValue(val)\n",
    "\n",
    "            def on_value_changed(val, name=a.name):\n",
    "                print(f'on_value_changed(val: {val}, name: {name})')\n",
    "                setattr(obj, name, val)\n",
    "                callback(obj)\n",
    "\n",
    "            ctrl.valueChanged.connect(on_value_changed)\n",
    "            layout.addRow(a.name, ctrl)\n",
    "\n",
    "    # END for a in at...\n",
    "    # layout.addRow(a.name, refresh_action)\n",
    "    return w\n",
    "\n",
    "\n",
    "def on_update_callback(updated_partition_result):\n",
    "    \"\"\" captures: all_examples_plot_data_dict, axs_dict\n",
    "    \"\"\"\n",
    "    print(f'updated_partition_result: {updated_partition_result}')\n",
    "    # updated_partition_result.compute() ## recompute\n",
    "    \n",
    "    # all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=15.0, n_pos_bins=57, debug_print=False) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "    # all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=15.0, n_pos_bins=57, debug_print=False) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "\n",
    "    for i, (a_name, a_partition_result) in enumerate(all_examples_plot_data_dict.items()):\n",
    "        # a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "        a_partition_result.same_thresh = updated_partition_result.same_thresh\n",
    "        a_partition_result.max_ignore_bins = updated_partition_result.max_ignore_bins\n",
    "        a_partition_result.max_jump_distance_cm = updated_partition_result.max_jump_distance_cm\n",
    "        a_partition_result.compute()\n",
    "        \n",
    "        # partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, **SubsequencesPartitioningResult_common_init_kwargs)\n",
    "        # Access the partitioned subsequences\n",
    "        subsequences = a_partition_result.split_positions_arrays\n",
    "        merged_subsequences = a_partition_result.merged_split_positions_arrays\n",
    "        print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "        print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "        print(a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False))\n",
    "        \n",
    "        # a_fig = list(figures_dict.values())[i]\n",
    "        an_ax_dict = axs_dict[a_name] # list(axs_dict.values())[i]\n",
    "        for an_ax_name, an_ax in an_ax_dict.items():\n",
    "            an_ax.clear()\n",
    "\n",
    "        _out = a_partition_result._plot_step_by_step_subsequence_partition_process(extant_ax_dict=an_ax_dict)\n",
    "        ui.update()\n",
    "        ui.draw()\n",
    "\n",
    "_out_w = create_controls(deepcopy(partition_result), callback=on_update_callback)\n",
    "toolbar = QToolBar(\"Controls\", ui)\n",
    "toolbar.addWidget(_out_w)\n",
    "refresh_action = QAction(\"Refresh\", ui)\n",
    "refresh_action.triggered.connect(lambda: [ctrl.setValue(ctrl.value()) for ctrl in _out_w.findChildren((QSpinBox, QDoubleSpinBox))])\n",
    "toolbar.addAction(refresh_action)\n",
    "ui.addToolBar(toolbar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.tabs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ui.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ui.tabs.__dict__\n",
    "# ui.tabs._items\n",
    "for k, v in ui.tabs._items.items():\n",
    "    print(f\"k: {k}, v: {v}\")\n",
    "    # v.figure\n",
    "    v.canvas.draw()\n",
    "    v.canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.update()\n",
    "ui.updateGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_w.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.compute()\n",
    "position_bins_info_df = deepcopy(partition_result.position_bins_info_df)\n",
    "position_changes_info_df = deepcopy(partition_result.position_changes_info_df)\n",
    "position_bins_info_df\n",
    "position_changes_info_df\n",
    "# partition_result.position_changes_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_changes_info_df['split_reason'] = '' # str column descripting why the split occured\n",
    "position_changes_info_df['direction'] = np.sign(position_changes_info_df['pos_diff']).astype(int)\n",
    "position_changes_info_df['exceeds_same_thresh'] = (np.abs(position_changes_info_df['pos_diff']) > partition_result.same_thresh)\n",
    "position_changes_info_df['did_accum_dir_change'] = (position_changes_info_df['direction'] != position_changes_info_df['direction'].shift()) & (position_changes_info_df['direction'] != 0)\n",
    "position_changes_info_df['should_split'] = np.logical_and(position_changes_info_df['did_accum_dir_change'], position_changes_info_df['exceeds_same_thresh'])\n",
    "if (partition_result.max_jump_distance_cm is not None):\n",
    "    position_changes_info_df['exceeds_jump_distance'] = (np.abs(position_changes_info_df['pos_diff']) > partition_result.max_jump_distance_cm)\n",
    "    position_changes_info_df['should_split'] = np.logical_or(position_changes_info_df['should_split'], position_changes_info_df['exceeds_jump_distance'])\n",
    "\n",
    "position_changes_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_changes_info_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_changes_info_df.diff\n",
    "# np.diff(position_bins_info_df['pos'])\n",
    "prev_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[:-1]\n",
    "next_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[1:]\n",
    "\n",
    "prev_bin_flat_idxs\n",
    "next_bin_flat_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[:-1]\n",
    "next_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[1:]\n",
    "\n",
    "position_changes_info_df = pd.DataFrame({'pos_diff': np.diff(position_bins_info_df['pos']),\n",
    "                                         'prev_bin_flat_idx': prev_bin_flat_idxs, 'next_bin_flat_idxs': next_bin_flat_idxs,\n",
    "})\n",
    "position_changes_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVE-2024-12-05\n",
    "partition_result.num_merged_subsequence_bins\n",
    "\n",
    "\n",
    "\n",
    "position_bins_info_df, position_changes_info_df = deepcopy(partition_result.rebuild_sequence_info_df())\n",
    "sequence_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_flat_indicies = position_bins_info_df[position_bins_info_df['is_intrusion']]['flat_idx'].to_numpy()\n",
    "is_longest_sequence_bin = (position_bins_info_df['subsequence_idx'] == partition_result.longest_sequence_subsequence_idx) #['flat_idx'].to_numpy()\n",
    "longest_sequence_flatindicies: NDArray = partition_result.longest_sequence_flatindicies\n",
    "longest_sequence_non_intrusion_flatindicies = np.setdiff1d(longest_sequence_flatindicies, intrusion_flat_indicies)\n",
    "longest_sequence_non_intrusion_flatindicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.split_position_flatindicies_arrays = np.split(partition_result.flat_position_indicies, partition_result.split_indicies)\n",
    "partition_result.merged_split_position_flatindicies_arrays = np.split(partition_result.flat_position_indicies, partition_result.split_indicies)\n",
    "\n",
    "print(partition_result.split_position_flatindicies_arrays) # [array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8, 9]), array([10, 11]), array([12, 13]), array([14]), array([15]), array([16]), array([17, 18, 19, 20, 21])]\n",
    "\n",
    "partition_result.longest_sequence_subsequence_excluding_intrusions\n",
    "partition_result.longest_subsequence_non_intrusion_nbins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_info_df: pd.DataFrame = partition_result.rebuild_sequence_info_df()\n",
    "sequence_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin by finding only the longest sequence\n",
    "flat_position_idxs = deepcopy(partition_result.flat_position_indicies)\n",
    "subsequences = deepcopy(partition_result.split_positions_arrays)\n",
    "n_tbins_list = np.array([len(v) for v in subsequences])\n",
    "longest_subsequence_idx: int = np.argmax(n_tbins_list)\n",
    "\n",
    "longest_subsequence_positions = subsequences[longest_subsequence_idx] \n",
    "# longest_subsequence_indicies = \n",
    "# longest_subsequence_positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import LongestSupersequenceFinder\n",
    "\n",
    "# Sample subsequences\n",
    "# subsequences = [\n",
    "#     [38.9801, 225.446, 225.446],\n",
    "#     [145.532, 137.921, 38.9801],\n",
    "#     [225.446, 217.835, 175.975, 107.478],\n",
    "#     [134.116, 145.532],\n",
    "#     [38.9801, 38.9801],\n",
    "#     [172.17],\n",
    "#     [149.337],\n",
    "#     [248.278],\n",
    "#     [225.446, 153.143, 145.532, 111.283, 69.4234]\n",
    "# ]\n",
    "\n",
    "# subsequences =[[38.9801,225.446,225.446], [145.532,137.921,38.9801], [225.446,217.835,175.975,107.478], [134.116,145.532], [38.9801,38.9801], [172.17], [149.337], [248.278], [225.446,153.143,145.532,111.283,69.4234]]\n",
    "# subsequences\n",
    "\n",
    "max_ignore_bins = 2  # Define your maximum ignore bins\n",
    "\n",
    "# Create an instance of the finder\n",
    "finder = LongestSupersequenceFinder([v.tolist() for v in subsequences], max_ignore_bins)\n",
    "\n",
    "# Find the longest supersequence\n",
    "longest_supersequence = finder.find_longest_supersequence()\n",
    "\n",
    "print(\"Longest Supersequence:\")\n",
    "print(longest_supersequence)\n",
    "print(\"\\nLength of the longest supersequence:\", len(longest_supersequence))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "partition_result. idx\n",
    "\n",
    "\n",
    "\n",
    "n_total_tbins: int = np.sum(n_tbins_list)\n",
    "is_subsequence_potential_intrusion = (n_tbins_list <= partition_result.max_ignore_bins) ## any subsequence shorter than the max ignore distance\n",
    "ignored_subsequence_idxs = np.where(is_subsequence_potential_intrusion)[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📈🎨🖼️ 2025-02-10 - 2D Decoded Posterior frames/snapshots by subdivision\n",
    "\n",
    "NOTE: ## INPUTS: test_epoch_specific_decoded_results2D_dict, continuous_specific_decoded_results2D_dict, new_decoder2D_dict, new_pf2Ds_dict # NOTE: 2D results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_divided_epochs_specific_decoded_results2D_dict, test_epoch_specific_decoded_results2D_dict, continuous_specific_decoded_results2D_dict, new_decoder2D_dict, new_pf2Ds_dict # NOTE: 2D results\n",
    "## `frame_divided_epochs_specific_decoded_results2D_dict` is most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔝🚧 2025-02-10 - Single Artist Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🔝🚧 2025-02-10 - Single Artist Approach\n",
    "\n",
    "assert 'global_subdivision_idx' in global_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standalone Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test Multi `DecodedTrajectoryMatplotlibPlotter` side-by-side\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackInstance, _perform_plot_matplotlib_2D_tracks\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "from neuropy.utils.matplotlib_helpers import build_or_reuse_figure\n",
    "\n",
    "## Figure Setup:\n",
    "fig = build_or_reuse_figure(fignum='Single Artist Approach', figsize=(10, 4), constrained_layout=True, clear=True) # \n",
    "gs = plt.GridSpec(1, 1, figure=fig)\n",
    "ax = plt.subplot(gs[0])\n",
    "\n",
    "# subfigs = fig.subfigures(actual_num_subfigures, 1, wspace=0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2025-02-14 - FINAL END OF DAY - `SingleArtistMultiEpochBatchHelpers`-based full ax plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import SingleArtistMultiEpochBatchHelpers\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "spike_raster_plt_2d: Spike2DRaster = spike_raster_window.spike_raster_plt_2d\n",
    "\n",
    "# active_track_identifier: str = 'NEW!! 2D repeat Tracks'\n",
    "print(f'track_dock_identifier: \"{track_dock_identifier}\"')\n",
    "# track_ax\n",
    "subdivide_bin_size: float = results2D.pos_df.attrs['subdivide_bin_size']\n",
    "desired_epoch_start_idx: int = 0\n",
    "# desired_epoch_end_idx: int = int(round(1/subdivide_bin_size)) * 60 * 8 # 8 minutes\n",
    "desired_epoch_end_idx: Optional[int] = None\n",
    "\n",
    "## INPUTS: subdivide_bin_size, results2D\n",
    "batch_plot_helper: SingleArtistMultiEpochBatchHelpers = SingleArtistMultiEpochBatchHelpers(results2D=results2D, active_ax=track_ax, frame_divide_bin_size=subdivide_bin_size, desired_epoch_start_idx=desired_epoch_start_idx,desired_epoch_end_idx=desired_epoch_end_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ax.set_facecolor('#333333')\n",
    "# track_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data = batch_plot_helper.add_all_track_plots(global_session=global_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data\n",
    "\n",
    "# df[\"y_scaled\"] = (df[\"y_scaled\"] - df[\"y_scaled\"].min()) / (df[\"y_scaled\"].max() - df[\"y_scaled\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data.measured_pos_line_artist.set_alpha(0.85)\n",
    "plots_data.measured_pos_line_artist.get_sizes()\n",
    "plots_data.measured_pos_line_artist.set_sizes([14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.clear_all_artists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.shared_build_flat_stacked_data(force_recompute=True, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print_keys_if_possible('batch_plot_helper', batch_plot_helper, max_depth=1)\n",
    "# batch_plot_helper: pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins.SingleArtistMultiEpochBatchHelpers  = SingleArtistMultiEpochBatchHelpers(results2D=DecodingResultND(ndim=2, pos_df=                 t           x           y     lin_pos      speed  lap  lap_dir        dt  velocity_x  acceleration_x  velocity_y  acceleration_y    x_smooth    y_smooth  velocity_x_smo...\n",
    "# │   ├── results2D: pyphoplacecellanalysis.SpecificResults.PendingNotebookCode.DecodingResultND  = DecodingResultND(ndim=2, pos_df=                 t           x           y     lin_pos      speed  lap  lap_dir        dt  velocity_x  acceleration_x  velocity_y  acceleration_y    x_smooth    y_smooth  velocity_x_smooth  acceleration_x_smooth  velocity_y_smooth...\n",
    "# │   ├── active_ax: matplotlib.axes._axes.Axes  = Axes(0,0;1x1)\n",
    "# │   ├── subdivide_bin_size: float  = 0.5\n",
    "# │   ├── rotate_to_vertical: bool  = True\n",
    "# │   ├── desired_epoch_start_idx: int  = 0\n",
    "# │   ├── desired_epoch_end_idx: int  = 960\n",
    "# │   ├── stacked_flat_global_pos_df: pandas.core.frame.DataFrame (children omitted) - (14381, 22)\n",
    "# │   ├── n_xbins: int  = 59\n",
    "# │   ├── n_ybins: int  = 8\n",
    "# │   ├── n_tbins: int  = 960\n",
    "# │   ├── flattened_n_xbins: int  = 59\n",
    "# │   ├── flattened_n_ybins: int  = 7680\n",
    "# │   ├── flattened_n_tbins: int  = 7680\n",
    "# │   ├── stacked_p_x_given_n: numpy.ndarray  = [[[0 0 0 ... 0.00272521 0.0027264 0.00272731]<br>  [0 0 0 ... 0.00271789 0.00272121 0.00272371]<br>  [0 0 0 ... 0.00269868 0.00270771 0.00271443]<br>  ...<br>  [1.7565e-18 2.1432e-17 2.30041e-16 ... 0.00272702 0.00272792 0.00272849]<br>  [2.60737e-21 3.18769e-20 3.42699e-19 ..... - (1, 59, 7680)\n",
    "# │   ├── stacked_flat_time_bin_centers: numpy.ndarray  = [[0.0125 0.0125 0.0125 ... 25.2375 25.2375 25.2375]] - (1, 7680)\n",
    "# │   ├── stacked_flat_xbin_centers: numpy.ndarray  = [2.43873 7.31618 12.1936 17.0711 21.9485 26.826 31.7035 36.5809 41.4584 46.3358 51.2133 56.0907 60.9682 65.8456 70.7231 75.6005 80.478 85.3554 90.2329 95.1104 99.9878 104.865 109.743 114.62 119.498 124.375 129.253 134.13 139.007 143.885 148.762 153.64 158.517 163.395 168.272 1... - (59,)\n",
    "# │   ├── stacked_flat_ybin_centers: numpy.ndarray  = [93.5252 93.5252 93.5252 ... 194.245 194.245 194.245] - (7680,)\n",
    "# │   ├── xbin_edges: numpy.ndarray  = [0 4.87745 9.75491 14.6324 19.5098 24.3873 29.2647 34.1422 39.0196 43.8971 48.7745 53.652 58.5294 63.4069 68.2844 73.1618 78.0393 82.9167 87.7942 92.6716 97.5491 102.427 107.304 112.181 117.059 121.936 126.814 131.691 136.569 141.446 146.324 151.201 156.079 160.956 165.833 170... - (60,)\n",
    "# │   ├── ybin_edges: numpy.ndarray  = [86.3309 100.719 115.108 129.496 143.885 158.273 172.662 187.05 201.439] - (9,)\n",
    "# │   ├── inverse_xbin_width: numpy.float64  = 287.7697841726619\n",
    "# │   ├── inverse_xbin_height: numpy.float64  = 115.10791366906471\n",
    "# │   ├── x0_offset: numpy.float64  = 0.0\n",
    "# │   ├── y0_offset: numpy.float64  = 86.33093525179856\n",
    "# │   ├── x1_offset: numpy.float64  = 287.7697841726619\n",
    "# │   ├── y1_offset: numpy.float64  = 201.43884892086328\n",
    "\n",
    "# [86.33093525179856, 201.43884892086328], how can I map each value of this column to a range [0.0, 1.0]?\n",
    "\n",
    "# y_range = [self.y0_offset, self.y1_offset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_shape_patch_collection_artists = batch_plot_helper.add_track_shapes(global_session=global_session, override_ax=None) ## does not seem to successfully synchronize to window\n",
    "# track_shape_patch_collection_artists = batch_plot_helper.add_track_shapes(global_session=global_session, override_ax=track_shapes_dock_track_ax) ## does not seem to successfully synchronize to window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_pos_line_artist, subdivision_epoch_separator_vlines = batch_plot_helper.add_track_positions(override_ax=None)\n",
    "# measured_pos_line_artist, subdivision_epoch_separator_vlines = batch_plot_helper.add_track_positions(override_ax=measured_pos_dock_track_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_artist_dict, image_extent, plots_data = batch_plot_helper.add_position_posteriors(posterior_masking_value=0.0025, override_ax=None, debug_print=True, defer_draw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.redraw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between `dict(ymin=self.xbin_edges[0], ymax=self.xbin_edges[-1])` and (0.0, 1.0)\n",
    "self.results2D.frame_divided_epochs_df['start'].to_numpy()\n",
    "\n",
    "self.stacked_flat_global_pos_df['global_subdivision_x_data_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch_plot_helper.x0_offset, batch_plot_helper.x1_offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_flat_global_pos_df = deepcopy(batch_plot_helper.stacked_flat_global_pos_df)\n",
    "\n",
    "stacked_flat_global_pos_df['x_scaled'] = (stacked_flat_global_pos_df['x'] - batch_plot_helper.y0_offset) / (batch_plot_helper.y1_offset - batch_plot_helper.y0_offset)\n",
    "# stacked_flat_global_pos_df['x_smooth_scaled'] = (stacked_flat_global_pos_df['x_smooth'] - batch_plot_helper.y0_offset) / (batch_plot_helper.y1_offset - batch_plot_helper.y0_offset)\n",
    "stacked_flat_global_pos_df['y_scaled'] = (stacked_flat_global_pos_df['y'] - batch_plot_helper.x0_offset) / (batch_plot_helper.x1_offset - batch_plot_helper.x0_offset)\n",
    "\n",
    "\n",
    "# stacked_flat_global_pos_df['y_scaled'] = (stacked_flat_global_pos_df['y'] - batch_plot_helper.y0_offset) / (batch_plot_helper.y1_offset - batch_plot_helper.y0_offset)\n",
    "# stacked_flat_global_pos_df['x_smooth_scaled'] = (stacked_flat_global_pos_df['x_smooth'] - batch_plot_helper.y0_offset) / (batch_plot_helper.y1_offset - batch_plot_helper.y0_offset)\n",
    "# stacked_flat_global_pos_df['x_scaled'] = (stacked_flat_global_pos_df['x'] - batch_plot_helper.x0_offset) / (batch_plot_helper.x1_offset - batch_plot_helper.x0_offset)\n",
    "\n",
    "\n",
    "## swap axes:\n",
    "stacked_flat_global_pos_df['y_temp'] = deepcopy(stacked_flat_global_pos_df['y'])\n",
    "stacked_flat_global_pos_df['y'] = deepcopy(stacked_flat_global_pos_df['x'])\n",
    "stacked_flat_global_pos_df['x'] = deepcopy(stacked_flat_global_pos_df['y_temp'])\n",
    "stacked_flat_global_pos_df.drop(columns=['y_temp'], inplace=True)\n",
    "\n",
    "stacked_flat_global_pos_df['y_scaled_temp'] = deepcopy(stacked_flat_global_pos_df['y_scaled'])\n",
    "stacked_flat_global_pos_df['y_scaled'] = deepcopy(stacked_flat_global_pos_df['x_scaled'])\n",
    "stacked_flat_global_pos_df['x_scaled'] = deepcopy(stacked_flat_global_pos_df['y_scaled_temp'])\n",
    "stacked_flat_global_pos_df.drop(columns=['y_scaled_temp'], inplace=True)\n",
    "\n",
    "\n",
    "stacked_flat_global_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_flat_global_pos_df.plot(x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.EpochComputationFunctions import DecodingResultND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(results2D.decoders['global'])\n",
    "a_result2D = results2D.frame_divided_epochs_results['global']\n",
    "a_new_global2D_decoder = results2D.decoders['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2D.decoders['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2D.a_result2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".a_result2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timebins, flat_time_bin_containers, flat_timebins_p_x_given_n = deepcopy(a_result2D).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_total_n_timebins, updated_is_masked_bin, updated_time_bin_containers, updated_timebins_p_x_given_n = deepcopy(a_result2D).flatten_to_masked_values()\n",
    "desired_total_n_timebins\n",
    "updated_time_bin_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_cmap='viridis'\n",
    "\n",
    "time_cmap_start_end_colors = [(0, 0.6, 0), (0, 0, 0)]  # first is green, second is black\n",
    "time_cmap = LinearSegmentedColormap.from_list(\"GreenToBlack\", time_cmap_start_end_colors, N=25) # Create a colormap (green to black).\n",
    "\n",
    "stacked_flat_global_pos_df = SingleArtistMultiEpochBatchHelpers.add_color_over_global_subdivision_idx_positions_to_stacked_flat_global_pos_df(stacked_flat_global_pos_df=stacked_flat_global_pos_df, time_cmap=time_cmap)\n",
    "# stacked_flat_global_pos_df\n",
    "new_stacked_flat_global_pos_df = SingleArtistMultiEpochBatchHelpers.add_nan_masked_rows_to_stacked_flat_global_pos_df(stacked_flat_global_pos_df=stacked_flat_global_pos_df)\n",
    "# new_stacked_flat_global_pos_df, color_formatting_dict = add_nan_masked_rows_to_stacked_flat_global_pos_df(stacked_flat_global_pos_df=stacked_flat_global_pos_df)\n",
    "new_stacked_flat_global_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_stacked_flat_global_pos_df = deepcopy(stacked_flat_global_pos_df)\n",
    "active_stacked_flat_global_pos_df = deepcopy(new_stacked_flat_global_pos_df)\n",
    "# extracted_colors_arr_flat: NDArray = active_stacked_flat_global_pos_df['color'].to_numpy()\n",
    "extracted_colors_arr: NDArray = np.array(active_stacked_flat_global_pos_df['color'].to_list()).astype(float) # .shape # (16299, 4)\n",
    "\n",
    "\n",
    "# extracted_colors_arr.T.shape # (16299,)\n",
    "# a_time_bin_centers = deepcopy(active_stacked_flat_global_pos_df['t'].to_numpy().astype(float))\n",
    "# a_time_bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_meas_pos_line, _meas_pos_out_markers = DecodedTrajectoryMatplotlibPlotter._perform_plot_measured_position_line_helper(an_ax, a_measured_pos_df, a_time_bin_centers, fake_y_lower_bound=None, fake_y_upper_bound=None, rotate_to_vertical=True, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get measured time bins from the dataframe\n",
    "a_measured_time_bin_centers: NDArray = np.atleast_1d([np.squeeze(a_measured_pos_df['t'].to_numpy())])\n",
    "# Determine X and Y positions based on dimensionality.\n",
    "if rotate_to_vertical is False:\n",
    "    # 1D: construct fake y values.\n",
    "    measured_fake_y_num_samples: int = len(a_measured_pos_df)\n",
    "    measured_fake_y_arr = np.linspace(fake_y_lower_bound, fake_y_upper_bound, measured_fake_y_num_samples)\n",
    "    x = np.atleast_1d([a_measured_pos_df['x'].to_numpy()])\n",
    "    y = np.atleast_1d([measured_fake_y_arr])\n",
    "else:\n",
    "    # 2D: take columns as is.\n",
    "    x = np.squeeze(a_measured_pos_df['x'].to_numpy())\n",
    "    y = np.squeeze(a_measured_pos_df['y'].to_numpy())\n",
    "\n",
    "# If in single-time-bin mode, restrict positions to those with t <= current time bin center.\n",
    "# n_time_bins: int = len(a_time_bin_centers)\n",
    "# Here, the caller is expected to ensure that time_bin_index is valid.\n",
    "# (This helper would be called after the check for single-time-bin mode.)\n",
    "# In a full implementation, one may pass time_bin_index as an argument.\n",
    "# For now, we only handle the non-restricted case.\n",
    "\n",
    "# Squeeze arrays down to rank 1.\n",
    "a_measured_time_bin_centers = np.squeeze(a_measured_time_bin_centers)\n",
    "x = np.squeeze(x)\n",
    "y = np.squeeze(y)\n",
    "if debug_print:\n",
    "    print(f'\\tFinal Shapes:')\n",
    "    print(f'\\tnp.shape(x): {np.shape(x)}, np.shape(y): {np.shape(y)}, np.shape(a_measured_time_bin_centers): {np.shape(a_measured_time_bin_centers)}')\n",
    "\n",
    "# Set pos_kwargs according to orientation.\n",
    "if not rotate_to_vertical:\n",
    "    pos_kwargs = dict(x=x, y=y)\n",
    "else:\n",
    "    pos_kwargs = dict(x=y, y=x)  # swap if vertical\n",
    "\n",
    "add_markers = True\n",
    "colors = [(0, 0.6, 0), (0, 0, 0)]  # first is green, second is black\n",
    "# Create a colormap (green to black).\n",
    "time_cmap = LinearSegmentedColormap.from_list(\"GreenToBlack\", colors, N=25)\n",
    "\n",
    "# Use the helper to add a gradient line.\n",
    "a_meas_pos_line, _meas_pos_out_markers = cls._helper_add_gradient_line(an_ax, t=a_measured_time_bin_centers, **pos_kwargs, add_markers=add_markers, time_cmap=time_cmap, zorder=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecodedTrajectoryMatplotlibPlotter._helper_add_gradient_line(ax=ax_dict[\"scaled_ax\"],\n",
    "    t=np.linspace(curr_lap_time_range[0], curr_lap_time_range[-1], len(laps_position_traces[curr_lap_id][0,:]))\n",
    "    x=laps_position_traces[curr_lap_id][0,:],\n",
    "    y=laps_position_traces[curr_lap_id][1,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num='xy-pos-test', layout=\"constrained\", clear=True)\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    [\n",
    "        [\"ax\",], # \"ax_LONG_activity_v_time\", \"ax_SHORT_activity_v_time\", \"ax_SHORT_pf_tuning_curve\"],\n",
    "\t\t['scaled_ax'],\n",
    "    ],\n",
    "    height_ratios=[1,1], # set the height ratios between the rows\n",
    "    # sharey=True,\n",
    "    gridspec_kw=dict(wspace=0, hspace=0.15) # `wspace=0`` is responsible for sticking the pf and the activity axes together with no spacing\n",
    ")\n",
    "ax_dict[\"ax\"].scatter(active_stacked_flat_global_pos_df[\"x\"], active_stacked_flat_global_pos_df[\"y\"], color=active_stacked_flat_global_pos_df[\"color\"].tolist())\n",
    "# ax_dict[\"scaled_ax\"].scatter(active_stacked_flat_global_pos_df[\"x_scaled\"], active_stacked_flat_global_pos_df[\"y_scaled\"])\n",
    "# active_stacked_flat_global_pos_df.plot(x='x', y='y', c=extracted_colors_arr, ax=ax_dict[\"ax\"])\n",
    "# active_stacked_flat_global_pos_df.plot(x='x_scaled', y='y_scaled', ax=ax_dict[\"scaled_ax\"])\n",
    "\n",
    "\n",
    "\n",
    "a_meas_pos_line, _meas_pos_out_markers = DecodedTrajectoryMatplotlibPlotter._perform_plot_measured_position_line_helper(ax_dict[\"scaled_ax\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t a_measured_pos_df=active_stacked_flat_global_pos_df, a_time_bin_centers=None, fake_y_lower_bound=None, fake_y_upper_bound=None, rotate_to_vertical=True, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num='xy-pos-xfirst-test', layout=\"constrained\", clear=True)\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    [\n",
    "        [\"ax\",], # \"ax_LONG_activity_v_time\", \"ax_SHORT_activity_v_time\", \"ax_SHORT_pf_tuning_curve\"],\n",
    "\t\t['scaled_ax'],\n",
    "    ],\n",
    "    height_ratios=[1,1], # set the height ratios between the rows\n",
    "    # sharey=True,\n",
    "    gridspec_kw=dict(wspace=0, hspace=0.15) # `wspace=0`` is responsible for sticking the pf and the activity axes together with no spacing\n",
    ")\n",
    "# active_stacked_flat_global_pos_df.plot(x='x', y='y', ax=ax_dict[\"ax\"])\n",
    "# active_stacked_flat_global_pos_df.plot(x='x_scaled', y='y_scaled', ax=ax_dict[\"scaled_ax\"])\n",
    "ax_dict[\"ax\"].scatter(active_stacked_flat_global_pos_df[\"x\"], active_stacked_flat_global_pos_df[\"y\"], color=active_stacked_flat_global_pos_df[\"color\"].tolist())\n",
    "ax_dict[\"scaled_ax\"].scatter(active_stacked_flat_global_pos_df[\"x_scaled\"], active_stacked_flat_global_pos_df[\"y_scaled\"], color=active_stacked_flat_global_pos_df[\"color\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num='xy-pos-real-fig-like-test', layout=\"constrained\", clear=True)\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    [\n",
    "        [\"ax\",], # \"ax_LONG_activity_v_time\", \"ax_SHORT_activity_v_time\", \"ax_SHORT_pf_tuning_curve\"],\n",
    "\t\t['scaled_ax'],\n",
    "    ],\n",
    "    height_ratios=[1,1], # set the height ratios between the rows\n",
    "    # sharey=True,\n",
    "    gridspec_kw=dict(wspace=0, hspace=0.15) # `wspace=0`` is responsible for sticking the pf and the activity axes together with no spacing\n",
    ")\n",
    "# active_stacked_flat_global_pos_df.plot(x='global_subdivision_x_data_offset', y='y_scaled', c='color', ax=ax_dict[\"ax\"])\n",
    "# active_stacked_flat_global_pos_df.plot(x='global_subdivision_x_data_offset', y='y_scaled', ax=ax_dict[\"scaled_ax\"])\n",
    "ax_dict[\"ax\"].scatter(active_stacked_flat_global_pos_df[\"global_subdivision_x_data_offset\"], active_stacked_flat_global_pos_df[\"y_scaled\"], color=active_stacked_flat_global_pos_df[\"color\"].tolist())\n",
    "ax_dict[\"scaled_ax\"].scatter(active_stacked_flat_global_pos_df[\"global_subdivision_x_data_offset\"], active_stacked_flat_global_pos_df[\"y_scaled\"], color=active_stacked_flat_global_pos_df[\"color\"].tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_pos_dock_track_ax.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_artist = measured_pos_dock_track_ax.scatter(active_stacked_flat_global_pos_df[\"global_subdivision_x_data_offset\"], active_stacked_flat_global_pos_df[\"y_scaled\"], color=active_stacked_flat_global_pos_df[\"color\"].tolist())\n",
    "_out_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_pos_dock_track_ax.get_figure().canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_dict[\"ax\"].sharex(measured_pos_dock_track_ax)  # Explicitly synchronize x-axis\n",
    "ax_dict[\"scaled_ax\"].sharex(measured_pos_dock_track_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(measured_pos_dock_track_ax.get_xlim(), measured_pos_dock_track_ax.get_ylim())\n",
    "\n",
    "ax_dict[\"ax\"].set_xlim(*measured_pos_dock_track_ax.get_xlim())\n",
    "ax_dict[\"scaled_ax\"].set_xlim(*measured_pos_dock_track_ax.get_xlim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_flat_global_pos_df['y_scaled'].hist() #.plot()\n",
    "stacked_flat_global_pos_df['x_scaled'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.custom_image_extent = [batch_plot_helper.desired_start_time_seconds, batch_plot_helper.desired_end_time_seconds, 0.0, 1.0] ## n\n",
    "batch_plot_helper.custom_image_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_all_rect_arr_dict = {k:v[(desired_epoch_start_idx*3):(desired_epoch_end_idx*3), :] for k, v in batch_plot_helper.track_all_normalized_rect_arr_dict.items()}\n",
    "track_all_rect_arr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.track_all_normalized_rect_arr_dict\n",
    "batch_plot_helper.inverse_normalized_track_all_rect_arr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_single_rects_tuples_list_dict = deepcopy(batch_plot_helper.track_single_rects_dict)\n",
    "track_single_rects_tuples_list_dict\n",
    "\n",
    "# np.array(track_single_rects_dict['long'])\n",
    "\n",
    "track_single_rects_dict = {a_name:np.array([a_tuples_list[:4] for a_tuples_list in a_track_single_rects_tuples]) for a_name, a_track_single_rects_tuples in track_single_rects_tuples_list_dict.items()}\n",
    "track_single_rects_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_t_axis_range = [0.0, 18.0] ## along t-axis \n",
    "expected_t_axis_range = [0.0, 480.0]\n",
    "## INPUTS: batch_plot_helper.custom_image_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.desired_start_time_seconds\n",
    "batch_plot_helper.desired_end_time_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ax = track_shapes_dock_track_ax\n",
    "track_all_normalized_rect_arr_dict = SingleArtistMultiEpochBatchHelpers.track_dict_all_stacked_rect_arr_normalization(batch_plot_helper.track_single_rects_dict, num_horizontal_repeats=batch_plot_helper.num_horizontal_repeats)\n",
    "# inverse_normalized_track_all_rect_arr_dict = SingleArtistMultiEpochBatchHelpers.track_dict_all_stacked_rect_arr_inverse_normalization_from_custom_extent(batch_plot_helper.track_all_normalized_rect_arr_dict, custom_image_extent=batch_plot_helper.custom_image_extent, num_active_horizontal_repeats=batch_plot_helper.num_horizontal_repeats)\n",
    "\n",
    "track_all_rect_arr_dict = {k:v[(batch_plot_helper.desired_epoch_start_idx*3):(batch_plot_helper.desired_epoch_end_idx*3), :] for k, v in track_all_normalized_rect_arr_dict.items()} ## just filter `track_all_normalized_rect_arr_dict` for the relevant active items\n",
    "\n",
    "filtered_epoch_range: NDArray = np.arange(start=batch_plot_helper.desired_epoch_start_idx, stop=batch_plot_helper.desired_epoch_end_idx)\n",
    "filtered_num_horizontal_repeats: int = len(filtered_epoch_range)\n",
    "filtered_num_output_rect_total_elements: int = filtered_num_horizontal_repeats * 3 # 3 parts to each track plot\n",
    "print(f'batch_plot_helper.num_horizontal_repeats: {batch_plot_helper.num_horizontal_repeats}')\n",
    "print(f'filtered_num_horizontal_repeats: {filtered_num_horizontal_repeats}')\n",
    "\n",
    "inverse_normalized_track_all_rect_arr_dict = SingleArtistMultiEpochBatchHelpers.track_dict_all_stacked_rect_arr_inverse_normalization_from_custom_extent(track_all_rect_arr_dict, custom_image_extent=batch_plot_helper.custom_image_extent, num_active_horizontal_repeats=filtered_num_horizontal_repeats)\n",
    "\n",
    "track_all_rect_arr_dict\n",
    "inverse_normalized_track_all_rect_arr_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "track_all_normalized_rect_arr_dict\n",
    "inverse_normalized_track_all_rect_arr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_all_normalized_rect_df_dict =  {a_track_name:pd.DataFrame(arr, columns=['x0', 'y0', 'width', 'height']) for a_track_name, arr in batch_plot_helper.track_all_normalized_rect_arr_dict.items()}\n",
    "inverse_normalized_track_all_rect_df_dict =  {a_track_name:pd.DataFrame(arr, columns=['x0', 'y0', 'width', 'height']) for a_track_name, arr in batch_plot_helper.inverse_normalized_track_all_rect_arr_dict.items()}\n",
    "\n",
    "track_all_normalized_rect_df_dict\n",
    "inverse_normalized_track_all_rect_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_all_normalized_rect_df_dict['long']\n",
    "inverse_normalized_track_all_rect_df_dict['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_plot_helper.track_all_normalized_rect_arr_dict['long'].shape\n",
    "\n",
    "# batch_plot_helper.track_all_normalized_rect_arr_dict['long']\n",
    "np.shape(batch_plot_helper.track_all_normalized_rect_arr_dict['short'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k, an_artist in track_shape_patch_collection_artists.items():\n",
    "\tan_artist.remove()\n",
    "batch_plot_helper.redraw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_window: SpikesDataframeWindow = spike_raster_window.spike_raster_plt_2d.spikes_window #.update_window_start_end(min_t, max_t)\n",
    "spikes_window.active_time_window # (75.38501800005326, 224.23115536324383)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right_sidebar_widget: Spike3DRasterRightSidebarWidget = spike_raster_window.right_sidebar_widget\n",
    "bottom_playback_bar_widget: Spike3DRasterBottomPlaybackControlBar = spike_raster_window.bottom_playback_control_bar_widget\n",
    "bottom_playback_bar_widget.on_window_changed(start_t="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "\n",
    "sync_connection = spike_raster_plt_2d.sync_matplotlib_render_plot_widget(identifier='NEW!! 2D repeat Tracks', sync_mode=SynchronizedPlotMode.TO_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_shape_patch_collection_artists['long'].set_visible(False)\n",
    "track_shape_patch_collection_artists['short'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.inverse_xbin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_pos_line_artist.get_bbox() # measured_pos_line_artist.get_bbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_xy: NDArray = measured_pos_line_artist.get_xydata()\n",
    "\n",
    "np.min(line_xy, axis=0)\n",
    "\n",
    "np.max(line_xy, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivision_epoch_separator_vlines.get_y() # measured_pos_line_artist.get_bbox() # LineCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_plot_helper.active_ax.remove(subdivision_epoch_separator_vlines)\n",
    "subdivision_epoch_separator_vlines.remove()\n",
    "\n",
    "measured_pos_line_artist.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.active_ax.set_autoscaley_on(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.active_ax.autoscale(axis='y', enable=True, tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.active_ax.set_ylim(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_plot_helper.redraw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_start_t, total_end_t, y_axis_min, y_axis_max = spike_raster_plt_2d.get_render_intervals_plot_range()\n",
    "\n",
    "t_window_duration: float = 18.0548 - 3.054774\n",
    "print(f't_window_duration: {t_window_duration}')\n",
    "## Compute the appopriate timeline width in seconds given the `t_window_duration` and `subdivide_bin_size`\n",
    "\n",
    "# t_window_duration: float = 0.5 # half-second\n",
    "num_subdivisions_per_window: int = int(round(t_window_duration/subdivide_bin_size))\n",
    "print(f'num_subdivisions_per_window: {num_subdivisions_per_window}')\n",
    "percent_window_subdivision_width: float = subdivide_bin_size / t_window_duration\n",
    "print(f'percent_window_subdivision_width: {percent_window_subdivision_width}') # percent_window_subdivision_width: what percentage of the window should each subdivisions axes take up? \n",
    "subdivide_bin_size\n",
    "percent_window_subdivision_width\n",
    "## INPUTS: track_ax\n",
    "# track_ax = ax\n",
    "## Build axes locators for each subdivision\n",
    "axes_inset_locators_list = deepcopy([(a_result2D.time_bin_containers[epoch_idx].edge_info.variable_extents[0], track_ax.get_ylim()[0], (a_result2D.time_bin_containers[epoch_idx].edge_info.variable_extents[-1] - a_result2D.time_bin_containers[epoch_idx].edge_info.variable_extents[0]), np.diff(track_ax.get_ylim())[0]) for epoch_idx in np.arange(a_result2D.num_filter_epochs-1)]) # [x0, y0, width, height], where [x0, y0] is the lower-left corner -- can do data_coords by adding `, transform=existing_ax.transData`\n",
    "axes_inset_locators_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.dict_representable import overriding_dict_with # required for safely_accepts_kwargs\n",
    "from pyphocorehelpers.geometry_helpers import point_tuple_mid_point, BoundsRect, is_point_in_rect\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import SingleArtistMultiEpochBatchHelpers\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import LongShortDisplayConfigManager, long_short_display_config_manager\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackInstance, _perform_plot_matplotlib_2D_tracks\n",
    "\n",
    "rotate_to_vertical: bool = True\n",
    "perform_autoscale: bool = False\n",
    "## INPUTS: track_ax, rotate_to_vertical, perform_autoscale\n",
    "\n",
    "long_track_inst, short_track_inst = LinearTrackInstance.init_tracks_from_session_config(deepcopy(global_session.config))\n",
    "\n",
    "long_short_display_config_manager = LongShortDisplayConfigManager()\n",
    "long_epoch_matplotlib_config = long_short_display_config_manager.long_epoch_config.as_matplotlib_kwargs()\n",
    "long_kwargs = deepcopy(long_epoch_matplotlib_config)\n",
    "long_kwargs = overriding_dict_with(lhs_dict=long_kwargs, **dict(linewidth=2, zorder=-99, alpha=0.5, facecolor='#0099ff07', edgecolor=long_kwargs['facecolor'], linestyle='dashed'))\n",
    "short_epoch_matplotlib_config = long_short_display_config_manager.short_epoch_config.as_matplotlib_kwargs()\n",
    "short_kwargs = deepcopy(short_epoch_matplotlib_config)\n",
    "short_kwargs = overriding_dict_with(lhs_dict=short_kwargs, **dict(linewidth=2, zorder=-98, alpha=0.5, facecolor='#f5161607', edgecolor=short_kwargs['facecolor'], linestyle='dashed'))\n",
    "track_kwargs_dict = {'long': long_kwargs, 'short': short_kwargs}\n",
    "\n",
    "# BEGIN PLOTTING _____________________________________________________________________________________________________ #\n",
    "# long_out_tuple = long_track_inst.plot_rects(plot_item=track_ax, matplotlib_rect_kwargs_override=long_kwargs, rotate_to_vertical=rotate_to_vertical, offset=None)\n",
    "# short_out_tuple = short_track_inst.plot_rects(plot_item=track_ax, matplotlib_rect_kwargs_override=short_kwargs, rotate_to_vertical=rotate_to_vertical, offset=None)\n",
    "# long_combined_item, long_rect_items, long_rects = long_out_tuple\n",
    "# short_combined_item, short_rect_items, short_rects = short_out_tuple\n",
    "\n",
    "long_rects = long_track_inst.build_rects(include_rendering_properties=False, rotate_to_vertical=rotate_to_vertical)\n",
    "short_rects = short_track_inst.build_rects(include_rendering_properties=False, rotate_to_vertical=rotate_to_vertical)\n",
    "track_single_rects_dict = {'long': long_rects, 'short': short_rects}\n",
    "\n",
    "# long_path = _build_track_1D_verticies(platform_length=22.0, track_length=170.0, track_1D_height=1.0, platform_1D_height=1.1, track_center_midpoint_x=long_track.grid_bin_bounds.center_point[0], track_center_midpoint_y=-1.0, debug_print=True)\n",
    "# short_path = _build_track_1D_verticies(platform_length=22.0, track_length=100.0, track_1D_height=1.0, platform_1D_height=1.1, track_center_midpoint_x=short_track.grid_bin_bounds.center_point[0], track_center_midpoint_y=1.0, debug_print=True)\n",
    "\n",
    "# ## Plot the tracks:\n",
    "# long_patch = patches.PathPatch(long_path, **long_track_color, alpha=0.5, lw=2)\n",
    "# ax.add_patch(long_patch)\n",
    "\n",
    "# short_patch = patches.PathPatch(short_path, **short_track_color, alpha=0.5, lw=2)\n",
    "# ax.add_patch(short_patch)\n",
    "# if perform_autoscale:\n",
    "#     track_ax.autoscale()\n",
    " \n",
    "# x_offset: float = -131.142\n",
    "# long_rect_arr = SingleArtistMultiEpochBatchHelpers.rect_tuples_to_NDArray(long_rects, x_offset=x_offset)\n",
    "# short_rect_arr = SingleArtistMultiEpochBatchHelpers.rect_tuples_to_NDArray(short_rects, x_offset=x_offset)\n",
    "\n",
    "num_horizontal_repeats: int = (a_result2D.num_filter_epochs-1)\n",
    "# num_horizontal_repeats: int = 20 ## hardcoded\n",
    "track_all_normalized_rect_arr_dict = SingleArtistMultiEpochBatchHelpers.track_dict_all_stacked_rect_arr_normalization(track_single_rects_dict, num_horizontal_repeats=num_horizontal_repeats)\n",
    "## INPUTS: filtered_num_horizontal_repeats\n",
    "inverse_normalized_track_all_rect_arr_dict = SingleArtistMultiEpochBatchHelpers.track_dict_all_stacked_rect_arr_inverse_normalization(track_all_normalized_rect_arr_dict, ax=track_ax, num_active_horizontal_repeats=num_horizontal_repeats)\n",
    "\n",
    "## OUTPUTS: track_all_normalized_rect_arr_dict, inverse_normalized_track_all_rect_arr_dict\n",
    "# track_all_normalized_rect_arr_dict\n",
    "\n",
    "## Slice a subset of the data epochs:\n",
    "desired_epoch_start_idx: int = 0\n",
    "# desired_epoch_end_idx: int = 20\n",
    "desired_epoch_end_idx: int = int(round(1/subdivide_bin_size)) * 60 * 8 # 8 minutes\n",
    "print(f'desired_epoch_start_idx: {desired_epoch_start_idx}, desired_epoch_end_idx: {desired_epoch_end_idx}')\n",
    "\n",
    "filtered_epoch_range = np.arange(start=desired_epoch_start_idx, stop=desired_epoch_end_idx)\n",
    "filtered_num_horizontal_repeats: int = len(filtered_epoch_range)\n",
    "filtered_num_output_rect_total_elements: int = filtered_num_horizontal_repeats * 3 # 3 parts to each track plot\n",
    "## OUTPUTS: filtered_epoch_range, filtered_num_horizontal_repeats, filtered_num_output_rect_total_elements\n",
    "filtered_num_output_rect_total_elements\n",
    "\n",
    "track_all_rect_arr_dict = {k:v[(desired_epoch_start_idx*3):(desired_epoch_end_idx*3), :] for k, v in track_all_normalized_rect_arr_dict.items()}\n",
    "# track_all_rect_arr_dict = {k:v[desired_epoch_start_idx:desired_epoch_end_idx, :] for k, v in track_all_rect_arr_dict.items()}\n",
    "# track_all_rect_arr_dict\n",
    "\n",
    "## INPUTS: filtered_num_horizontal_repeats\n",
    "inverse_normalized_track_all_rect_arr_dict = SingleArtistMultiEpochBatchHelpers.track_dict_all_stacked_rect_arr_inverse_normalization(track_all_rect_arr_dict, ax=track_ax, num_active_horizontal_repeats=filtered_num_horizontal_repeats)\n",
    "## OUTPUTS: inverse_normalized_track_all_rect_arr_dict\n",
    "## INPUTS: track_kwargs_dict, inverse_normalized_track_all_rect_arr_dict\n",
    "track_shape_patch_collection_artists = SingleArtistMultiEpochBatchHelpers.add_batch_track_shapes(ax=track_ax, inverse_normalized_track_all_rect_arr_dict=inverse_normalized_track_all_rect_arr_dict, track_kwargs_dict=track_kwargs_dict) # start (x0: 0.0, 20 of them span to exactly x=1.0)\n",
    "# track_shape_patch_collection_artists = SingleArtistMultiEpochBatchHelpers.add_batch_track_shapes(ax=ax, inverse_normalized_track_all_rect_arr_dict=inverse_normalized_track_all_rect_arr_dict, track_kwargs_dict=track_kwargs_dict, transform=ax.transData) # start (x0: 31.0, 20 of them span to about x=1000.0)\n",
    "# track_shape_patch_collection_artists = SingleArtistMultiEpochBatchHelpers.add_batch_track_shapes(ax=ax, inverse_normalized_track_all_rect_arr_dict=inverse_normalized_track_all_rect_arr_dict, track_kwargs_dict=track_kwargs_dict, transform=ax.transAxes) # start (x0: 31.0, 20 of them span to about x=1000.0)\n",
    "track_ax.get_figure().canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ax.get_figure().canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_normalized_track_all_rect_arr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.autoscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.clear()\n",
    "ax.autoscale(True)\n",
    "fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.clear()\n",
    "fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axes_inset_locators_list = np.vstack(deepcopy([(a_result.time_bin_containers[epoch_idx].edge_info.variable_extents[0], track_ax.get_ylim()[0], (a_result.time_bin_containers[epoch_idx].edge_info.variable_extents[-1] - a_result.time_bin_containers[epoch_idx].edge_info.variable_extents[0]), np.diff(track_ax.get_ylim())[0]) for epoch_idx in np.arange(a_result.num_filter_epochs-1)])) # [x0, y0, width, height], where [x0, y0] is the lower-left corner -- can do data_coords by adding `, transform=existing_ax.transData`\n",
    "# axes_inset_locators_list\n",
    "\n",
    "# repeated_axes_inset_locators_list = np.repeat(axes_inset_locators_list, 3, axis=0)\n",
    "# repeated_axes_inset_locators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "# verts = np.stack((np.column_stack((all_long_rect_arr[:,0],all_long_rect_arr[:,1])),\n",
    "#                    np.column_stack((all_long_rect_arr[:,0]+all_long_rect_arr[:,2],all_long_rect_arr[:,1])),\n",
    "#                    np.column_stack((all_long_rect_arr[:,0]+all_long_rect_arr[:,2],all_long_rect_arr[:,1]+all_long_rect_arr[:,3])),\n",
    "#                    np.column_stack((all_long_rect_arr[:,0],all_long_rect_arr[:,1]+all_long_rect_arr[:,3]))), axis=1)\n",
    "\n",
    "# pc = PolyCollection(verts, edgecolors='k', facecolors='none', transform=ax.transData)\n",
    "# ax.add_collection(pc)\n",
    "\n",
    "\n",
    "long_pc: PolyCollection = SingleArtistMultiEpochBatchHelpers.add_rectangles(ax=track_ax, rect_arr=all_long_rect_arr, facecolors='red')\n",
    "# short_pc: PolyCollection = SingleArtistMultiEpochBatchHelpers.add_rectangles(ax=track_ax, rect_arr=all_short_rect_arr, facecolors='blue')\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "track_fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long_pc.remove()\n",
    "track_fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_rect_arr = SingleArtistMultiEpochBatchHelpers.rect_tuples_to_NDArray(long_rects)\n",
    "x0s = long_rect_arr[:, 0] # x0\n",
    "widths = long_rect_arr[:, 2] # w\n",
    "heights = long_rect_arr[:, 3] # w\n",
    "\n",
    "x1s = x0s + widths\n",
    "x0s\n",
    "widths\n",
    "x1s\n",
    "\n",
    "single_subdiv_width: float = np.max(widths)\n",
    "single_subdiv_height: float = np.max(heights)\n",
    "padding_x: float = 0.0\n",
    "single_subdiv_offset_x: float = single_subdiv_width + padding_x\n",
    "\n",
    "\n",
    "## OUTPUTS: single_subdiv_width, single_subdiv_height, single_subdiv_offset_x\n",
    "all_long_rect_arr = np.vstack(deepcopy([((epoch_idx * single_subdiv_offset_x), 0, single_subdiv_width, single_subdiv_height) for epoch_idx in np.arange(a_result.num_filter_epochs-1)])) # [x0, y0, width, height], where [x0, y0] is the lower-left corner -- can do data_coords by adding `, transform=existing_ax.transData`\n",
    "all_long_rect_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long_rect_arr = SingleArtistMultiEpochBatchHelpers.rect_tuples_to_NDArray(long_rects, offset=)\n",
    "np.shape(long_rect_arr)\n",
    "long_rect_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = track_ax.get_xlim(); ylim = track_ax.get_ylim(); track_ax.clear(); track_ax.set_xlim(xlim); track_ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_axes: int = 1\n",
    "## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict, a_new_global_decoder2D\n",
    "xbin = deepcopy(a_new_global_decoder2D.xbin)\n",
    "xbin_centers = deepcopy(a_new_global_decoder2D.xbin_centers)\n",
    "ybin_centers = deepcopy(a_new_global_decoder2D.ybin_centers)\n",
    "ybin = deepcopy(a_new_global_decoder2D.ybin)\n",
    "num_filter_epochs: int = a_result2D.num_filter_epochs\n",
    "\n",
    "\n",
    "assert len(xbin_centers) == np.shape(a_result2D.p_x_given_n_list[an_epoch_idx])[0], f\"np.shape(a_result.p_x_given_n_list[an_epoch_idx]): {np.shape(self.a_result.p_x_given_n_list[an_epoch_idx])}, len(xbin_centers): {len(self.xbin_centers)}\"\n",
    "\n",
    "a_p_x_given_n = a_result2D.p_x_given_n_list[an_epoch_idx] # (76, 40, n_epoch_t_bins)\n",
    "a_most_likely_positions = a_result2D.most_likely_positions_list[an_epoch_idx] # (n_epoch_t_bins, n_pos_dims) \n",
    "a_time_bin_edges = a_result2D.time_bin_edges[an_epoch_idx] # (n_epoch_t_bins+1, )\n",
    "a_time_bin_centers = a_result2D.time_bin_containers[an_epoch_idx].centers # (n_epoch_t_bins, )\n",
    "has_measured_positions: bool = hasattr(a_result2D, 'measured_positions_list')\n",
    "if has_measured_positions:\n",
    "    a_measured_pos_df: pd.DataFrame = a_result2D.measured_positions_list[an_epoch_idx]\n",
    "    # assert len(a_measured_pos_df) == len(a_time_bin_centers)\n",
    "else:\n",
    "    a_measured_pos_df = None\n",
    "\n",
    "\n",
    "a_p_x_given_n = deepcopy(stacked_p_x_given_n)\n",
    "a_measured_pos_df = None\n",
    "a_most_likely_positions = None\n",
    "a_measured_pos_df = None\n",
    "\n",
    "\n",
    "curr_artist_dict = {}\n",
    "## Perform the plot:\n",
    "curr_artist_dict['prev_heatmaps'], (a_meas_pos_line, a_line), (_meas_pos_out_markers, _out_markers) = DecodedTrajectoryMatplotlibPlotter._perform_add_decoded_posterior_and_trajectory(ax, xbin_centers=xbin_centers, a_p_x_given_n=a_p_x_given_n,\n",
    "                                                                    a_time_bin_centers=a_time_bin_centers, a_most_likely_positions=a_most_likely_positions, a_measured_pos_df=a_measured_pos_df, ybin_centers=ybin_centers,\n",
    "                                                                    include_most_likely_pos_line=None, time_bin_index=None, rotate_to_vertical=True) # , allow_time_slider=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=a_result2D, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, rotate_to_vertical=True)\n",
    "fig, axes, decoded_epochs_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(global_session, curr_num_subplots=n_axes, active_page_index=0, plot_actual_lap_lines=False, use_theoretical_tracks_instead=True, fixed_columns=n_axes)\n",
    "# a_decoded_traj_plotter.fig = fig\n",
    "# a_decoded_traj_plotter.axs = axes\n",
    "\n",
    "for i in np.arange(n_axes):\n",
    "    print(f'plotting epoch[{i}]')\n",
    "    ax = a_decoded_traj_plotter.axs[0][i]\n",
    "    # a_decoded_traj_plotter.plot_epoch(an_epoch_idx=i, include_most_likely_pos_line=None, time_bin_index=None)\n",
    "    a_decoded_traj_plotter.plot_epoch(an_epoch_idx=i, include_most_likely_pos_line=None, time_bin_index=None, override_ax=ax)\n",
    "    \n",
    "a_decoded_traj_plotter.fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2025-01-30 - Multi-Axes, Multi-Artist Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Visualizing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import SynchronizedPlotMode\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DecoderIdentityColors\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_plot_multi_decoder_meas_pred_position_track\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions, plot_slices_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import AddNewDecodedPosteriors_MatplotlibPlotCommand\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "\n",
    "## INPUTS: test_epoch_specific_decoded_results2D_dict, continuous_specific_decoded_results2D_dict, new_decoder2D_dict, new_pf2Ds_dict # NOTE: 2D results\n",
    "\n",
    "## INPUTS: frame_divided_epochs_specific_decoded_results_dict, new_decoder2D_dict, global_pos_df\n",
    "# a_result = test_epoch_specific_decoded_results_dict['global']\n",
    "# a_result2D = frame_divided_epochs_specific_decoded_results2D_dict['global']\n",
    "a_result2D = results2D.a_result2D\n",
    "a_new_global_decoder2D = results2D.a_new_global2D_decoder\n",
    "# delattr(a_result, 'measured_positions_list')\n",
    "a_result2D.measured_positions_list = deepcopy([global_pos_df[global_pos_df['global_subdivision_idx'] == epoch_idx] for epoch_idx in np.arange(a_result2D.num_filter_epochs)]) ## add a List[pd.DataFrame] to plot as the measured positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Multi `DecodedTrajectoryMatplotlibPlotter` side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackInstance, _perform_plot_matplotlib_2D_tracks\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import multi_DecodedTrajectoryMatplotlibPlotter_side_by_side\n",
    "\n",
    "n_axes: int = 10\n",
    "posterior_masking_value: float = 0.02 # for 2D\n",
    "a_decoded_traj_plotter, (fig, axs, decoded_epochs_pages) = multi_DecodedTrajectoryMatplotlibPlotter_side_by_side(a_result2D=results2D.a_result2D, a_new_global_decoder2D=results2D.a_new_global2D_decoder,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  global_session=global_session, n_axes=n_axes, posterior_masking_value=posterior_masking_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_traj_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df: pd.DataFrame = results2D.a_result2D.filter_epochs.to_dataframe() #.num_filter_epochs\n",
    "epochs_df['start_t']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Firing Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_spikes_df\n",
    "# curr_active_pipeline\n",
    "epoch_spikes_df = deepcopy(long_one_step_decoder_1D.spikes_df)\n",
    "# filter_epoch_spikes_df_L\n",
    "# filter_epoch_spikes_df_S\n",
    "epoch_spikes_df\n",
    "\n",
    "epochs_df_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_specific_binned_spike_rate_df, unit_specific_binned_spike_counts_df, time_window_edges, time_window_edges_binning_info = SpikeRateTrends.compute_simple_time_binned_firing_rates_df(epoch_spikes_df, time_bin_size_seconds=0.005, debug_print=False)\n",
    "# unit_specific_binned_spike_rate_df.to_numpy() # (160580, 45)\n",
    "\n",
    "# Compute average firing rate for each neuron\n",
    "unit_avg_firing_rates = np.nanmean(unit_specific_binned_spike_rate_df.to_numpy(), axis=0) # (n_neurons, )\n",
    "unit_avg_firing_rates = np.nanmax(unit_specific_binned_spike_rate_df.to_numpy(), axis=0) # (n_neurons, )\n",
    "unit_avg_firing_rates            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "\n",
    "def _compute_epochs_cell_firing_rates_metastats(epoch_inst_fr_df_list, minimal_active_firing_rate_Hz = 1e-3):\n",
    "    # epoch_inst_fr_df_list # List[pd.DataFrame] - where each df is of shape: (n_epoch_time_bins[i], n_cells) -- list of length n_epochs\n",
    "    # len(epoch_inst_fr_df_list) # n_epochs\n",
    "    # an_epoch = epoch_inst_fr_df_list[0] ## df has aclus as columns\n",
    "    n_active_aclus_per_epoch = [(an_epoch > minimal_active_firing_rate_Hz).sum(axis=1).values for an_epoch in epoch_inst_fr_df_list] # (n_epochs, ) # (n_epoch_time_bins[i], )\n",
    "    n_active_aclus_avg_per_epoch_time_bin = np.array([np.mean((an_epoch > minimal_active_firing_rate_Hz).sum(axis=1).values) for an_epoch in epoch_inst_fr_df_list]) # (n_epochs, )\n",
    "    \n",
    "    ## OUTPUTS: n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin\n",
    "    \n",
    "    \n",
    "    return n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin\n",
    "\n",
    "\n",
    "\n",
    "# instantaneous_time_bin_size_seconds = 0.005\n",
    "instantaneous_time_bin_size_seconds = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_epochs_df = deepcopy(active_replay_epochs_df)\n",
    "replay_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "epoch_inst_fr_df_list, epoch_inst_fr_signal_list, epoch_avg_firing_rates_list = SpikeRateTrends.compute_epochs_unit_avg_inst_firing_rates(spikes_df=replay_spikes_df, filter_epochs=replay_epochs_df, included_neuron_ids=EITHER_subset.track_exclusive_aclus, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds, use_instantaneous_firing_rate=True, debug_print=True)\n",
    "# epoch_inst_fr_df_list, epoch_inst_fr_signal_list, epoch_avg_firing_rates_list = SpikeRateTrends.compute_epochs_unit_avg_inst_firing_rates(spikes_df=filter_epoch_spikes_df_L, filter_epochs=epochs_df_L, included_neuron_ids=EITHER_subset.track_exclusive_aclus, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds, use_instantaneous_firing_rate=False, debug_print=False)\n",
    "# epoch_avg_firing_rates_list # (294, 42), (n_filter_epochs, n_neurons)\n",
    "# epoch_avg_firing_rates_list\n",
    "\n",
    "# epoch_avg_firing_rates_list\n",
    "# laps_all_epoch_bins_marginals_df\n",
    "n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin = _compute_epochs_cell_firing_rates_metastats(epoch_inst_fr_df_list=epoch_inst_fr_df_list)\n",
    "# n_active_aclus_avg_per_epoch_time_bin # (n_epochs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_epoch_avg_firing_rates = np.mean(epoch_avg_firing_rates_list, 0) # (42,)\n",
    "across_epoch_avg_firing_rates\n",
    "# unit_specific_binned_spike_rate_df\n",
    "# unit_specific_binned_spike_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Laps\n",
    "# laps_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "laps_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "# laps_filter_epochs = ensure_dataframe(deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)) \n",
    "epoch_inst_fr_df_list, epoch_inst_fr_signal_list, epoch_avg_firing_rates_list = SpikeRateTrends.compute_epochs_unit_avg_inst_firing_rates(spikes_df=laps_spikes_df, filter_epochs=ensure_dataframe(global_any_laps_epochs_obj),\n",
    "                                                                                                                                           included_neuron_ids=EITHER_subset.track_exclusive_aclus, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds, use_instantaneous_firing_rate=True, debug_print=False)\n",
    "# epoch_avg_firing_rates_list\n",
    "# laps_all_epoch_bins_marginals_df\n",
    "n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin = _compute_epochs_cell_firing_rates_metastats(epoch_inst_fr_df_list=epoch_inst_fr_df_list)\n",
    "n_active_aclus_avg_per_epoch_time_bin # (n_epochs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_df_S\n",
    "epochs_df_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_active_aclus_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cells_active_per_epoch = np.sum((epoch_avg_firing_rates_list > 0.1), axis=1) # find the number of neurons active in each time bin. (n_filter_epochs, )\n",
    "num_cells_active_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_avg_firing_rates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch_inst_fr_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch_inst_fr_df_list) # (n_epoch_time_bins[i], n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import LongShortDisplayConfigManager, long_short_display_config_manager\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColorFormatConverter, debug_print_color, build_adjusted_color\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import apply_LR_to_RL_adjustment\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers\n",
    "\n",
    "\n",
    "additional_cmap_names = dict(zip(TrackTemplates.get_decoder_names(), ['red', 'purple', 'green', 'orange'])) # {'long_LR': 'red', 'long_RL': 'purple', 'short_LR': 'green', 'short_RL': 'orange'}\n",
    "\n",
    "long_epoch_config = long_short_display_config_manager.long_epoch_config.as_pyqtgraph_kwargs()\n",
    "short_epoch_config = long_short_display_config_manager.short_epoch_config.as_pyqtgraph_kwargs()\n",
    "\n",
    "color_dict = {'long_LR': long_epoch_config['brush'].color(), 'long_RL': apply_LR_to_RL_adjustment(long_epoch_config['brush'].color()),\n",
    "                'short_LR': short_epoch_config['brush'].color(), 'short_RL': apply_LR_to_RL_adjustment(short_epoch_config['brush'].color())}\n",
    "additional_cmap_names = {k: ColorFormatConverter.qColor_to_hexstring(v) for k, v in color_dict.items()}\n",
    "\n",
    "additional_cmaps = {k: ColormapHelpers.create_transparent_colormap(color_literal_name=v, lower_bound_alpha=0.1) for k, v in additional_cmap_names.items()}\n",
    "additional_cmaps['long_LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].num_filter_epochs ## 84 laps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.data_exporting import HeatmapExportConfig\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.Pho2D.data_exporting import PosteriorExporting\n",
    "\n",
    "# custom_export_formats: Dict[str, HeatmapExportConfig] = None\n",
    "# custom_export_formats: Dict[str, HeatmapExportConfig] = {\n",
    "# \t# 'greyscale': HeatmapExportConfig.init_greyscale(),\n",
    "#     'color': HeatmapExportConfig(colormap='Oranges', desired_height=400),\n",
    "#     # 'color': HeatmapExportConfig(colormap=additional_cmaps['long_LR']),\n",
    "# \t# 'color': HeatmapExportConfig(colormap=cmap1, desired_height=200),\n",
    "# }\n",
    "\n",
    "# custom_exports_dict['color'].to_dict()\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_stacked_epoch_slices')\n",
    "# _out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_stacked_epoch_slices', custom_export_formats=custom_export_formats) # directional_decoded_stacked_epoch_slices\n",
    "_out\n",
    "# {'export_paths': {'laps': {'long_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/long_LR'),\n",
    "#    'long_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/long_RL'),\n",
    "#    'short_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/short_LR'),\n",
    "#    'short_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/short_RL')},\n",
    "#   'ripple': {'long_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/long_LR'),\n",
    "#    'long_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/long_RL'),\n",
    "#    'short_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/short_LR'),\n",
    "#    'short_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/short_RL')}},\n",
    "#  'parent_output_folder': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors'),\n",
    "#  'parent_specific_session_output_folder': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize New Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import General2DRenderTimeEpochs, inline_mkColor\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.EpochRenderingMixin import EpochRenderingMixin, RenderedEpochsItemsContainer\n",
    "from pyphoplacecellanalysis.General.Model.Datasources.IntervalDatasource import IntervalsDatasource\n",
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_new_training_df_dict, a_new_test_df_dict\n",
    "a_new_global_training_df = deepcopy(a_new_training_df)\n",
    "a_new_global_test_df = deepcopy(a_new_test_df)\n",
    "\n",
    "a_new_global_training_df.attrs\n",
    "a_new_global_test_df.attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.remove_all_epoch_interval_rect_legends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_legends_dict = active_2d_plot.build_or_update_all_epoch_interval_rect_legends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_kwargs = {'y_location': -5.0, 'height': 0.9}\n",
    "active_2d_plot.add_rendered_intervals(a_new_global_training_df, name=a_new_global_training_df.attrs['interval_datasource_name'], pen_color=inline_mkColor('purple', 0.8), brush_color=inline_mkColor('purple', 0.5), **shared_kwargs)\n",
    "active_2d_plot.add_rendered_intervals(a_new_global_test_df, name=a_new_global_test_df.attrs['interval_datasource_name'], pen_color=inline_mkColor('pink', 0.8), brush_color=inline_mkColor('pink', 0.5), **shared_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis_column_kwarg_keys = ['y_location', 'height', 'pen_color', 'brush_color']\n",
    "\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties({\n",
    "    # 'GlobalNonPBE':dict(pen_color=inline_mkColor('green', 0.8), brush_color=inline_mkColor('green', 0.5), **shared_kwargs),\n",
    "    'GlobalNonPBE_TRAIN':dict(pen_color=inline_mkColor('purple', 0.8), brush_color=inline_mkColor('purple', 0.5), **shared_kwargs),\n",
    "    'GlobalNonPBE_TEST':dict(pen_color=inline_mkColor('pink', 0.8), brush_color=inline_mkColor('pink', 0.5), **shared_kwargs),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_2d_plot.add_rendered_intervals(inter_lap_non_PBE_epoch_df, name=f'InterLapNonPBE')\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties({\n",
    "    'InterLapNonPBE':dict(pen_color=inline_mkColor('white', 0.8), brush_color=inline_mkColor('white', 0.5)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.add_rendered_intervals(global_epoch_only_non_PBE_epoch_df, name=f'GlobalNonPBE')\n",
    "active_2d_plot.add_rendered_intervals(a_new_training_df, name=f'GlobalNonPBE_TRAIN')\n",
    "active_2d_plot.add_rendered_intervals(a_new_test_df, name=f'GlobalNonPBE_TEST')\n",
    "\n",
    "vis_column_kwarg_keys = ['y_location', 'height', 'pen_color', 'brush_color']\n",
    "\n",
    "shared_kwargs = {'y_location': -5.0, 'height': 0.9}\n",
    "\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties({\n",
    "    # 'GlobalNonPBE':dict(pen_color=inline_mkColor('green', 0.8), brush_color=inline_mkColor('green', 0.5), **shared_kwargs),\n",
    "    'GlobalNonPBE_TRAIN':dict(pen_color=inline_mkColor('purple', 0.8), brush_color=inline_mkColor('purple', 0.5), **shared_kwargs),\n",
    "    'GlobalNonPBE_TEST':dict(pen_color=inline_mkColor('pink', 0.8), brush_color=inline_mkColor('pink', 0.5), **shared_kwargs),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_interval_keys = ['_', 'SessionEpochs', 'Laps', '_', 'PBEs', 'Ripples', 'Replays'] # '_' indicates a vertical spacer\n",
    "desired_interval_height_ratios = [0.2, 0.5, 1.0, 0.1, 1.0, 1.0, 1.0] # ratio of heights to each interval (and the vertical spacers)\n",
    "stacked_epoch_layout_dict = active_2d_plot.apply_stacked_epoch_layout(rendered_interval_keys, desired_interval_height_ratios, epoch_render_stack_height=20.0, interval_stack_location='below')\n",
    "stacked_epoch_layout_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_x_min, curr_x_max, curr_y_min, curr_y_max = active_2d_plot.get_render_intervals_plot_range()\n",
    "(curr_x_min, curr_x_max, curr_y_min, curr_y_max) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upper_extreme_vertical_offset, lower_extreme_vertical_offsets = active_2d_plot.extract_interval_bottom_top_area()  \n",
    "(upper_extreme_vertical_offset, lower_extreme_vertical_offsets) # (-24.16666666666667, -5.0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interval Display Config Widget in SpikeRasterWindow's Right Sidebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_sidebar_contents_container: pg.LayoutWidget = spike_raster_window.right_sidebar_contents_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spike_raster_window.build_epoch_intervals_visual_configs_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.epochs_plotting_mixins import EpochDisplayConfig\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.EpochRenderConfigWidget.EpochRenderConfigWidget import EpochRenderConfigsListWidget, EpochRenderConfigWidget\n",
    "\n",
    "epochs_render_configs_widget: EpochRenderConfigsListWidget = active_2d_plot.ui.epochs_render_configs_widget\n",
    "epochs_render_configs_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.update_epochs_from_configs_widget()\n",
    "# spike_raster_window.update_epoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_render_configs_widget.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_render_configs_widget_configs: Dict[str, EpochDisplayConfig] = deepcopy(epochs_render_configs_widget.configs)\n",
    "epochs_render_configs_widget_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "out_configs_dict = active_2d_plot.extract_interval_display_config_lists()\n",
    "out_configs_dict\n",
    "pn.Row(*[pn.Column(*[pn.Param(a_sub_v) for a_sub_v in v]) for k,v in out_configs_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_render_configs_widget_configs: Dict[str, EpochDisplayConfig] = active_2d_plot.ui.epochs_render_configs_widget.configs_from_states()\n",
    "epochs_render_configs_widget_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_sidebar_widget: Spike3DRasterRightSidebarWidget = spike_raster_window.right_sidebar_widget\n",
    "right_sidebar_contents_container_dockarea = spike_raster_window.right_sidebar_contents_container_dockarea"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
