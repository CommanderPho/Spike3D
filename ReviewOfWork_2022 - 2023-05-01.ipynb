{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = find_first_extant_path([Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')])\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "tags": [
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load",
     "REQUIRED"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-13_14-42-6', '2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_3-23-37', '2006-6-09_1-22-43', '2006-6-12_15-55-31']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# # Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ## Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13e68b4-03e0-4388-a35c-87a352a6e6b3",
   "metadata": {
    "tags": [
     "load",
     "single_session",
     "REQUIRED"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.spikes.mat... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:140: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Computing linear positions for all active epochs for session... Saving updated position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.position.npy... 2006-6-07_11-26-53.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.interpolated_spike_positions.npy... 2006-6-07_11-26-53.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/2006-6-07_11-26-53.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/ripple_df.pkl.\n",
      "Loading success: .mua.npy.\n",
      "Loading success: .pbe.npy.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1739.1533641185379)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['maze_id']] = laps_df[['maze_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['maze_id']] = laps_df[['maze_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1739.1533641185379, end: 1932.4200048116618)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1932.4200048116618)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['maze_id']] = laps_df[['maze_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using provided computation_functions_name_whitelist: ['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_time_dependent_pf_sequential_surprise_computation_perform_two_step_position_decoding_computation']\n",
      "due to whitelist, including only 5 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19156,) should be less than time_window_edges: (52175,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19156,) should be less than time_window_edges: (52175,)!\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 5 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 5 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19917,) should be less than time_window_edges: (57970,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19917,) should be less than time_window_edges: (57970,)!\n",
      "updating computation_results...\n",
      "done.\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53/loadedSessPickle.pkl... done.\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[1] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# # Read if possible:\n",
    "# saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "# force_reload = False\n",
    "\n",
    "# Force write:\n",
    "saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "force_reload = True\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                          skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e403d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sane_midpoint_x: 118.9705186457787, hardcoded_track_midpoint_x: None, track_min_max_x: (23.923329354140844, 261.8643666456982)\n",
      "hardcoded_track_midpoint_x is None, falling back to sane_midpoint_x... 118.9705186457787\n",
      "desc_crossings_x: (22,), asc_crossings_x: (23,)\n",
      "WARNING: must drop last asc_crossing_midpoints.\n",
      "setting new computation epochs because laps changed.\n"
     ]
    }
   ],
   "source": [
    "from neuropy.analyses.placefields import PfND\n",
    "from PendingNotebookCode import constrain_to_laps\n",
    "from PendingNotebookCode import compute_short_long_constrained_decoders\n",
    "\n",
    "curr_active_pipeline = constrain_to_laps(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda48261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7697,) should be less than time_window_edges: (50547,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7697,) should be less than time_window_edges: (50547,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "self will be re-binned to match target_pf...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7697,) should be less than time_window_edges: (50547,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7697,) should be less than time_window_edges: (50547,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "self will be re-binned to match target_pf...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (758,) should be less than time_window_edges: (5792,)!\n"
     ]
    }
   ],
   "source": [
    "(long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "# 3m 40.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_one_step_decoder_1D.pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_results.pf1D_Decoder.pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_results.pf2D_Decoder.pf.plot_ratemaps_2D()\n",
    "short_one_step_decoder_2D.pf.plot_ratemaps_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) # AttributeError: 'PfND_TimeDependent' object has no attribute '_included_thresh_neurons_indx'\n",
    "# TypeError: cannot pickle 'MplMultiTab' object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a92c8",
   "metadata": {},
   "source": [
    "# 2023-03-16 - Explore passing in long/short decoders specifically to `perform_full_session_leave_one_out_decoding_analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26804fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "n_neurons = 33, neurons_colors_array.shape =(4, 33)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 33, n_all_epoch_timebins = 1328)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:321: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/.cache/pypoetry/virtualenvs/spike3d-xhKu_1Lg-py3.9/lib64/python3.9/site-packages/scipy/spatial/distance.py:1300: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 33, n_all_epoch_timebins = 1328)\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_full_session_leave_one_out_decoding_analysis\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import SurpriseAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import build_neurons_color_map # for plot_short_v_long_pf1D_comparison\n",
    "\n",
    "# Get existing long/short decoders from the cell under \"# 2023-02-24 Decoders\"\n",
    "long_decoder, short_decoder = deepcopy(long_one_step_decoder_1D), deepcopy(short_one_step_decoder_1D)\n",
    "assert np.all(long_decoder.xbin == short_decoder.xbin)\n",
    "\n",
    "# Backup and replace loaded replays with computed ones:\n",
    "long_replays, short_replays, global_replays = [a_session.replace_session_replays_with_estimates(require_intersecting_epoch=None, debug_print=False) for a_session in [long_session, short_session, global_session]]\n",
    "\n",
    "### Need to prune to only the cells active in both epochs ahead of time:\n",
    "# Prune to the shared aclus in both epochs (short/long):\n",
    "long_shared_aclus_only_decoder, short_shared_aclus_only_decoder = [BasePositionDecoder.init_from_stateful_decoder(a_decoder) for a_decoder in (long_decoder, short_decoder)]\n",
    "shared_aclus, (long_shared_aclus_only_decoder, short_shared_aclus_only_decoder), long_short_pf_neurons_diff = BasePositionDecoder.prune_to_shared_aclus_only(long_shared_aclus_only_decoder, short_shared_aclus_only_decoder)\n",
    "n_neurons = len(shared_aclus)\n",
    "# for plotting purposes, build colors only for the common (present in both, the intersection) neurons:\n",
    "neurons_colors_array = build_neurons_color_map(n_neurons, sortby=None, cmap=None)\n",
    "print(f'{n_neurons = }, {neurons_colors_array.shape =}')\n",
    "# from viztracer import VizTracer\n",
    "\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "long_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=long_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_long', perform_cache_load=False) # , perform_cache_load=False\n",
    "short_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=short_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_short', perform_cache_load=False) # , perform_cache_load=False\n",
    "\n",
    "# 4m 22.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7742482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-05-01 - Test my interneuron hypothesis by looking at interneurons across the long/short divide\n",
    "\n",
    "\n",
    "# 1. Interneuron identity should be best determinant of active environment.\n",
    "\n",
    "# 2. Do people know anything about interneurons that activate in a given environment? For example, to interneurons activate alongside pyramidal cells when replaying a given environment?\n",
    "\n",
    "# 3. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d06a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d69eed8a",
   "metadata": {},
   "source": [
    "## 2023-04-13 - Shuffled Surprise\n",
    "\"\"\" \n",
    "Relevant Functions:\n",
    "`perform_full_session_leave_one_out_decoding_analysis`:\n",
    "\t`perform_leave_one_aclu_out_decoding_analysis`:\tfrom pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_leave_one_aclu_out_decoding_analysis\n",
    "\t`_analyze_leave_one_out_decoding_results`: from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions import _analyze_leave_one_out_decoding_results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-04-18 - Refactored into decoder_result\n",
    "result, result_df, result_df_grouped = long_results_obj.result, long_results_obj.result_df, long_results_obj.result_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c61634",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import DiagnosticDistanceMetricFigure\n",
    "\n",
    "## Render the internactive slider that allows selecting the timebin index to debug\n",
    "n_timebins = np.sum(long_results_obj.all_epochs_num_epoch_time_bins)\n",
    "timebinned_neuron_info = long_results_obj.timebinned_neuron_info\n",
    "result = long_results_obj.new_result\n",
    "active_fig_obj, update_function = DiagnosticDistanceMetricFigure.build_interactive_diagnostic_distance_metric_figure(long_results_obj, timebinned_neuron_info, result)\n",
    "active_fig_obj.integer_slider(n_timebins=n_timebins, update_func=update_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get set of cells active in a given time bin, for each compute the surprise of its placefield with the leave-one-out decoded posterior.\n",
    "\n",
    "# 2. From the remainder of cells (those not active), randomly choose one to grab the placefield of and compute the surprise with that and the same posterior.\n",
    "\n",
    "# Expectation: The cells that are included in the time bin are expected to have a lower surprise (be less correlated with) the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff50bc",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import build_pyqtgraph_epoch_indicator_regions # Add session indicators to pyqtgraph plot\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.MultiContextComparingDisplayFunctions import _helper_add_long_short_session_indicator_regions\n",
    "\n",
    "from pyphocorehelpers.print_helpers import generate_html_string\n",
    "# Long Short\n",
    "# TODO 2023-04-18 - Can Refactor in terms of `plot_long_short_any_values`?\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "# 'time_bin_indices': valid_time_bin_indicies, 'posterior_to_pf_mean_surprise': one_left_out_posterior_to_pf_surprises_mean, 'posterior_to_scrambled_pf_mean_surprise': one_left_out_posterior_to_scrambled_pf_surprises_mean}\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.MultiContextComparingDisplayFunctions import plot_long_short_expected_vs_observed_firing_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "win, plots_tuple, legend = plot_long_short_expected_vs_observed_firing_rates(long_results_obj, short_results_obj)\n",
    "long_epoch_indicator_region_items, short_epoch_indicator_region_items = _helper_add_long_short_session_indicator_regions(win, long_epoch=curr_active_pipeline.filtered_epochs[long_epoch_name], short_epoch=curr_active_pipeline.filtered_epochs[short_epoch_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4efa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_long_short_surprise_difference_plot(long_results_obj, short_results_obj):\n",
    "\t\"\"\" captures `curr_active_pipeline` \"\"\"\n",
    "\t# Private Subfunctions _______________________________________________________________________________________________ #\n",
    "\tdef _subfn_add_difference_plot_series(win, plots, result_df_grouped, series_suffix, **kwargs):\n",
    "\t\t\"\"\" captures nothing\n",
    "\t\tmodifies `plots` \"\"\"\n",
    "\t\tx=result_df_grouped.time_bin_centers.to_numpy()\n",
    "\t\ty=result_df_grouped['surprise_diff'].to_numpy()\n",
    "\t\tseries_id_str = f'difference_{series_suffix}'\n",
    "\t\tplots[series_id_str] = win.plot(x=x, y=y, name=series_id_str, alpha=0.5, **kwargs) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\t# BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "\n",
    "\t# make a separate symbol_brush color for each cell:\n",
    "\t# cell_color_symbol_brush = [pg.intColor(i,hues=9, values=3, alpha=180) for i, aclu in enumerate(long_results_obj.original_1D_decoder.neuron_IDs)] # maxValue=128\n",
    "\t# All properties in common:\n",
    "\twin = pg.plot() # PlotWidget\n",
    "\twin.setWindowTitle('Long Sanity Check - Leave-one-out Custom Surprise Plot')\n",
    "\t# legend_size = (80,60) # fixed size legend\n",
    "\tlegend_size = None # auto-sizing legend to contents\n",
    "\tlegend = pg.LegendItem(legend_size, offset=(-1,0)) # do this instead of # .addLegend\n",
    "\tlegend.setParentItem(win.graphicsItem())\n",
    "\n",
    "\tplots = {}\n",
    "\tlabel_prefix_list = ['normal', 'scrambled']\n",
    "\tlong_short_symbol_list = ['t', 'o'] # note: 's' is a square. 'o', 't1': triangle pointing upwards0\n",
    "\n",
    "\t# Use mean time_bin and surprise for each epoch\n",
    "\t# plots['normal'] = win.plot(x=valid_time_bin_indicies, y=one_left_out_posterior_to_pf_surprises_mean, pen=None, symbol='t', symbolBrush=pg.intColor(1,6,maxValue=128), name=f'normal', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\t# plots['scrambled'] = win.plot(x=valid_time_bin_indicies, y=one_left_out_posterior_to_scrambled_pf_surprises_mean, pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), name=f'scrambled', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\t# curr_surprise_difference = one_left_out_posterior_to_scrambled_pf_surprises_mean - one_left_out_posterior_to_pf_surprises_mean\n",
    "\n",
    "\t# x=valid_time_bin_indicies\n",
    "\t# y=curr_surprise_difference\n",
    "\t# x=result_df_grouped.time_bin_indices.to_numpy()\n",
    "\n",
    "\t\n",
    "\t_subfn_add_difference_plot_series(win, plots, long_results_obj.result_df_grouped, series_suffix='_long', **dict(pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), clickable=True, hoverable=True, hoverSize=7))\n",
    "\n",
    "\t_subfn_add_difference_plot_series(win, plots, short_results_obj.result_df_grouped, series_suffix='_short', **dict(pen=None, symbol='o', symbolBrush=pg.intColor(3,6,maxValue=128), clickable=True))\n",
    "\n",
    "\t# dict(pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128))\n",
    "\n",
    "\n",
    "\t# x=result_df_grouped.time_bin_centers.to_numpy()\n",
    "\t# y=result_df_grouped['surprise_diff'].to_numpy()\n",
    "\t# plots['difference'] = win.plot(x=x, y=y, pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), name=f'difference', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\t# long_results_obj.result, long_results_obj.result_df, long_results_obj.result_df_grouped\n",
    "\n",
    "\t# short_results_obj.result, short_results_obj.result_df, short_results_obj.result_df_grouped\n",
    "\n",
    "\n",
    "\tfor k, v in plots.items():\n",
    "\t\tlegend.addItem(v, f'{k}')\n",
    "\n",
    "\twin.graphicsItem().setLabel(axis='left', text='Normal v. Random - Surprise (Custom)')\n",
    "\twin.graphicsItem().setLabel(axis='bottom', text='time')\n",
    "\n",
    "\twin.showGrid(True, True)  # Show grid for reference\n",
    "\n",
    "\t# Emphasize the y=0 crossing by drawing a horizontal line at y=0\n",
    "\tvline = pg.InfiniteLine(pos=0, angle=0, movable=False, pen=pg.mkPen(color='w', width=2, style=pg.QtCore.Qt.DashLine))\n",
    "\twin.addItem(vline)\n",
    "\n",
    "\t# Add session indicators to pyqtgraph plot\n",
    "\tlong_epoch = curr_active_pipeline.filtered_epochs[long_epoch_name]\n",
    "\tshort_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "\tlong_epoch_indicator_region_items, short_epoch_indicator_region_items = _helper_add_long_short_session_indicator_regions(win, long_epoch, short_epoch)\n",
    "\n",
    "\t# epoch_linear_region, epoch_region_label = build_pyqtgraph_epoch_indicator_regions(win, t_start=curr_active_pipeline.filtered_epochs[long_epoch_name].t_start, t_stop=curr_active_pipeline.filtered_epochs[long_epoch_name].t_stop, epoch_label='long', **dict(pen=pg.mkPen('#0b0049'), brush=pg.mkBrush('#0099ff42'), hoverBrush=pg.mkBrush('#fff400'), hoverPen=pg.mkPen('#00ff00')))\n",
    "\t# epoch_linear_region, epoch_region_label = build_pyqtgraph_epoch_indicator_regions(win, t_start=curr_active_pipeline.filtered_epochs[short_epoch_name].t_start, t_stop=curr_active_pipeline.filtered_epochs[short_epoch_name].t_stop, epoch_label='short', **dict(pen=pg.mkPen('#490000'), brush=pg.mkBrush('#f5161659'), hoverBrush=pg.mkBrush('#fff400'), hoverPen=pg.mkPen('#00ff00')))\n",
    "\n",
    "\ti_str = generate_html_string('i', color='white', bold=True)\n",
    "\tj_str = generate_html_string('j', color='red', bold=True)\n",
    "\ttitle_str = generate_html_string(f'JSD(p_x_given_n, pf[{i_str}]) - JSD(p_x_given_n, pf[{j_str}]) where {j_str} non-firing')\n",
    "\twin.setTitle(title_str)\n",
    "\n",
    "\twin.setWindowTitle('Long Sanity Check - Leave-one-out Custom Surprise Plot - JSD')\n",
    "\n",
    "\treturn win, plots\n",
    "\n",
    "win, plots = plot_long_short_surprise_difference_plot(long_results_obj, short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d937f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _helper_make_scatterplot_clickable(main_scatter_plot, enable_hover:bool=False):\n",
    "\t\"\"\" pyqtgraph \"\"\"\n",
    "\t# Highlights the hovered spikes white:\n",
    "\t# main_scatter_plot.addPoints(hoverable=True,\n",
    "\t# \t# hoverSymbol=vtick, # hoverSymbol='s',\n",
    "\t# \thoverSize=7, # default is 5\n",
    "\t# \t)\n",
    "\n",
    "\n",
    "\t## Clickable/Selectable Spikes:\n",
    "\t# Will make all plots clickable\n",
    "\tclickedPen = pg.mkPen('#DDD', width=2)\n",
    "\tlastClicked = []\n",
    "\tdef _test_scatter_plot_clicked(plot, points):\n",
    "\t\tglobal lastClicked\n",
    "\t\tfor p in lastClicked:\n",
    "\t\t\tp.resetPen()\n",
    "\t\tprint(\"clicked points\", points)\n",
    "\t\tfor p in points:\n",
    "\t\t\tp.setPen(clickedPen)\n",
    "\t\tlastClicked = points\n",
    "\n",
    "\tmain_scatter_clicked_connection = main_scatter_plot.sigClicked.connect(_test_scatter_plot_clicked)\n",
    "\n",
    "\t## Hoverable Spikes:\n",
    "\tif enable_hover:\n",
    "\t\tdef _test_scatter_plot_hovered(plt, points, ev):\n",
    "\t\t\t# sigHovered(self, points, ev)\n",
    "\t\t\tprint(f'_test_scatter_plot_hovered(plt: {plt}, points: {points}, ev: {ev})')\n",
    "\t\t\tif (len(points) > 0):\n",
    "\t\t\t\tcurr_point = points[0]\n",
    "\t\t\t\t# self.\n",
    "\t\t\t\t# curr_point.index\n",
    "\t\tmain_scatter_hovered_connection = main_scatter_plot.sigHovered.connect(_test_scatter_plot_hovered)\n",
    "\telse:\n",
    "\t\tmain_scatter_hovered_connection = None\n",
    "\n",
    "\treturn lastClicked, clickedPen, (main_scatter_hovered_connection, main_scatter_clicked_connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a84888",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot = plots['difference__long']  # PlotDataItem \n",
    "a_curve = a_plot.curve # PlotCurveItem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot.sigPointsClicked\n",
    "# a_plot.sigPointsHovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe350d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve.curve.setClickable(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will make all plots clickable\n",
    "clickedPen = pg.mkPen('#DDD', width=2)\n",
    "lastClicked = []\n",
    "def _test_scatter_plot_clicked(plot, points):\n",
    "\tglobal lastClicked\n",
    "\tfor p in lastClicked:\n",
    "\t\tp.resetPen()\n",
    "\tprint(\"clicked points\", points)\n",
    "\tfor p in points:\n",
    "\t\tp.setPen(clickedPen)\n",
    "\tlastClicked = points\n",
    "\n",
    "main_scatter_clicked_connection = a_plot.sigClicked.connect(_test_scatter_plot_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastClicked, clickedPen, (main_scatter_hovered_connection, main_scatter_clicked_connection) = _helper_make_scatterplot_clickable(a_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d54370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to map from time_bin_indicies to times\n",
    "a_plot = plots['difference'] # PlotDataItem \n",
    "# a_plot.setLa\n",
    "# win.graphicsItem().setLabel(axis='left', text='Normal v. Random - Surprise (Custom)')\n",
    "\n",
    "# # Set the plot title with a LaTeX formula\n",
    "# title = pg.LabelItem(justify='center')\n",
    "# title.setText(r'<font size=\"4\">JSD Surprise Diff: $\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$</font>')\n",
    "# win.addItem(title)\n",
    "\n",
    "# win.graphicsItem().setLabel(axis='top', text=r'$\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot # works pretty well seemingly\n",
    "\n",
    "export_pyqtgraph_plot(win)\n",
    "# pg.setConfigOption('leftTitle', 'MathText')\n",
    "# win.setTitle(r'JSD(p_x_given_n, pf[<font size=\"4\"><b><span style=\"color:red;\">i</span></b></font>]) - JSD(p_x_given_n, pf[<font size=\"4\"><b>j</b></font>]) where <font size=\"4\"><b>j</b></font> non-firing')\n",
    "\n",
    "# win.setTitle(generate_html_string(f'JSD(p_x_given_n, pf[{i_str}]) - JSD(p_x_given_n, pf[{j_str}]) where {j_str} non-firing', font_size=8))\n",
    "\n",
    "# r'$\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$'\n",
    "# title_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqtgraph.GraphicsScene import exportDialog\n",
    "\n",
    "# active_item = active_fig_obj.win\n",
    "active_item = active_fig_obj.plot_dict['curr_cell_pf_curve']['plot_item']\n",
    "exportDialog = exportDialog.ExportDialog(active_item.scene())\n",
    "exportDialog.show(active_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_fig_obj.plot_dict['curr_cell_pf_curve']['plot_item']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d68ce1",
   "metadata": {},
   "source": [
    "$\\fn{JSD}{\\matr{pf}_{i}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}} - \\fn{JSD}{\\matr{pf}_{j}, \\Pr{\\vec{x}_{t}|\\vec{n}_{t}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "win.setBackground\n",
    "win.setWindowTitle\n",
    "win.setBackgroundBrush\n",
    "win.setXRange\n",
    "win.setYRange\n",
    "\n",
    "\"Show X Grid\"\n",
    "\"Show Y Grid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_config = dict(pen=pg.mkPen('#fff'), brush=pg.mkBrush('#f004'), hoverBrush=pg.mkBrush('#fff4'), hoverPen=pg.mkPen('#f00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_epochs[short_epoch_name].t_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_epochs[long_epoch_name].start_end_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f996fd9",
   "metadata": {},
   "source": [
    "## Pre 2023-04-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import plot_kourosh_activity_style_figure\n",
    "\n",
    "from neuropy.core.neurons import NeuronType\n",
    "# # Include only pyramidal aclus:\n",
    "# print(f'all shared_aclus: {len(shared_aclus)}\\nshared_aclus: {shared_aclus}')\n",
    "# shared_aclu_neuron_type = long_session.neurons.neuron_type[np.isin(long_session.neurons.neuron_ids, shared_aclus)]\n",
    "# assert len(shared_aclu_neuron_type) == len(shared_aclus)\n",
    "# # Find only the aclus that are pyramidal:\n",
    "# is_shared_aclu_pyramidal = (shared_aclu_neuron_type == NeuronType.PYRAMIDAL)\n",
    "# pyramidal_only_shared_aclus = shared_aclus[is_shared_aclu_pyramidal]\n",
    "# print(f'num pyramidal_only_shared_aclus: {len(pyramidal_only_shared_aclus)}\\npyramidal_only_shared_aclus: {pyramidal_only_shared_aclus}')\n",
    "\n",
    "\n",
    "## Drop Pyramidal but don't use only shared aclus:\n",
    "all_aclus = deepcopy(long_session.neurons.neuron_ids)\n",
    "neuron_type = long_session.neurons.neuron_type\n",
    "assert len(neuron_type) == len(all_aclus)\n",
    "# Find only the aclus that are pyramidal:\n",
    "is_aclu_pyramidal = (neuron_type == NeuronType.PYRAMIDAL)\n",
    "pyramidal_only_all_aclus = all_aclus[is_aclu_pyramidal]\n",
    "print(f'num pyramidal_only_all_aclus: {len(pyramidal_only_all_aclus)}\\npyramidal_only_all_aclus: {pyramidal_only_all_aclus}')\n",
    "\n",
    "is_aclu_interneuron = (neuron_type == NeuronType.INTERNEURONS)\n",
    "interneuron_only_all_aclus = all_aclus[is_aclu_interneuron]\n",
    "print(f'num interneuron_only_all_aclus: {len(interneuron_only_all_aclus)}\\ninterneurons: {interneuron_only_all_aclus}')\n",
    "\n",
    "# app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, shared_aclus, epoch_idx=5, callout_epoch_IDXs=[0,1,2,3], skip_rendering_callouts=True)\n",
    "# app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_shared_aclus, epoch_idx=2, callout_epoch_IDXs=[0,4], skip_rendering_callouts=False)\n",
    "app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_all_aclus, epoch_idx=6, callout_epoch_IDXs=[2,4,6], skip_rendering_callouts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b0a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985977d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the Jupyter Index Thing\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import DiagnosticDistanceMetricFigure\n",
    "\n",
    "## Render the internactive slider that allows selecting the timebin index to debug\n",
    "n_timebins = np.sum(long_results_obj.all_epochs_num_epoch_time_bins)\n",
    "timebinned_neuron_info = long_results_obj.timebinned_neuron_info\n",
    "result = long_results_obj.new_result\n",
    "active_fig_obj, update_function = DiagnosticDistanceMetricFigure.build_interactive_diagnostic_distance_metric_figure(long_results_obj, timebinned_neuron_info, result)\n",
    "active_fig_obj.integer_slider(n_timebins=n_timebins, update_func=update_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1aa692",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_fig_obj.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea983fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_all_aclus, epoch_idx=11, callout_epoch_IDXs=[0,1,2, 3, 4, 5], skip_rendering_callouts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519e392",
   "metadata": {},
   "source": [
    "# 2023-04-13 Show Surprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.MultiContextComparingDisplayFunctions import plot_long_short, plot_long_short_any_values, plot_long_short_expected_vs_observed_firing_rates, _helper_add_long_short_session_indicator_regions\n",
    "# plot_long_short(long_results_obj, short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_long_short_any_values(long_results_obj=long_results_obj, short_results_obj=short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_long_short_expected_vs_observed_firing_rates(long_results_obj=long_results_obj, short_results_obj=short_results_obj, limit_aclus=[89]) # 4, 89, 28, 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fn = lambda a_results_obj: a_results_obj.all_epochs_decoded_epoch_time_bins_mean[:,0]\n",
    "# y_fn = lambda a_results_obj: a_results_obj.all_epochs_all_cells_one_left_out_posterior_to_scrambled_pf_surprises_mean\n",
    "y_fn = lambda a_results_obj: a_results_obj.all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean\n",
    "# y_fn = lambda a_results_obj: a_results_obj.all_epochs_computed_one_left_out_posterior_to_pf_surprises\n",
    "\n",
    "# (time_bins, neurons), (epochs, neurons), (epochs)\n",
    "# all_epochs_computed_one_left_out_posterior_to_pf_surprises, all_epochs_computed_cell_one_left_out_posterior_to_pf_surprises_mean, all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean\n",
    "win, plots_tuple, legend = plot_long_short_any_values(long_results_obj, short_results_obj, x=x_fn, y=y_fn, limit_aclus=[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe80559",
   "metadata": {},
   "source": [
    "# 2023-04-13 - Find Good looking epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(long_results_obj.active_filter_epochs, long_results_obj.all_included_filter_epochs_decoder_result, global_pos_df=global_session.position.df, variable_name='lin_pos', xbin=long_results_obj.original_1D_decoder.xbin,\n",
    "                                                        name='stacked_epoch_slices_long_results_obj', debug_print=False, debug_test_max_num_slices=32)\n",
    "curr_viz_params, _curr_plot_data, _curr_plots, _curr_ui_container = laps_plot_tuple\n",
    "_curr_plots.axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [23, 27, 29, ]\n",
    "[16, 17, 18, 20, 21, 22, 23, 25, 26, 29]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e6526e9",
   "metadata": {},
   "source": [
    "# 2023-03-28 - Playing around with older computations/visualizations from the `_display_short_long_firing_rate_index_comparison` era:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2e9eb6",
   "metadata": {},
   "source": [
    "\n",
    "2023-04-20 - Encountered issue with the replays in session '2006-6-08_14-26-15' where they are duplicated exactly twice, like the first half of the rows are legitimate entries and the second half are directly repeated versions of the first with the only difference appearing to be the 'epoch_id' column changes from 1 to 2. 'rel_id' column seems incorrect but different for some reason. It must be how the MATLAB script exports the values.\n",
    "\n",
    "Also when I'm looking at only the `short_session.replay` there are many non-2 'epoch_id' values, which is strange. \n",
    "\n",
    "TODO: It could have something to do with Jonathan's code maybe? Because the 'replay_r' and 'replay_p' columns he added are different. SEEMS FALSE. It's this way even without running Jonathan's code, although the values might have been saved later?\n",
    "\tAlso flat_replay_idx jumps from 689 to 1087 at the transition from epoch_id 1 to 2\n",
    "\n",
    "UPDATE: the 'replay_r' and 'replay_p' columns aren't from Joanthan, they're in the original .replay_info.mat that's imported!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a13cccd",
   "metadata": {},
   "source": [
    "# Plot long|short firing rate index using 'long_short_fr_indicies_analyses':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd397788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included whitelist is specified: ['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/epoch.py:404: UserWarning: curr_epochs already empty prior to filtering by firing rate or minimum active units\n",
      "  warn(f'curr_epochs already empty prior to filtering by firing rate or minimum active units')\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:590: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:590: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No short laps!\t long: (laps: True, replays: 172), \t short: (laps: 0, replays: 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveWrapper.py:498\u001b[0m, in \u001b[0;36mbatch_extended_computations\u001b[0;34m(curr_active_pipeline, include_whitelist, include_global_functions, fail_on_exception, progress_print, debug_print)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[39m## Get global `long_short_fr_indicies_analysis` results:\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     long_short_fr_indicies_analysis_results \u001b[39m=\u001b[39m curr_active_pipeline\u001b[39m.\u001b[39;49mglobal_computation_results\u001b[39m.\u001b[39;49mcomputed_data[\u001b[39m'\u001b[39;49m\u001b[39mlong_short_fr_indicies_analysis\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    499\u001b[0m     x_frs_index, y_frs_index \u001b[39m=\u001b[39m long_short_fr_indicies_analysis_results[\u001b[39m'\u001b[39m\u001b[39mx_frs_index\u001b[39m\u001b[39m'\u001b[39m], long_short_fr_indicies_analysis_results[\u001b[39m'\u001b[39m\u001b[39my_frs_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# use the all_results_dict as the computed data value\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:34\u001b[0m, in \u001b[0;36mDynamicParameters.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDynamicParameters.__getitem__(self, key): key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'long_short_fr_indicies_analysis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# 2023-01-* - Call extended computations to build `_display_short_long_firing_rate_index_comparison` figures:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m extended_computations_include_whitelist\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mlong_short_fr_indicies_analyses\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjonathan_firing_rate_analysis\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# do only specifiedl\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m newly_computed_values \u001b[39m=\u001b[39m batch_extended_computations(curr_active_pipeline, include_whitelist\u001b[39m=\u001b[39;49mextended_computations_include_whitelist, include_global_functions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fail_on_exception\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, progress_print\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, debug_print\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m newly_computed_values\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveWrapper.py:510\u001b[0m, in \u001b[0;36mbatch_extended_computations\u001b[0;34m(curr_active_pipeline, include_whitelist, include_global_functions, fail_on_exception, progress_print, debug_print)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m progress_print \u001b[39mor\u001b[39;00m debug_print:\n\u001b[1;32m    509\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Recomputing \u001b[39m\u001b[39m{\u001b[39;00m_comp_name\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 510\u001b[0m curr_active_pipeline\u001b[39m.\u001b[39;49mperform_specific_computation(computation_functions_name_whitelist\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m_perform_short_long_firing_rate_analyses\u001b[39;49m\u001b[39m'\u001b[39;49m], fail_on_exception\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, debug_print\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# fail_on_exception MUST be True or error handling is all messed up \u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m done.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    512\u001b[0m long_short_fr_indicies_analysis_results \u001b[39m=\u001b[39m curr_active_pipeline\u001b[39m.\u001b[39mglobal_computation_results\u001b[39m.\u001b[39mcomputed_data[\u001b[39m'\u001b[39m\u001b[39mlong_short_fr_indicies_analysis\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:796\u001b[0m, in \u001b[0;36mPipelineWithComputedPipelineStageMixin.perform_specific_computation\u001b[0;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_whitelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" perform a specific computation (specified in computation_functions_name_whitelist) in a minimally destructive manner using the previously recomputed results:\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[39mPassthrough wrapper to self.stage.perform_specific_computation(...) with the same arguments.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \n\u001b[1;32m    792\u001b[0m \u001b[39mUpdates:\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[39m    curr_active_pipeline.computation_results\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39m# self.stage is of type ComputedPipelineStage\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstage\u001b[39m.\u001b[39;49mperform_specific_computation(active_computation_params\u001b[39m=\u001b[39;49mactive_computation_params, enabled_filter_names\u001b[39m=\u001b[39;49menabled_filter_names, computation_functions_name_whitelist\u001b[39m=\u001b[39;49mcomputation_functions_name_whitelist, computation_kwargs_list\u001b[39m=\u001b[39;49mcomputation_kwargs_list, fail_on_exception\u001b[39m=\u001b[39;49mfail_on_exception, debug_print\u001b[39m=\u001b[39;49mdebug_print)\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:466\u001b[0m, in \u001b[0;36mComputedPipelineStage.perform_specific_computation\u001b[0;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_whitelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[39m## TODO: ERROR: `owning_pipeline_reference=self` is not CORRECT as self is of type `ComputedPipelineStage` (or `DisplayPipelineStage`) and not `NeuropyPipeline`\u001b[39;00m\n\u001b[1;32m    463\u001b[0m         \u001b[39m# this has been fine for all the global functions so far because the majority of the properties are defined on the stage anyway, but any pipeline properties will be missing! \u001b[39;00m\n\u001b[1;32m    464\u001b[0m     global_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(owning_pipeline_reference\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, global_computation_results\u001b[39m=\u001b[39mprevious_computation_result, computation_results\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomputation_results, active_configs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_configs, include_whitelist\u001b[39m=\u001b[39menabled_filter_names, debug_print\u001b[39m=\u001b[39mdebug_print)\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_computation_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_specific_computations_single_context(global_kwargs, computation_functions_name_whitelist\u001b[39m=\u001b[39;49mcomputation_functions_name_whitelist, are_global\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fail_on_exception\u001b[39m=\u001b[39;49mfail_on_exception, debug_print\u001b[39m=\u001b[39;49mdebug_print)\n\u001b[1;32m    467\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Non-global functions:\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39mfor\u001b[39;00m a_select_config_name, a_filtered_session \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiltered_sessions\u001b[39m.\u001b[39mitems():                \n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:251\u001b[0m, in \u001b[0;36mComputedPipelineStage.run_specific_computations_single_context\u001b[0;34m(self, previous_computation_result, computation_functions_name_whitelist, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[1;32m    249\u001b[0m     progress_logger_callback(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrun_specific_computations_single_context(including only \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(active_computation_functions)\u001b[39m}\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregistered_computation_function_names)\u001b[39m}\u001b[39;00m\u001b[39m registered computation functions): active_computation_functions: \u001b[39m\u001b[39m{\u001b[39;00mactive_computation_functions\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[39m# Perform the computations:\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[39mreturn\u001b[39;00m ComputedPipelineStage\u001b[39m.\u001b[39;49m_execute_computation_functions(active_computation_functions, previous_computation_result\u001b[39m=\u001b[39;49mprevious_computation_result, computation_kwargs_list\u001b[39m=\u001b[39;49mcomputation_kwargs_list, fail_on_exception\u001b[39m=\u001b[39;49mfail_on_exception, progress_logger_callback\u001b[39m=\u001b[39;49mprogress_logger_callback, are_global\u001b[39m=\u001b[39;49mare_global, debug_print\u001b[39m=\u001b[39;49mdebug_print)\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:533\u001b[0m, in \u001b[0;36mComputedPipelineStage._execute_computation_functions\u001b[0;34m(active_computation_functions, previous_computation_result, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m progress_logger_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m         progress_logger_callback(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExecuting [\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_num_funcs\u001b[39m}\u001b[39;00m\u001b[39m]: \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 533\u001b[0m     previous_computation_result \u001b[39m=\u001b[39m f(previous_computation_result, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomputation_kwargs_list[i])\n\u001b[1;32m    535\u001b[0m \u001b[39m# Since there's no error handling, gettin ghere means that there were no accumulated errors\u001b[39;00m\n\u001b[1;32m    536\u001b[0m accumulated_errors \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/MultiContextComputationFunctions.py:52\u001b[0m, in \u001b[0;36m_wrap_multi_context_computation_function.<locals>._\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(x):\n\u001b[1;32m     51\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m>\u001b[39m \u001b[39m4\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m     x[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m global_comp_fcn(\u001b[39m*\u001b[39;49mx) \u001b[39m# update global_computation_results\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/MultiContextComputationFunctions.py:314\u001b[0m, in \u001b[0;36mMultiContextComputationFunctions._perform_short_long_firing_rate_analyses\u001b[0;34m(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_whitelist, debug_print)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" \u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39mRequires:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# New unified `pipeline_complete_compute_long_short_fr_indicies(...)` method for entire pipeline:\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m x_frs_index, y_frs_index, active_context, all_results_dict \u001b[39m=\u001b[39m pipeline_complete_compute_long_short_fr_indicies(owning_pipeline_reference) \u001b[39m# use the all_results_dict as the computed data value\u001b[39;00m\n\u001b[1;32m    315\u001b[0m global_computation_results\u001b[39m.\u001b[39mcomputed_data[\u001b[39m'\u001b[39m\u001b[39mlong_short_fr_indicies_analysis\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m DynamicParameters\u001b[39m.\u001b[39minit_from_dict({\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_results_dict, \u001b[39m'\u001b[39m\u001b[39mactive_context\u001b[39m\u001b[39m'\u001b[39m: active_context})\n\u001b[1;32m    316\u001b[0m \u001b[39mreturn\u001b[39;00m global_computation_results\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/MultiContextComputationFunctions.py:1389\u001b[0m, in \u001b[0;36mpipeline_complete_compute_long_short_fr_indicies\u001b[0;34m(curr_active_pipeline, temp_save_filename)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mif\u001b[39;00m temp_save_filename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1387\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtemp_save_filename: \u001b[39m\u001b[39m{\u001b[39;00mtemp_save_filename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1389\u001b[0m x_frs_index, y_frs_index, updated_all_results_dict \u001b[39m=\u001b[39m _compute_long_short_firing_rate_indicies(spikes_df, long_laps, long_replays, short_laps, short_replays, save_path\u001b[39m=\u001b[39;49mtemp_save_filename) \u001b[39m# 'temp_2023-01-24_results.pkl'\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m all_results_dict\u001b[39m.\u001b[39mupdate(updated_all_results_dict) \u001b[39m# append the results dict\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[39m# all_results_dict.update(dict(zip(['x_frs_index', 'y_frs_index'], [x_frs_index, y_frs_index]))) # append the indicies to the results dict\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/MultiContextComputationFunctions.py:1254\u001b[0m, in \u001b[0;36m_compute_long_short_firing_rate_indicies\u001b[0;34m(spikes_df, long_laps, long_replays, short_laps, short_replays, save_path)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[39m@function_attributes\u001b[39m(short_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_compute_long_short_firing_rate_indicies\u001b[39m\u001b[39m'\u001b[39m, tags\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mlong_short\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcompute\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfr_index\u001b[39m\u001b[39m'\u001b[39m], input_requires\u001b[39m=\u001b[39m[], output_provides\u001b[39m=\u001b[39m[], uses\u001b[39m=\u001b[39m[], used_by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpipeline_complete_compute_long_short_fr_indicies\u001b[39m\u001b[39m'\u001b[39m], creation_date\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2023-01-19 00:00\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_long_short_firing_rate_indicies\u001b[39m(spikes_df, long_laps, long_replays, short_laps, short_replays, save_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A computation for the long/short firing rate index that Kamran and I discussed as one of three metrics during our meeting on 2023-01-19.\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \n\u001b[1;32m   1237\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \n\u001b[1;32m   1253\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1254\u001b[0m     \u001b[39massert\u001b[39;00m short_laps\u001b[39m.\u001b[39mn_epochs \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo short laps!\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m long: (laps: \u001b[39m\u001b[39m{\u001b[39;00mlong_laps\u001b[39m.\u001b[39mn_epochs\u001b[39m \u001b[39m\u001b[39m>\u001b[39m\u001b[39m \u001b[39m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, replays: \u001b[39m\u001b[39m{\u001b[39;00mlong_replays\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m), \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m short: (laps: \u001b[39m\u001b[39m{\u001b[39;00mshort_laps\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m, replays: \u001b[39m\u001b[39m{\u001b[39;00mshort_replays\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m     \u001b[39massert\u001b[39;00m long_laps\u001b[39m.\u001b[39mn_epochs \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo long laps!\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m long: (laps: \u001b[39m\u001b[39m{\u001b[39m\u001b[39mlong_laps.n_epochs > 0}, replays: \u001b[39m\u001b[39m{long_replays.n_epochs}\u001b[39;00m\u001b[39m), \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m short: (laps: \u001b[39m\u001b[39m{short_laps.n_epochs}\u001b[39;00m\u001b[39m, replays: \u001b[39m\u001b[39m{short_replays.n_epochs}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m     \u001b[39massert\u001b[39;00m long_replays\u001b[39m.\u001b[39mn_epochs \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo short replays!\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m long: (laps: \u001b[39m\u001b[39m{\u001b[39;00mlong_laps\u001b[39m.\u001b[39mn_epochs\u001b[39m \u001b[39m\u001b[39m>\u001b[39m\u001b[39m \u001b[39m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, replays: \u001b[39m\u001b[39m{\u001b[39;00mlong_replays\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m), \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m short: (laps: \u001b[39m\u001b[39m{\u001b[39;00mshort_laps\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m, replays: \u001b[39m\u001b[39m{\u001b[39;00mshort_replays\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No short laps!\t long: (laps: True, replays: 172), \t short: (laps: 0, replays: 30)"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import create_daily_programmatic_display_function_testing_folder_if_needed, session_context_to_relative_path\n",
    "# 2023-01-* - Call extended computations to build `_display_short_long_firing_rate_index_comparison` figures:\n",
    "extended_computations_include_whitelist=['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis'] # do only specifiedl\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_whitelist=extended_computations_include_whitelist, include_global_functions=True, fail_on_exception=True, progress_print=True, debug_print=False)\n",
    "newly_computed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_save_parent_path = Path(r'E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Results from 2023-01-20 - LongShort Firing Rate Indicies')\n",
    "## Get the output path (active_session_figures_out_path) for this session (and all of its filtered_contexts as well):\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "figures_parent_out_path = create_daily_programmatic_display_function_testing_folder_if_needed()\n",
    "active_session_figures_out_path = session_context_to_relative_path(figures_parent_out_path, active_identifying_session_ctx)\n",
    "print(f'curr_session_parent_out_path: {active_session_figures_out_path}')\n",
    "active_session_figures_out_path.mkdir(parents=True, exist_ok=True) # make folder if needed\n",
    "curr_active_pipeline.display('_display_short_long_firing_rate_index_comparison', curr_active_pipeline.sess.get_context(), fig_save_parent_path=active_session_figures_out_path)\n",
    "# plt.close() # closes the current figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_short_long_firing_rate_analyses'], fail_on_exception=True, debug_print=False) # fail_on_exception MUST be True or error handling is all messed up \n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "x_frs_index, y_frs_index = long_short_fr_indicies_analysis_results['x_frs_index'], long_short_fr_indicies_analysis_results['y_frs_index'] # use the all_results_dict as the computed data value\n",
    "active_context = long_short_fr_indicies_analysis_results['active_context']\n",
    "long_short_fr_indicies_analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0409e08",
   "metadata": {},
   "source": [
    "# Other Programmatic Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_extended_programmatic_figures(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_programmatic_figures(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the 2D placefields for my presentation\n",
    "from neuropy.core.neuron_identities import PlotStringBrevityModeEnum\n",
    "from neuropy.plotting.ratemaps import BackgroundRenderingOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# '_display_2d_placefield_result_plot_ratemaps_2D'\n",
    "# %matplotlib inline\n",
    "%matplotlib qt\n",
    "# curr_active_pipeline.display('_display_2d_placefield_result_plot_ratemaps_2D', long_epoch_context) # MatplotlibRenderPlots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48029c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# long_pf2D.plot_occupancy()\n",
    "_out = short_one_step_decoder_2D.pf.plot_ratemaps_2D(included_unit_neuron_IDs=[2,4,5], bg_rendering_mode=BackgroundRenderingOptions.EMPTY, use_special_overlayed_title=False, missing_aclu_string_formatter=None, debug_print=False, brev_mode=PlotStringBrevityModeEnum.NONE)\n",
    "\n",
    "## Single column output: subplots=(None, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14977e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "active_display_to_pdf_fn(curr_active_pipeline, curr_display_function_name='_display_1d_placefields', debug_print=False) #  Now seems to be working and saving to PDF!! Still using matplotlib.use('Qt5Agg') mode and plots still appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2686e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_shared_aclus_only_decoder.pf.plot_occupancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "long_one_step_decoder_2D.pf.plot_occupancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8006610",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_one_step_decoder_2D.pf.plot_ratemaps_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49dcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.display('_display_short_long_pf1D_comparison', active_session_configuration_context=global_epoch_context, single_figure=False)\n",
    "\n",
    "short_one_step_decoder_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402834bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "long_single_cell_pfmap_processing_fn = None\n",
    "short_single_cell_pfmap_processing_fn = None\n",
    "\n",
    "# long_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: 0.5 * pfmap # flip over the y-axis\n",
    "# short_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: -0.5 * pfmap # flip over the y-axis\n",
    "\n",
    "# pad = 1\n",
    "# long_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (0.5 * pfmap) + (0.5*pad) # shift the baseline up by half\n",
    "# short_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (-0.5 * pfmap * pad) + (0.5*pad) # flip over the y-axis, shift the baseline down by half\n",
    "\n",
    "# pad = 1\n",
    "# long_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (0.5 * pfmap * pad) + (0.5*pad) # shift the baseline up by half\n",
    "# short_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (0.5 * pfmap * pad) + (0.5*pad) # flip over the y-axis, shift the baseline down by half\n",
    "# long_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (0.5 * pfmap * pad) # shift the baseline up by half\n",
    "# short_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (0.5 * pfmap * pad) # flip over the y-axis, shift the baseline down by half\n",
    "\n",
    "\n",
    "# long_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (1.0 * pfmap * pad) # shift the baseline up by half\n",
    "# short_single_cell_pfmap_processing_fn = lambda i, aclu, pfmap: (-1.0 * pfmap * pad) + (1.0*pad) # this does not work and results in short being fully filled. I think this is because the fill_between gets reversed since everything is below baseline\n",
    "\n",
    "sort_idx = None\n",
    "\n",
    "out = curr_active_pipeline.display('_display_short_long_pf1D_comparison', active_identifying_session_ctx, single_figure=False, debug_print=False, fignum='Short v Long pf1D Comparison',\n",
    "                                   long_kwargs={'sortby': sort_idx, 'single_cell_pfmap_processing_fn': long_single_cell_pfmap_processing_fn},\n",
    "                                   short_kwargs={'sortby': sort_idx, 'single_cell_pfmap_processing_fn': short_single_cell_pfmap_processing_fn, 'curve_hatch_style': {'hatch':'///', 'edgecolor':'k'}},\n",
    "                                  )\n",
    "\n",
    "# ax = out.axes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfbb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(product_overlap_scalars_df.prod_overlap.to_numpy())[::-1] # the `[::-1]` term reverses the array, which by defaul is returned in ascending order and we want descending\n",
    "sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe831b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "curr_ratemap = long_one_step_decoder_1D.pf.ratemap\n",
    "curr_ratemap.get_sort_indicies()\n",
    "# .pf1D.ratemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8629a",
   "metadata": {},
   "source": [
    "# 2023-04-27 - Idea: Candidate Replay \"Quality\" Metric\n",
    "Observed that I visually distinguish \"good\" replays from bad ones based on their mostly-monotonically increasing nature.\n",
    "\n",
    "#### They don't have to:\n",
    "\tSpan the whole track\n",
    "\tStart or end at an end-cap\n",
    "\tIncrease linearly\n",
    "\n",
    "#### The algorithm must:\n",
    "\ttolerate ocasional jumps in an otherwise linear sequence\n",
    "\tallow sequences to \"start\" only halfway through. They should extract out the coherent sequence\n",
    "\n",
    "#### Ideas:\n",
    "\t- just a linear fit\n",
    "\t- a monotonicity check using some sort of cumulative sum over the differences in position.\n",
    "\t- \"radon transform\"?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-xhKu_1Lg-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
